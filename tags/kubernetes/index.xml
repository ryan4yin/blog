<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:dc="http://purl.org/dc/elements/1.1/" version="2.0"><channel><title>Kubernetes - 标签 - Ryan4Yin's Space</title><link>https://ryan4yin.space/tags/kubernetes/</link><description>Kubernetes - 标签 - Ryan4Yin's Space</description><generator>Hugo -- gohugo.io</generator><language>zh-CN</language><managingEditor>xiaoyin_c@qq.com (ryan4yin)</managingEditor><webMaster>xiaoyin_c@qq.com (ryan4yin)</webMaster><lastBuildDate>Tue, 25 Jan 2022 01:37:00 +0800</lastBuildDate><atom:link href="https://ryan4yin.space/tags/kubernetes/" rel="self" type="application/rss+xml"/><item><title>部署一个 Kubernetes 集群</title><link>https://ryan4yin.space/posts/kubernetes-deployemnt-using-kubeadm/</link><pubDate>Tue, 25 Jan 2022 01:37:00 +0800</pubDate><author>xiaoyin_c@qq.com</author><dc:creator>ryan4yin</dc:creator><guid>https://ryan4yin.space/posts/kubernetes-deployemnt-using-kubeadm/</guid><description><![CDATA[<blockquote>
<p>本文由个人笔记 <a href="https://github.com/ryan4yin/knowledge/tree/master/kubernetes" target="_blank" rel="noopener noreferrer">ryan4yin/knowledge</a> 整理而来，不保证正确</p>
</blockquote>
<h2 id="本地-kubernetes-集群安装工具">本地 Kubernetes 集群安装工具</h2>
<blockquote>
<p>云上的 Kubernetes 集群，基本上各云厂商都支持一键部署。这里主要关注本地部署，或者叫做裸机(baremetal)部署</p>
</blockquote>
<blockquote>
<p>本文介绍的方法适合开发测试使用，安全性、稳定性、长期可用性等方案都可能还有问题。</p>
</blockquote>
<blockquote>
<p>本文未考虑国内网络环境，建议在路由器上整个科学代理，或者自行调整文中的部分命令。</p>
</blockquote>
<p>kubernetes 是一个组件化的系统，安装过程有很大的灵活性，很多组件都有多种实现，这些实现各有特点，让初学者眼花缭乱。</p>
<p>而且要把这些组件一个个安装配置好并且能协同工作，也是很不容易的。</p>
<p>因此社区出现了各种各样的安装方案，下面介绍下几种支持裸机（Baremetal）部署的工具：</p>
<ol>
<li><a href="https://kuboard.cn/install/install-k8s.html" target="_blank" rel="noopener noreferrer">kubeadm</a>: 社区的集群安装工具，目前已经很成熟了。
<ol>
<li>使用难度：简单</li>
</ol>
</li>
<li><a href="https://github.com/k3s-io/k3s" target="_blank" rel="noopener noreferrer">k3s</a>: 轻量级 kubernetes，资源需求小，部署非常简单，适合开发测试用或者边缘环境
<ol>
<li>支持 airgap 离线部署</li>
<li>使用难度：超级简单</li>
</ol>
</li>
<li><a href="https://github.com/alibaba/sealer" target="_blank" rel="noopener noreferrer">alibaba/sealer</a>: 支持将整个 kubernetes 打包成一个镜像进行交付，而且部署也非常简单。
<ol>
<li>使用难度：超级简单</li>
<li>这个项目目前还在发展中，不过貌似已经有很多 toB 的公司在使用它进行 k8s 应用的交付了。</li>
</ol>
</li>
<li><a href="https://github.com/kubernetes-sigs/kubespray" target="_blank" rel="noopener noreferrer">kubespray</a>: 适合自建生产级别的集群，是一个大而全的 kubernetes 安装方案，自动安装容器运行时、k8s、网络插件等组件，而且各组件都有很多方案可选，但是感觉有点复杂。
<ol>
<li>使用难度：中等</li>
<li>支持 airgap 离线部署，但是以前我试用过是有坑，现在不知道咋样了</li>
<li>底层使用了 kubeadm 部署集群</li>
</ol>
</li>
</ol>
<p>笔者为了学习 Kuberntes，下面采用官方的 kubeadm 进行部署（不要问为啥不二进制部署，问就是懒），容器运行时使用 containerd，网络插件则使用目前最潮的基于 eBPF 的 Cilium.</p>
<p>kubernetes 官方介绍了两种高可用集群的拓扑结构：「Stacked etcd topology」和「External etcd topology」，简单起见，本文使用第一种「堆叠 Etcd 拓扑」结构，创建一个三 master 的高可用集群。</p>
<p>参考：</p>
<ul>
<li><a href="https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/install-kubeadm/" target="_blank" rel="noopener noreferrer">Kubernetes Docs - Installing kubeadm</a></li>
<li><a href="https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/high-availability/" target="_blank" rel="noopener noreferrer">Kubernetes Docs - Creating Highly Available clusters with kubeadm</a></li>
</ul>
<h2 id="1-节点的环境准备">1. 节点的环境准备</h2>
<p>首先准备三台 Linux 虚拟机，系统按需选择，然后调整这三台机器的设置：</p>
<ul>
<li>节点配置：
<ul>
<li>master：不低于 2c/3g，硬盘 20G
<ul>
<li>主节点性能也受集群 Pods 个数的影响，上述配置应该可以支撑到每个 Worker 节点跑 100 个 Pod.</li>
</ul>
</li>
<li>worker：看需求，建议不低于 2c/4g，硬盘不小于 20G，资源充分的话建议 40G.</li>
</ul>
</li>
<li>处于同一网络内并可互通（通常是同一局域网）</li>
<li>各主机的 hostname 和 mac/ip 地址以及 <code>/sys/class/dmi/id/product_uuid</code>，都必须唯一
<ul>
<li>这里最容易出问题的，通常是 hostname 冲突！</li>
</ul>
</li>
<li><strong>必须</strong>关闭 swap，kubelet 才能正常工作！</li>
</ul>
<p>方便起见，我直接使用 <a href="https://github.com/ryan4yin/pulumi-libvirt#examples" target="_blank" rel="noopener noreferrer">ryan4yin/pulumi-libvirt</a> 自动创建了五个虚拟机，并设置好了 ip/hostname.</p>
<p>本文使用了 opensuse leap 15.3 的 KVM cloud image 进行安装测试。</p>
<h3 id="11-iptables-设置">1.1 iptables 设置</h3>
<p>目前 kubernetes 的容器网络，默认使用的是 bridge 模式，这种模式下，需要使 <code>iptables</code> 能够接管 bridge 上的流量。</p>
<p>配置如下：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-shell" data-lang="shell">sudo modprobe br_netfilter
cat <span class="s">&lt;&lt;EOF | sudo tee /etc/modules-load.d/k8s.conf
</span><span class="s">br_netfilter
</span><span class="s">EOF</span>

cat <span class="s">&lt;&lt;EOF | sudo tee /etc/sysctl.d/k8s.conf
</span><span class="s">net.bridge.bridge-nf-call-ip6tables = 1
</span><span class="s">net.bridge.bridge-nf-call-iptables = 1
</span><span class="s">EOF</span>
sudo sysctl --system
</code></pre></td></tr></table>
</div>
</div><h3 id="12-开放节点端口">1.2 开放节点端口</h3>
<blockquote>
<p>局域网环境的话，建议直接关闭防火墙。这样所有端口都可用，方便快捷。</p>
</blockquote>
<blockquote>
<p>通常我们的云上集群，也是关闭防火墙的，只是会通过云服务提供的「安全组」来限制客户端 ip</p>
</blockquote>
<p>Control-plane 节点，也就是 master，需要开放如下端口：</p>
<table>
<thead>
<tr>
<th>Protocol</th>
<th>Direction</th>
<th>Port Range</th>
<th>Purpose</th>
<th>Used By</th>
</tr>
</thead>
<tbody>
<tr>
<td>TCP</td>
<td>Inbound</td>
<td>6443*</td>
<td>Kubernetes API server</td>
<td>All</td>
</tr>
<tr>
<td>TCP</td>
<td>Inbound</td>
<td>2379-2380</td>
<td>etcd server client API</td>
<td>kube-apiserver, etcd</td>
</tr>
<tr>
<td>TCP</td>
<td>Inbound</td>
<td>10250</td>
<td>kubelet API</td>
<td>Self, Control plane</td>
</tr>
<tr>
<td>TCP</td>
<td>Inbound</td>
<td>10251</td>
<td>kube-scheduler</td>
<td>Self</td>
</tr>
<tr>
<td>TCP</td>
<td>Inbound</td>
<td>10252</td>
<td>kube-controller-manager</td>
<td>Self</td>
</tr>
</tbody>
</table>
<p>Worker 节点需要开发如下端口：</p>
<table>
<thead>
<tr>
<th>Protocol</th>
<th>Direction</th>
<th>Port Range</th>
<th>Purpose</th>
<th>Used By</th>
</tr>
</thead>
<tbody>
<tr>
<td>TCP</td>
<td>Inbound</td>
<td>10250</td>
<td>kubelet API</td>
<td>Self, Control plane</td>
</tr>
<tr>
<td>TCP</td>
<td>Inbound</td>
<td>30000-32767</td>
<td>NodePort Services†</td>
<td>All</td>
</tr>
</tbody>
</table>
<p>另外通常我们本地测试的时候，可能更想直接在 <code>80</code> <code>443</code> <code>8080</code> 等端口上使用 <code>NodePort</code>，
就需要修改 kube-apiserver 的 <code>--service-node-port-range</code> 参数来自定义 NodePort 的端口范围，相应的 Worker 节点也得开放这些端口。</p>
<h2 id="2-安装-containerd">2. 安装 containerd</h2>
<p>首先是环境配置：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-shell" data-lang="shell">cat <span class="s">&lt;&lt;EOF | sudo tee /etc/modules-load.d/containerd.conf
</span><span class="s">overlay
</span><span class="s">br_netfilter
</span><span class="s">nf_conntrack
</span><span class="s">EOF</span>

sudo modprobe overlay
sudo modprobe br_netfilter
sudo modprobe nf_conntrack

<span class="c1"># Setup required sysctl params, these persist across reboots.</span>
cat <span class="s">&lt;&lt;EOF | sudo tee /etc/sysctl.d/99-kubernetes-cri.conf
</span><span class="s">net.bridge.bridge-nf-call-iptables  = 1
</span><span class="s">net.ipv4.ip_forward                 = 1
</span><span class="s">net.bridge.bridge-nf-call-ip6tables = 1
</span><span class="s">EOF</span>

<span class="c1"># Apply sysctl params without reboot</span>
sudo sysctl --system
</code></pre></td></tr></table>
</div>
</div><p>安装 containerd+nerdctl:</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-shell" data-lang="shell">wget https://github.com/containerd/nerdctl/releases/download/v0.11.1/nerdctl-full-0.11.1-linux-amd64.tar.gz
tar -axvf nerdctl-full-0.11.1-linux-amd64.tar.gz
<span class="c1"># 这里简单起见，rootless 相关的东西也一起装进去了，测试嘛就无所谓了...</span>
mv bin/* /usr/local/bin/
mv lib/systemd/system/containerd.service /usr/lib/systemd/system/

systemctl <span class="nb">enable</span> containerd
systemctl start containerd
</code></pre></td></tr></table>
</div>
</div><p><code>nerdctl</code> 是一个 containerd 的命令行工具，但是它的容器、镜像与 Kubernetes 的容器、镜像是完全隔离的，不能互通！</p>
<p>目前只能通过 <code>crictl</code> 来查看、拉取 Kubernetes 的容器、镜像，下一节会介绍 crictl 的安装。</p>
<h2 id="3-安装-kubeletkubeadmkubectl">3. 安装 kubelet/kubeadm/kubectl</h2>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-shell" data-lang="shell"><span class="c1"># 一些全局都需要用的变量</span>
<span class="nv">CNI_VERSION</span><span class="o">=</span><span class="s2">&#34;v0.8.2&#34;</span>
<span class="nv">CRICTL_VERSION</span><span class="o">=</span><span class="s2">&#34;v1.17.0&#34;</span>
<span class="c1"># kubernetes 的版本号</span>
<span class="c1"># RELEASE=&#34;$(curl -sSL https://dl.k8s.io/release/stable.txt)&#34;</span>
<span class="nv">RELEASE</span><span class="o">=</span><span class="s2">&#34;1.22.1&#34;</span>
<span class="c1"># kubelet 配置文件的版本号</span>
<span class="nv">RELEASE_VERSION</span><span class="o">=</span><span class="s2">&#34;v0.4.0&#34;</span>
<span class="c1"># 架构</span>
<span class="nv">ARCH</span><span class="o">=</span><span class="s2">&#34;amd64&#34;</span>
<span class="c1">#　安装目录</span>
<span class="nv">DOWNLOAD_DIR</span><span class="o">=</span>/usr/local/bin


<span class="c1"># CNI 插件</span>
sudo mkdir -p /opt/cni/bin
curl -L <span class="s2">&#34;https://github.com/containernetworking/plugins/releases/download/</span><span class="si">${</span><span class="nv">CNI_VERSION</span><span class="si">}</span><span class="s2">/cni-plugins-linux-</span><span class="si">${</span><span class="nv">ARCH</span><span class="si">}</span><span class="s2">-</span><span class="si">${</span><span class="nv">CNI_VERSION</span><span class="si">}</span><span class="s2">.tgz&#34;</span> <span class="p">|</span> sudo tar -C /opt/cni/bin -xz

<span class="c1"># crictl 相关工具</span>
curl -L <span class="s2">&#34;https://github.com/kubernetes-sigs/cri-tools/releases/download/</span><span class="si">${</span><span class="nv">CRICTL_VERSION</span><span class="si">}</span><span class="s2">/crictl-</span><span class="si">${</span><span class="nv">CRICTL_VERSION</span><span class="si">}</span><span class="s2">-linux-</span><span class="si">${</span><span class="nv">ARCH</span><span class="si">}</span><span class="s2">.tar.gz&#34;</span> <span class="p">|</span> sudo tar -C <span class="nv">$DOWNLOAD_DIR</span> -xz

<span class="c1"># kubelet/kubeadm/kubectl</span>
<span class="nb">cd</span> <span class="nv">$DOWNLOAD_DIR</span>
sudo curl -L --remote-name-all https://storage.googleapis.com/kubernetes-release/release/<span class="si">${</span><span class="nv">RELEASE</span><span class="si">}</span>/bin/linux/<span class="si">${</span><span class="nv">ARCH</span><span class="si">}</span>/<span class="o">{</span>kubeadm,kubelet,kubectl<span class="o">}</span>
sudo chmod +x <span class="o">{</span>kubeadm,kubelet,kubectl<span class="o">}</span>

<span class="c1"># kubelet/kubeadm 配置</span>
curl -sSL <span class="s2">&#34;https://raw.githubusercontent.com/kubernetes/release/</span><span class="si">${</span><span class="nv">RELEASE_VERSION</span><span class="si">}</span><span class="s2">/cmd/kubepkg/templates/latest/deb/kubelet/lib/systemd/system/kubelet.service&#34;</span> <span class="p">|</span> sed <span class="s2">&#34;s:/usr/bin:</span><span class="si">${</span><span class="nv">DOWNLOAD_DIR</span><span class="si">}</span><span class="s2">:g&#34;</span> <span class="p">|</span> sudo tee /etc/systemd/system/kubelet.service
sudo mkdir -p /etc/systemd/system/kubelet.service.d
curl -sSL <span class="s2">&#34;https://raw.githubusercontent.com/kubernetes/release/</span><span class="si">${</span><span class="nv">RELEASE_VERSION</span><span class="si">}</span><span class="s2">/cmd/kubepkg/templates/latest/deb/kubeadm/10-kubeadm.conf&#34;</span> <span class="p">|</span> sed <span class="s2">&#34;s:/usr/bin:</span><span class="si">${</span><span class="nv">DOWNLOAD_DIR</span><span class="si">}</span><span class="s2">:g&#34;</span> <span class="p">|</span> sudo tee /etc/systemd/system/kubelet.service.d/10-kubeadm.conf

systemctl <span class="nb">enable</span> --now kubelet
<span class="c1"># 验证 kubelet 启动起来了，但是目前还没有初始化配置，过一阵就会重启一次</span>
systemctl status kubelet
</code></pre></td></tr></table>
</div>
</div><p>试用 crictl:</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-shell" data-lang="shell"><span class="nb">export</span> <span class="nv">CONTAINER_RUNTIME_ENDPOINT</span><span class="o">=</span><span class="s1">&#39;unix:///var/run/containerd/containerd.sock&#39;</span>
<span class="c1"># 列出所有 pods，现在应该啥也没</span>
crictl  pods

<span class="c1"># 列出所有镜像</span>
crictl images
</code></pre></td></tr></table>
</div>
</div><h2 id="4-为-master-的-kube-apiserver-创建负载均衡实现高可用">4. 为 master 的 kube-apiserver 创建负载均衡实现高可用</h2>
<p>根据 kubeadm 官方文档 <a href="https://github.com/kubernetes/kubeadm/blob/master/docs/ha-considerations.md#kube-vip" target="_blank" rel="noopener noreferrer">Kubeadm Docs - High Availability Considerations</a> 介绍，要实现 kube-apiserver 的高可用，目前最知名的负载均衡方式是 keepalived+haproxy，另外也可以考虑使用 kube-vip 等更简单的工具。</p>
<p>简单起见，我们直接用 kube-vip 吧，参考了 kube-vip 的官方文档：<a href="https://kube-vip.io/install_static/" target="_blank" rel="noopener noreferrer">Kube-vip as a Static Pod with Kubelet</a>.</p>
<blockquote>
<p>P.S. 我也见过有的安装工具会直接抛弃 keepalived，直接在每个节点上跑一个 nginx 做负载均衡，配置里写死了所有 master 的地址&hellip;</p>
</blockquote>
<p>首先使用如下命令生成 kube-vip 的配置文件，以 ARP 为例（生产环境建议换成 BGP）：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-shell" data-lang="shell">cat <span class="s">&lt;&lt;EOF | sudo tee add-kube-vip.sh
</span><span class="s"># 你的虚拟机网卡，opensuse/centos 等都是 eth0，但是 ubuntu 可能是 ens3
</span><span class="s">export INTERFACE=eth0
</span><span class="s">
</span><span class="s"># 用于实现高可用的 vip，需要和前面的网络接口在同一网段内，否则就无法路由了。
</span><span class="s">export VIP=192.168.122.200
</span><span class="s">
</span><span class="s"># 生成 static-pod 的配置文件
</span><span class="s">mkdir -p /etc/kubernetes/manifests
</span><span class="s">nerdctl run --rm --network=host --entrypoint=/kube-vip ghcr.io/kube-vip/kube-vip:v0.3.8 \
</span><span class="s">  manifest pod \
</span><span class="s">  --interface $INTERFACE \
</span><span class="s">  --vip $VIP \
</span><span class="s">  --controlplane \
</span><span class="s">  --services \
</span><span class="s">  --arp \
</span><span class="s">  --leaderElection | tee  /etc/kubernetes/manifests/kube-vip.yaml
</span><span class="s">EOF</span>

bash add-kube-vip.sh
</code></pre></td></tr></table>
</div>
</div><p>三个 master 节点都需要跑下上面的命令（worker 不需要），创建好 kube-vip 的 static-pod 配置文件。
在完成 kubeadm 初始化后，kubelet 会自动把它们拉起为 static pod.</p>
<h2 id="5-使用-kubeadm-创建集群">5. 使用 kubeadm 创建集群</h2>
<p>其实需要运行的就是这条命令：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-shell" data-lang="shell"><span class="c1"># 极简配置：</span>
cat <span class="s">&lt;&lt;EOF | sudo tee kubeadm-config.yaml
</span><span class="s">apiVersion: kubeadm.k8s.io/v1beta3
</span><span class="s">kind: InitConfiguration
</span><span class="s">nodeRegistration:
</span><span class="s">  criSocket: &#34;/var/run/containerd/containerd.sock&#34;
</span><span class="s">  imagePullPolicy: IfNotPresent
</span><span class="s">---
</span><span class="s">kind: ClusterConfiguration
</span><span class="s">apiVersion: kubeadm.k8s.io/v1beta3
</span><span class="s">kubernetesVersion: v1.22.1
</span><span class="s">clusterName: kubernetes
</span><span class="s">certificatesDir: /etc/kubernetes/pki
</span><span class="s">imageRepository: k8s.gcr.io
</span><span class="s">controlPlaneEndpoint: &#34;192.168.122.200:6443&#34;  # 填 apiserver 的 vip 地址，或者整个域名也行，但是就得加 /etc/hosts 或者内网 DNS 解析
</span><span class="s">networking:
</span><span class="s">  serviceSubnet: &#34;10.96.0.0/16&#34;
</span><span class="s">  podSubnet: &#34;10.244.0.0/16&#34;
</span><span class="s">etcd:
</span><span class="s">  local:
</span><span class="s">    dataDir: /var/lib/etcd
</span><span class="s">---
</span><span class="s">apiVersion: kubelet.config.k8s.io/v1beta1
</span><span class="s">kind: KubeletConfiguration
</span><span class="s">cgroupDriver: systemd
</span><span class="s"># 让 kubelet 从 certificates.k8s.io 申请由集群 CA Root 签名的 tls 证书，而非直接使用自签名证书
</span><span class="s"># 如果不启用这个， 安装 metrics-server 时就会遇到证书报错，后面会详细介绍。
</span><span class="s">serverTLSBootstrap: true
</span><span class="s">EOF</span>

<span class="c1"># 查看 kubeadm 默认的完整配置，供参考</span>
kubeadm config print init-defaults &gt; init.default.yaml

<span class="c1"># 执行集群的初始化，这会直接将当前节点创建为 master</span>
<span class="c1"># 成功运行的前提：前面该装的东西都装好了，而且 kubelet 已经在后台运行了</span>
<span class="c1"># `--upload-certs` 会将生成的集群证书上传到 kubeadm 服务器，在两小时内加入集群的 master 节点会自动拉证书，主要是方便集群创建。</span>
kubeadm init --config kubeadm-config.yaml --upload-certs
</code></pre></td></tr></table>
</div>
</div><p>kubeadm 应该会报错，提示你有些依赖不存在，下面先安装好依赖项。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-shell" data-lang="shell">sudo zypper in -y socat ebtables conntrack-tools
</code></pre></td></tr></table>
</div>
</div><p>再重新运行前面的 kubeadm 命令，应该就能正常执行了，它做的操作有：</p>
<ul>
<li>拉取控制面的容器镜像</li>
<li>生成 ca 根证书</li>
<li>使用根证书为 etcd/apiserver 等一票工具生成 tls 证书</li>
<li>为控制面的各个组件生成 kubeconfig 配置</li>
<li>生成 static pod 配置，kubelet 会根据这些配置自动拉起 kube-proxy 以及其他所有的 k8s master 组件</li>
</ul>
<p>运行完会给出三部分命令：</p>
<ul>
<li>将 <code>kubeconfig</code> 放到 <code>$HOME/.kube/config</code> 下，<code>kubectl</code> 需要使用该配置文件连接 kube-apiserver</li>
<li>control-plane 节点加入集群的命令:
<ul>
<li>这里由于我们提前添加了 kube-vip 的 static-pod 配置，这里的 preflight-check 会报错，需要添加此参数忽略该报错 - <code>--ignore-preflight-errors=DirAvailable--etc-kubernetes-manifests</code></li>
</ul>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-shell" data-lang="shell">kubeadm join 192.168.122.200:6443 --token &lt;token&gt; <span class="se">\
</span><span class="se"></span>  --discovery-token-ca-cert-hash sha256:&lt;hash&gt; <span class="se">\
</span><span class="se"></span>  --control-plane --certificate-key &lt;key&gt; <span class="se">\
</span><span class="se"></span>  --ignore-preflight-errors<span class="o">=</span>DirAvailable--etc-kubernetes-manifests
</code></pre></td></tr></table>
</div>
</div></li>
<li>worker 节点加入集群的命令:
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-shell" data-lang="shell">kubeadm join 192.168.122.200:6443 --token &lt;token&gt; <span class="se">\
</span><span class="se"></span>      --discovery-token-ca-cert-hash sha256:&lt;hash&gt; 
</code></pre></td></tr></table>
</div>
</div></li>
</ul>
<p>跑完第一部分 <code>kubeconfig</code> 的处理命令后，就可以使用 kubectl 查看集群状况了：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-shell" data-lang="shell">k8s-master-0:~/kubeadm <span class="c1"># kubectl get no</span>
NAME           STATUS     ROLES                  AGE   VERSION
k8s-master-0   NotReady   control-plane,master   79s   v1.22.1
k8s-master-0:~/kubeadm <span class="c1"># kubectl get po --all-namespaces</span>
NAMESPACE     NAME                                   READY   STATUS    RESTARTS   AGE
kube-system   coredns-78fcd69978-6tlnw               0/1     Pending   <span class="m">0</span>          83s
kube-system   coredns-78fcd69978-hxtvs               0/1     Pending   <span class="m">0</span>          83s
kube-system   etcd-k8s-master-0                      1/1     Running   <span class="m">6</span>          90s
kube-system   kube-apiserver-k8s-master-0            1/1     Running   <span class="m">4</span>          90s
kube-system   kube-controller-manager-k8s-master-0   1/1     Running   <span class="m">4</span>          90s
kube-system   kube-proxy-6w2bx                       1/1     Running   <span class="m">0</span>          83s
kube-system   kube-scheduler-k8s-master-0            1/1     Running   <span class="m">7</span>          97s
</code></pre></td></tr></table>
</div>
</div><p>现在在其他节点运行前面打印出的加入集群的命令，就可以搭建好一个高可用的集群了。</p>
<p>所有节点都加入集群后，通过 kubectl 查看，应该是三个控制面 master，两个 worker：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-shell" data-lang="shell">k8s-master-0:~/kubeadm <span class="c1"># kubectl get node</span>
NAME           STATUS     ROLES                  AGE     VERSION
k8s-master-0   NotReady   control-plane,master   26m     v1.22.1
k8s-master-1   NotReady   control-plane,master   7m2s    v1.22.1
k8s-master-2   NotReady   control-plane,master   2m10s   v1.22.1
k8s-worker-0   NotReady   &lt;none&gt;                 97s     v1.22.1
k8s-worker-1   NotReady   &lt;none&gt;                 86s     v1.22.1
</code></pre></td></tr></table>
</div>
</div><p>现在它们都还处于 NotReady 状态，需要等到我们把网络插件安装好，才会 Ready.</p>
<p>现在再看下集群的证书签发状态：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-shell" data-lang="shell">❯ kubectl get csr --sort-by<span class="o">=</span><span class="s1">&#39;{.spec.username}&#39;</span>
NAME        AGE     SIGNERNAME                                    REQUESTOR                  REQUESTEDDURATION   CONDITION
csr-95hll   6m58s   kubernetes.io/kube-apiserver-client-kubelet   system:bootstrap:q8ivnz    &lt;none&gt;              Approved,Issued
csr-tklnr   7m5s    kubernetes.io/kube-apiserver-client-kubelet   system:bootstrap:q8ivnz    &lt;none&gt;              Approved,Issued
csr-w92jv   9m15s   kubernetes.io/kube-apiserver-client-kubelet   system:bootstrap:q8ivnz    &lt;none&gt;              Approved,Issued
csr-rv7sj   8m11s   kubernetes.io/kube-apiserver-client-kubelet   system:bootstrap:q8ivnz    &lt;none&gt;              Approved,Issued
csr-nxkgx   10m     kubernetes.io/kube-apiserver-client-kubelet   system:node:k8s-master-0   &lt;none&gt;              Approved,Issued
csr-cd22c   10m     kubernetes.io/kubelet-serving                 system:node:k8s-master-0   &lt;none&gt;              Pending
csr-wjrnr   9m53s   kubernetes.io/kubelet-serving                 system:node:k8s-master-0   &lt;none&gt;              Pending
csr-sjq42   9m8s    kubernetes.io/kubelet-serving                 system:node:k8s-master-1   &lt;none&gt;              Pending
csr-xtv8f   8m56s   kubernetes.io/kubelet-serving                 system:node:k8s-master-1   &lt;none&gt;              Pending
csr-f2dsf   8m3s    kubernetes.io/kubelet-serving                 system:node:k8s-master-2   &lt;none&gt;              Pending
csr-xl8dg   6m58s   kubernetes.io/kubelet-serving                 system:node:k8s-worker-0   &lt;none&gt;              Pending
csr-p9g24   6m52s   kubernetes.io/kubelet-serving                 system:node:k8s-worker-1   &lt;none&gt;              Pending
</code></pre></td></tr></table>
</div>
</div><p>能看到有好几个 <code>kubernetes.io/kubelet-serving</code> 的证书还处于 pending 状态，
这是因为我们在 kubeadm 配置文件中，设置了 <code>serverTLSBootstrap: true</code>，让 Kubelet 从集群中申请 CA 签名证书，而不是自签名导致的。</p>
<p>设置这个参数的主要目的，是为了让 metrics-server 等组件能使用 https 协议与 kubelet 通信，避免为 metrics-server 添加参数 <code>--kubelet-insecure-tls</code>.</p>
<p>目前 kubeadm 不支持自动批准 kubelet 申请的证书，需要我们手动批准一下：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-shell" data-lang="shell"><span class="c1"># 批准 Kubelet 申请的所有证书</span>
kubectl certificate approve csr-cd22c csr-wjrnr csr-sjq42 csr-xtv8f csr-f2dsf csr-xl8dg csr-p9g24
</code></pre></td></tr></table>
</div>
</div><p>在未批准这些证书之前，所有需要调用 kubelet api 的功能都将无法使用，比如：</p>
<ul>
<li>查看 pod 日志</li>
<li>获取节点 metrics</li>
<li>等等</li>
</ul>
<h3 id="51-常见问题">5.1 常见问题</h3>
<h4 id="511-使用国内镜像源">5.1.1 使用国内镜像源</h4>
<p>如果你没有科学环境，kubeadm 默认的镜像仓库在国内是拉不了的。
如果对可靠性要求高，最好是自建私有镜像仓库，把镜像推送到私有仓库。</p>
<p>可以通过如下命令列出所有需要用到的镜像地址：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-shell" data-lang="shell">❯ kubeadm config images list --kubernetes-version v1.22.1
k8s.gcr.io/kube-apiserver:v1.22.1
k8s.gcr.io/kube-controller-manager:v1.22.1
k8s.gcr.io/kube-scheduler:v1.22.1
k8s.gcr.io/kube-proxy:v1.22.1
k8s.gcr.io/pause:3.5
k8s.gcr.io/etcd:3.5.0-0
k8s.gcr.io/coredns/coredns:v1.8.4
</code></pre></td></tr></table>
</div>
</div><p>使用 <code>skopeo</code> 等工具或脚本将上述镜像拷贝到你的私有仓库，或者图方便（测试环境）也可以考虑网上找找别人同步好的镜像地址。将镜像地址添加到 <code>kubeadm-config.yaml</code> 中再部署。</p>
<h4 id="512-重置集群配置">5.1.2 重置集群配置</h4>
<p>创建集群的过程中出现任何问题，都可以通过在所有节点上运行 <code>kubeadm reset</code> 来还原配置，然后重新走 kubeadm 的集群创建流程。</p>
<p>但是要注意几点：</p>
<ul>
<li><code>kubeadm reset</code> 会清除包含 kube-vip 配置在内的所有 static-pod 配置文件，所以 master 节点需要重新跑下前面给的 kube-vip 命令，生成下 kube-vip 配置。</li>
<li><code>kubeadm reset</code> 不会重置网络接口的配置，master 节点需要手动清理下 kube-vip 添加的 vip: <code>ip addr del 192.168.122.200/32 dev eth0</code>.</li>
<li>如果你在安装了网络插件之后希望重装集群，顺序如下：
<ul>
<li>通过 <code>kubectl delete -f xxx.yaml</code>/<code>helm uninstall</code> 删除所有除网络之外的其他应用配置</li>
<li>删除网络插件</li>
<li>先重启一遍所有节点，或者手动重置所有节点的网络配置
<ul>
<li>建议重启，因为我不知道该怎么手动重置&hellip; 试了 <code>systemctl restart network</code> 并不会清理所有虚拟网络接口。</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>如此操作后，再重新执行集群安装，应该就没啥毛病了。</p>
<h2 id="6-验证集群的高可用性">6. 验证集群的高可用性</h2>
<p>虽然网络插件还没装导致集群所有节点都还没 ready，但是我们已经可以通过 kubectl 命令来简单验证集群的高可用性了。</p>
<p>首先，我们将前面放置在 k8s-master-0 的认证文件 <code>$HOME/.kube/config</code> 以及 kunbectl 安装在另一台机器上，比如我直接放我的宿主机。</p>
<p>然后在宿主机上跑 <code>kubectl get node</code> 命令验证集群的高可用性：</p>
<ul>
<li>三个主节点都正常运行时，kubectl 命令也正常</li>
<li>pause 或者 stop 其中一个 master，kubectl 命令仍然能正常运行</li>
<li>再 pause 第二个 master，kubectl 命令应该就会卡住，并且超时，无法使用了</li>
<li>resume 恢复停掉的两个 master 之一，会发现 kubectl 命令又能正常运行了</li>
</ul>
<p>到这里 kubeadm 的工作就完成了，接下来再安装网络插件，集群就可用了。</p>
<h2 id="7-安装网络插件">7. 安装网络插件</h2>
<p>社区有很多种网络插件可选，比较知名且性能也不错的，应该是 Calico 和 Cilium，其中 Cilium 主打基于 eBPF 的高性能与高可观测性。</p>
<p>下面分别介绍这两个插件的安装方法。（注意只能安装其中一个网络插件，不能重复安装。）</p>
<p>需要提前在本机安装好 helm，我这里使用宿主机，因此只需要在宿主机安装:</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-shell" data-lang="shell"><span class="c1"># 一行命令安装，也可以自己手动下载安装包，都行</span>
curl https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3 <span class="p">|</span> bash

<span class="c1"># 或者 opensuse 直接用包管理器安装</span>
sudo zypper in helm
</code></pre></td></tr></table>
</div>
</div><h3 id="71-安装-cilium">7.1 安装 Cilium</h3>
<blockquote>
<p>官方文档：https://docs.cilium.io/en/v1.10/gettingstarted/k8s-install-kubeadm/</p>
</blockquote>
<p>cilium 通过 eBPF 提供了高性能与高可观测的 k8s 集群网络，
另外 cilium 还提供了比 kube-proxy 更高效的实现，可以完全替代 kube-proxy.</p>
<p>这里我们还是先使用 kube-proxy 模式，先熟悉下 cilium 的使用：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-shell" data-lang="shell">helm repo add cilium https://helm.cilium.io/
helm search repo cilium/cilium -l <span class="p">|</span> head

helm install cilium cilium/cilium --version 1.10.4 --namespace kube-system
</code></pre></td></tr></table>
</div>
</div><p>可以通过 <code>kubectl get pod -A</code> 查看 cilium 的安装进度，当所有 pod 都 ready 后，集群就 ready 了~</p>
<p>cilium 也提供了专用的客户端：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-shell" data-lang="shell">curl -L --remote-name-all https://github.com/cilium/cilium-cli/releases/latest/download/cilium-linux-amd64.tar.gz<span class="o">{</span>,.sha256sum<span class="o">}</span>
sha256sum --check cilium-linux-amd64.tar.gz.sha256sum
sudo tar xzvfC cilium-linux-amd64.tar.gz /usr/local/bin
rm cilium-linux-amd64.tar.gz<span class="o">{</span>,.sha256sum<span class="o">}</span>
</code></pre></td></tr></table>
</div>
</div><p>然后使用 cilium 客户端检查网络插件的状态：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-shell" data-lang="shell"> $ cilium status --wait
    /¯¯<span class="se">\
</span><span class="se"></span> /¯¯<span class="se">\_</span>_/¯¯<span class="se">\ </span>   Cilium:         OK
 <span class="se">\_</span>_/¯¯<span class="se">\_</span>_/    Operator:       OK
 /¯¯<span class="se">\_</span>_/¯¯<span class="se">\ </span>   Hubble:         disabled
 <span class="se">\_</span>_/¯¯<span class="se">\_</span>_/    ClusterMesh:    disabled
    <span class="se">\_</span>_/

DaemonSet         cilium             Desired: 5, Ready: 5/5, Available: 5/5
Deployment        cilium-operator    Desired: 2, Ready: 2/2, Available: 2/2
Containers:       cilium             Running: <span class="m">5</span>
                  cilium-operator    Running: <span class="m">2</span>
Cluster Pods:     2/2 managed by Cilium
Image versions    cilium             quay.io/cilium/cilium:v1.10.4@sha256:7d354052ccf2a7445101d78cebd14444c7c40129ce7889f2f04b89374dbf8a1d: <span class="m">5</span>
                  cilium-operator    quay.io/cilium/operator-generic:v1.10.4@sha256:c49a14e34634ff1a494c84b718641f27267fb3a0291ce3d74352b44f8a8d2f93: <span class="m">2</span>
</code></pre></td></tr></table>
</div>
</div><p>cilium 还提供了命令，自动创建 pod 进行集群网络的连接性测试：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-shell" data-lang="shell">❯ cilium connectivity <span class="nb">test</span>
ℹ️  Monitor aggregation detected, will skip some flow validation steps
✨ <span class="o">[</span>kubernetes<span class="o">]</span> Creating namespace <span class="k">for</span> connectivity check...
✨ <span class="o">[</span>kubernetes<span class="o">]</span> Deploying echo-same-node service...
✨ <span class="o">[</span>kubernetes<span class="o">]</span> Deploying same-node deployment...
✨ <span class="o">[</span>kubernetes<span class="o">]</span> Deploying client deployment...
✨ <span class="o">[</span>kubernetes<span class="o">]</span> Deploying client2 deployment...
✨ <span class="o">[</span>kubernetes<span class="o">]</span> Deploying echo-other-node service...
✨ <span class="o">[</span>kubernetes<span class="o">]</span> Deploying other-node deployment...
...
ℹ️  Expose Relay locally with:
   cilium hubble <span class="nb">enable</span>
   cilium status --wait
   cilium hubble port-forward<span class="p">&amp;</span>
🏃 Running tests...
...
---------------------------------------------------------------------------------------------------------------------
✅ All <span class="m">11</span> tests <span class="o">(</span><span class="m">134</span> actions<span class="o">)</span> successful, <span class="m">0</span> tests skipped, <span class="m">0</span> scenarios skipped.
</code></pre></td></tr></table>
</div>
</div><p>通过 <code>kubectl get po -A</code> 能观察到，这个测试命令会自动创建一个 <code>cilium-test</code> 名字空间，并在启动创建若干 pod 进行详细的测试。</p>
<p>整个测试流程大概会持续 5 分多钟，测试完成后，相关 Pod 不会自动删除，使用如下命令手动删除：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-shell" data-lang="shell">kubectl delete namespace cilium-test
</code></pre></td></tr></table>
</div>
</div><h3 id="72-安装-calico">7.2 安装 Calico</h3>
<blockquote>
<p>官方文档：https://docs.projectcalico.org/getting-started/kubernetes/self-managed-onprem/onpremises</p>
</blockquote>
<p>也就两三行命令。安装确实特别简单，懒得介绍了，看官方文档吧。</p>
<p>但是实际上 calico 的细节还蛮多的，建议通读下它的官方文档，了解下 calico 的架构。</p>
<h2 id="8-查看集群状态">8. 查看集群状态</h2>
<p>官方的 dashboard 个人感觉不太好用，建议直接在本地装个 k9s 用，特别爽。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-shell" data-lang="shell">sudo zypper in k9s
</code></pre></td></tr></table>
</div>
</div><p>然后就可以愉快地玩耍了。</p>
<h2 id="9-安装-metrics-server">9. 安装 metrics-server</h2>
<blockquote>
<p>这一步可能遇到的问题：<a href="https://kubernetes.io/docs/tasks/administer-cluster/kubeadm/kubeadm-certs/#kubelet-serving-certs" target="_blank" rel="noopener noreferrer">Enabling signed kubelet serving certificates</a></p>
</blockquote>
<p>如果需要使用 HPA 以及简单的集群监控，那么 metrics-server 是必须安装的，现在我们安装一下它。</p>
<p>首先，跑 kubectl 的监控命令应该会报错：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-shell" data-lang="shell">❯ kubectl top node
error: Metrics API not available
</code></pre></td></tr></table>
</div>
</div><p>k9s 里面应该也看不到任何监控指标。</p>
<p>现在通过 helm 安装它：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-shell" data-lang="shell">helm repo add metrics-server https://kubernetes-sigs.github.io/metrics-server/
helm search repo metrics-server/metrics-server -l <span class="p">|</span> head

helm upgrade --install metrics-server metrics-server/metrics-server --version 3.5.0 --namespace kube-system
</code></pre></td></tr></table>
</div>
</div><blockquote>
<p>metrics-server 默认只会部署一个实例，如果希望高可用，请参考官方配置：<a href="https://github.com/kubernetes-sigs/metrics-server/tree/master/manifests/high-availability" target="_blank" rel="noopener noreferrer">metrics-server - high-availability manifests</a></p>
</blockquote>
<p>等 metrics-server 启动好后，就可以使用 <code>kubectl top</code> 命令啦：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span><span class="lnt">43
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-shell" data-lang="shell">❯ kubectl top node
NAME           CPU<span class="o">(</span>cores<span class="o">)</span>   CPU%   MEMORY<span class="o">(</span>bytes<span class="o">)</span>   MEMORY%   
k8s-master-0   327m         16%    1465Mi          50%       
k8s-master-1   263m         13%    1279Mi          44%       
k8s-master-2   289m         14%    1282Mi          44%       
k8s-worker-0   62m          3%     518Mi           13%       
k8s-worker-1   115m         2%     659Mi           8%        

❯ kubectl top pod
No resources found in default namespace.

❯ kubectl top pod -A
NAMESPACE     NAME                                   CPU<span class="o">(</span>cores<span class="o">)</span>   MEMORY<span class="o">(</span>bytes<span class="o">)</span>   
kube-system   cilium-45nw4                           9m           135Mi           
kube-system   cilium-5x7jf                           6m           154Mi           
kube-system   cilium-84sr2                           7m           160Mi           
kube-system   cilium-operator-78f45675-dp4b6         2m           30Mi            
kube-system   cilium-operator-78f45675-fpm5g         1m           30Mi            
kube-system   cilium-tkhl4                           6m           141Mi           
kube-system   cilium-zxbvm                           5m           138Mi           
kube-system   coredns-78fcd69978-dpxxk               3m           16Mi            
kube-system   coredns-78fcd69978-ptd9p               1m           18Mi            
kube-system   etcd-k8s-master-0                      61m          88Mi            
kube-system   etcd-k8s-master-1                      50m          85Mi            
kube-system   etcd-k8s-master-2                      55m          83Mi            
kube-system   kube-apiserver-k8s-master-0            98m          462Mi           
kube-system   kube-apiserver-k8s-master-1            85m          468Mi           
kube-system   kube-apiserver-k8s-master-2            85m          423Mi           
kube-system   kube-controller-manager-k8s-master-0   22m          57Mi            
kube-system   kube-controller-manager-k8s-master-1   2m           23Mi            
kube-system   kube-controller-manager-k8s-master-2   2m           23Mi            
kube-system   kube-proxy-j2s76                       1m           24Mi            
kube-system   kube-proxy-k6d6z                       1m           18Mi            
kube-system   kube-proxy-k85rx                       1m           23Mi            
kube-system   kube-proxy-pknsc                       1m           20Mi            
kube-system   kube-proxy-xsq4m                       1m           15Mi            
kube-system   kube-scheduler-k8s-master-0            3m           25Mi            
kube-system   kube-scheduler-k8s-master-1            4m           21Mi            
kube-system   kube-scheduler-k8s-master-2            5m           21Mi            
kube-system   kube-vip-k8s-master-0                  4m           17Mi            
kube-system   kube-vip-k8s-master-1                  2m           16Mi            
kube-system   kube-vip-k8s-master-2                  2m           17Mi            
kube-system   metrics-server-559f85484-5b6xf         7m           27Mi    
</code></pre></td></tr></table>
</div>
</div><h2 id="10-为-etcd-添加定期备份能力">10. 为 etcd 添加定期备份能力</h2>
<p>请移步 <a href="https://github.com/ryan4yin/knowledge/blob/master/datastore/etcd/etcd%20%E7%9A%84%E5%A4%87%E4%BB%BD%E4%B8%8E%E6%81%A2%E5%A4%8D.md" target="_blank" rel="noopener noreferrer">etcd 的备份与恢复</a></p>
<h2 id="11-安装-volume-provisioner">11. 安装 Volume Provisioner</h2>
<p>在我们学习使用 Prometheus/MinIO/Tekton 等有状态应用时，它们默认情况下会通过 PVC 声明需要的数据卷。</p>
<p>为了支持这个能力，我们需要在集群中部署一个 Volume Provisioner.</p>
<p>对于云上环境，直接接入云服务商提供的 Volume Provisioner 就 OK 了，方便省事而且足够可靠。</p>
<p>而对于 bare-metal 环境，比较有名的应该是 rook-ceph，但是这个玩意部署复杂，维护难度又高，不适合用来测试学习，也不适合生产环境。</p>
<p>对于开发、测试环境，或者个人集群，建议使用：</p>
<ul>
<li>local 数据卷，适合数据可丢失，且不要求分布式的场景，如开发测试环境
<ul>
<li><a href="https://github.com/kubernetes-sigs/sig-storage-local-static-provisioner">https://github.com/kubernetes-sigs/sig-storage-local-static-provisioner</a></li>
<li><a href="https://github.com/rancher/local-path-provisioner">https://github.com/rancher/local-path-provisioner</a></li>
</ul>
</li>
<li>NFS 数据卷，适合数据可丢失，对性能要求不高，并且要求分布式的场景。比如开发测试环境、或者线上没啥压力的应用
<ul>
<li><a href="https://github.com/kubernetes-sigs/nfs-subdir-external-provisioner">https://github.com/kubernetes-sigs/nfs-subdir-external-provisioner</a></li>
<li><a href="https://github.com/kubernetes-csi/csi-driver-nfs">https://github.com/kubernetes-csi/csi-driver-nfs</a></li>
<li>NFS 数据的可靠性依赖于外部 NFS 服务器，企业通常使用群晖等 NAS 来做 NFS 服务器</li>
<li>如果外部 NFS 服务器出问题，应用就会崩。</li>
</ul>
</li>
<li>直接使用云上的对象存储，适合希望数据不丢失、对性能要求不高的场景。
<ul>
<li>直接使用 <a href="https://github.com/rclone/rclone">https://github.com/rclone/rclone</a> mount 模式来保存数据，或者直接同步文件夹数据到云端（可能会有一定数据丢失）。</li>
</ul>
</li>
</ul>
]]></description></item><item><title>Kubernetes 微服务最佳实践</title><link>https://ryan4yin.space/posts/kubernetes-best-practices/</link><pubDate>Tue, 25 Jan 2022 00:13:00 +0800</pubDate><author>xiaoyin_c@qq.com</author><dc:creator>ryan4yin</dc:creator><guid>https://ryan4yin.space/posts/kubernetes-best-practices/</guid><description><![CDATA[<blockquote>
<p>本文由个人笔记 <a href="https://github.com/ryan4yin/knowledge/tree/master/kubernetes" target="_blank" rel="noopener noreferrer">ryan4yin/knowledge</a> 整理而来</p>
</blockquote>
<p>本文主要介绍下我个人在使用 Kubernetes 的过程中，总结出的一套「Kubernetes 配置」，是我个人的「最佳实践」。
其中大部分内容都经历过线上环境的考验，但是也有少部分还只在我脑子里模拟过，请谨慎参考。</p>
<p>阅读前的几个注意事项：</p>
<ul>
<li>这份文档比较长，囊括了很多内容，建议当成参考手册使用，先参照目录简单读一读，有需要再细读相关内容。</li>
<li>这份文档需要一定的 Kubernetes 基础才能理解，而且如果没有过实践经验的话，看上去可能会比较枯燥。
<ul>
<li>而有过实践经验的大佬，可能会跟我有不同的见解，欢迎各路大佬评论~</li>
</ul>
</li>
</ul>
<p>我会视情况不定期更新这份文档。</p>
<h2 id="零示例">零、示例</h2>
<p>首先，这里给出一些本文遵守的前提，这些前提只是契合我遇到的场景，可灵活变通：</p>
<ul>
<li>这里只讨论无状态服务，有状态服务不在讨论范围内</li>
<li>我们不使用 Deployment 的滚动更新能力，而是为每个服务的每个版本，都创建不同的 Deployment + HPA + PodDisruptionBudget，这是为了方便做金丝雀/灰度发布</li>
<li>我们的服务可能会使用 IngressController / Service Mesh 来进行服务的负载均衡、流量切分</li>
</ul>
<p>下面先给出一个 Deployment + HPA + PodDisruptionBudget 的 demo，后面再拆开详细说下：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">  1
</span><span class="lnt">  2
</span><span class="lnt">  3
</span><span class="lnt">  4
</span><span class="lnt">  5
</span><span class="lnt">  6
</span><span class="lnt">  7
</span><span class="lnt">  8
</span><span class="lnt">  9
</span><span class="lnt"> 10
</span><span class="lnt"> 11
</span><span class="lnt"> 12
</span><span class="lnt"> 13
</span><span class="lnt"> 14
</span><span class="lnt"> 15
</span><span class="lnt"> 16
</span><span class="lnt"> 17
</span><span class="lnt"> 18
</span><span class="lnt"> 19
</span><span class="lnt"> 20
</span><span class="lnt"> 21
</span><span class="lnt"> 22
</span><span class="lnt"> 23
</span><span class="lnt"> 24
</span><span class="lnt"> 25
</span><span class="lnt"> 26
</span><span class="lnt"> 27
</span><span class="lnt"> 28
</span><span class="lnt"> 29
</span><span class="lnt"> 30
</span><span class="lnt"> 31
</span><span class="lnt"> 32
</span><span class="lnt"> 33
</span><span class="lnt"> 34
</span><span class="lnt"> 35
</span><span class="lnt"> 36
</span><span class="lnt"> 37
</span><span class="lnt"> 38
</span><span class="lnt"> 39
</span><span class="lnt"> 40
</span><span class="lnt"> 41
</span><span class="lnt"> 42
</span><span class="lnt"> 43
</span><span class="lnt"> 44
</span><span class="lnt"> 45
</span><span class="lnt"> 46
</span><span class="lnt"> 47
</span><span class="lnt"> 48
</span><span class="lnt"> 49
</span><span class="lnt"> 50
</span><span class="lnt"> 51
</span><span class="lnt"> 52
</span><span class="lnt"> 53
</span><span class="lnt"> 54
</span><span class="lnt"> 55
</span><span class="lnt"> 56
</span><span class="lnt"> 57
</span><span class="lnt"> 58
</span><span class="lnt"> 59
</span><span class="lnt"> 60
</span><span class="lnt"> 61
</span><span class="lnt"> 62
</span><span class="lnt"> 63
</span><span class="lnt"> 64
</span><span class="lnt"> 65
</span><span class="lnt"> 66
</span><span class="lnt"> 67
</span><span class="lnt"> 68
</span><span class="lnt"> 69
</span><span class="lnt"> 70
</span><span class="lnt"> 71
</span><span class="lnt"> 72
</span><span class="lnt"> 73
</span><span class="lnt"> 74
</span><span class="lnt"> 75
</span><span class="lnt"> 76
</span><span class="lnt"> 77
</span><span class="lnt"> 78
</span><span class="lnt"> 79
</span><span class="lnt"> 80
</span><span class="lnt"> 81
</span><span class="lnt"> 82
</span><span class="lnt"> 83
</span><span class="lnt"> 84
</span><span class="lnt"> 85
</span><span class="lnt"> 86
</span><span class="lnt"> 87
</span><span class="lnt"> 88
</span><span class="lnt"> 89
</span><span class="lnt"> 90
</span><span class="lnt"> 91
</span><span class="lnt"> 92
</span><span class="lnt"> 93
</span><span class="lnt"> 94
</span><span class="lnt"> 95
</span><span class="lnt"> 96
</span><span class="lnt"> 97
</span><span class="lnt"> 98
</span><span class="lnt"> 99
</span><span class="lnt">100
</span><span class="lnt">101
</span><span class="lnt">102
</span><span class="lnt">103
</span><span class="lnt">104
</span><span class="lnt">105
</span><span class="lnt">106
</span><span class="lnt">107
</span><span class="lnt">108
</span><span class="lnt">109
</span><span class="lnt">110
</span><span class="lnt">111
</span><span class="lnt">112
</span><span class="lnt">113
</span><span class="lnt">114
</span><span class="lnt">115
</span><span class="lnt">116
</span><span class="lnt">117
</span><span class="lnt">118
</span><span class="lnt">119
</span><span class="lnt">120
</span><span class="lnt">121
</span><span class="lnt">122
</span><span class="lnt">123
</span><span class="lnt">124
</span><span class="lnt">125
</span><span class="lnt">126
</span><span class="lnt">127
</span><span class="lnt">128
</span><span class="lnt">129
</span><span class="lnt">130
</span><span class="lnt">131
</span><span class="lnt">132
</span><span class="lnt">133
</span><span class="lnt">134
</span><span class="lnt">135
</span><span class="lnt">136
</span><span class="lnt">137
</span><span class="lnt">138
</span><span class="lnt">139
</span><span class="lnt">140
</span><span class="lnt">141
</span><span class="lnt">142
</span><span class="lnt">143
</span><span class="lnt">144
</span><span class="lnt">145
</span><span class="lnt">146
</span><span class="lnt">147
</span><span class="lnt">148
</span><span class="lnt">149
</span><span class="lnt">150
</span><span class="lnt">151
</span><span class="lnt">152
</span><span class="lnt">153
</span><span class="lnt">154
</span><span class="lnt">155
</span><span class="lnt">156
</span><span class="lnt">157
</span><span class="lnt">158
</span><span class="lnt">159
</span><span class="lnt">160
</span><span class="lnt">161
</span><span class="lnt">162
</span><span class="lnt">163
</span><span class="lnt">164
</span><span class="lnt">165
</span><span class="lnt">166
</span><span class="lnt">167
</span><span class="lnt">168
</span><span class="lnt">169
</span><span class="lnt">170
</span><span class="lnt">171
</span><span class="lnt">172
</span><span class="lnt">173
</span><span class="lnt">174
</span><span class="lnt">175
</span><span class="lnt">176
</span><span class="lnt">177
</span><span class="lnt">178
</span><span class="lnt">179
</span><span class="lnt">180
</span><span class="lnt">181
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-yaml" data-lang="yaml"><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l">apps/v1</span><span class="w">
</span><span class="w"></span><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l">Deployment</span><span class="w">
</span><span class="w"></span><span class="nt">metadata</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">my-app-v3</span><span class="w">
</span><span class="w">  </span><span class="nt">namespace</span><span class="p">:</span><span class="w"> </span><span class="l">prod</span><span class="w">
</span><span class="w">  </span><span class="nt">labels</span><span class="p">:</span><span class="w">
</span><span class="w">    </span><span class="nt">app</span><span class="p">:</span><span class="w"> </span><span class="l">my-app</span><span class="w">
</span><span class="w"></span><span class="nt">spec</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">replicas</span><span class="p">:</span><span class="w"> </span><span class="m">3</span><span class="w">
</span><span class="w">  </span><span class="nt">strategy</span><span class="p">:</span><span class="w">
</span><span class="w">    </span><span class="nt">type</span><span class="p">:</span><span class="w"> </span><span class="l">RollingUpdate</span><span class="w">
</span><span class="w">    </span><span class="c"># 因为服务的每个版本都使用各自的 Deployment，服务更新时其实是用不上这里的滚动更新策略的</span><span class="w">
</span><span class="w">    </span><span class="c"># 这个配置应该只在 SRE 手动修改 Deployment 配置时才会生效（通常不应该发生这种事）</span><span class="w">
</span><span class="w">    </span><span class="nt">rollingUpdate</span><span class="p">:</span><span class="w">
</span><span class="w">      </span><span class="nt">maxSurge</span><span class="p">:</span><span class="w"> </span><span class="m">10</span><span class="l">% </span><span class="w"> </span><span class="c"># 滚动更新时，每次最多更新 10% 的 Pods</span><span class="w">
</span><span class="w">      </span><span class="nt">maxUnavailable</span><span class="p">:</span><span class="w"> </span><span class="m">0</span><span class="w">  </span><span class="c"># 滚动更新时，不允许出现不可用的 Pods，也就是说始终要维持 3 个可用副本</span><span class="w">
</span><span class="w">  </span><span class="nt">selector</span><span class="p">:</span><span class="w">
</span><span class="w">    </span><span class="nt">matchLabels</span><span class="p">:</span><span class="w">
</span><span class="w">      </span><span class="nt">app</span><span class="p">:</span><span class="w"> </span><span class="l">my-app</span><span class="w">
</span><span class="w">      </span><span class="nt">version</span><span class="p">:</span><span class="w"> </span><span class="l">v3</span><span class="w">
</span><span class="w">  </span><span class="nt">template</span><span class="p">:</span><span class="w">
</span><span class="w">    </span><span class="nt">metadata</span><span class="p">:</span><span class="w">
</span><span class="w">      </span><span class="nt">labels</span><span class="p">:</span><span class="w">
</span><span class="w">        </span><span class="nt">app</span><span class="p">:</span><span class="w"> </span><span class="l">my-app</span><span class="w">
</span><span class="w">        </span><span class="nt">version</span><span class="p">:</span><span class="w"> </span><span class="l">v3</span><span class="w">
</span><span class="w">    </span><span class="nt">spec</span><span class="p">:</span><span class="w">
</span><span class="w">      </span><span class="nt">affinity</span><span class="p">:</span><span class="w">
</span><span class="w">        </span><span class="nt">podAffinity</span><span class="p">:</span><span class="w">
</span><span class="w">          </span><span class="nt">preferredDuringSchedulingIgnoredDuringExecution</span><span class="p">:</span><span class="w"> </span><span class="c"># 非强制性条件</span><span class="w">
</span><span class="w">          </span>- <span class="nt">weight</span><span class="p">:</span><span class="w"> </span><span class="m">100</span><span class="w">  </span><span class="c"># weight 用于为节点评分，会优先选择评分最高的节点（只有一条规则的情况下，这个值没啥意义）</span><span class="w">
</span><span class="w">            </span><span class="nt">podAffinityTerm</span><span class="p">:</span><span class="w">
</span><span class="w">              </span><span class="nt">labelSelector</span><span class="p">:</span><span class="w">
</span><span class="w">                </span><span class="nt">matchExpressions</span><span class="p">:</span><span class="w">
</span><span class="w">                </span>- <span class="nt">key</span><span class="p">:</span><span class="w"> </span><span class="l">app</span><span class="w">
</span><span class="w">                  </span><span class="nt">operator</span><span class="p">:</span><span class="w"> </span><span class="l">In</span><span class="w">
</span><span class="w">                  </span><span class="nt">values</span><span class="p">:</span><span class="w">
</span><span class="w">                  </span>- <span class="l">my-app</span><span class="w">
</span><span class="w">                </span>- <span class="nt">key</span><span class="p">:</span><span class="w"> </span><span class="l">version</span><span class="w">
</span><span class="w">                  </span><span class="nt">operator</span><span class="p">:</span><span class="w"> </span><span class="l">In</span><span class="w">
</span><span class="w">                  </span><span class="nt">values</span><span class="p">:</span><span class="w">
</span><span class="w">                  </span>- <span class="l">v3</span><span class="w">
</span><span class="w">              </span><span class="c"># pod 尽量使用同一种节点类型，也就是尽量保证节点的性能一致</span><span class="w">
</span><span class="w">              </span><span class="nt">topologyKey</span><span class="p">:</span><span class="w"> </span><span class="l">node.kubernetes.io/instance-type</span><span class="w">
</span><span class="w">        </span><span class="nt">podAntiAffinity</span><span class="p">:</span><span class="w">
</span><span class="w">          </span><span class="nt">preferredDuringSchedulingIgnoredDuringExecution</span><span class="p">:</span><span class="w"> </span><span class="c"># 非强制性条件</span><span class="w">
</span><span class="w">          </span>- <span class="nt">weight</span><span class="p">:</span><span class="w"> </span><span class="m">100</span><span class="w">  </span><span class="c"># weight 用于为节点评分，会优先选择评分最高的节点（只有一条规则的情况下，这个值没啥意义）</span><span class="w">
</span><span class="w">            </span><span class="nt">podAffinityTerm</span><span class="p">:</span><span class="w">
</span><span class="w">              </span><span class="nt">labelSelector</span><span class="p">:</span><span class="w">
</span><span class="w">                </span><span class="nt">matchExpressions</span><span class="p">:</span><span class="w">
</span><span class="w">                </span>- <span class="nt">key</span><span class="p">:</span><span class="w"> </span><span class="l">app</span><span class="w">
</span><span class="w">                  </span><span class="nt">operator</span><span class="p">:</span><span class="w"> </span><span class="l">In</span><span class="w">
</span><span class="w">                  </span><span class="nt">values</span><span class="p">:</span><span class="w">
</span><span class="w">                  </span>- <span class="l">my-app</span><span class="w">
</span><span class="w">                </span>- <span class="nt">key</span><span class="p">:</span><span class="w"> </span><span class="l">version</span><span class="w">
</span><span class="w">                  </span><span class="nt">operator</span><span class="p">:</span><span class="w"> </span><span class="l">In</span><span class="w">
</span><span class="w">                  </span><span class="nt">values</span><span class="p">:</span><span class="w">
</span><span class="w">                  </span>- <span class="l">v3</span><span class="w">
</span><span class="w">              </span><span class="c"># 将 pod 尽量打散在多个可用区</span><span class="w">
</span><span class="w">              </span><span class="nt">topologyKey</span><span class="p">:</span><span class="w"> </span><span class="l">topology.kubernetes.io/zone</span><span class="w">
</span><span class="w">          </span><span class="nt">requiredDuringSchedulingIgnoredDuringExecution</span><span class="p">:</span><span class="w">  </span><span class="c"># 强制性要求（这个建议按需添加）</span><span class="w">
</span><span class="w">          </span><span class="c"># 注意这个没有 weights，必须满足列表中的所有条件</span><span class="w">
</span><span class="w">          </span>- <span class="nt">labelSelector</span><span class="p">:</span><span class="w">
</span><span class="w">              </span><span class="nt">matchExpressions</span><span class="p">:</span><span class="w">
</span><span class="w">              </span>- <span class="nt">key</span><span class="p">:</span><span class="w"> </span><span class="l">app</span><span class="w">
</span><span class="w">                </span><span class="nt">operator</span><span class="p">:</span><span class="w"> </span><span class="l">In</span><span class="w">
</span><span class="w">                </span><span class="nt">values</span><span class="p">:</span><span class="w">
</span><span class="w">                </span>- <span class="l">my-app</span><span class="w">
</span><span class="w">              </span>- <span class="nt">key</span><span class="p">:</span><span class="w"> </span><span class="l">version</span><span class="w">
</span><span class="w">                </span><span class="nt">operator</span><span class="p">:</span><span class="w"> </span><span class="l">In</span><span class="w">
</span><span class="w">                </span><span class="nt">values</span><span class="p">:</span><span class="w">
</span><span class="w">                </span>- <span class="l">v3</span><span class="w">
</span><span class="w">            </span><span class="c"># Pod 必须运行在不同的节点上</span><span class="w">
</span><span class="w">            </span><span class="nt">topologyKey</span><span class="p">:</span><span class="w"> </span><span class="l">kubernetes.io/hostname</span><span class="w">
</span><span class="w">      </span><span class="nt">securityContext</span><span class="p">:</span><span class="w">
</span><span class="w">        </span><span class="c"># runAsUser: 1000  # 设定用户</span><span class="w">
</span><span class="w">        </span><span class="c"># runAsGroup: 1000  # 设定用户组</span><span class="w">
</span><span class="w">        </span><span class="nt">runAsNonRoot</span><span class="p">:</span><span class="w"> </span><span class="kc">true</span><span class="w">  </span><span class="c"># Pod 必须以非 root 用户运行</span><span class="w">
</span><span class="w">        </span><span class="nt">seccompProfile</span><span class="p">:</span><span class="w">  </span><span class="c"># security compute mode</span><span class="w">
</span><span class="w">          </span><span class="nt">type</span><span class="p">:</span><span class="w"> </span><span class="l">RuntimeDefault</span><span class="w">
</span><span class="w">      </span><span class="nt">nodeSelector</span><span class="p">:</span><span class="w">
</span><span class="w">        </span><span class="nt">eks.amazonaws.com/nodegroup</span><span class="p">:</span><span class="w"> </span><span class="l">common </span><span class="w"> </span><span class="c"># 使用专用节点组，如果希望使用多个节点组，可改用节点亲和性</span><span class="w">
</span><span class="w">      </span><span class="nt">volumes</span><span class="p">:</span><span class="w">
</span><span class="w">      </span>- <span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">tmp-dir</span><span class="w">
</span><span class="w">        </span><span class="nt">emptyDir</span><span class="p">:</span><span class="w"> </span>{}<span class="w">
</span><span class="w">      </span><span class="nt">containers</span><span class="p">:</span><span class="w">
</span><span class="w">      </span>- <span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">my-app-v3</span><span class="w">
</span><span class="w">        </span><span class="nt">image</span><span class="p">:</span><span class="w"> </span><span class="l">my-app:v3 </span><span class="w"> </span><span class="c"># 建议使用私有镜像仓库，规避 docker.io 的镜像拉取限制</span><span class="w">
</span><span class="w">        </span><span class="nt">imagePullPolicy</span><span class="p">:</span><span class="w"> </span><span class="l">IfNotPresent</span><span class="w">
</span><span class="w">        </span><span class="nt">volumeMounts</span><span class="p">:</span><span class="w">
</span><span class="w">        </span>- <span class="nt">mountPath</span><span class="p">:</span><span class="w"> </span><span class="l">/tmp</span><span class="w">
</span><span class="w">          </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">tmp-dir</span><span class="w">
</span><span class="w">        </span><span class="nt">lifecycle</span><span class="p">:</span><span class="w">
</span><span class="w">          </span><span class="nt">preStop</span><span class="p">:</span><span class="w">
</span><span class="w">            </span><span class="nt">exec</span><span class="p">:</span><span class="w">
</span><span class="w">              </span><span class="nt">command</span><span class="p">:</span><span class="w">
</span><span class="w">              </span>- <span class="l">/bin/sh</span><span class="w">
</span><span class="w">              </span>- -<span class="l">c</span><span class="w">
</span><span class="w">              </span>- <span class="s2">&#34;while [ $(netstat -plunt | grep tcp | wc -l | xargs) -ne 0 ]; do sleep 1; done&#34;</span><span class="w">
</span><span class="w">        </span><span class="nt">resources</span><span class="p">:</span><span class="w">  </span><span class="c"># 资源请求与限制</span><span class="w">
</span><span class="w">          </span><span class="c"># 对于核心服务，建议设置 requests = limits，避免资源竞争</span><span class="w">
</span><span class="w">          </span><span class="nt">requests</span><span class="p">:</span><span class="w">
</span><span class="w">            </span><span class="c"># HPA 会使用 requests 计算资源利用率</span><span class="w">
</span><span class="w">            </span><span class="c"># 建议将 requests 设为服务正常状态下的 CPU 使用率，HPA 的目前指标设为 80%</span><span class="w">
</span><span class="w">            </span><span class="c"># 所有容器的 requests 总量不建议为 2c/4G 4c/8G 等常见值，因为节点通常也是这个配置，这会导致 Pod 只能调度到更大的节点上，适当调小 requests 等扩充可用的节点类型，从而扩充节点池。 </span><span class="w">
</span><span class="w">            </span><span class="nt">cpu</span><span class="p">:</span><span class="w"> </span><span class="l">1000m</span><span class="w">
</span><span class="w">            </span><span class="nt">memory</span><span class="p">:</span><span class="w"> </span><span class="l">1Gi</span><span class="w">
</span><span class="w">          </span><span class="nt">limits</span><span class="p">:</span><span class="w">
</span><span class="w">            </span><span class="c"># limits - requests 为允许超卖的资源量，建议为 requests 的 1 到 2 倍，酌情配置。</span><span class="w">
</span><span class="w">            </span><span class="nt">cpu</span><span class="p">:</span><span class="w"> </span><span class="l">1000m</span><span class="w">
</span><span class="w">            </span><span class="nt">memory</span><span class="p">:</span><span class="w"> </span><span class="l">1Gi</span><span class="w">
</span><span class="w">        </span><span class="nt">securityContext</span><span class="p">:</span><span class="w">
</span><span class="w">          </span><span class="c"># 将容器层设为只读，防止容器文件被篡改</span><span class="w">
</span><span class="w">          </span><span class="c">## 如果需要写入临时文件，建议额外挂载 emptyDir 来提供可读写的数据卷</span><span class="w">
</span><span class="w">          </span><span class="nt">readOnlyRootFilesystem</span><span class="p">:</span><span class="w"> </span><span class="kc">true</span><span class="w">
</span><span class="w">          </span><span class="c"># 禁止 Pod 做任何权限提升</span><span class="w">
</span><span class="w">          </span><span class="nt">allowPrivilegeEscalation</span><span class="p">:</span><span class="w"> </span><span class="kc">false</span><span class="w">
</span><span class="w">          </span><span class="nt">capabilities</span><span class="p">:</span><span class="w">
</span><span class="w">            </span><span class="c"># drop ALL 的权限比较严格，可按需修改</span><span class="w">
</span><span class="w">            </span><span class="nt">drop</span><span class="p">:</span><span class="w">
</span><span class="w">            </span>- <span class="l">ALL</span><span class="w">
</span><span class="w">        </span><span class="nt">startupProbe</span><span class="p">:</span><span class="w">  </span><span class="c"># 要求 kubernetes 1.18+</span><span class="w">
</span><span class="w">          </span><span class="nt">httpGet</span><span class="p">:</span><span class="w">
</span><span class="w">            </span><span class="nt">path</span><span class="p">:</span><span class="w"> </span><span class="l">/actuator/health </span><span class="w"> </span><span class="c"># 直接使用健康检查接口即可</span><span class="w">
</span><span class="w">            </span><span class="nt">port</span><span class="p">:</span><span class="w"> </span><span class="m">8080</span><span class="w">
</span><span class="w">          </span><span class="nt">periodSeconds</span><span class="p">:</span><span class="w"> </span><span class="m">5</span><span class="w">
</span><span class="w">          </span><span class="nt">timeoutSeconds</span><span class="p">:</span><span class="w"> </span><span class="m">1</span><span class="w">
</span><span class="w">          </span><span class="nt">failureThreshold</span><span class="p">:</span><span class="w"> </span><span class="m">20</span><span class="w">  </span><span class="c"># 最多提供给服务 5s * 20 的启动时间</span><span class="w">
</span><span class="w">          </span><span class="nt">successThreshold</span><span class="p">:</span><span class="w"> </span><span class="m">1</span><span class="w">
</span><span class="w">        </span><span class="nt">livenessProbe</span><span class="p">:</span><span class="w">
</span><span class="w">          </span><span class="nt">httpGet</span><span class="p">:</span><span class="w">
</span><span class="w">            </span><span class="nt">path</span><span class="p">:</span><span class="w"> </span><span class="l">/actuator/health </span><span class="w"> </span><span class="c"># spring 的通用健康检查路径</span><span class="w">
</span><span class="w">            </span><span class="nt">port</span><span class="p">:</span><span class="w"> </span><span class="m">8080</span><span class="w">
</span><span class="w">          </span><span class="nt">periodSeconds</span><span class="p">:</span><span class="w"> </span><span class="m">5</span><span class="w">
</span><span class="w">          </span><span class="nt">timeoutSeconds</span><span class="p">:</span><span class="w"> </span><span class="m">1</span><span class="w">
</span><span class="w">          </span><span class="nt">failureThreshold</span><span class="p">:</span><span class="w"> </span><span class="m">5</span><span class="w">
</span><span class="w">          </span><span class="nt">successThreshold</span><span class="p">:</span><span class="w"> </span><span class="m">1</span><span class="w">
</span><span class="w">        </span><span class="c"># Readiness probes are very important for a RollingUpdate to work properly,</span><span class="w">
</span><span class="w">        </span><span class="nt">readinessProbe</span><span class="p">:</span><span class="w">
</span><span class="w">          </span><span class="nt">httpGet</span><span class="p">:</span><span class="w">
</span><span class="w">            </span><span class="nt">path</span><span class="p">:</span><span class="w"> </span><span class="l">/actuator/health </span><span class="w"> </span><span class="c"># 简单起见可直接使用 livenessProbe 相同的接口，当然也可额外定义</span><span class="w">
</span><span class="w">            </span><span class="nt">port</span><span class="p">:</span><span class="w"> </span><span class="m">8080</span><span class="w">
</span><span class="w">          </span><span class="nt">periodSeconds</span><span class="p">:</span><span class="w"> </span><span class="m">5</span><span class="w">
</span><span class="w">          </span><span class="nt">timeoutSeconds</span><span class="p">:</span><span class="w"> </span><span class="m">1</span><span class="w">
</span><span class="w">          </span><span class="nt">failureThreshold</span><span class="p">:</span><span class="w"> </span><span class="m">5</span><span class="w">
</span><span class="w">          </span><span class="nt">successThreshold</span><span class="p">:</span><span class="w"> </span><span class="m">1</span><span class="w">
</span><span class="w"></span><span class="nn">---</span><span class="w">
</span><span class="w"></span><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l">autoscaling/v2beta2</span><span class="w">
</span><span class="w"></span><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l">HorizontalPodAutoscaler</span><span class="w">
</span><span class="w"></span><span class="nt">metadata</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">labels</span><span class="p">:</span><span class="w">
</span><span class="w">    </span><span class="nt">app</span><span class="p">:</span><span class="w"> </span><span class="l">my-app</span><span class="w">
</span><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">my-app-v3</span><span class="w">
</span><span class="w">  </span><span class="nt">namespace</span><span class="p">:</span><span class="w"> </span><span class="l">prod</span><span class="w">
</span><span class="w"></span><span class="nt">spec</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">scaleTargetRef</span><span class="p">:</span><span class="w">
</span><span class="w">    </span><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l">apps/v1</span><span class="w">
</span><span class="w">    </span><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l">Deployment</span><span class="w">
</span><span class="w">    </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">my-app-v3</span><span class="w">
</span><span class="w">  </span><span class="nt">maxReplicas</span><span class="p">:</span><span class="w"> </span><span class="m">50</span><span class="w">
</span><span class="w">  </span><span class="nt">minReplicas</span><span class="p">:</span><span class="w"> </span><span class="m">3</span><span class="w">
</span><span class="w">  </span><span class="nt">metrics</span><span class="p">:</span><span class="w">
</span><span class="w">  </span>- <span class="nt">type</span><span class="p">:</span><span class="w"> </span><span class="l">Resource</span><span class="w">
</span><span class="w">    </span><span class="nt">resource</span><span class="p">:</span><span class="w">
</span><span class="w">      </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">cpu</span><span class="w">
</span><span class="w">      </span><span class="nt">target</span><span class="p">:</span><span class="w">
</span><span class="w">        </span><span class="nt">type</span><span class="p">:</span><span class="w"> </span><span class="l">Utilization</span><span class="w">
</span><span class="w">        </span><span class="nt">averageUtilization</span><span class="p">:</span><span class="w"> </span><span class="m">70</span><span class="w">
</span><span class="w"></span><span class="nn">---</span><span class="w">
</span><span class="w"></span><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l">policy/v1</span><span class="w">
</span><span class="w"></span><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l">PodDisruptionBudget</span><span class="w">
</span><span class="w"></span><span class="nt">metadata</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">my-app-v3</span><span class="w">
</span><span class="w">  </span><span class="nt">namespace</span><span class="p">:</span><span class="w"> </span><span class="l">prod</span><span class="w">
</span><span class="w">  </span><span class="nt">labels</span><span class="p">:</span><span class="w">
</span><span class="w">    </span><span class="nt">app</span><span class="p">:</span><span class="w"> </span><span class="l">my-app</span><span class="w">
</span><span class="w"></span><span class="nt">spec</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">minAvailable</span><span class="p">:</span><span class="w"> </span><span class="m">75</span><span class="l">%</span><span class="w">
</span><span class="w">  </span><span class="nt">selector</span><span class="p">:</span><span class="w">
</span><span class="w">    </span><span class="nt">matchLabels</span><span class="p">:</span><span class="w">
</span><span class="w">      </span><span class="nt">app</span><span class="p">:</span><span class="w"> </span><span class="l">my-app</span><span class="w">
</span><span class="w">      </span><span class="nt">version</span><span class="p">:</span><span class="w"> </span><span class="l">v3</span><span class="w">
</span></code></pre></td></tr></table>
</div>
</div><h2 id="一优雅停止gracful-shutdown与-502504-报错">一、优雅停止（Gracful Shutdown）与 502/504 报错</h2>
<p>如果 Pod 正在处理大量请求（比如 1000 QPS+）时，因为节点故障或「竞价节点」被回收等原因被重新调度，
你可能会观察到在容器被 terminate 的一段时间内出现少量 502/504。</p>
<p>为了搞清楚这个问题，需要先理解清楚 terminate 一个 Pod 的流程：</p>
<ol>
<li>Pod 的状态被设为「Terminating」，（几乎）同时该 Pod 被从所有关联的 Service Endpoints 中移除</li>
<li><code>preStop</code> 钩子被执行，它可以是一个命令，或者一个对 Pod 中容器的 http 调用
<ol>
<li>如果你的程序在收到 SIGTERM 信号时，无法优雅退出，就可以考虑使用 <code>preStop</code></li>
<li>如果让程序本身支持优雅退出比较麻烦的话，用 <code>preStop</code> 实现优雅退出是一个非常好的方式</li>
</ol>
</li>
<li>将 SIGTERM 发送给 Pod 中的所有容器</li>
<li>继续等待，直到超过 <code>spec.terminationGracePeriodSeconds</code> 设定好的时间，这个值默认为 30s
<ol>
<li>需要注意的是，这个优雅退出的等待计时是与 <code>preStop</code> 同步开始的！而且它也不会等待 <code>preStop</code> 结束！</li>
</ol>
</li>
<li>如果超过了 <code>spec.terminationGracePeriodSeconds</code> 容器仍然没有停止，k8s 将会发送 SIGKILL 信号给容器</li>
<li>进程全部终止后，整个 Pod 完全被清理掉</li>
</ol>
<p><strong>注意</strong>：1 和 2 两个工作是异步发生的，所以可能会出现「Pod 还在 Service Endpoints 中，但是 <code>preStop</code> 已经执行了」的情况，我们需要考虑到这种状况的发生。</p>
<p>了解了上面的流程后，我们就能分析出两种错误码出现的原因：</p>
<ul>
<li>502：应用程序在收到 SIGTERM 信号后直接终止了运行，导致部分还没有被处理完的请求直接中断，代理层返回 502 表示这种情况</li>
<li>504：Service Endpoints 移除不够及时，在 Pod 已经被终止后，仍然有个别请求被路由到了该 Pod，得不到响应导致 504</li>
</ul>
<p>通常的解决方案是，在 Pod 的 <code>preStop</code> 步骤加一个 15s 的等待时间。
其原理是：在 Pod 处理 terminating 状态的时候，就会被从 Service Endpoints 中移除，也就不会再有新的请求过来了。
在 <code>preStop</code> 等待 15s，基本就能保证所有的请求都在容器死掉之前被处理完成（一般来说，绝大部分请求的处理时间都在 300ms 以内吧）。</p>
<p>一个简单的示例如下，它使 Pod 被终止时，总是先等待 15s，再发送 SIGTERM 信号给容器：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span><span class="lnt">9
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-yaml" data-lang="yaml"><span class="w">    </span><span class="nt">containers</span><span class="p">:</span><span class="w">
</span><span class="w">    </span>- <span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">my-app</span><span class="w">
</span><span class="w">      </span><span class="c"># 添加下面这部分</span><span class="w">
</span><span class="w">      </span><span class="nt">lifecycle</span><span class="p">:</span><span class="w">
</span><span class="w">        </span><span class="nt">preStop</span><span class="p">:</span><span class="w">
</span><span class="w">          </span><span class="nt">exec</span><span class="p">:</span><span class="w">
</span><span class="w">            </span><span class="nt">command</span><span class="p">:</span><span class="w">
</span><span class="w">            </span>- <span class="l">/bin/sleep</span><span class="w">
</span><span class="w">            </span>- <span class="s2">&#34;15&#34;</span><span class="w">
</span></code></pre></td></tr></table>
</div>
</div><p>更好的解决办法，是直接等待所有 tcp 连接都关闭（需要镜像中有 netstat）：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-yaml" data-lang="yaml"><span class="w">    </span><span class="nt">containers</span><span class="p">:</span><span class="w">
</span><span class="w">    </span>- <span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">my-app</span><span class="w">
</span><span class="w">      </span><span class="c"># 添加下面这部分</span><span class="w">
</span><span class="w">      </span><span class="nt">lifecycle</span><span class="p">:</span><span class="w">
</span><span class="w">      </span><span class="nt">preStop</span><span class="p">:</span><span class="w">
</span><span class="w">          </span><span class="nt">exec</span><span class="p">:</span><span class="w">
</span><span class="w">            </span><span class="nt">command</span><span class="p">:</span><span class="w">
</span><span class="w">            </span>- <span class="l">/bin/sh</span><span class="w">
</span><span class="w">            </span>- -<span class="l">c</span><span class="w">
</span><span class="w">            </span>- <span class="s2">&#34;while [ $(netstat -plunt | grep tcp | wc -l | xargs) -ne 0 ]; do sleep 1; done&#34;</span><span class="w">
</span></code></pre></td></tr></table>
</div>
</div><h3 id="k8s-istio-pod-prestop">如果我的服务还使用了 Sidecar 代理网络请求，该怎么处理？</h3>
<p>以服务网格 Istio 为例，在 Envoy 代理了 Pod 流量的情况下，502/504 的问题会变得更复杂一点——还需要考虑 Sidecar 与主容器的关闭顺序：</p>
<ul>
<li>如果在 Envoy 已关闭后，有新的请求再进来，将会导致 504（没人响应这个请求了）
<ul>
<li>所以 Envoy 最好在 Terminating 至少 3s 后才能关，确保 Istio 网格配置已完全更新</li>
</ul>
</li>
<li>如果在 Envoy 还没停止时，主容器先关闭，然后又有新的请求再进来，Envoy 将因为无法连接到 upstream 导致 503
<ul>
<li>所以主容器也最好在 Terminating 至少 3s 后，才能关闭。</li>
</ul>
</li>
<li>如果主容器处理还未处理完遗留请求时，Envoy 或者主容器的其中一个停止了，会因为 tcp 连接直接断开连接导致 502
<ul>
<li>因此 Envoy 必须在主容器处理完遗留请求后（即没有 tcp 连接时），才能关闭</li>
</ul>
</li>
</ul>
<p>所以总结下：Envoy 及主容器的 <code>preStop</code> 都至少得设成 3s，并且在「没有 tcp 连接」时，才能关闭，避免出现 502/503/504.</p>
<p>主容器的修改方法在前文中已经写过了，下面介绍下 Envoy 的修改方法。</p>
<p>和主容器一样，Envoy 也能直接加 <code>preStop</code>，修改 <code>istio-sidecar-injector</code> 这个 <code>configmap</code>，在 sidecar 里添加 preStop sleep 命令:</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-yaml" data-lang="yaml"><span class="w">    </span><span class="nt">containers</span><span class="p">:</span><span class="w">
</span><span class="w">    </span>- <span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">istio-proxy</span><span class="w">
</span><span class="w">      </span><span class="c"># 添加下面这部分</span><span class="w">
</span><span class="w">      </span><span class="nt">lifecycle</span><span class="p">:</span><span class="w">
</span><span class="w">      </span><span class="nt">preStop</span><span class="p">:</span><span class="w">
</span><span class="w">          </span><span class="nt">exec</span><span class="p">:</span><span class="w">
</span><span class="w">            </span><span class="nt">command</span><span class="p">:</span><span class="w">
</span><span class="w">            </span>- <span class="l">/bin/sh</span><span class="w">
</span><span class="w">            </span>- -<span class="l">c</span><span class="w">
</span><span class="w">            </span>- <span class="s2">&#34;while [ $(netstat -plunt | grep tcp | grep -v envoy | wc -l | xargs) -ne 0 ]; do sleep 1; done&#34;</span><span class="w">
</span></code></pre></td></tr></table>
</div>
</div><h3 id="参考">参考</h3>
<ul>
<li><a href="https://cloud.google.com/blog/products/containers-kubernetes/kubernetes-best-practices-terminating-with-grace" target="_blank" rel="noopener noreferrer">Kubernetes best practices: terminating with grace</a></li>
<li><a href="https://medium.com/flant-com/kubernetes-graceful-shutdown-nginx-php-fpm-d5ab266963c2" target="_blank" rel="noopener noreferrer">Graceful shutdown in Kubernetes is not always trivial</a></li>
</ul>
<h2 id="k8s-hpa">二、服务的伸缩配置 - HPA</h2>
<p>Kubernetes 官方主要支持基于 Pod CPU 的伸缩，这是应用最为广泛的伸缩指标，需要部署 <a href="https://github.com/kubernetes-sigs/metrics-server" target="_blank" rel="noopener noreferrer">metrics-server</a> 才可使用。</p>
<p>先回顾下前面给出的示例：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-yaml" data-lang="yaml"><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l">autoscaling/v2beta2 </span><span class="w"> </span><span class="c"># k8s 1.23+ 此 API 已经 GA</span><span class="w">
</span><span class="w"></span><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l">HorizontalPodAutoscaler</span><span class="w">
</span><span class="w"></span><span class="nt">metadata</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">labels</span><span class="p">:</span><span class="w">
</span><span class="w">    </span><span class="nt">app</span><span class="p">:</span><span class="w"> </span><span class="l">my-app</span><span class="w">
</span><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">my-app-v3</span><span class="w">
</span><span class="w">  </span><span class="nt">namespace</span><span class="p">:</span><span class="w"> </span><span class="l">prod</span><span class="w">
</span><span class="w"></span><span class="nt">spec</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">scaleTargetRef</span><span class="p">:</span><span class="w">
</span><span class="w">    </span><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l">apps/v1</span><span class="w">
</span><span class="w">    </span><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l">Deployment</span><span class="w">
</span><span class="w">    </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">my-app-v3</span><span class="w">
</span><span class="w">  </span><span class="nt">maxReplicas</span><span class="p">:</span><span class="w"> </span><span class="m">50</span><span class="w">
</span><span class="w">  </span><span class="nt">minReplicas</span><span class="p">:</span><span class="w"> </span><span class="m">3</span><span class="w">
</span><span class="w">  </span><span class="nt">metrics</span><span class="p">:</span><span class="w">
</span><span class="w">  </span>- <span class="nt">type</span><span class="p">:</span><span class="w"> </span><span class="l">Resource</span><span class="w">
</span><span class="w">    </span><span class="nt">resource</span><span class="p">:</span><span class="w">
</span><span class="w">      </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">cpu</span><span class="w">
</span><span class="w">      </span><span class="nt">target</span><span class="p">:</span><span class="w">
</span><span class="w">        </span><span class="nt">type</span><span class="p">:</span><span class="w"> </span><span class="l">Utilization</span><span class="w">
</span><span class="w">        </span><span class="nt">averageUtilization</span><span class="p">:</span><span class="w"> </span><span class="m">70</span><span class="w">
</span></code></pre></td></tr></table>
</div>
</div><h3 id="1-当前指标值的计算方式">1. 当前指标值的计算方式</h3>
<p>HPA 默认使用 Pod 的当前指标进行计算，以 CPU 为例，其计算公式为：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback">「Pod 的 CPU 利用率」= 100% * 「所有 Container 的 CPU 用量之和」/「所有 Container 的 CPU requests 之和」
</code></pre></td></tr></table>
</div>
</div><p>注意分母是总的 requests 量，而不是 limits.</p>
<h4 id="11-存在的问题与解决方法">1.1 存在的问题与解决方法</h4>
<p>在 Pod 只有一个容器时这没啥问题，但是当 Pod 注入了 envoy 等 sidecar 时，这就会有问题了。</p>
<p>因为 Istio 的 Sidecar requests 默认为 <code>100m</code> 也就是 0.1 核。
在未 tuning 的情况下，服务负载一高，sidecar 的实际用量很容易就能涨到 0.2-0.4 核。
把这两个值代入前面的公式，会发现 <strong>对于 QPS 较高的服务，添加 Sidecar 后，「Pod 的 CPU 利用率」可能会高于「应用容器的 CPU 利用率」</strong>，造成不必要的扩容。</p>
<p>解决方法：</p>
<ul>
<li>方法一：针对每个服务的 CPU 使用情况，为 sidecar 设置不同的 requests/limits</li>
<li>方法二：使用 KEDA 等第三方组件，获取到应用程序的 CPU 利用率（排除掉 Sidecar），使用它进行扩缩容</li>
<li>方法三：使用 k8s 1.20 提供的 alpha 特性：<a href="https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/#container-resource-metrics" target="_blank" rel="noopener noreferrer">Container Resourse Metrics</a>.</li>
</ul>
<h4 id="2-hpa-的扩缩容算法">2. HPA 的扩缩容算法</h4>
<p>HPA 什么时候会扩容，这一点是很好理解的。但是 HPA 的缩容策略，会有些迷惑，下面简单分析下。</p>
<ol>
<li>HPA 的「目标指标」可以使用两种形式：绝对度量指标和资源利用率。
<ul>
<li>绝对度量指标：比如 CPU，就是设定绝对核数。</li>
<li>资源利用率（资源使用量/资源请求 * 100%）：在 Pod 设置了资源请求时，可以使用资源利用率进行 Pod 伸缩。</li>
</ul>
</li>
<li>HPA 的「当前指标」是一段时间内所有 Pods 的平均值，不是峰值。Pod 的指标是其中所有容器指标之和。</li>
</ol>
<p>HPA 的扩缩容算法为：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback">期望副本数 = ceil[当前副本数 * ( 当前指标 / 目标指标 )]
</code></pre></td></tr></table>
</div>
</div><p>从上面的参数可以看到：</p>
<ol>
<li>只要「当前指标」超过了目标指标，就一定会发生扩容。</li>
<li><code>当前指标 / 目标指标</code>要小到一定的程度，才会触发缩容。
<ol>
<li>比如双副本的情况下，上述比值要小于等于 1/2，才会缩容到单副本。</li>
<li>三副本的情况下，上述比值的临界点是 2/3。</li>
<li>五副本时临界值是 4/5，100副本时临界值是 99/100，依此类推。</li>
<li>如果 <code>当前指标 / 目标指标</code> 从 1 降到 0.5，副本的数量将会减半。（虽然说副本数越多，发生这么大变化的可能性就越小。）</li>
</ol>
</li>
<li><code>当前副本数 / 目标指标</code>的值越大，「当前指标」的波动对「期望副本数」的影响就越大。</li>
</ol>
<p>为了防止扩缩容过于敏感，它还有几个延时相关的参数：</p>
<ol>
<li>HPA Loop 延时：默认 15 秒，每 15 秒钟进行一次 HPA 扫描。</li>
<li><code>--horizontal-pod-autoscaler-cpu-initialization-period</code>:</li>
<li>缩容冷却时间：默认 5 分钟。</li>
</ol>
<h3 id="3-hpa-的期望值设成多少合适">3. HPA 的期望值设成多少合适</h3>
<p>这个需要针对每个服务的具体情况，具体分析。</p>
<p>以最常用的按 CPU 值伸缩为例，</p>
<ul>
<li>核心服务
<ul>
<li>requests/limits 值: 建议设成相等的，保证<a href="https://kubernetes.io/docs/tasks/configure-pod-container/quality-service-pod/" target="_blank" rel="noopener noreferrer">服务质量等级</a>为 Guaranteed
<ul>
<li>需要注意 CPU 跟 Memory 的 limits 限制策略是不同的，CPU 是真正地限制了上限，而 Memory 是用超了就干掉容器（OOMKilled）</li>
<li>k8s 一直使用 cgroups v1 (<code>cpu_shares</code>/<code>memory.limit_in_bytes</code>)来限制 cpu/memory，但是对于 <code>Guaranteed</code> 的 Pods 而言，内存并不能完全预留，资源竞争总是有可能发生的。1.22 有 alpha 特性改用 cgroups v2，可以关注下。</li>
</ul>
</li>
<li>HPA: 一般来说，期望值设为 60% 到 70% 可能是比较合适的，最小副本数建议设为 2 - 5. （仅供参考）</li>
<li>PodDisruptionBudget: 建议按服务的健壮性与 HPA 期望值，来设置 PDB，后面会详细介绍，这里就先略过了</li>
</ul>
</li>
<li>非核心服务
<ul>
<li>requests/limits 值: 建议 requests 设为 limits 的 0.6 - 0.9 倍（仅供参考），对应的服务质量等级为 Burstable
<ul>
<li>也就是超卖了资源，这样做主要的考量点是，很多非核心服务负载都很低，根本跑不到 limits 这么高，降低 requests 可以提高集群资源利用率，也不会损害服务稳定性。</li>
</ul>
</li>
<li>HPA: 因为 requests 降低了，我们可以提高 HPA 到期望值，比如 80% ~ 90%，最小副本数建议设为 1 - 3. （仅供参考）</li>
<li>PodDisruptionBudget: 非核心服务嘛，保证最少副本数为 1 就行了。</li>
</ul>
</li>
</ul>
<h3 id="4-hpa-的常见问题">4. HPA 的常见问题</h3>
<h4 id="41-pod-扩容---预热陷阱">4.1. Pod 扩容 - 预热陷阱</h4>
<blockquote>
<p>预热：Java/C# 这类运行在虚拟机上的语言，第一次使用到某些功能时，往往需要初始化一些资源，例如「JIT 即时编译」。
如果代码里还应用了动态类加载之类的功能，就很可能导致微服务某些 API 第一次被调用时，响应特别慢（要动态编译 class）。
因此 Pod 在提供服务前，需要提前「预热（slow_start）」一次这些接口，将需要用到的资源提前初始化好。</p>
</blockquote>
<p>在负载很高的情况下，HPA 会自动扩容。
但是如果扩容的 Pod 需要预热，就可能会遇到「预热陷阱」。</p>
<p>在有大量用户访问的时候，不论使用何种负载均衡策略，只要请求被转发到新建的 Pod 上，这个请求就会「卡住」。
如果请求速度太快，Pod 启动的瞬间「卡住」的请求就越多，这将会导致新建 Pod 因为压力过大而垮掉。
然后 Pod 一重启就被压垮，进入 CrashLoopBackoff 循环。</p>
<p>如果是在使用多线程做负载测试时，效果更明显：50 个线程在不间断地请求，
别的 Pod 响应时间是「毫秒级」，而新建的 Pod 的首次响应是「秒级」。几乎是一瞬间，50 个线程就会全部陷在新建的 Pod 这里。
而新建的 Pod 在启动的瞬间可能特别脆弱，瞬间的 50 个并发请求就可以将它压垮。
然后 Pod 一重启就被压垮，进入 CrashLoopBackoff 循环。</p>
<p><strong>解决方法</strong>：</p>
<p>可以在「应用层面」解决：</p>
<ol>
<li>在启动探针 API 的后端控制器里面，依次调用所有需要预热的接口或者其他方式，提前初始化好所有资源。
<ol>
<li>启动探针的控制器中，可以通过 <code>localhost</code> 回环地址调用它自身的接口。</li>
</ol>
</li>
<li>使用「AOT 预编译」技术：预热，通常都是因为「JIT 即时编译」导致的问题，在需要用到时它才编译。而 AOT 是预先编译，在使用前完成编译，因此 AOT 能解决预热的问题。</li>
</ol>
<p>也可以在「基础设施层面」解决：</p>
<ol>
<li>像 AWS ALB TargetGroup 以及其他云服务商的 ALB 服务，通常都可以设置 <code>slow_start</code> 时长，即对新加入的实例，使用一定时间慢慢地把流量切过去，最终达到预期的负载均衡状态。这个可以解决服务预热问题。</li>
<li>Envoy 也已经支持 <code>slow_start</code> 模式，支持在一个设置好的时间窗口内，把流量慢慢负载到新加入的实例上，达成预热效果。</li>
</ol>
<h4 id="42-hpa-扩缩容过于敏感导致-pod-数量震荡">4.2. HPA 扩缩容过于敏感，导致 Pod 数量震荡</h4>
<p>通常来讲，EKS 上绝大部分负载都应该选择使用 CPU 进行扩缩容。因为 CPU 通常能很好的反映服务的负载情况</p>
<p>但是有些服务会存在其他影响 CPU 使用率的因素，导致使用 CPU 扩缩容变得不那么可靠，比如：</p>
<ul>
<li>有些 Java 服务堆内存设得很大，GC pause 也设得比较长，因此内存 GC 会造成 CPU 间歇性飙升，CPU 监控会有大量的尖峰。</li>
<li>有些服务有定时任务，定时任务一运行 CPU 就涨，但是这跟服务的 QPS 是无关的</li>
<li>有些服务可能一运行 CPU 就会立即处于一个高位状态，它可能希望使用别的业务侧指标来进行扩容，而不是 CPU.</li>
</ul>
<p>因为上述问题存在，使用 CPU 扩缩容，就可能会造成服务频繁的扩容然后缩容，或者无限扩容。
而有些服务（如我们的「推荐服务」），对「扩容」和「缩容」都是比较敏感的，每次扩缩都会造成服务可用率抖动。</p>
<p>对这类服务而言，HPA 有这几种调整策略：</p>
<ul>
<li>选择使用 <strong>QPS</strong> 等相对比较平滑，没有 GC 这类干扰的指标来进行扩缩容，这需要借助 KEDA 等社区组件。</li>
<li>对 kubernetes 1.18+，可以直接使用 HPA 的 <code>behavior.scaleDown</code> 和 <code>behavior.scaleUp</code> 两个参数，控制每次扩缩容的最多 pod 数量或者比例。 示例如下：</li>
</ul>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-yaml" data-lang="yaml"><span class="nn">---</span><span class="w">
</span><span class="w"></span><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l">autoscaling/v2beta2</span><span class="w">
</span><span class="w"></span><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l">HorizontalPodAutoscaler</span><span class="w">
</span><span class="w"></span><span class="nt">metadata</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">podinfo</span><span class="w">
</span><span class="w">  </span><span class="nt">namespace</span><span class="p">:</span><span class="w"> </span><span class="l">default</span><span class="w">
</span><span class="w"></span><span class="nt">spec</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">scaleTargetRef</span><span class="p">:</span><span class="w">
</span><span class="w">    </span><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l">apps/v1</span><span class="w">
</span><span class="w">    </span><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l">Deployment</span><span class="w">
</span><span class="w">    </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">podinfo</span><span class="w">
</span><span class="w">  </span><span class="nt">minReplicas</span><span class="p">:</span><span class="w"> </span><span class="m">3</span><span class="w">
</span><span class="w">  </span><span class="nt">maxReplicas</span><span class="p">:</span><span class="w"> </span><span class="m">50</span><span class="w">
</span><span class="w">  </span><span class="nt">metrics</span><span class="p">:</span><span class="w">
</span><span class="w">  </span>- <span class="nt">type</span><span class="p">:</span><span class="w"> </span><span class="l">Resource</span><span class="w">
</span><span class="w">    </span><span class="nt">resource</span><span class="p">:</span><span class="w">
</span><span class="w">      </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">cpu</span><span class="w">
</span><span class="w">      </span><span class="nt">target</span><span class="p">:</span><span class="w">
</span><span class="w">        </span><span class="nt">type</span><span class="p">:</span><span class="w"> </span><span class="l">Utilization</span><span class="w">
</span><span class="w">        </span><span class="nt">averageUtilization</span><span class="p">:</span><span class="w"> </span><span class="m">50</span><span class="w">  </span><span class="c"># 期望的 CPU 平均值</span><span class="w">
</span><span class="w">  </span><span class="nt">behavior</span><span class="p">:</span><span class="w">
</span><span class="w">    </span><span class="nt">scaleUp</span><span class="p">:</span><span class="w">
</span><span class="w">      </span><span class="nt">stabilizationWindowSeconds</span><span class="p">:</span><span class="w"> </span><span class="m">0</span><span class="w">  </span><span class="c"># 默认为 0，只使用当前值进行扩缩容</span><span class="w">
</span><span class="w">      </span><span class="nt">policies</span><span class="p">:</span><span class="w">
</span><span class="w">      </span>- <span class="nt">periodSeconds</span><span class="p">:</span><span class="w"> </span><span class="m">180</span><span class="w">  </span><span class="c"># 每 3 分钟最多扩容 5% 的 Pods</span><span class="w">
</span><span class="w">        </span><span class="nt">type</span><span class="p">:</span><span class="w"> </span><span class="l">Percent</span><span class="w">
</span><span class="w">        </span><span class="nt">value</span><span class="p">:</span><span class="w"> </span><span class="m">5</span><span class="w">
</span><span class="w">      </span>- <span class="nt">periodSeconds</span><span class="p">:</span><span class="w"> </span><span class="m">60</span><span class="w">  </span><span class="c"># 每分钟最多扩容 1 个 Pod，扩的慢一点主要是为了一个个地预热，避免一次扩容太多未预热的 Pods 导致服务可用率剧烈抖动</span><span class="w">
</span><span class="w">        </span><span class="nt">type</span><span class="p">:</span><span class="w"> </span><span class="l">Pods</span><span class="w">
</span><span class="w">        </span><span class="nt">value</span><span class="p">:</span><span class="w"> </span><span class="m">1</span><span class="w">
</span><span class="w">      </span><span class="nt">selectPolicy</span><span class="p">:</span><span class="w"> </span><span class="l">Min </span><span class="w"> </span><span class="c"># 选择最小的策略</span><span class="w">
</span><span class="w">    </span><span class="c"># 以下的一切配置，都是为了更平滑地缩容</span><span class="w">
</span><span class="w">    </span><span class="nt">scaleDown</span><span class="p">:</span><span class="w">
</span><span class="w">      </span><span class="nt">stabilizationWindowSeconds</span><span class="p">:</span><span class="w"> </span><span class="m">600</span><span class="w">  </span><span class="c"># 使用过去 10 mins 的最大 cpu 值进行缩容计算，避免过快缩容</span><span class="w">
</span><span class="w">      </span><span class="nt">policies</span><span class="p">:</span><span class="w">
</span><span class="w">      </span>- <span class="nt">type</span><span class="p">:</span><span class="w"> </span><span class="l">Percent </span><span class="w"> </span><span class="c"># 每 3 mins 最多缩容 `ceil[当前副本数 * 5%]` 个 pod（20 个 pod 以内，一次只缩容 1 个 pod）</span><span class="w">
</span><span class="w">        </span><span class="nt">value</span><span class="p">:</span><span class="w"> </span><span class="m">5</span><span class="w">
</span><span class="w">        </span><span class="nt">periodSeconds</span><span class="p">:</span><span class="w"> </span><span class="m">180</span><span class="w">
</span><span class="w">      </span>- <span class="nt">type</span><span class="p">:</span><span class="w"> </span><span class="l">Pods </span><span class="w"> </span><span class="c"># 每 1 mins 最多缩容 1 个 pod</span><span class="w">
</span><span class="w">        </span><span class="nt">value</span><span class="p">:</span><span class="w"> </span><span class="m">1</span><span class="w">
</span><span class="w">        </span><span class="nt">periodSeconds</span><span class="p">:</span><span class="w"> </span><span class="m">60</span><span class="w">
</span><span class="w">      </span><span class="nt">selectPolicy</span><span class="p">:</span><span class="w"> </span><span class="l">Min </span><span class="w"> </span><span class="c"># 上面的 policies 列表，只生效其中最小的值作为缩容限制（保证平滑缩容）</span><span class="w">
</span></code></pre></td></tr></table>
</div>
</div><p>而对于扩容不够平滑这个问题，可以考虑提供类似 AWS ALB TargetGroup <code>slow_start</code> 的功能，在扩容时缓慢将流量切到新 Pod 上，以实现预热服务（JVM 预热以及本地缓存预热），这样就能达到比较好的平滑扩容效果。</p>
<h3 id="5-hpa-注意事项">5. HPA 注意事项</h3>
<p>注意 kubectl 1.23 以下的版本，默认使用 <code>hpa.v1.autoscaling</code> 来查询 HPA 配置，<code>v2beta2</code> 相关的参数会被编码到 <code>metadata.annotations</code> 中。</p>
<p>比如 <code>behavior</code> 就会被编码到 <code>autoscaling.alpha.kubernetes.io/behavior</code> 这个 key 所对应的值中。</p>
<p>因此如果使用了 v2beta2 的 HPA，一定要明确指定使用 <code>v2beta2</code> 版本的 HPA：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-shell" data-lang="shell">kubectl get hpa.v2beta2.autoscaling
</code></pre></td></tr></table>
</div>
</div><p>否则不小心动到 <code>annotations</code> 中编码的某些参数，可能会产生意料之外的效果，甚至直接把控制面搞崩&hellip;
比如这个 issue: <a href="https://github.com/kubernetes/kubernetes/issues/107038" target="_blank" rel="noopener noreferrer">Nil pointer dereference in KCM after v1 HPA patch request</a></p>
<h3 id="6-参考">6. 参考</h3>
<ul>
<li><a href="https://kubernetes.io/zh/docs/tasks/run-application/horizontal-pod-autoscale/" target="_blank" rel="noopener noreferrer">Pod 水平自动伸缩 - Kubernetes Docs</a></li>
<li><a href="https://kubernetes.io/zh/docs/tasks/run-application/horizontal-pod-autoscale-walkthrough/" target="_blank" rel="noopener noreferrer">Horizontal Pod Autoscaler演练 - Kubernetes Docs</a></li>
</ul>
<h2 id="k8s-PodDistruptionBuget">三、<a href="https://kubernetes.io/zh/docs/tasks/run-application/configure-pdb/" target="_blank" rel="noopener noreferrer">节点维护与Pod干扰预算</a></h2>
<p>在我们通过 <code>kubectl drain</code> 将某个节点上的容器驱逐走的时候，
kubernetes 会依据 Pod 的「PodDistruptionBuget」来进行 Pod 的驱逐。</p>
<p>如果不设置任何明确的 PodDistruptionBuget，Pod 将会被直接杀死，然后在别的节点重新调度，<strong>这可能导致服务中断！</strong></p>
<p>PDB 是一个单独的 CR 自定义资源，示例如下：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-yaml" data-lang="yaml"><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l">policy/v1beta1</span><span class="w">
</span><span class="w"></span><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l">PodDisruptionBudget</span><span class="w">
</span><span class="w"></span><span class="nt">metadata</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">podinfo-pdb</span><span class="w">
</span><span class="w"></span><span class="nt">spec</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="c"># 如果不满足 PDB，Pod 驱逐将会失败！</span><span class="w">
</span><span class="w">  </span><span class="nt">minAvailable</span><span class="p">:</span><span class="w"> </span><span class="m">1</span><span class="w">      </span><span class="c"># 最少也要维持一个 Pod 可用</span><span class="w">
</span><span class="w"></span><span class="c">#   maxUnavailable: 1  # 最大不可用的 Pod 数，与 minAvailable 不能同时配置！二选一</span><span class="w">
</span><span class="w">  </span><span class="nt">selector</span><span class="p">:</span><span class="w">
</span><span class="w">    </span><span class="nt">matchLabels</span><span class="p">:</span><span class="w">
</span><span class="w">      </span><span class="nt">app</span><span class="p">:</span><span class="w"> </span><span class="l">podinfo</span><span class="w">
</span></code></pre></td></tr></table>
</div>
</div><p>如果在进行节点维护时(kubectl drain)，Pod 不满足 PDB，drain 将会失败，示例：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-shell" data-lang="shell">&gt; kubectl drain node-205 --ignore-daemonsets --delete-local-data
node/node-205 cordoned
WARNING: ignoring DaemonSet-managed Pods: kube-system/calico-node-nfhj7, kube-system/kube-proxy-94dz5
evicting pod default/podinfo-7c84d8c94d-h9brq
evicting pod default/podinfo-7c84d8c94d-gw6qf
error when evicting pod <span class="s2">&#34;podinfo-7c84d8c94d-h9brq&#34;</span> <span class="o">(</span>will retry after 5s<span class="o">)</span>: Cannot evict pod as it would violate the pod<span class="s1">&#39;s disruption budget.
</span><span class="s1">evicting pod default/podinfo-7c84d8c94d-h9brq
</span><span class="s1">error when evicting pod &#34;podinfo-7c84d8c94d-h9brq&#34; (will retry after 5s): Cannot evict pod as it would violate the pod&#39;</span>s disruption budget.
evicting pod default/podinfo-7c84d8c94d-h9brq
error when evicting pod <span class="s2">&#34;podinfo-7c84d8c94d-h9brq&#34;</span> <span class="o">(</span>will retry after 5s<span class="o">)</span>: Cannot evict pod as it would violate the pod<span class="err">&#39;</span>s disruption budget.
evicting pod default/podinfo-7c84d8c94d-h9brq
pod/podinfo-7c84d8c94d-gw6qf evicted
pod/podinfo-7c84d8c94d-h9brq evicted
node/node-205 evicted
</code></pre></td></tr></table>
</div>
</div><p>上面的示例中，podinfo 一共有两个副本，都运行在 node-205 上面。我给它设置了干扰预算 PDB <code>minAvailable: 1</code>。</p>
<p>然后使用 <code>kubectl drain</code> 驱逐 Pod 时，其中一个 Pod 被立即驱逐走了，而另一个 Pod 大概在 15 秒内一直驱逐失败。
因为第一个 Pod 还没有在新的节点上启动完成，它不满足干扰预算 PDB <code>minAvailable: 1</code> 这个条件。</p>
<p>大约 15 秒后，最先被驱逐走的 Pod 在新节点上启动完成了，另一个 Pod 满足了 PDB 所以终于也被驱逐了。这才完成了一个节点的 drain 操作。</p>
<blockquote>
<p>ClusterAutoscaler 等集群节点伸缩组件，在缩容节点时也会考虑 PodDisruptionBudget. 如果你的集群使用了 ClusterAutoscaler 等动态扩缩容节点的组件，强烈建议设置为所有服务设置 PodDisruptionBudget.</p>
</blockquote>
<h4 id="在-pdb-中使用百分比的注意事项">在 PDB 中使用百分比的注意事项</h4>
<p>在使用百分比时，计算出的实例数都会被向上取整，这会造成两个现象：</p>
<ul>
<li>如果使用 <code>minAvailable</code>，实例数较少的情况下，可能会导致 ALLOWED DISRUPTIONS 为 0</li>
<li>如果使用 <code>maxUnavailable</code>，因为是向上取整，ALLOWED DISRUPTIONS 的值一定不会低于 1</li>
</ul>
<p>因此从便于驱逐的角度看，如果你的服务至少有 2-3 个实例，建议在 PDB 中使用百分比配置 <code>maxUnavailable</code>，而不是 <code>minAvailable</code>.</p>
<h3 id="最佳实践-deployment--hpa--poddisruptionbudget">最佳实践 Deployment + HPA + PodDisruptionBudget</h3>
<p>一般而言，一个服务的每个版本，都应该包含如下三个资源：</p>
<ul>
<li>Deployment: 管理服务自身的 Pods 嘛</li>
<li>HPA: 负责 Pods 的扩缩容，通常使用 CPU 指标进行扩缩容</li>
<li>PodDisruptionBudget(PDB): 建议按照 HPA 的目标值，来设置 PDB.
<ul>
<li>比如 HPA CPU 目标值为 60%，就可以考虑设置 PDB <code>minAvailable=65%</code>，保证至少有 65% 的 Pod 可用。这样理论上极限情况下 QPS 均摊到剩下 65% 的 Pods 上也不会造成雪崩（这里假设 QPS 和 CPU 是完全的线性关系）</li>
</ul>
</li>
</ul>
<h2 id="k8s-affinity">四、节点亲和性与节点组</h2>
<p>我们一个集群，通常会使用不同的标签为节点组进行分类，比如 kubernetes 自动生成的一些节点标签：</p>
<ul>
<li><code>kubernetes.io/os</code>: 通常都用 <code>linux</code></li>
<li><code>kubernetes.io/arch</code>: <code>amd64</code>, <code>arm64</code></li>
<li><code>topology.kubernetes.io/region</code> 和 <code>topology.kubernetes.io/zone</code>: 云服务的区域及可用区</li>
</ul>
<p>我们使用得比较多的，是「节点亲和性」以及「Pod 反亲和性」，另外两个策略视情况使用。</p>
<h3 id="1-节点亲和性">1. 节点亲和性</h3>
<p>如果你使用的是 aws，那 aws 有一些自定义的节点标签：</p>
<ul>
<li><code>eks.amazonaws.com/nodegroup</code>: aws eks 节点组的名称，同一个节点组使用同样的 aws ec2 实例模板
<ul>
<li>比如 arm64 节点组、amd64/x64 节点组</li>
<li>内存比例高的节点组如 m 系实例，计算性能高的节点组如 c 系列</li>
<li>竞价实例节点组：这个省钱啊，但是动态性很高，随时可能被回收</li>
<li>按量付费节点组：这类实例贵，但是稳定。</li>
</ul>
</li>
</ul>
<p>假设你希望优先选择竞价实例跑你的 Pod，如果竞价实例暂时跑满了，就选择按量付费实例。
那 <code>nodeSelector</code> 就满足不了你的需求了，你需要使用 <code>nodeAffinity</code>，示例如下:</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span><span class="lnt">43
</span><span class="lnt">44
</span><span class="lnt">45
</span><span class="lnt">46
</span><span class="lnt">47
</span><span class="lnt">48
</span><span class="lnt">49
</span><span class="lnt">50
</span><span class="lnt">51
</span><span class="lnt">52
</span><span class="lnt">53
</span><span class="lnt">54
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-yaml" data-lang="yaml"><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l">apps/v1</span><span class="w">
</span><span class="w"></span><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l">Deployment</span><span class="w">
</span><span class="w"></span><span class="nt">metadata</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">xxx</span><span class="w">
</span><span class="w">  </span><span class="nt">namespace</span><span class="p">:</span><span class="w"> </span><span class="l">xxx</span><span class="w">
</span><span class="w"></span><span class="nt">spec</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="c"># ...</span><span class="w">
</span><span class="w">  </span><span class="nt">template</span><span class="p">:</span><span class="w">
</span><span class="w">    </span><span class="c"># ...</span><span class="w">
</span><span class="w">    </span><span class="nt">spec</span><span class="p">:</span><span class="w">
</span><span class="w">      </span><span class="nt">affinity</span><span class="p">:</span><span class="w">
</span><span class="w">        </span><span class="nt">nodeAffinity</span><span class="p">:</span><span class="w">
</span><span class="w">          </span><span class="c"># 优先选择 spot-group-c 的节点</span><span class="w">
</span><span class="w">          </span><span class="nt">preferredDuringSchedulingIgnoredDuringExecution</span><span class="p">:</span><span class="w">
</span><span class="w">          </span>- <span class="nt">preference</span><span class="p">:</span><span class="w">
</span><span class="w">              </span><span class="nt">matchExpressions</span><span class="p">:</span><span class="w">
</span><span class="w">              </span>- <span class="nt">key</span><span class="p">:</span><span class="w"> </span><span class="l">eks.amazonaws.com/nodegroup</span><span class="w">
</span><span class="w">                </span><span class="nt">operator</span><span class="p">:</span><span class="w"> </span><span class="l">In</span><span class="w">
</span><span class="w">                </span><span class="nt">values</span><span class="p">:</span><span class="w">
</span><span class="w">                </span>- <span class="l">spot-group-c</span><span class="w">
</span><span class="w">            </span><span class="nt">weight</span><span class="p">:</span><span class="w"> </span><span class="m">80</span><span class="w">  </span><span class="c"># weight 用于为节点评分，会优先选择评分最高的节点</span><span class="w">
</span><span class="w">          </span>- <span class="nt">preference</span><span class="p">:</span><span class="w">
</span><span class="w">              </span><span class="nt">matchExpressions</span><span class="p">:</span><span class="w">
</span><span class="w">              </span><span class="c"># 优先选择 aws c6i 的机器</span><span class="w">
</span><span class="w">              </span>- <span class="nt">key</span><span class="p">:</span><span class="w"> </span><span class="l">node.kubernetes.io/instance-type</span><span class="w">
</span><span class="w">                </span><span class="nt">operator</span><span class="p">:</span><span class="w"> </span><span class="l">In</span><span class="w">
</span><span class="w">                </span><span class="nt">values</span><span class="p">:</span><span class="w">
</span><span class="w">                </span>- <span class="s2">&#34;c6i.xlarge&#34;</span><span class="w">
</span><span class="w">                </span>- <span class="s2">&#34;c6i.2xlarge&#34;</span><span class="w">
</span><span class="w">                </span>- <span class="s2">&#34;c6i.4xlarge&#34;</span><span class="w">
</span><span class="w">                </span>- <span class="s2">&#34;c6i.8xlarge&#34;</span><span class="w">
</span><span class="w">            </span><span class="nt">weight</span><span class="p">:</span><span class="w"> </span><span class="m">70</span><span class="w">
</span><span class="w">          </span>- <span class="nt">preference</span><span class="p">:</span><span class="w">
</span><span class="w">              </span><span class="nt">matchExpressions</span><span class="p">:</span><span class="w">
</span><span class="w">              </span><span class="c"># 其次选择 aws c5 的机器</span><span class="w">
</span><span class="w">              </span>- <span class="nt">key</span><span class="p">:</span><span class="w"> </span><span class="l">node.kubernetes.io/instance-type</span><span class="w">
</span><span class="w">                </span><span class="nt">operator</span><span class="p">:</span><span class="w"> </span><span class="l">In</span><span class="w">
</span><span class="w">                </span><span class="nt">values</span><span class="p">:</span><span class="w">
</span><span class="w">                </span>- <span class="s2">&#34;c5.xlarge&#34;</span><span class="w">
</span><span class="w">                </span>- <span class="s2">&#34;c5.2xlarge&#34;</span><span class="w">
</span><span class="w">                </span>- <span class="s2">&#34;c5.4xlarge&#34;</span><span class="w">
</span><span class="w">                </span>- <span class="s2">&#34;c5.9xlarge&#34;</span><span class="w">
</span><span class="w">            </span><span class="nt">weight</span><span class="p">:</span><span class="w"> </span><span class="m">60</span><span class="w">
</span><span class="w">         </span><span class="c"># 如果没 spot-group-c 可用，也可选择 ondemand-group-c 的节点跑</span><span class="w">
</span><span class="w">          </span><span class="nt">requiredDuringSchedulingIgnoredDuringExecution</span><span class="p">:</span><span class="w">
</span><span class="w">            </span><span class="nt">nodeSelectorTerms</span><span class="p">:</span><span class="w">
</span><span class="w">            </span>- <span class="nt">matchExpressions</span><span class="p">:</span><span class="w">
</span><span class="w">              </span>- <span class="nt">key</span><span class="p">:</span><span class="w"> </span><span class="l">eks.amazonaws.com/nodegroup</span><span class="w">
</span><span class="w">                </span><span class="nt">operator</span><span class="p">:</span><span class="w"> </span><span class="l">In</span><span class="w">
</span><span class="w">                </span><span class="nt">values</span><span class="p">:</span><span class="w">
</span><span class="w">                </span>- <span class="l">spot-group-c</span><span class="w">
</span><span class="w">                </span>- <span class="l">ondemand-group-c</span><span class="w">
</span><span class="w">      </span><span class="nt">containers</span><span class="p">:</span><span class="w">
</span><span class="w">        </span><span class="c"># ...</span><span class="w">
</span></code></pre></td></tr></table>
</div>
</div><h3 id="2-pod-反亲和性">2. Pod 反亲和性</h3>
<p>通常建议为每个 Deployment 的 template 配置 Pod 反亲和性，把 Pods 打散在所有节点上：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-yaml" data-lang="yaml"><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l">apps/v1</span><span class="w">
</span><span class="w"></span><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l">Deployment</span><span class="w">
</span><span class="w"></span><span class="nt">metadata</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">xxx</span><span class="w">
</span><span class="w">  </span><span class="nt">namespace</span><span class="p">:</span><span class="w"> </span><span class="l">xxx</span><span class="w">
</span><span class="w"></span><span class="nt">spec</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="c"># ...</span><span class="w">
</span><span class="w">  </span><span class="nt">template</span><span class="p">:</span><span class="w">
</span><span class="w">    </span><span class="c"># ...</span><span class="w">
</span><span class="w">    </span><span class="nt">spec</span><span class="p">:</span><span class="w">
</span><span class="w">      </span><span class="nt">replicas</span><span class="p">:</span><span class="w"> </span><span class="m">3</span><span class="w">
</span><span class="w">      </span><span class="nt">affinity</span><span class="p">:</span><span class="w">
</span><span class="w">        </span><span class="nt">podAntiAffinity</span><span class="p">:</span><span class="w">
</span><span class="w">          </span><span class="nt">preferredDuringSchedulingIgnoredDuringExecution</span><span class="p">:</span><span class="w"> </span><span class="c"># 非强制性条件</span><span class="w">
</span><span class="w">          </span>- <span class="nt">weight</span><span class="p">:</span><span class="w"> </span><span class="m">100</span><span class="w">  </span><span class="c"># weight 用于为节点评分，会优先选择评分最高的节点</span><span class="w">
</span><span class="w">            </span><span class="nt">podAffinityTerm</span><span class="p">:</span><span class="w">
</span><span class="w">              </span><span class="nt">labelSelector</span><span class="p">:</span><span class="w">
</span><span class="w">                </span><span class="nt">matchExpressions</span><span class="p">:</span><span class="w">
</span><span class="w">                </span>- <span class="nt">key</span><span class="p">:</span><span class="w"> </span><span class="l">app</span><span class="w">
</span><span class="w">                  </span><span class="nt">operator</span><span class="p">:</span><span class="w"> </span><span class="l">In</span><span class="w">
</span><span class="w">                  </span><span class="nt">values</span><span class="p">:</span><span class="w">
</span><span class="w">                  </span>- <span class="l">xxx</span><span class="w">
</span><span class="w">                </span>- <span class="nt">key</span><span class="p">:</span><span class="w"> </span><span class="l">version</span><span class="w">
</span><span class="w">                  </span><span class="nt">operator</span><span class="p">:</span><span class="w"> </span><span class="l">In</span><span class="w">
</span><span class="w">                  </span><span class="nt">values</span><span class="p">:</span><span class="w">
</span><span class="w">                  </span>- <span class="l">v12</span><span class="w">
</span><span class="w">              </span><span class="c"># 将 pod 尽量打散在多个可用区</span><span class="w">
</span><span class="w">              </span><span class="nt">topologyKey</span><span class="p">:</span><span class="w"> </span><span class="l">topology.kubernetes.io/zone</span><span class="w">
</span><span class="w">          </span><span class="nt">requiredDuringSchedulingIgnoredDuringExecution</span><span class="p">:</span><span class="w">  </span><span class="c"># 强制性要求</span><span class="w">
</span><span class="w">          </span><span class="c"># 注意这个没有 weights，必须满足列表中的所有条件</span><span class="w">
</span><span class="w">          </span>- <span class="nt">labelSelector</span><span class="p">:</span><span class="w">
</span><span class="w">              </span><span class="nt">matchExpressions</span><span class="p">:</span><span class="w">
</span><span class="w">              </span>- <span class="nt">key</span><span class="p">:</span><span class="w"> </span><span class="l">app</span><span class="w">
</span><span class="w">                </span><span class="nt">operator</span><span class="p">:</span><span class="w"> </span><span class="l">In</span><span class="w">
</span><span class="w">                </span><span class="nt">values</span><span class="p">:</span><span class="w">
</span><span class="w">                </span>- <span class="l">xxx</span><span class="w">
</span><span class="w">              </span>- <span class="nt">key</span><span class="p">:</span><span class="w"> </span><span class="l">version</span><span class="w">
</span><span class="w">                </span><span class="nt">operator</span><span class="p">:</span><span class="w"> </span><span class="l">In</span><span class="w">
</span><span class="w">                </span><span class="nt">values</span><span class="p">:</span><span class="w">
</span><span class="w">                </span>- <span class="l">v12</span><span class="w">
</span><span class="w">            </span><span class="c"># Pod 必须运行在不同的节点上</span><span class="w">
</span><span class="w">            </span><span class="nt">topologyKey</span><span class="p">:</span><span class="w"> </span><span class="l">kubernetes.io/hostname</span><span class="w">
</span></code></pre></td></tr></table>
</div>
</div><h2 id="k8s-container-probe">五、Pod 的就绪探针、存活探针与启动探针</h2>
<p>Pod 提供如下三种探针，均支持使用 Command、HTTP API、TCP Socket 这三种手段来进行服务可用性探测。</p>
<ul>
<li><code>startupProbe</code> 启动探针（Kubernetes v1.18 [beta]）: 此探针通过后，「就绪探针」与「存活探针」才会进行存活性与就绪检查
<ul>
<li>用于对慢启动容器进行存活性检测，避免它们在启动运行之前就被杀掉
<ul>
<li>startupProbe 显然比 livenessProbe 的 initialDelaySeconds 参数更灵活。</li>
<li>同时它也能延迟 readinessProbe 的生效时间，这主要是为了避免无意义的探测。容器都还没 startUp，显然是不可能就绪的。</li>
</ul>
</li>
<li>程序将最多有 <code>failureThreshold * periodSeconds</code> 的时间用于启动，比如设置 <code>failureThreshold=20</code>、<code>periodSeconds=5</code>，程序启动时间最长就为 100s，如果超过 100s 仍然未通过「启动探测」，容器会被杀死。</li>
</ul>
</li>
<li><code>readinessProbe</code> 就绪探针:
<ul>
<li>就绪探针失败次数超过 <code>failureThreshold</code> 限制（默认三次），服务将被暂时从 Service 的 Endpoints 中踢出，直到服务再次满足 <code>successThreshold</code>.</li>
</ul>
</li>
<li><code>livenessProbe</code> 存活探针: 检测服务是否存活，它可以捕捉到死锁等情况，及时杀死这种容器。
<ul>
<li>存活探针失败可能的原因：
<ul>
<li>服务发生死锁，对所有请求均无响应</li>
<li>服务线程全部卡在对外部 redis/mysql 等外部依赖的等待中，导致请求无响应</li>
</ul>
</li>
<li>存活探针失败次数超过 <code>failureThreshold</code> 限制（默认三次），容器将被杀死，随后根据重启策略执行重启。
<ul>
<li><code>kubectl describe pod</code> 会显示重启原因为 <code>State.Last State.Reason = Error, Exit Code=137</code>，同时 Events 中会有 <code>Liveness probe failed: ...</code> 这样的描述。</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>上述三类探测器的参数都是通用的，五个时间相关的参数列举如下：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-yaml" data-lang="yaml"><span class="c"># 下面的值就是 k8s 的默认值</span><span class="w">
</span><span class="w"></span><span class="nt">initialDelaySeconds</span><span class="p">:</span><span class="w"> </span><span class="m">0</span><span class="w">  </span><span class="c"># 默认没有 delay 时间</span><span class="w">
</span><span class="w"></span><span class="nt">periodSeconds</span><span class="p">:</span><span class="w"> </span><span class="m">10</span><span class="w">
</span><span class="w"></span><span class="nt">timeoutSeconds</span><span class="p">:</span><span class="w"> </span><span class="m">1</span><span class="w">
</span><span class="w"></span><span class="nt">failureThreshold</span><span class="p">:</span><span class="w"> </span><span class="m">3</span><span class="w">
</span><span class="w"></span><span class="nt">successThreshold</span><span class="p">:</span><span class="w"> </span><span class="m">1</span><span class="w">
</span></code></pre></td></tr></table>
</div>
</div><p>示例：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-yaml" data-lang="yaml"><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l">apps/v1</span><span class="w">
</span><span class="w"></span><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l">Deployment</span><span class="w">
</span><span class="w"></span><span class="nt">metadata</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">my-app-v3</span><span class="w">
</span><span class="w"></span><span class="nt">spec</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="c"># ...</span><span class="w">
</span><span class="w">  </span><span class="nt">template</span><span class="p">:</span><span class="w">
</span><span class="w">    </span><span class="c">#  ...</span><span class="w">
</span><span class="w">    </span><span class="nt">spec</span><span class="p">:</span><span class="w">
</span><span class="w">      </span><span class="nt">containers</span><span class="p">:</span><span class="w">
</span><span class="w">      </span>- <span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">my-app-v3</span><span class="w">
</span><span class="w">        </span><span class="nt">image</span><span class="p">:</span><span class="w"> </span><span class="l">xxx.com/app/my-app:v3</span><span class="w">
</span><span class="w">        </span><span class="nt">imagePullPolicy</span><span class="p">:</span><span class="w"> </span><span class="l">IfNotPresent </span><span class="w">
</span><span class="w">        </span><span class="c"># ... 省略若干配置</span><span class="w">
</span><span class="w">        </span><span class="nt">startupProbe</span><span class="p">:</span><span class="w">
</span><span class="w">          </span><span class="nt">httpGet</span><span class="p">:</span><span class="w">
</span><span class="w">            </span><span class="nt">path</span><span class="p">:</span><span class="w"> </span><span class="l">/actuator/health </span><span class="w"> </span><span class="c"># 直接使用健康检查接口即可</span><span class="w">
</span><span class="w">            </span><span class="nt">port</span><span class="p">:</span><span class="w"> </span><span class="m">8080</span><span class="w">
</span><span class="w">          </span><span class="nt">periodSeconds</span><span class="p">:</span><span class="w"> </span><span class="m">5</span><span class="w">
</span><span class="w">          </span><span class="nt">timeoutSeconds</span><span class="p">:</span><span class="w"> </span><span class="m">1</span><span class="w">
</span><span class="w">          </span><span class="nt">failureThreshold</span><span class="p">:</span><span class="w"> </span><span class="m">20</span><span class="w">  </span><span class="c"># 最多提供给服务 5s * 20 的启动时间</span><span class="w">
</span><span class="w">          </span><span class="nt">successThreshold</span><span class="p">:</span><span class="w"> </span><span class="m">1</span><span class="w">
</span><span class="w">        </span><span class="nt">livenessProbe</span><span class="p">:</span><span class="w">
</span><span class="w">          </span><span class="nt">httpGet</span><span class="p">:</span><span class="w">
</span><span class="w">            </span><span class="nt">path</span><span class="p">:</span><span class="w"> </span><span class="l">/actuator/health </span><span class="w"> </span><span class="c"># spring 的通用健康检查路径</span><span class="w">
</span><span class="w">            </span><span class="nt">port</span><span class="p">:</span><span class="w"> </span><span class="m">8080</span><span class="w">
</span><span class="w">          </span><span class="nt">periodSeconds</span><span class="p">:</span><span class="w"> </span><span class="m">5</span><span class="w">
</span><span class="w">          </span><span class="nt">timeoutSeconds</span><span class="p">:</span><span class="w"> </span><span class="m">1</span><span class="w">
</span><span class="w">          </span><span class="nt">failureThreshold</span><span class="p">:</span><span class="w"> </span><span class="m">5</span><span class="w">
</span><span class="w">          </span><span class="nt">successThreshold</span><span class="p">:</span><span class="w"> </span><span class="m">1</span><span class="w">
</span><span class="w">        </span><span class="c"># Readiness probes are very important for a RollingUpdate to work properly,</span><span class="w">
</span><span class="w">        </span><span class="nt">readinessProbe</span><span class="p">:</span><span class="w">
</span><span class="w">          </span><span class="nt">httpGet</span><span class="p">:</span><span class="w">
</span><span class="w">            </span><span class="nt">path</span><span class="p">:</span><span class="w"> </span><span class="l">/actuator/health </span><span class="w"> </span><span class="c"># 简单起见可直接使用 livenessProbe 相同的接口，当然也可额外定义</span><span class="w">
</span><span class="w">            </span><span class="nt">port</span><span class="p">:</span><span class="w"> </span><span class="m">8080</span><span class="w">
</span><span class="w">          </span><span class="nt">periodSeconds</span><span class="p">:</span><span class="w"> </span><span class="m">5</span><span class="w">
</span><span class="w">          </span><span class="nt">timeoutSeconds</span><span class="p">:</span><span class="w"> </span><span class="m">1</span><span class="w">
</span><span class="w">          </span><span class="nt">failureThreshold</span><span class="p">:</span><span class="w"> </span><span class="m">5</span><span class="w">
</span><span class="w">          </span><span class="nt">successThreshold</span><span class="p">:</span><span class="w"> </span><span class="m">1</span><span class="w">
</span></code></pre></td></tr></table>
</div>
</div><p>在 Kubernetes 1.18 之前，通用的手段是为「就绪探针」添加较长的 <code>initialDelaySeconds</code> 来实现类似「启动探针」的功能动，避免容器因为启动太慢，存活探针失败导致容器被重启。示例如下：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-yaml" data-lang="yaml"><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l">apps/v1</span><span class="w">
</span><span class="w"></span><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l">Deployment</span><span class="w">
</span><span class="w"></span><span class="nt">metadata</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">my-app-v3</span><span class="w">
</span><span class="w"></span><span class="nt">spec</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="c"># ...</span><span class="w">
</span><span class="w">  </span><span class="nt">template</span><span class="p">:</span><span class="w">
</span><span class="w">    </span><span class="c">#  ...</span><span class="w">
</span><span class="w">    </span><span class="nt">spec</span><span class="p">:</span><span class="w">
</span><span class="w">      </span><span class="nt">containers</span><span class="p">:</span><span class="w">
</span><span class="w">      </span>- <span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">my-app-v3</span><span class="w">
</span><span class="w">        </span><span class="nt">image</span><span class="p">:</span><span class="w"> </span><span class="l">xxx.com/app/my-app:v3</span><span class="w">
</span><span class="w">        </span><span class="nt">imagePullPolicy</span><span class="p">:</span><span class="w"> </span><span class="l">IfNotPresent </span><span class="w">
</span><span class="w">        </span><span class="c"># ... 省略若干配置</span><span class="w">
</span><span class="w">        </span><span class="nt">livenessProbe</span><span class="p">:</span><span class="w">
</span><span class="w">          </span><span class="nt">httpGet</span><span class="p">:</span><span class="w">
</span><span class="w">            </span><span class="nt">path</span><span class="p">:</span><span class="w"> </span><span class="l">/actuator/health </span><span class="w"> </span><span class="c"># spring 的通用健康检查路径</span><span class="w">
</span><span class="w">            </span><span class="nt">port</span><span class="p">:</span><span class="w"> </span><span class="m">8080</span><span class="w">
</span><span class="w">          </span><span class="nt">initialDelaySeconds</span><span class="p">:</span><span class="w"> </span><span class="m">120</span><span class="w">  </span><span class="c"># 前两分钟，都假设服务健康，避免 livenessProbe 失败导致服务重启</span><span class="w">
</span><span class="w">          </span><span class="nt">periodSeconds</span><span class="p">:</span><span class="w"> </span><span class="m">5</span><span class="w">
</span><span class="w">          </span><span class="nt">timeoutSeconds</span><span class="p">:</span><span class="w"> </span><span class="m">1</span><span class="w">
</span><span class="w">          </span><span class="nt">failureThreshold</span><span class="p">:</span><span class="w"> </span><span class="m">5</span><span class="w">
</span><span class="w">          </span><span class="nt">successThreshold</span><span class="p">:</span><span class="w"> </span><span class="m">1</span><span class="w">
</span><span class="w">        </span><span class="c"># 容器一启动，Readiness probes 就会不断进行检测</span><span class="w">
</span><span class="w">        </span><span class="nt">readinessProbe</span><span class="p">:</span><span class="w">
</span><span class="w">          </span><span class="nt">httpGet</span><span class="p">:</span><span class="w">
</span><span class="w">            </span><span class="nt">path</span><span class="p">:</span><span class="w"> </span><span class="l">/actuator/health</span><span class="w">
</span><span class="w">            </span><span class="nt">port</span><span class="p">:</span><span class="w"> </span><span class="m">8080</span><span class="w">
</span><span class="w">          </span><span class="nt">initialDelaySeconds</span><span class="p">:</span><span class="w"> </span><span class="m">3</span><span class="w">  </span><span class="c"># readiness probe 不需要设太长时间，使 Pod 尽快加入到 Endpoints.</span><span class="w">
</span><span class="w">          </span><span class="nt">periodSeconds</span><span class="p">:</span><span class="w"> </span><span class="m">5</span><span class="w">
</span><span class="w">          </span><span class="nt">timeoutSeconds</span><span class="p">:</span><span class="w"> </span><span class="m">1</span><span class="w">
</span><span class="w">          </span><span class="nt">failureThreshold</span><span class="p">:</span><span class="w"> </span><span class="m">5</span><span class="w">
</span><span class="w">          </span><span class="nt">successThreshold</span><span class="p">:</span><span class="w"> </span><span class="m">1</span><span class="w">
</span></code></pre></td></tr></table>
</div>
</div><h2 id="k8s-pod-security">六、Pod 安全</h2>
<p>这里只介绍 Pod 中安全相关的参数，其他诸如集群全局的安全策略，不在这里讨论。</p>
<h3 id="1-pod-securitycontexthttpskubernetesiodocstasksconfigure-pod-containersecurity-context">1. <a href="https://kubernetes.io/docs/tasks/configure-pod-container/security-context/" target="_blank" rel="noopener noreferrer">Pod SecurityContext</a></h3>
<p>通过设置 Pod 的 SecurityContext，可以为每个 Pod 设置特定的安全策略。</p>
<p>SecurityContext 有两种类型：</p>
<ol>
<li><code>spec.securityContext</code>: 这是一个 <a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.20/#podsecuritycontext-v1-core" target="_blank" rel="noopener noreferrer">PodSecurityContext</a> 对象
<ul>
<li>顾名思义，它对 Pod 中的所有 contaienrs 都有效。</li>
</ul>
</li>
<li><code>spec.containers[*].securityContext</code>: 这是一个 <a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.20/#securitycontext-v1-core" target="_blank" rel="noopener noreferrer">SecurityContext</a> 对象
<ul>
<li>container 私有的 SecurityContext</li>
</ul>
</li>
</ol>
<p>这两个 SecurityContext 的参数只有部分重叠，重叠的部分 <code>spec.containers[*].securityContext</code> 优先级更高。</p>
<p>我们比较常遇到的一些<strong>提升权限</strong>的安全策略：</p>
<ol>
<li>特权容器：<code>spec.containers[*].securityContext.privileged</code></li>
<li>添加（Capabilities）可选的系统级能力: <code>spec.containers[*].securityContext.capabilities.add</code>
<ol>
<li>只有 ntp 同步服务等少数容器，可以开启这项功能。请注意这非常危险。</li>
</ol>
</li>
<li>Sysctls: 系统参数: <code>spec.securityContext.sysctls</code></li>
</ol>
<p><strong>权限限制</strong>相关的安全策略有（<strong>强烈建议在所有 Pod 上按需配置如下安全策略！</strong>）：</p>
<ol>
<li><code>spec.volumes</code>: 所有的数据卷都可以设定读写权限</li>
<li><code>spec.securityContext.runAsNonRoot: true</code> Pod 必须以非 root 用户运行</li>
<li><code>spec.containers[*].securityContext.readOnlyRootFileSystem:true</code> <strong>将容器层设为只读，防止容器文件被篡改。</strong>
<ol>
<li>如果微服务需要读写文件，建议额外挂载 <code>emptydir</code> 类型的数据卷。</li>
</ol>
</li>
<li><code>spec.containers[*].securityContext.allowPrivilegeEscalation: false</code> 不允许 Pod 做任何权限提升！</li>
<li><code>spec.containers[*].securityContext.capabilities.drop</code>: 移除（Capabilities）可选的系统级能力</li>
</ol>
<p>还有其他诸如指定容器的运行用户(user)/用户组(group)等功能未列出，请自行查阅 Kubernetes 相关文档。</p>
<p>一个无状态的微服务 Pod 配置举例：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-yaml" data-lang="yaml"><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l">v1</span><span class="w">
</span><span class="w"></span><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l">Pod</span><span class="w">
</span><span class="w"></span><span class="nt">metadata</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">&lt;Pod name&gt;</span><span class="w">
</span><span class="w"></span><span class="nt">spec</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">containers</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">- name</span><span class="p">:</span><span class="w"> </span><span class="l">&lt;container name&gt;</span><span class="w">
</span><span class="w">    </span><span class="nt">image</span><span class="p">:</span><span class="w"> </span><span class="l">&lt;image&gt;</span><span class="w">
</span><span class="w">    </span><span class="nt">imagePullPolicy</span><span class="p">:</span><span class="w"> </span><span class="l">IfNotPresent </span><span class="w">
</span><span class="w">    </span><span class="c"># ......此处省略 500 字</span><span class="w">
</span><span class="w">    </span><span class="nt">securityContext</span><span class="p">:</span><span class="w">
</span><span class="w">      </span><span class="nt">readOnlyRootFilesystem</span><span class="p">:</span><span class="w"> </span><span class="kc">true</span><span class="w">  </span><span class="c"># 将容器层设为只读，防止容器文件被篡改。</span><span class="w">
</span><span class="w">      </span><span class="nt">allowPrivilegeEscalation</span><span class="p">:</span><span class="w"> </span><span class="kc">false</span><span class="w">  </span><span class="c"># 禁止 Pod 做任何权限提升</span><span class="w">
</span><span class="w">      </span><span class="nt">capabilities</span><span class="p">:</span><span class="w">
</span><span class="w">        </span><span class="nt">drop</span><span class="p">:</span><span class="w">
</span><span class="w">        </span><span class="c"># 禁止容器使用 raw 套接字，通常只有 hacker 才会用到 raw 套接字。</span><span class="w">
</span><span class="w">        </span><span class="c"># raw_socket 可自定义网络层数据，避开 tcp/udp 协议栈，直接操作底层的 ip/icmp 数据包。可实现 ip 伪装、自定义协议等功能。</span><span class="w">
</span><span class="w">        </span><span class="c"># 去掉 net_raw 会导致 tcpdump 无法使用，无法进行容器内抓包。需要抓包时可临时去除这项配置</span><span class="w">
</span><span class="w">        </span>- <span class="l">NET_RAW</span><span class="w">
</span><span class="w">        </span><span class="c"># 更好的选择：直接禁用所有 capabilities</span><span class="w">
</span><span class="w">        </span><span class="c"># - ALL</span><span class="w">
</span><span class="w">  </span><span class="nt">securityContext</span><span class="p">:</span><span class="w">
</span><span class="w">    </span><span class="c"># runAsUser: 1000  # 设定用户</span><span class="w">
</span><span class="w">    </span><span class="c"># runAsGroup: 1000  # 设定用户组</span><span class="w">
</span><span class="w">    </span><span class="nt">runAsNonRoot</span><span class="p">:</span><span class="w"> </span><span class="kc">true</span><span class="w">  </span><span class="c"># Pod 必须以非 root 用户运行</span><span class="w">
</span><span class="w">    </span><span class="nt">seccompProfile</span><span class="p">:</span><span class="w">  </span><span class="c"># security compute mode</span><span class="w">
</span><span class="w">      </span><span class="nt">type</span><span class="p">:</span><span class="w"> </span><span class="l">RuntimeDefault</span><span class="w">
</span></code></pre></td></tr></table>
</div>
</div><h3 id="2-seccomp-security-compute-mode">2. seccomp: security compute mode</h3>
<p>seccomp 和 seccomp-bpf 允许对系统调用进行过滤，可以防止用户的二进制文对主机操作系统件执行通常情况下并不需要的危险操作。它和 Falco 有些类似，不过 Seccomp 没有为容器提供特别的支持。</p>
<p>视频:</p>
<ul>
<li><a href="https://www.youtube.com/watch?v=Ro4QRx7VPsY&amp;list=PLj6h78yzYM2Pn8RxfLh2qrXBDftr6Qjut$index=22" target="_blank" rel="noopener noreferrer">Seccomp: What Can It Do For You? - Justin Cormack, Docker</a></li>
</ul>
<h2 id="其他问题">其他问题</h2>
<ul>
<li>不同节点类型的性能有差距，导致 QPS 均衡的情况下，CPU 负载不均衡
<ul>
<li>解决办法（未验证）：
<ul>
<li>尽量使用性能相同的实例类型：通过 <code>podAffinity</code> 及 <code>nodeAffinity</code> 添加节点类型的亲和性</li>
</ul>
</li>
</ul>
</li>
</ul>
]]></description></item><item><title>使用 Istio 进行 JWT 身份验证（充当 API 网关）</title><link>https://ryan4yin.space/posts/use-istio-for-jwt-auth/</link><pubDate>Mon, 06 Apr 2020 21:48:26 +0800</pubDate><author>xiaoyin_c@qq.com</author><dc:creator>ryan4yin</dc:creator><guid>https://ryan4yin.space/posts/use-istio-for-jwt-auth/</guid><description><![CDATA[<blockquote>
<p>本文基于 Istio1.5 编写测试</p>
</blockquote>
<p>Istio 支持使用 JWT 对终端用户进行身份验证（Istio End User Authentication），支持多种 JWT 签名算法。</p>
<p>目前主流的 JWT 算法是 RS256/ES256。（请忽略 HS256，该算法不适合分布式 JWT 验证）</p>
<p>这里以 RSA256 算法为例进行介绍，ES256 的配置方式也是一样的。</p>
<h3 id="1-介绍-jwk-与-jwks">1. 介绍 JWK 与 JWKS</h3>
<p>Istio 要求提供 JWKS 格式的信息，用于 JWT 签名验证。因此这里得先介绍一下 JWK 和 JWKS.</p>
<p>JWKS ，也就是 JWK Set，json 结构如下：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback">{
&#34;keys&#34;: [
  &lt;jwk-1&gt;,
  &lt;jwk-2&gt;,
  ...
]}
</code></pre></td></tr></table>
</div>
</div><p>JWKS 描述一组 JWK 密钥。它能同时描述多个可用的公钥，应用场景之一是密钥的 Rotate.</p>
<p>而 JWK，全称是 Json Web Key，它描述了一个加密密钥（公钥或私钥）的各项属性，包括密钥的值。</p>
<p>Istio 使用 JWK 描述验证 JWT 签名所需要的信息。在使用 RSA 签名算法时，JWK 描述的应该是用于验证的 RSA 公钥。</p>
<p>一个 RSA 公钥的 JWK 描述如下：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback">{
    &#34;alg&#34;: &#34;RS256&#34;,  # 算法「可选参数」
    &#34;kty&#34;: &#34;RSA&#34;,    # 密钥类型
    &#34;use&#34;: &#34;sig&#34;,    # 被用于签名「可选参数」
    &#34;kid&#34;: &#34;NjVBRjY5MDlCMUIwNzU4RTA2QzZFMDQ4QzQ2MDAyQjVDNjk1RTM2Qg&#34;,  # key 的唯一 id
    &#34;n&#34;: &#34;yeNlzlub94YgerT030codqEztjfU_S6X4DbDA_iVKkjAWtYfPHDzz_sPCT1Axz6isZdf3lHpq_gYX4Sz-cbe4rjmigxUxr-FgKHQy3HeCdK6hNq9ASQvMK9LBOpXDNn7mei6RZWom4wo3CMvvsY1w8tjtfLb-yQwJPltHxShZq5-ihC9irpLI9xEBTgG12q5lGIFPhTl_7inA1PFK97LuSLnTJzW0bj096v_TMDg7pOWm_zHtF53qbVsI0e3v5nmdKXdFf9BjIARRfVrbxVxiZHjU6zL6jY5QJdh1QCmENoejj_ytspMmGW7yMRxzUqgxcAqOBpVm0b-_mW3HoBdjQ&#34;,
    &#34;e&#34;: &#34;AQAB&#34;
}
</code></pre></td></tr></table>
</div>
</div><p>RSA 是基于大数分解的加密/签名算法，上述参数中，<code>e</code> 是公钥的模数(modulus)，<code>n</code> 是公钥的指数(exponent)，两个参数都是 base64 字符串。</p>
<p>JWK 中 RSA 公钥的具体定义参见 <a href="https://tools.ietf.org/html/rfc7518#page-30" target="_blank" rel="noopener noreferrer">RSA Keys - JSON Web Algorithms (JWA)</a></p>
<h3 id="2-jwk-的生成">2. JWK 的生成</h3>
<p>要生成 JWK 公钥，需要先生成私钥，生成方法参见 <a href="/posts/jwt-algorithm-key-generation/#使用-openssl-生成-rsaecc-公私钥" rel="">JWT 签名算法 HS256、RS256 及 ES256 及密钥生成</a>。</p>
<blockquote>
<p>公钥不需要用上述方法生成，因为我们需要的是 JWK 格式的公钥。后面会通过 python 生成出 JWK 公钥。</p>
</blockquote>
<p>上面的命令会将生成出的 RSA 私钥写入 key.pem 中，查看一下私钥内容。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback">ryan@RYAN-MI-DESKTOP:~/istio$ cat key.pem
-----BEGIN RSA PRIVATE KEY-----
MIIEpAIBAAKCAQEAt1cKkQqPh8iOv5BhKh7Rx6A2+1ldpO/jczML/0GBKu4X+lHr
Y8YbJrt29jyAXlWM8vHC7tXsqgUG+WziRD0D8nhnh10XC14SeH+3mVuBqph+TqhX
TWsh9gtAIbeUHJjEI4I79QK4/wquPHHIGZBQDQQnuMh6vAS3VaUYJdEIoKvUBnAy
Y35kJZgyJSbrxLsEExL2zujUD/OY+/In2bq/3rFtDGNlgHyC7Gu2zXSXvfOA4O5m
9BBXOc7eEqj7PoOKNaTxLN3YcuRtgR6NIXL4KLb6oyvIzoeiprt4+9q7sc3Dnkc5
EV9kwWlEW2DHzhP6VYca0WXIIXc53U1AM3ewxwIDAQABAoIBABIKhaaqJF+XM7zU
B0uuxrPfJynqrFVbqcUfQ9H1bzF7Rm7CeuhRiUBxeA5Y+8TMpFcPxT/dWzGL1xja
RxWx715/zKg8V9Uth6HF55o2r/bKlLtGw3iBz1C34LKwrul1eu+HlEDS6MNoGKco
BynE0qvFOedsCu/Pgv7xhQPLow60Ty1uM0AhbcPgi6yJ5ksRB1XjtEnW0t+c8yQS
nU3mU8k230SdMhf4Ifud/5TPLjmXdFpyPi9uYiVdJ5oWsmMWEvekXoBnHWDDF/eT
VkVMiTBorT4qn+Ax1VjHL2VOMO5ZbXEcpbIc3Uer7eZAaDQ0NPZK37IkIn9TiZ21
cqzgbCkCgYEA5enHZbD5JgfwSNWCaiNrcBhYjpCtvfbT82yGW+J4/Qe/H+bY/hmJ
RRTKf0kVPdRwZzq7GphVMWIuezbOk0aFGhk/SzIveW8QpLY0FV/5xFnGNjV9AuNc
xrmgVshUsyQvr1TFkbdkC6yuvNgQfXfnbEoaPsXYEMCii2zqdF5lWGUCgYEAzCR2
6g8vEQx0hdRS5d0zD2/9IRYNzfP5oK0+F3KHH2OuwlmQVIo7IhCiUgqserXNBDef
hj+GNcU8O/yXLomAXG7VG/cLWRrpY8d9bcRMrwb0/SkNr0yNrkqHiWQ/PvR+2MLk
viWFZTTp8YizPA+8pSC/oFd1jkZF0UhKVAREM7sCgYB5+mfxyczFopyW58ADM7uC
g0goixXCnTuiAEfgY+0wwXVjJYSme0HaxscQdOOyJA1ml0BBQeShCKgEcvVyKY3g
ZNixunR5hrVbzdcgKAVJaR/CDuq+J4ZHYKByqmJVkLND4EPZpWSM1Rb31eIZzw2W
5FG8UBbr/GfAdQ6GorY+CQKBgQCzWQHkBmz6VG/2t6AQ9LIMSP4hWEfOfh78q9dW
MDdIO4JomtkzfLIQ7n49B8WalShGITwUbLDTgrG1neeQahsMmg6+X99nbD5JfBTV
H9WjG8CWvb+ZF++NhUroSNtLyu+6LhdaeopkbQVvPwMArG62wDu6ebv8v/5MrG8o
uwrUSwKBgQCxV43ZqTRnEuDlF7jMN+2JZWhpbrucTG5INoMPOC0ZVatePszZjYm8
LrmqQZHer2nqtFpyslwgKMWgmVLJTH7sVf0hS9po0+iSYY/r8e/c85UdUreb0xyT
x8whrOnMMODCAqu4W/Rx1Lgf2vXIx0pZmlt8Df9i2AVg/ePR6jO3Nw==
-----END RSA PRIVATE KEY-----
</code></pre></td></tr></table>
</div>
</div><p>接下来通过 Python 编程生成 RSA Public Key 和 JWK（jwk 其实就是公钥的另一个表述形式而已）:</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback"># 需要先安装依赖: pip install jwcrypto
from jwcrypto.jwk import JWK
from pathlib import Path

private_key = Path(&#34;key.pem&#34;).read_bytes()
jwk = JWK.from_pem(private_key)

# 导出公钥 RSA Public Key
public_key = jwk.public().export_to_pem()
print(public_key)

print(&#34;=&#34;*30)

# 导出 JWK
jwk_bytes = jwk.public().export()
print(jwk_bytes)
</code></pre></td></tr></table>
</div>
</div><p>Istio 需要 JWK 进行 JWT 验证，而我们手动验证 JWT 时一般需要用到 Public Key. 方便起见，上述代码把这两个都打印了出来。内容如下：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback"># Public Key 内容，不包含这行注释
-----BEGIN PUBLIC KEY-----
MIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEAt1cKkQqPh8iOv5BhKh7R
x6A2+1ldpO/jczML/0GBKu4X+lHrY8YbJrt29jyAXlWM8vHC7tXsqgUG+WziRD0D
8nhnh10XC14SeH+3mVuBqph+TqhXTWsh9gtAIbeUHJjEI4I79QK4/wquPHHIGZBQ
DQQnuMh6vAS3VaUYJdEIoKvUBnAyY35kJZgyJSbrxLsEExL2zujUD/OY+/In2bq/
3rFtDGNlgHyC7Gu2zXSXvfOA4O5m9BBXOc7eEqj7PoOKNaTxLN3YcuRtgR6NIXL4
KLb6oyvIzoeiprt4+9q7sc3Dnkc5EV9kwWlEW2DHzhP6VYca0WXIIXc53U1AM3ew
xwIDAQAB
-----END PUBLIC KEY-----
</code></pre></td></tr></table>
</div>
</div><div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback"># jwk 内容
{
 &#39;e&#39;: &#39;AQAB&#39;,
 &#39;kid&#39;: &#39;oyYwZSLCLVVPHdVp0jXIcLNpGn6dMCumlY-6wSenmFo&#39;,
 &#39;kty&#39;: &#39;RSA&#39;,
 &#39;n&#39;: &#39;t1cKkQqPh8iOv5BhKh7Rx6A2-1ldpO_jczML_0GBKu4X-lHrY8YbJrt29jyAXlWM8vHC7tXsqgUG-WziRD0D8nhnh10XC14SeH-3mVuBqph-TqhXTWsh9gtAIbeUHJjEI4I79QK4_wquPHHIGZBQDQQnuMh6vAS3VaUYJdEIoKvUBnAyY35kJZgyJSbrxLsEExL2zujUD_OY-_In2bq_3rFtDGNlgHyC7Gu2zXSXvfOA4O5m9BBXOc7eEqj7PoOKNaTxLN3YcuRtgR6NIXL4KLb6oyvIzoeiprt4-9q7sc3Dnkc5EV9kwWlEW2DHzhP6VYca0WXIIXc53U1AM3ewxw&#39;
}
</code></pre></td></tr></table>
</div>
</div><h3 id="4-测试密钥可用性">4. 测试密钥可用性</h3>
<p>接下来在 <a href="https://jwt.io" target="_blank" rel="noopener noreferrer">jwt.io</a> 中填入测试用的公钥私钥，还有 Header/Payload。一是测试公私钥的可用性，二是生成出 JWT 供后续测试 Istio JWT 验证功能的可用性。
</p>
<p>可以看到左下角显示「Signature Verified」，成功地生成出了 JWT。后续可以使用这个 JWT 访问 Istio 网关，测试 Istio JWT 验证功能。</p>
<h3 id="5-启用-istio-的身份验证">5. 启用 Istio 的身份验证</h3>
<p>编写 istio 配置：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-yaml" data-lang="yaml"><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;security.istio.io/v1beta1&#34;</span><span class="w">
</span><span class="w"></span><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;RequestAuthentication&#34;</span><span class="w">
</span><span class="w"></span><span class="nt">metadata</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;jwt-example&#34;</span><span class="w">
</span><span class="w">  </span><span class="nt">namespace</span><span class="p">:</span><span class="w"> </span><span class="l">istio-system </span><span class="w"> </span><span class="c"># istio-system 名字空间中的配置，默认情况下会应用到所有名字空间</span><span class="w">
</span><span class="w"></span><span class="nt">spec</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">selector</span><span class="p">:</span><span class="w">
</span><span class="w">    </span><span class="nt">matchLabels</span><span class="p">:</span><span class="w">
</span><span class="w">      </span><span class="nt">istio</span><span class="p">:</span><span class="w"> </span><span class="l">ingressgateway</span><span class="w">
</span><span class="w">  </span><span class="nt">jwtRules</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="c"># issuer 即签发者，需要和 JWT payload 中的 iss 属性完全一致。</span><span class="w">
</span><span class="w">  </span>- <span class="nt">issuer</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;testing@secure.istio.io&#34;</span><span class="w">
</span><span class="w">    </span><span class="nt">jwks</span><span class="p">:</span><span class="w"> </span><span class="p">|</span><span class="sd">
</span><span class="sd">    {
</span><span class="sd">        &#34;keys&#34;: [
</span><span class="sd">            {
</span><span class="sd">                &#34;e&#34;: &#34;AQAB&#34;,
</span><span class="sd">                &#34;kid&#34;: &#34;oyYwZSLCLVVPHdVp0jXIcLNpGn6dMCumlY-6wSenmFo&#34;,  # kid 需要与 jwt header 中的 kid 完全一致。
</span><span class="sd">                &#34;kty&#34;: &#34;RSA&#34;,
</span><span class="sd">                &#34;n&#34;: &#34;t1cKkQqPh8iOv5BhKh7Rx6A2-1ldpO_jczML_0GBKu4X-lHrY8YbJrt29jyAXlWM8vHC7tXsqgUG-WziRD0D8nhnh10XC14SeH-3mVuBqph-TqhXTWsh9gtAIbeUHJjEI4I79QK4_wquPHHIGZBQDQQnuMh6vAS3VaUYJdEIoKvUBnAyY35kJZgyJSbrxLsEExL2zujUD_OY-_In2bq_3rFtDGNlgHyC7Gu2zXSXvfOA4O5m9BBXOc7eEqj7PoOKNaTxLN3YcuRtgR6NIXL4KLb6oyvIzoeiprt4-9q7sc3Dnkc5EV9kwWlEW2DHzhP6VYca0WXIIXc53U1AM3ewxw&#34;
</span><span class="sd">            }
</span><span class="sd">        ]
</span><span class="sd">    }
</span><span class="sd">      # jwks 或 jwksUri 二选其一
</span><span class="sd">      # jwksUri: &#34;http://nginx.test.local/istio/jwks.json&#34;</span><span class="w">    
</span></code></pre></td></tr></table>
</div>
</div><p>现在 <code>kubectl apply</code> 一下，JWT 验证就添加到全局了。</p>
<p>可以看到 jwtRules 是一个列表，因此可以为每个 issuers 配置不同的 jwtRule.</p>
<p>对同一个 issuers（jwt 签发者），可以通过 jwks 设置多个公钥，以实现JWT签名密钥的轮转。
JWT 的验证规则是：</p>
<ol>
<li>JWT 的 payload 中有 issuer 属性，首先通过 issuer 匹配到对应的 istio 中配置的 jwks。</li>
<li>JWT 的 header 中有 kid 属性，第二步在 jwks 的公钥列表中，中找到 kid 相同的公钥。</li>
<li>使用找到的公钥进行 JWT 签名验证。</li>
</ol>
<h3 id="6-启用-payload-转发authorization-转发">6. 启用 Payload 转发/Authorization 转发</h3>
<p>默认情况下，Istio 在完成了身份验证之后，会去掉 Authorization 请求头再进行转发。
这将导致我们的后端服务获取不到对应的 Payload，无法判断 End User 的身份。
因此我们需要启用 Istio 的 Authorization 请求头的转发功能，在前述的 <code>RequestAuthentication</code> yaml 配置中添加一个参数就行：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-yaml" data-lang="yaml"><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;security.istio.io/v1beta1&#34;</span><span class="w">
</span><span class="w"></span><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;RequestAuthentication&#34;</span><span class="w">
</span><span class="w"></span><span class="nt">metadata</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;jwt-example&#34;</span><span class="w">
</span><span class="w">  </span><span class="nt">namespace</span><span class="p">:</span><span class="w"> </span><span class="l">istio-system</span><span class="w">
</span><span class="w"></span><span class="nt">spec</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">selector</span><span class="p">:</span><span class="w">
</span><span class="w">    </span><span class="nt">matchLabels</span><span class="p">:</span><span class="w">
</span><span class="w">      </span><span class="nt">istio</span><span class="p">:</span><span class="w"> </span><span class="l">ingressgateway</span><span class="w">
</span><span class="w">  </span><span class="nt">jwtRules</span><span class="p">:</span><span class="w">
</span><span class="w">  </span>- <span class="nt">issuer</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;testing@secure.istio.io&#34;</span><span class="w">
</span><span class="w">    </span><span class="nt">jwks</span><span class="p">:</span><span class="w"> </span><span class="p">|</span><span class="sd">
</span><span class="sd">    {
</span><span class="sd">        &#34;keys&#34;: [
</span><span class="sd">            {
</span><span class="sd">                &#34;e&#34;: &#34;AQAB&#34;,
</span><span class="sd">                &#34;kid&#34;: &#34;oyYwZSLCLVVPHdVp0jXIcLNpGn6dMCumlY-6wSenmFo&#34;,
</span><span class="sd">                &#34;kty&#34;: &#34;RSA&#34;,
</span><span class="sd">                &#34;n&#34;: &#34;t1cKkQqPh8iOv5BhKh7Rx6A2-1ldpO_jczML_0GBKu4X-lHrY8YbJrt29jyAXlWM8vHC7tXsqgUG-WziRD0D8nhnh10XC14SeH-3mVuBqph-TqhXTWsh9gtAIbeUHJjEI4I79QK4_wquPHHIGZBQDQQnuMh6vAS3VaUYJdEIoKvUBnAyY35kJZgyJSbrxLsEExL2zujUD_OY-_In2bq_3rFtDGNlgHyC7Gu2zXSXvfOA4O5m9BBXOc7eEqj7PoOKNaTxLN3YcuRtgR6NIXL4KLb6oyvIzoeiprt4-9q7sc3Dnkc5EV9kwWlEW2DHzhP6VYca0WXIIXc53U1AM3ewxw&#34;
</span><span class="sd">            }
</span><span class="sd">        ]
</span><span class="sd">    }</span><span class="w">    
</span><span class="w"></span><span class="c"># ===================== 添加如下参数===========================</span><span class="w">
</span><span class="w">    </span><span class="nt">forwardOriginalToken</span><span class="p">:</span><span class="w"> </span><span class="kc">true</span><span class="w">  </span><span class="c"># 转发 Authorization 请求头</span><span class="w">
</span></code></pre></td></tr></table>
</div>
</div><p>加了转发后，流程图如下（需要 mermaid 渲染）：</p>
<div class="mermaid" id="id-1"></div>
<h2 id="其他问题">其他问题</h2>
<h3 id="1-authorizationpolicy">1. AuthorizationPolicy</h3>
<p>Istio 的 JWT 验证规则，默认情况下会直接忽略不带 Authorization 请求头的流量，因此这类流量能直接进入网格内部。如果需要禁止不带 Authorization 头的流量，需要额外配置 AuthorizationPolicy 策略。</p>
<p>RequestsAuthentication 验证失败的请求，Istio 会返回 401 状态码。
AuthorizationPolicy 验证失败的请求，Istio 会返回 403 状态码。</p>
<p>这会导致在使用 AuthorizationPolicy 禁止了不带 Authorization 头的流量后，这类请求会直接被返回 403。。。在使用 RESTful API 时，这种情况可能会造成一定的问题。</p>
<h3 id="2-response-headers">2. Response Headers</h3>
<p>RequestsAuthentication 不支持自定义响应头信息，这导致对于前后端分离的 Web API 而言，
一旦 JWT 失效，Istio 会直接将 401 返回给前端 Web 页面。
因为响应头中不包含 <code>Access-Crontrol-Allow-Origin</code>，响应将被浏览器拦截！</p>
<p>这可能需要通过 EnvoyFilter 自定义响应头，添加跨域信息。</p>
<h2 id="参考">参考</h2>
<ul>
<li><a href="https://auth0.com/docs/tokens/references/jwks-properties" target="_blank" rel="noopener noreferrer">JSON Web Key Set Properties - Auth0</a></li>
<li><a href="https://tools.ietf.org/html/rfc7517" target="_blank" rel="noopener noreferrer">JWK - RFC7517</a></li>
<li><a href="https://github.com/istio/istio/tree/master/security/tools/jwt/samples" target="_blank" rel="noopener noreferrer">Sample JWT and JWKS data for demo - Istio Security</a></li>
<li><a href="https://istio.io/docs/tasks/security/authentication/authn-policy/#end-user-authentication" target="_blank" rel="noopener noreferrer">End User Authentication - Istio</a></li>
<li><a href="https://istio.io/docs/reference/config/security/jwt/" target="_blank" rel="noopener noreferrer">JWTRule - Istio</a></li>
<li><a href="https://jwt.io/" target="_blank" rel="noopener noreferrer">jwt.io - 动态生成 jwt</a></li>
</ul>]]></description></item><item><title>Kubernetes 常见错误、原因及处理方法</title><link>https://ryan4yin.space/posts/kubernetes-common-errors-and-solutions/</link><pubDate>Sun, 24 Nov 2019 19:26:54 +0800</pubDate><author>xiaoyin_c@qq.com</author><dc:creator>ryan4yin</dc:creator><guid>https://ryan4yin.space/posts/kubernetes-common-errors-and-solutions/</guid><description><![CDATA[<h2 id="pod-常见错误">Pod 常见错误</h2>
<ol>
<li>OOMKilled: Pod 的内存使用超出了 resources.limits 中的限制，被强制杀死。</li>
<li><a href="https://cloud.tencent.com/developer/article/1411527" target="_blank" rel="noopener noreferrer">SandboxChanged: Pod sandbox changed, it will be killed and re-created</a>: 很可能是由于内存限制导致容器被 OOMKilled，或者其他资源不足
<ol>
<li>如果是 OOM，容器通常会被重启，<code>kubectl describe</code> 能看到容器上次被重启的原因 <code>State.Last State.Reason = OOMKilled, Exit Code=137</code>.</li>
</ol>
</li>
<li>Pod 不断被重启，<code>kubectl describe</code> 显示重启原因 <code>State.Last State.Reason = Error, Exit Code=137</code>，137 对应 SIGKILL(<code>kill -9</code>) 信号，说明容器被强制重启。可能的原因：
<ol>
<li>最有可能的原因是，存活探针（livenessProbe）检查失败</li>
<li>节点资源不足，内核强制关闭了进程以释放资源，这种情况可以通过 <code>journalctl -k</code> 查看详细的系统日志。</li>
</ol>
</li>
<li>CrashLoopBackoff: Pod 进入 <strong>崩溃-重启</strong>循环，重启间隔时间从 10 20 40 80 一直翻倍到上限 300 秒，然后以 300 秒为间隔无限重启。</li>
<li>Pod 一直 Pending: 这说明没有任何节点能满足 Pod 的要求，容器无法被调度。比如端口被别的容器用 hostPort 占用，节点有污点等。</li>
<li><a href="" rel="">FailedCreateSandBox: Failed create pod sandbox: rpc error: code = DeadlineExceeded desc = context deadline exceeded</a>：很可能是 CNI 网络插件的问题（比如 ip 地址溢出），</li>
<li><a href="https://github.com/kubernetes/kubernetes/issues/55094" target="_blank" rel="noopener noreferrer">FailedSync: error determining status: rpc error: code = DeadlineExceeded desc = context deadline exceeded</a>: 常和前两个错误先后出现，很可能是 CNI 网络插件的问题。</li>
<li>开发集群，一次性部署所有服务时，各 Pod 互相争抢资源，导致 Pod 生存探针失败，不断重启，重启进一步加重资源使用。恶性循环。
<ul>
<li><strong>需要给每个 Pod 加上 resources.requests，这样资源不足时，后续 Pod 会停止调度，直到资源恢复正常。</strong></li>
</ul>
</li>
<li>Pod 出现大量的 Failed 记录，Deployment 一直重复建立 Pod: 通过 <code>kubectl describe/edit pod &lt;pod-name&gt;</code> 查看 pod <code>Events</code> 和 <code>Status</code>，一般会看到失败信息，如节点异常导致 Pod 被驱逐。</li>
<li><a href="https://zhuanlan.zhihu.com/p/70031676" target="_blank" rel="noopener noreferrer">Kubernetes 问题排查：Pod 状态一直 Terminating</a></li>
<li>创建了 Deployment 后，却没有自动创建 Pod: 缺少某些创建 Pod 必要的东西，比如设定的 ServiceAccount 不存在。</li>
<li>Pod 运行失败，状态为 MatchNodeSelector: 对主节点进行关机、迁移等操作，导致主调度器下线时，会在一段时间内导致 Pod 调度失败，调度失败会报这个错。</li>
<li>Pod 仍然存在，但是 <code>Service</code> 的 Endpoints 却为空，找不到对应的 Pod IPs: 遇到过一次，是因为时间跳变（从未来的时间改回了当前时间）导致的问题。</li>
</ol>
<h3 id="控制面故障可能会导致各类奇怪的异常现象">控制面故障可能会导致各类奇怪的异常现象</h3>
<p>对于生产环境的集群，因为有高可用，通常我们比较少遇到控制面故障问题。但是一旦控制面发生故障，就可能会导致各类奇怪的异常现象。
如果能在排查问题时，把控制面异常考虑进来，在这种情况下，就能节约大量的排查时间，快速定位到问题。</p>
<p>其中比较隐晦的就是 controller-manager 故障导致的异常：</p>
<ol>
<li>节点的服务器已经被终止，但是 Kuberntes 里还显示 node 为 Ready 状态，不会更新为 NotReady.</li>
<li>被删除的 Pods 可能会卡在 Terminating 状态，只有强制删除才能删除掉它们。并且确认 Pod 没有 <code>metadata.finalizers</code> 属性</li>
<li>HPA 的动态伸缩功能失效</li>
<li>&hellip;</li>
</ol>
<p>如果这些现象同时发生，就要怀疑是否是 kube-controller-manager 出问题了.</p>
<p>其他控制面异常的详细分析，参见 <a href="./kubernetes%20%e6%8e%a7%e5%88%b6%e9%9d%a2%e6%95%85%e9%9a%9c%e7%8e%b0%e8%b1%a1%e5%8f%8a%e5%88%86%e6%9e%90.md" rel="">kubernetes 控制面故障现象及分析</a></p>
<h3 id="pod-无法删除">Pod 无法删除</h3>
<p>可能是某些资源无法被GC，这会导致容器已经 Exited 了，但是 Pod 一直处于 Terminating 状态。</p>
<p>这个问题在网上能搜到很多案例,但大都只是提供了如下的强制清理命令，未分析具体原因：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-shell" data-lang="shell">kubectl delete pods &lt;pod&gt; --grace-period<span class="o">=</span><span class="m">0</span> --force
</code></pre></td></tr></table>
</div>
</div><p>最近找到几篇详细的原因分析文章，值得一看：</p>
<ul>
<li><a href="https://cloud.tencent.com/developer/article/1680612" target="_blank" rel="noopener noreferrer">腾讯云原生 -【Pod Terminating原因追踪系列】之 containerd 中被漏掉的 runc 错误信息</a></li>
<li><a href="https://cloud.tencent.com/developer/article/1680613" target="_blank" rel="noopener noreferrer">腾讯云原生 -【Pod Terminating原因追踪系列之二】exec连接未关闭导致的事件阻塞</a></li>
<li><a href="https://cloud.tencent.com/developer/article/1689486" target="_blank" rel="noopener noreferrer">腾讯云原生 -【Pod Terminating原因追踪系列之三】让docker事件处理罢工的cancel状态码</a></li>
<li><a href="https://www.likakuli.com/posts/docker-pod-terminating/" target="_blank" rel="noopener noreferrer">Pod terminating - 问题排查 - KaKu Li</a></li>
</ul>
<p>大致总结一下，主要原因来自 docker 18.06 以及 kubernetes 的 docker-shim 运行时的底层逻辑，已经在新版本被修复了。</p>
<h3 id="initcontainers-不断-restart但是-containers-却都显示已-ready">initContainers 不断 restart，但是 Containers 却都显示已 ready</h3>
<p>Kubernetes 应该确保所有 initContainers 都 Completed，然后才能启动 Containers.</p>
<p>但是我们发现有一个节点上，所有包含 initContainers 的 Pod，状态全都是 <code>Init:CrashLoopBackOff</code> 或者 <code>Init:Error</code>.</p>
<p>而且进一步 <code>kubectl describe po</code> 查看细节，发现 initContainer 的状态为:</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback">...
    State:          Waiting
      Reason:       CrashLoopBackOff
    Last State:     Terminated
      Reason:       Error
      Exit Code:    2
      Started:      Tue, 03 Aug 2021 06:02:42 +0000
      Finished:     Tue, 03 Aug 2021 06:02:42 +0000
    Ready:          False
    Restart Count:  67
...
</code></pre></td></tr></table>
</div>
</div><p>而 Containers 的状态居然是 ready:</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback">...
    Host Port:      0/TCP
    State:          Running
      Started:      Tue, 03 Aug 2021 00:35:30 +0000
    Ready:          True
    Restart Count:  0
...
</code></pre></td></tr></table>
</div>
</div><p>initContainers 还未运行成功，而 Containers 却 Ready 了，非常疑惑。</p>
<p>仔细想了下，早上因为磁盘余量告警，有手动运行过 <code>docker system prune</code> 命令，那么问题可能就是这条命令清理掉了已经 exited 的 initContainers 容器，导致 k8s 故障，不断尝试重启该容器。</p>
<p>网上一搜确实有相关的信息：</p>
<ul>
<li><a href="https://stackoverflow.com/questions/62333064/cant-delete-exited-init-container">https://stackoverflow.com/questions/62333064/cant-delete-exited-init-container</a></li>
<li><a href="https://github.com/kubernetes/kubernetes/issues/62362">https://github.com/kubernetes/kubernetes/issues/62362</a></li>
</ul>
<p>结论：使用外部的垃圾清理命令可能导致 k8s 行为异常。</p>
<h2 id="节点常见错误">节点常见错误</h2>
<ol>
<li><a href="https://kubernetes.io/docs/tasks/administer-cluster/out-of-resource/#node-conditions" target="_blank" rel="noopener noreferrer">DiskPressure</a>：节点的可用空间不足。（通过<code>df -h</code> 查看，保证可用空间不小于 15%）</li>
<li>The node was low on resource: ephemeral-storage: 同上，节点的存储空间不够了。</li>
</ol>
<p>节点存储告警可能的原因：</p>
<ol>
<li>kubelet 的资源 GC 设置有问题，遗留的镜像等资源未及时 GC 导致告警</li>
<li>存在运行的 pod 使用了大量存储空间，在节点上通过 <code>docker ps -a --size | grep G</code> 可以查看到</li>
<li>如果使用的是 EKS，并且磁盘告警的挂载点为 <code>/var/lib/kubelet/plugins/kubernetes.io/aws-ebs/mounts/aws/us-east-1b/vol-xxxxx</code>
<ol>
<li>显然是 EBS 存储卷快满了导致的</li>
<li>可通过 <code> kubectl get pv -A -o yaml | grep -C 30 vol-xxxxx</code> 来定位到具体的存储卷</li>
</ol>
</li>
</ol>
<h2 id="网络常见错误">网络常见错误</h2>
<h3 id="1-ingressistio-gateway-返回值">1. Ingress/Istio Gateway 返回值</h3>
<ol>
<li>404：不存在该 Service/Istio Gateway，或者是服务自身返回 404</li>
<li>500：大概率是服务自身的错误导致 500，小概率是代理（Sidecar/Ingress 等）的错误</li>
<li>503：服务不可用，有如下几种可能的原因：
<ol>
<li>Service 对应的 Pods 不存在，endpoints 为空</li>
<li>Service 对应的 Pods 全部都 NotReady，导致 endpoints 为空</li>
<li>也有可能是服务自身出错返回的 503</li>
<li>如果你使用了 envoy sidecar， 503 可能的原因就多了。基本上 sidecar 与主容器通信过程中的任何问题都会使 envoy 返回 503，使客户端重试。
<ol>
<li>详见 <a href="https://blog.fleeto.us/post/istio-503-uc-debug/" target="_blank" rel="noopener noreferrer">Istio：503、UC 和 TCP</a></li>
</ol>
</li>
</ol>
</li>
<li>502：Bad Gateway，通常是由于上游未返回正确的响应导致的，可能的根本原因：
<ol>
<li>应用程序未正确处理 SIGTERM 信号，在请求未处理完毕时直接终止了进程。详见 <a href="./%e6%9c%80%e4%bd%b3%e5%ae%9e%e8%b7%b5.md" rel="">优雅停止（Gracful Shutdown）与 502/504 报错 - K8s 最佳实践</a></li>
<li>网络插件 bug</li>
</ol>
</li>
<li>504：网关请求 upstream 超时，主要有两种可能
<ol>
<li>考虑是不是 Ingress Controller 的 IP 列表未更新，将请求代理到了不存在的 ip，导致得不到响应</li>
<li>Service Endpoints 移除不够及时，在 Pod 已经被终止后，仍然有个别请求被路由到了该 Pod，得不到响应导致 504。详见 <a href="./%e6%9c%80%e4%bd%b3%e5%ae%9e%e8%b7%b5.md" rel="">优雅停止（Gracful Shutdown）与 502/504 报错 - K8s 最佳实践</a></li>
<li>Pod 响应太慢，代码问题</li>
</ol>
</li>
</ol>
<p>再总结一下常见的几种错误：</p>
<ul>
<li>未设置优雅停止，导致 Pod 被重新终止时，有概率出现 502/504</li>
<li>服务的所有 Pods 的状态在「就绪」和「未就绪」之间摆动，导致间歇性地出现大量 503 错误</li>
<li>服务返回 5xx 错误导致客户端不断重试，请求流量被放大，导致服务一直起不来
<ul>
<li>解决办法：限流、熔断（网关层直接返回固定的相应内容）</li>
</ul>
</li>
</ul>
<p>Ingress 相关网络问题的排查流程：</p>
<ol>
<li>Which ingress controller?</li>
<li>Timeout between client and ingress controller, or between ingress controller and backend service/pod?</li>
<li>HTTP/504 generated by the ingress controller, proven by logs from the ingress controller?</li>
<li>If you port-forward to skip the internet between client and ingress controller, does the timeout still happen?</li>
</ol>
<h3 id="2-上了-istio-sidecar-后应用程序偶尔间隔几天半个月会-redis-连接相关的错误">2. 上了 istio sidecar 后，应用程序偶尔（间隔几天半个月）会 redis 连接相关的错误</h3>
<p>考虑是否和 tcp 长时间使用有关，比如连接长时间空闲的话，可能会被 istio sidecar 断开。
如果程序自身的重连机制有问题，就会导致这种现象。</p>
<p>确认方法：</p>
<ol>
<li>检查 istio 的 <code>idleTimeout</code> 时长（默认 1h）</li>
<li>创建三五个没流量的 Pod 放置 1h（与 istio idleTimeout 时长一致），看看是否会准时开始报 redis 的错。</li>
<li>对照组：创建三五个同样没流量的 Pod，但是不注入 istio sidecar，应该一直很正常</li>
</ol>
<p>这样就能确认问题，后续处理：</p>
<ol>
<li>抓包观察程序在出错后的 tcp 层行为</li>
<li>查阅 redis sdk 的相关 issue、代码，通过升级 SDK 应该能解决问题。</li>
</ol>
<h2 id="名字空间常见错误">名字空间常见错误</h2>
<h3 id="名字空间无法删除">名字空间无法删除</h3>
<p>这通常是某些资源如 CR(custom resources)/存储等资源无法释放导致的。
比如常见的 monitoring 名字空间无法删除，应该就是 CR 无法 GC 导致的。</p>
<p>可手动删除 namespace 配置中的析构器（spec.finalizer，在名字空间生命周期结束前会生成的配置项），这样名字空间就会直接跳过 GC 步骤：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-shell" data-lang="shell"><span class="c1"># 编辑名字空间的配置</span>
kubectl edit namespace &lt;ns-name&gt;
<span class="c1"># 将 spec.finalizers 改成空列表 []</span>
</code></pre></td></tr></table>
</div>
</div><p>如果上述方法也无法删除名字空间，也找不到具体的问题，就只能直接从 etcd 中删除掉它了(有风险，谨慎操作！)。方法如下：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-shell" data-lang="shell"><span class="c1"># 登录到 etcd 容器中，执行如下命令：</span>
<span class="nb">export</span> <span class="nv">ETCDCTL_API</span><span class="o">=</span><span class="m">3</span>
<span class="nb">cd</span> /etc/kubernetes/pki/etcd/
<span class="c1"># 列出所有名字空间</span>
etcdctl --cacert ca.crt --cert peer.crt --key peer.key get /registry/namespaces --prefix --keys-only

<span class="c1"># （谨慎操作！！！）强制删除名字空间 `monitoring`。这可能导致相关资源无法被 GC！</span>
etcdctl --cacert ca.crt --cert peer.crt --key peer.key del /registry/namespaces/monitoring
</code></pre></td></tr></table>
</div>
</div><h2 id="kubectlistioctl-等客户端工具异常">kubectl/istioctl 等客户端工具异常</h2>
<ol>
<li><code>socat not found</code>: kubectl 使用 <code>socat</code> 进行端口转发，集群的所有节点，以及本机都必须安装有 <code>socat</code> 工具。</li>
</ol>
<h2 id="批量清理-evicted-记录">批量清理 Evicted 记录</h2>
<p>有时候 Pod 因为节点选择器的问题，被不断调度到有问题的 Node 上，就会不断被 Evicted，导致出现大量的 Evicted Pods。
排查完问题后，需要手动清理掉这些 Evicted Pods.</p>
<p>批量删除 Evicted 记录:</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-shell" data-lang="shell">kubectl get pods <span class="p">|</span> grep Evicted <span class="p">|</span> awk <span class="s1">&#39;{print $1}&#39;</span> <span class="p">|</span> xargs kubectl delete pod
</code></pre></td></tr></table>
</div>
</div><h2 id="容器镜像gcpod驱逐以及节点压力">容器镜像GC、Pod驱逐以及节点压力</h2>
<p>节点压力 DiskPressure 会导致 Pod 被驱逐，也会触发容器镜像的 GC。</p>
<p>根据官方文档 <a href="https://kubernetes.io/zh/docs/tasks/administer-cluster/out-of-resource" target="_blank" rel="noopener noreferrer">配置资源不足时的处理方式</a>，Kubelet 提供如下用于配置容器 GC 及 Evicetion 的阈值：</p>
<ol>
<li><code>--eviction-hard</code> 和 <code>eviction-soft</code>: 对应旧参数 <code>--image-gc-high-threshold</code>，这两个参数配置镜像 GC 及驱逐的触发阈值。磁盘使用率的阈值默认为 85%
<ol>
<li>区别在于 <code>eviction-hard</code> 是立即驱逐，而 <code>eviction-soft</code> 在超过 <code>eviction-soft-grace-period</code> 之后才驱逐。</li>
</ol>
</li>
<li><code>--eviction-minimum-reclaim</code>: 对应旧参数 <code>--image-gc-low-threshold</code>。这是进行资源回收（镜像GC、Pod驱逐等）后期望达到的磁盘使用率百分比。磁盘使用率的阈值默认值为 80%。</li>
</ol>
<p>问：能否为 ImageGC 设置一个比 DiskPressure 更低的阈值？因为我们希望能自动进行镜像 GC，但是不想立即触发 Pod 驱逐。</p>
<p>答：这应该可以通过设置 <code>eviction-soft</code> 和长一点的 <code>eviction-soft-grace-period</code> 来实现。
另外 <code>--eviction-minimum-reclaim</code> 也可以设小一点，清理得更干净。示例如下：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-shell" data-lang="shell">--eviction-soft<span class="o">=</span>memory.available&lt;1Gi,nodefs.available&lt;2Gi,imagefs.available&lt;200Gi
--eviction-soft-grace-period<span class="o">=</span>3m
--eviction-minimum-reclaim<span class="o">=</span>memory.available<span class="o">=</span>0Mi,nodefs.available<span class="o">=</span>1Gi,imagefs.available<span class="o">=</span>2Gi
</code></pre></td></tr></table>
</div>
</div><h2 id="其他问题">其他问题</h2>
<h2 id="隔天-istio-等工具的-sidecar-自动注入莫名其妙失效了">隔天 Istio 等工具的 sidecar 自动注入莫名其妙失效了</h2>
<p>如果服务器晚上会关机，可能导致第二天网络插件出问题，导致 sidecar 注入器无法观察到 pod 的创建，也就无法完成 sidecar 注入。</p>
<h3 id="如何重新运行一个-job">如何重新运行一个 Job？</h3>
<p>我们有一个 Job 因为外部原因运行失败了，修复好后就需要重新运行它。</p>
<p>方法是：删除旧的 Job，再使用同一份配置重建 Job.</p>
<p>如果你使用的是 fluxcd 这类 GitOps 工具，就只需要手工删除旧 Pod，fluxcd 会定时自动 apply 所有配置，这就完成了 Job 的重建。</p>
<h2 id="参考">参考</h2>
<ul>
<li><a href="https://yq.aliyun.com/articles/703971?type=2" target="_blank" rel="noopener noreferrer">Kubernetes管理经验</a></li>
<li><a href="https://www.reddit.com/r/kubernetes/comments/ced0py/504_gateway_timeout_when_accessing_workload_via/" target="_blank" rel="noopener noreferrer">504 Gateway Timeout when accessing workload via ingress</a></li>
<li><a href="https://k8s.af/" target="_blank" rel="noopener noreferrer">Kubernetes Failure Stories</a></li>
<li><a href="https://blog.fleeto.us/post/istio-503-uc-debug/" target="_blank" rel="noopener noreferrer">Istio：503、UC 和 TCP</a></li>
</ul>
]]></description></item></channel></rss>