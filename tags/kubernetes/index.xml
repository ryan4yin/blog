<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"><channel><title>Kubernetes - 标签 - This Cute World</title><link>https://thiscute.world/tags/kubernetes/</link><description>Kubernetes - 标签 - This Cute World</description><generator>Hugo -- gohugo.io</generator><language>zh-CN</language><lastBuildDate>Tue, 27 Aug 2024 10:10:22 +0800</lastBuildDate><atom:link href="https://thiscute.world/tags/kubernetes/" rel="self" type="application/rss+xml"/><item><title>KubeCon China 2024 之旅</title><link>https://thiscute.world/posts/kubecon-china-2024/</link><pubDate>Tue, 27 Aug 2024 10:10:22 +0800</pubDate><author><name>作者</name></author><guid>https://thiscute.world/posts/kubecon-china-2024/</guid><description><![CDATA[<div class="featured-image">
                <img src="/posts/kubecon-china-2024/kubecon-china-2024-linus.webp" referrerpolicy="no-referrer">
            </div><h2 id="前言" class="headerLink">
    <a href="#%e5%89%8d%e8%a8%80" class="header-mark"></a>前言</h2><p>很早就有了解到今年的 KubeCon China 会在香港举办，虽然有些兴趣，但我最初是有被 KubeCon 高昂的门票价格劝退了的。</p>
<p>有时候不得不相信运气的魔力，机缘巧合之下，我从朋友 @Kev 处得知了 KubeCon 的「最终用户门票计划」并借此 0 元购了门票，又邀上了 <a href="https://0xffff.one/" target="_blank" rel="noopener noreferrer">0xFFFF 社区</a> 的<a href="https://0xffff.one/u/Chever-John" target="_blank" rel="noopener noreferrer">@Chever-John</a>
<a href="https://0xffff.one/u/0xdeadbeef" target="_blank" rel="noopener noreferrer">@0xdeadbeef</a> <a href="https://0xffff.one/u/MingLLuo" target="_blank" rel="noopener noreferrer">@茗洛</a> 三位朋友一起参加，在香港租了个 airbnb 住宿，期间也逛了香港城市中的不少地方，收货颇丰。</p>]]></description></item><item><title>Kubernetes 集群伸缩组件 - Karpenter</title><link>https://thiscute.world/posts/kubernetes-cluster-autoscaling-1-karpenter/</link><pubDate>Wed, 10 Jul 2024 09:17:31 +0800</pubDate><author><name>作者</name></author><guid>https://thiscute.world/posts/kubernetes-cluster-autoscaling-1-karpenter/</guid><description><![CDATA[<div class="featured-image">
                <img src="/posts/kubernetes-cluster-autoscaling-1-karpenter/karpenter.png" referrerpolicy="no-referrer">
            </div><h2 id="前言" class="headerLink">
    <a href="#%e5%89%8d%e8%a8%80" class="header-mark"></a>前言</h2><p>Kubernetes 具有非常丰富的动态伸缩能力，这体现在多个层面：</p>
<ol>
<li>Workloads 的伸缩：通过 Horizontal Pod Autoscaler（HPA）和 Vertical Pod
Autoscaler（VPA）等资源，可以根据资源使用情况自动调整 Pod 的数量和资源配置。
<ul>
<li>相关项目：
<ul>
<li><a href="https://github.com/kubernetes-sigs/metrics-server" target="_blank" rel="noopener noreferrer">metrics-server</a>: 采集指标数据供
HPA 使用</li>
<li><a href="https://github.com/kedacore/keda" target="_blank" rel="noopener noreferrer">KEDA</a>: 用于支持更多的指标数据源与触发方式</li>
<li><a href="https://github.com/kubernetes/autoscaler" target="_blank" rel="noopener noreferrer">kubernetes/autoscaler</a>: 提供 VPA 功能</li>
</ul>
</li>
</ul>
</li>
<li>Nodes 的伸缩：根据集群的负载情况，可以自动增加或减少 Nodes 的数量，以适应负载的变化。
<ul>
<li>相关项目：
<ul>
<li><a href="https://github.com/kubernetes/autoscaler" target="_blank" rel="noopener noreferrer">kubernetes/autoscaler</a>: 目前最流行的
Node 伸缩方案，支持绝大多数云厂商。</li>
<li><a href="https://github.com/kubernetes-sigs/karpenter" target="_blank" rel="noopener noreferrer">karpenter</a>: AWS 捐给 CNCF 的一个新兴
Node 伸缩方案，目前仅支持 AWS/Azure，但基于其核心库可以很容易地扩展支持其他云厂商。</li>
</ul>
</li>
</ul>
</li>
</ol>
<p>本文主要介绍新兴 Node 伸缩与管理方案 Karpenter 的优势、应用场景及使用方法。</p>]]></description></item><item><title>Kubernetes 中的证书管理工具 - cert-manager</title><link>https://thiscute.world/posts/kubernetes-cert-management/</link><pubDate>Sun, 31 Jul 2022 15:11:46 +0800</pubDate><author><name>作者</name></author><guid>https://thiscute.world/posts/kubernetes-cert-management/</guid><description><![CDATA[<div class="featured-image">
                <img src="/posts/kubernetes-cert-management/cert-manager.webp" referrerpolicy="no-referrer">
            </div><p>我在之前的文章<a href="https://thiscute.world/posts/about-tls-cert/" target="_blank" rel="noopener noreferrer">写给开发人员的实用密码学（八）—— 数字证书与 TLS 协议</a>
中，介绍了如何使用 openssl 生成与管理各种用途的数字证书，也简单介绍了如何通过 certbot 等工具与 ACME 证书申请与管理协议，进行数字证书的申请与自动更新（autorenew）。</p>]]></description></item><item><title>FinOps for Kubernetes - 如何拆分 Kubernetes 成本</title><link>https://thiscute.world/posts/finops-for-kubernetes/</link><pubDate>Wed, 04 May 2022 23:15:00 +0800</pubDate><author><name>作者</name></author><guid>https://thiscute.world/posts/finops-for-kubernetes/</guid><description><![CDATA[<div class="featured-image">
                <img src="/posts/finops-for-kubernetes/finops-for-kubernetes.webp" referrerpolicy="no-referrer">
            </div><blockquote>
  <p>FinOps 是一种不断发展的云财务管理学科和文化实践，通过帮助工程、财务、技术和业务团队在数据驱动的预算分配上进行协作，使成本预算能够产生最大的业务价值。</p>]]></description></item><item><title>部署一个 Kubernetes 集群</title><link>https://thiscute.world/posts/kubernetes-deployment-using-kubeadm/</link><pubDate>Tue, 25 Jan 2022 01:37:00 +0800</pubDate><author><name>作者</name></author><guid>https://thiscute.world/posts/kubernetes-deployment-using-kubeadm/</guid><description><![CDATA[<div class="featured-image">
                <img src="/posts/kubernetes-deployment-using-kubeadm/adopt-kubernetes.webp" referrerpolicy="no-referrer">
            </div><blockquote>
  <p>本文完成于 2022-01-25，其中部分内容已经过时，仅供参考。</p>

</blockquote><blockquote>
  <p>本文由个人笔记<a href="https://github.com/ryan4yin/knowledge/tree/master/kubernetes" target="_blank" rel="noopener noreferrer">ryan4yin/knowledge</a> 整理而来，不保证正确</p>

</blockquote><h2 id="本地-kubernetes-集群安装工具" class="headerLink">
    <a href="#%e6%9c%ac%e5%9c%b0-kubernetes-%e9%9b%86%e7%be%a4%e5%ae%89%e8%a3%85%e5%b7%a5%e5%85%b7" class="header-mark"></a>本地 Kubernetes 集群安装工具</h2><blockquote>
  <p>云上的 Kubernetes 集群，基本上各云厂商都支持一键部署。这里主要关注本地部署，或者叫做裸机
(baremetal)部署</p>]]></description></item><item><title>Kubernetes 微服务最佳实践</title><link>https://thiscute.world/posts/kubernetes-best-practices/</link><pubDate>Tue, 25 Jan 2022 00:13:00 +0800</pubDate><author><name>作者</name></author><guid>https://thiscute.world/posts/kubernetes-best-practices/</guid><description><![CDATA[<div class="featured-image">
                <img src="/posts/kubernetes-best-practices/kubernetes-best-practices.webp" referrerpolicy="no-referrer">
            </div><blockquote>
  <p>本文由个人笔记<a href="https://github.com/ryan4yin/knowledge/tree/master/kubernetes" target="_blank" rel="noopener noreferrer">ryan4yin/knowledge</a> 整理而来</p>

</blockquote><p>本文主要介绍我个人在使用 Kubernetes 的过程中，总结出的一套「Kubernetes 配置」，是我个人的「最佳实践」。其中大部分内容都经历过线上环境的考验，但是也有少部分还只在我脑子里模拟过，请谨慎参考。</p>]]></description></item><item><title>云原生流水线 Argo Workflows 的安装、使用以及个人体验</title><link>https://thiscute.world/posts/experience-of-argo-workflows/</link><pubDate>Wed, 27 Jan 2021 15:37:27 +0800</pubDate><author><name>作者</name></author><guid>https://thiscute.world/posts/experience-of-argo-workflows/</guid><description><![CDATA[<div class="featured-image">
                <img src="/posts/experience-of-argo-workflows/argo-workflows.webp" referrerpolicy="no-referrer">
            </div><blockquote>
  <p>注意：这篇文章并不是一篇入门教程，学习 Argo Workflows 请移步官方文档<a href="https://argoproj.github.io/argo-workflows/" target="_blank" rel="noopener noreferrer">Argo Documentation</a></p>

</blockquote><p><a href="https://github.com/argoproj/argo-workflows/" target="_blank" rel="noopener noreferrer">Argo Workflows</a> 是一个云原生工作流引擎，专注于<strong>编排并行任务</strong>。它的特点如下：</p>]]></description></item><item><title>使用 Istio 进行 JWT 身份验证（充当 API 网关）</title><link>https://thiscute.world/posts/use-istio-for-jwt-auth/</link><pubDate>Mon, 06 Apr 2020 21:48:26 +0800</pubDate><author><name>作者</name></author><guid>https://thiscute.world/posts/use-istio-for-jwt-auth/</guid><description><![CDATA[<div class="featured-image">
                <img src="/posts/use-istio-for-jwt-auth/istio.webp" referrerpolicy="no-referrer">
            </div><blockquote>
  <p>本文基于 Istio1.5 编写测试</p>

</blockquote><p>Istio 支持使用 JWT 对终端用户进行身份验证（Istio End User Authentication），支持多种 JWT
签名算法。</p>
<p>目前主流的 JWT 算法是 RS256/ES256。（请忽略 HS256，该算法不适合分布式 JWT 验证）</p>
<p>这里以 RSA256 算法为例进行介绍，ES256 的配置方式也是一样的。</p>]]></description></item><item><title>Kubernetes 常见错误、原因及处理方法</title><link>https://thiscute.world/posts/kubernetes-common-errors-and-solutions/</link><pubDate>Sun, 24 Nov 2019 19:26:54 +0800</pubDate><author><name>作者</name></author><guid>https://thiscute.world/posts/kubernetes-common-errors-and-solutions/</guid><description><![CDATA[<div class="featured-image">
                <img src="/posts/kubernetes-common-errors-and-solutions/featured-image.webp" referrerpolicy="no-referrer">
            </div><h2 id="pod-常见错误" class="headerLink">
    <a href="#pod-%e5%b8%b8%e8%a7%81%e9%94%99%e8%af%af" class="header-mark"></a>Pod 常见错误</h2><ol>
<li>OOMKilled: Pod 的内存使用超出了 resources.limits 中的限制，被强制杀死。</li>
<li><a href="https://cloud.tencent.com/developer/article/1411527" target="_blank" rel="noopener noreferrer">SandboxChanged: Pod sandbox changed, it will be killed and re-created</a>:
很可能是由于内存限制导致容器被 OOMKilled，或者其他资源不足
<ol>
<li>如果是 OOM，容器通常会被重启，<code>kubectl describe</code> 能看到容器上次被重启的原因<code>State.Last State.Reason = OOMKilled, Exit Code=137</code>.</li>
</ol>
</li>
<li>Pod 不断被重启，<code>kubectl describe</code> 显示重启原因<code>State.Last State.Reason = Error, Exit Code=137</code>，137 对应 SIGKILL(<code>kill -9</code>) 信号，说明容器被强制重启。可能的原因：
<ol>
<li>最有可能的原因是，存活探针（livenessProbe）检查失败</li>
<li>节点资源不足，内核强制关闭了进程以释放资源，这种情况可以通过 <code>journalctl -k</code> 查看详细的系统日志。</li>
</ol>
</li>
<li>CrashLoopBackoff: Pod 进入 <strong>崩溃-重启</strong>循环，重启间隔时间从 10 20 40 80 一直翻倍到上限
300 秒，然后以 300 秒为间隔无限重启。</li>
<li>Pod 一直 Pending: 这说明没有任何节点能满足 Pod 的要求，容器无法被调度。比如端口被别的容器用 hostPort 占用，节点有污点等。</li>
<li><a href="" rel="">FailedCreateSandBox: Failed create pod sandbox: rpc error: code = DeadlineExceeded desc = context deadline exceeded</a>：
很可能是 CNI 网络插件的问题（比如 ip 地址溢出），</li>
<li><a href="https://github.com/kubernetes/kubernetes/issues/55094" target="_blank" rel="noopener noreferrer">FailedSync: error determining status: rpc error: code = DeadlineExceeded desc = context deadline exceeded</a>:
常和前两个错误先后出现，很可能是 CNI 网络插件的问题。</li>
<li>开发集群，一次性部署所有服务时，各 Pod 互相争抢资源，导致 Pod 生存探针失败，不断重启，
重启进一步加重资源使用。恶性循环。
<ul>
<li><strong>需要给每个 Pod 加上 resources.requests，这样资源不足时，后续 Pod 会停止调度，直到资源恢复正常</strong>。</li>
</ul>
</li>
<li>Pod 出现大量的 Failed 记录，Deployment 一直重复建立 Pod: 通过<code>kubectl describe/edit pod &lt;pod-name&gt;</code> 查看 pod <code>Events</code> 和 <code>Status</code>，一般会看到失败信息，如节点异常导致 Pod 被驱逐。</li>
<li><a href="https://zhuanlan.zhihu.com/p/70031676" target="_blank" rel="noopener noreferrer">Kubernetes 问题排查：Pod 状态一直 Terminating</a></li>
<li>创建了 Deployment 后，却没有自动创建 Pod: 缺少某些创建 Pod 必要的东西，比如设定的
ServiceAccount 不存在。</li>
<li>Pod 运行失败，状态为 MatchNodeSelector: 对主节点进行关机、迁移等操作，导致主调度器下线时，会在一段时间内导致 Pod 调度失败，调度失败会报这个错。</li>
<li>Pod 仍然存在，但是 <code>Service</code> 的 Endpoints 却为空，找不到对应的 Pod IPs: 遇到过一次，是因为时间跳变（从未来的时间改回了当前时间）导致的问题。</li>
<li>Pod 无法调度，报错 <code>x node(s) had volume node affinity conflict</code>: 说明该 pod 所绑定的
PV 有 nodeAffinity 无法满足，可以 check 对应的 PV yaml. 通常原因是 PV 所在的可用区，没有可用的节点，导致 Pod 无法调度。
<ol>
<li>最简单的解决方法是，在对应的可用区补充节点</li>
<li>如果数据可以丢，也可以考虑直接删除重建 PV/PVC</li>
</ol>
</li>
</ol>
<h3 id="控制面故障可能会导致各类奇怪的异常现象" class="headerLink">
    <a href="#%e6%8e%a7%e5%88%b6%e9%9d%a2%e6%95%85%e9%9a%9c%e5%8f%af%e8%83%bd%e4%bc%9a%e5%af%bc%e8%87%b4%e5%90%84%e7%b1%bb%e5%a5%87%e6%80%aa%e7%9a%84%e5%bc%82%e5%b8%b8%e7%8e%b0%e8%b1%a1" class="header-mark"></a>控制面故障可能会导致各类奇怪的异常现象</h3><p>对于生产环境的集群，因为有高可用，通常我们比较少遇到控制面故障问题。但是一旦控制面发生故障，就可能会导致各类奇怪的异常现象。如果能在排查问题时，把控制面异常考虑进来，在这种情况下，就能节约大量的排查时间，快速定位到问题。</p>]]></description></item></channel></rss>