[{"categories":["tech"],"content":" 个人笔记，不保证正确！ 谈到分布式数据库，不论是 Etcd/Zookeeper 这样的中心化数据库，还是 Ethereum 区块链这样的去中心化数据库，都避免不了两个关键词：「一致性」跟「共识」。 本文是笔者学习「一致性」和「共识」以及相关的理论知识时记录的笔记，这些知识能帮助我们了解 Etcd/Zookeeper/Consul/MySQL/PostgreSQL/DynamoDB/Cassandra/MongoDB/CockroachDB/TiDB 等一众数据库的区别，理解各数据库的优势与局限性，搞懂数据库隔离级别的含义以及应该如何设置，并使我们能在各种应用场景中选择出适用的数据库。 如果你对区块链感兴趣，那这篇文章也能帮助你了解区块链这样的去中心化数据库，跟业界流行的分布式数据库在技术上有何区别，又有哪些共同点，具体是如何实现。 ","date":"2022-08-07","objectID":"/posts/consistency-and-consensus-algorithm/:0:0","series":null,"tags":["共识","一致性","分布式","数据库","区块链"],"title":"分布式数据库的一致性问题与共识算法","uri":"/posts/consistency-and-consensus-algorithm/#"},{"categories":["tech"],"content":" 一、一致性 - Consistency「一致性」本身是一个比较模糊的定义，视使用场景的不同，存在许多不同的含义。 由于数据库仍然是一个新兴领域，目前存在许多不同的一致性模型，其中的一些术语描述的一致性之间可能还有重叠关系，这些关系甚至会困扰专业的数据库开发人员。 但是究其根本，实际上在谈论一致性时，我们是在谈论事务一致性跟数据一致性，下面我们分别介绍下这两个一致性。 ","date":"2022-08-07","objectID":"/posts/consistency-and-consensus-algorithm/:1:0","series":null,"tags":["共识","一致性","分布式","数据库","区块链"],"title":"分布式数据库的一致性问题与共识算法","uri":"/posts/consistency-and-consensus-algorithm/#一一致性---consistency"},{"categories":["tech"],"content":" 1. 事务一致性 - Transactions Consistency「事务一致性」指的是数据库中事务的一致性，它是 ACID 理论中最不起眼的特性，也并不是本文的重点。 但是这里就写这么一句话也说不过去，所以下面就仔细介绍下事务与 ACID 理论。 事务与 ACID 理论事务是一种「要么全部完成，要么完全不做（All or Nothing）」的指令运行机制。 ACID 理论定义，拥有如下四个特性的「数据库指令序列」，就被称为事务： 原子性 Atomicity：事务是一个不可分割的工作单元，事务中的所有操作，要么全部完成，要么全部不完成，不可能停滞在某个中间状态。 比如 A 转账 100 元给 B，要么转账失败，要么转账成功，不可能卡在 A 被扣除了 100 元，而 B 还没收到 100 元的中间状态。 原子性在单机数据库上已得到妥善解决，但是在分布式数据库上它成为一项新的挑战。要在分布式架构下支持原子性并不容易，所以不少 NoSQL 产品都选择绕过这个问题，聚焦到那些对原子性不敏感的细分场景。 一致性 Consistency：也叫数据的「正确性 Correctness」或者完整性，指事务对数据库状态的变更必须满足所有预定义的规则，包括「约束 constraints」、「级联 cascades」、「触发器 triggers」以及这些规则的任何组合。 比如如果用户为某个字段设置了约束条件 unique，那么事务对该表的所有修改都必须保证此约束成立，否则它将会失败。 是存在感最低的特性 隔离性 Isolation：并发执行的多个事务之间是完全隔离的，它们的执行效果跟按事务的开始顺序串行执行完全一致。 事务中最复杂的特性 持久性 Durability：事务执行完毕后，结果就保存不变了。这个最好理解。 ACID 是传统的单机数据库的核心特性，比如 MySQL/PostgreSQL. ACID 中最复杂的特性 - 隔离性完全地实现 ACID 得到的数据库，性能是非常差的。 因此在关系数据库中，设计者通常会选择牺牲相对不重要的「隔离性」来获取更好的性能。 而一旦隔离不够彻底，就可能会遇到一些事务之间互相影响的异常情况，这些异常被分为如下几种： 脏写 Dirty writes：即事务 T1 跟事务 T2 同时在原数据的基础上更新同一个数据，导致结果不符合预期。 案例：两个事务同时尝试从账户中扣款 1000 元，但是它们读到的初始状态都是 5000 元，于是都尝试将账户修改为 4000 元，结果就是少扣了 1000 元。 最简单的解决方法：针对 UPDATE table SET field = field - 1000 WHERE id = 1 这类数据增删改的逻辑，需要对被更新的行加一把「行写锁」，使其他需要写此数据的事务等待。 脏读 Dirty reads：事务 T1 读取了事务 T2 未提交的数据。这个数据不一定准确，被称为脏数据，因为假如事务 T2 回滚了，T1 拿到的就是一个错误的数据 案例：假设小明小红在一个银行账户存了 5000 元，小明小红在用这同一个账户消费 1000 元，这中间小明付款的事务读取到账户已经被小红的事务修改为了 4000 元，于是它把余额修改为 3000 元，然后付款成功。但是在小明的付款事务成功后，小红的付款失败回滚了，余额又从 3000 被修改回 5000 元。小明就完成了 0 元购的壮举。 最简单的解决方法：事务 T2 写数据时对被修改的行加「行写锁」，T2 结束后再释放锁，这样事务 T1 的读取就会被阻塞，直到锁释放。 不可重复读 Non-repeatable reads：事务 T1 读取数据后，紧接着事务 T2 就更新了数据并提交，事务 T1 再次读取的时候发现数据不一致了 案例： 小明在京东上抢购商品，抢购事务启动时事务读到还剩 36 件商品，于是继续执行抢购逻辑，之后事务因为某种原因需要再读一次商品数量，结果发现商品数量已经变成 0 了，抢购失败。 更麻烦的是，不可重复读导致 SELECT 跟 UPDATE 之间也可能出现数据变更，如果你在事务中先通过 SELECT field INTO myvar FROM mytable WHERE uid = 1 读到余额，再在此基础上通过 UPDATE 去更新余额，很可能导致数据变得一团糟！ 正确的做法是使用 UPDATE mytable SET field = field - 1000 WHERE id = 1，因为每一条 SQL 命令本身都是原子的，这个 SQL 不会有问题。 最简单的解决办法：事务 T1 读数据时，也加一把「行」锁，直到不再需要读该数据了，再释放锁。 幻读 Phantom reads：事务 T1 在多次批量读数据时，事务 T2 往其中执行了插入/删除操作，导致 T1 读到的是旧数据的一个残影，而非当前真实的数据状态。 最简单的解决办法：事务 T1 在批量读数据时，先加一把范围锁，在事务 T1 结束读取之后，再释放这把锁。这能同时解决「幻读」跟「不可重复读」的问题。 根据隔离程度，ANSI SQL-92 标准中将「隔离性」细分为四个等级（避免「脏写」是数据库的必备要求，因此未记录在下面的四个等级中）： 串行化 Serializable：也就是完全的隔离，只要事务之间存在互相影响的可能，就（通过锁机制）强制它们串行执行。 可重复读 Repeatable read：可避免脏读、不可重复读的发生，但是解决不了幻读的问题。 读已提交 Read committed：只能避免脏读 读未提交 Read uncommitted：最低级别，完全放弃隔离性 MySQL 默认的隔离级别为「可重复读 Repeatable Read」，PostgreSQL 和 Oracle 默认隔离级别为「读已提交 Read committed」。 为什么 MySQL/PostgreSQL/Oracle 的默认隔离级别是这样设置的呢？该如何选择正确的隔离级别呢？ 我们针对普通的高并发业务场景做个简单分析： 首先，「脏读」是必须避免的，它会使事务读到错误的数据！最低的「读未提交」级别直接排除 「串行化」的性能太差，也直接排除 只要 SQL 用得对，「不可重复读」问题对业务逻辑的正确性通常并无影响，所以是可以容忍的。 因此一般「读已提交」是最佳的隔离级别，这也是 PostgreSQL/Oracle 将其设为默认隔离级别的原因。 那么为什么 MySQL 这么特立独行，将默认隔离级别提高到了「可重复读」呢？为啥阿里这种大的互联网公司又会把 MySQL 默认的隔离级别改成「读已提交」？ 根据网上查到的资料，这是 MySQL 的历史问题导致的。MySQL 5.0 之前只支持 statement 这种 binlog 格式，此格式在「读已提交」的隔离级别下会出现诸多问题，最明显的就是可能会导致主从数据库的数据不一致。 除了设置默认的隔离级别外，MySQL 还禁止在使用 statement 格式的 binlog 时，使用 READ COMMITTED 作为事务隔离级别，尝试修改隔离级别会报错 Transaction level 'READ-COMMITTED' in InnoDB is not safe for binlog mode 'STATEMENT' 而互联网公司将隔离级别改为「读已提交」的原因也很好理解，正如前文所述「读已提交」是最佳的隔离级别，这样修改能够提升数据库的性能。 「隔离性」的本质其实就是事务的并发控制，不同的隔离级别代表了对并发事务的隔离程度，主要的实现手段是「多版本并发控制 MVCC」与「锁」。 锁机制前面已经简单介绍过了，而 MVCC 其实就是为每个事务创建一个特定隔离级别的快照，这样读写不会互相阻塞，性能就提升了。（MVCC 暂时也是超纲知识，后面再研究吧 emmmm） ANSI SQL-92 对异常现象的分析仍然太过简单了，1995 年新发布的论文 A Critique of ANSI SQL Isolation Levels 丰富和细化了 SQL-92 的内容，定义了六种隔离级别和八种异常现象（有大佬强烈建议通读此论文，重点是文中的快照隔离（Snapshot Isolation, SI）级别）。 ","date":"2022-08-07","objectID":"/posts/consistency-and-consensus-algorithm/:1:1","series":null,"tags":["共识","一致性","分布式","数据库","区块链"],"title":"分布式数据库的一致性问题与共识算法","uri":"/posts/consistency-and-consensus-algorithm/#1-事务一致性---transactions-consistency"},{"categories":["tech"],"content":" 1. 事务一致性 - Transactions Consistency「事务一致性」指的是数据库中事务的一致性，它是 ACID 理论中最不起眼的特性，也并不是本文的重点。 但是这里就写这么一句话也说不过去，所以下面就仔细介绍下事务与 ACID 理论。 事务与 ACID 理论事务是一种「要么全部完成，要么完全不做（All or Nothing）」的指令运行机制。 ACID 理论定义，拥有如下四个特性的「数据库指令序列」，就被称为事务： 原子性 Atomicity：事务是一个不可分割的工作单元，事务中的所有操作，要么全部完成，要么全部不完成，不可能停滞在某个中间状态。 比如 A 转账 100 元给 B，要么转账失败，要么转账成功，不可能卡在 A 被扣除了 100 元，而 B 还没收到 100 元的中间状态。 原子性在单机数据库上已得到妥善解决，但是在分布式数据库上它成为一项新的挑战。要在分布式架构下支持原子性并不容易，所以不少 NoSQL 产品都选择绕过这个问题，聚焦到那些对原子性不敏感的细分场景。 一致性 Consistency：也叫数据的「正确性 Correctness」或者完整性，指事务对数据库状态的变更必须满足所有预定义的规则，包括「约束 constraints」、「级联 cascades」、「触发器 triggers」以及这些规则的任何组合。 比如如果用户为某个字段设置了约束条件 unique，那么事务对该表的所有修改都必须保证此约束成立，否则它将会失败。 是存在感最低的特性 隔离性 Isolation：并发执行的多个事务之间是完全隔离的，它们的执行效果跟按事务的开始顺序串行执行完全一致。 事务中最复杂的特性 持久性 Durability：事务执行完毕后，结果就保存不变了。这个最好理解。 ACID 是传统的单机数据库的核心特性，比如 MySQL/PostgreSQL. ACID 中最复杂的特性 - 隔离性完全地实现 ACID 得到的数据库，性能是非常差的。 因此在关系数据库中，设计者通常会选择牺牲相对不重要的「隔离性」来获取更好的性能。 而一旦隔离不够彻底，就可能会遇到一些事务之间互相影响的异常情况，这些异常被分为如下几种： 脏写 Dirty writes：即事务 T1 跟事务 T2 同时在原数据的基础上更新同一个数据，导致结果不符合预期。 案例：两个事务同时尝试从账户中扣款 1000 元，但是它们读到的初始状态都是 5000 元，于是都尝试将账户修改为 4000 元，结果就是少扣了 1000 元。 最简单的解决方法：针对 UPDATE table SET field = field - 1000 WHERE id = 1 这类数据增删改的逻辑，需要对被更新的行加一把「行写锁」，使其他需要写此数据的事务等待。 脏读 Dirty reads：事务 T1 读取了事务 T2 未提交的数据。这个数据不一定准确，被称为脏数据，因为假如事务 T2 回滚了，T1 拿到的就是一个错误的数据 案例：假设小明小红在一个银行账户存了 5000 元，小明小红在用这同一个账户消费 1000 元，这中间小明付款的事务读取到账户已经被小红的事务修改为了 4000 元，于是它把余额修改为 3000 元，然后付款成功。但是在小明的付款事务成功后，小红的付款失败回滚了，余额又从 3000 被修改回 5000 元。小明就完成了 0 元购的壮举。 最简单的解决方法：事务 T2 写数据时对被修改的行加「行写锁」，T2 结束后再释放锁，这样事务 T1 的读取就会被阻塞，直到锁释放。 不可重复读 Non-repeatable reads：事务 T1 读取数据后，紧接着事务 T2 就更新了数据并提交，事务 T1 再次读取的时候发现数据不一致了 案例： 小明在京东上抢购商品，抢购事务启动时事务读到还剩 36 件商品，于是继续执行抢购逻辑，之后事务因为某种原因需要再读一次商品数量，结果发现商品数量已经变成 0 了，抢购失败。 更麻烦的是，不可重复读导致 SELECT 跟 UPDATE 之间也可能出现数据变更，如果你在事务中先通过 SELECT field INTO myvar FROM mytable WHERE uid = 1 读到余额，再在此基础上通过 UPDATE 去更新余额，很可能导致数据变得一团糟！ 正确的做法是使用 UPDATE mytable SET field = field - 1000 WHERE id = 1，因为每一条 SQL 命令本身都是原子的，这个 SQL 不会有问题。 最简单的解决办法：事务 T1 读数据时，也加一把「行」锁，直到不再需要读该数据了，再释放锁。 幻读 Phantom reads：事务 T1 在多次批量读数据时，事务 T2 往其中执行了插入/删除操作，导致 T1 读到的是旧数据的一个残影，而非当前真实的数据状态。 最简单的解决办法：事务 T1 在批量读数据时，先加一把范围锁，在事务 T1 结束读取之后，再释放这把锁。这能同时解决「幻读」跟「不可重复读」的问题。 根据隔离程度，ANSI SQL-92 标准中将「隔离性」细分为四个等级（避免「脏写」是数据库的必备要求，因此未记录在下面的四个等级中）： 串行化 Serializable：也就是完全的隔离，只要事务之间存在互相影响的可能，就（通过锁机制）强制它们串行执行。 可重复读 Repeatable read：可避免脏读、不可重复读的发生，但是解决不了幻读的问题。 读已提交 Read committed：只能避免脏读 读未提交 Read uncommitted：最低级别，完全放弃隔离性 MySQL 默认的隔离级别为「可重复读 Repeatable Read」，PostgreSQL 和 Oracle 默认隔离级别为「读已提交 Read committed」。 为什么 MySQL/PostgreSQL/Oracle 的默认隔离级别是这样设置的呢？该如何选择正确的隔离级别呢？ 我们针对普通的高并发业务场景做个简单分析： 首先，「脏读」是必须避免的，它会使事务读到错误的数据！最低的「读未提交」级别直接排除 「串行化」的性能太差，也直接排除 只要 SQL 用得对，「不可重复读」问题对业务逻辑的正确性通常并无影响，所以是可以容忍的。 因此一般「读已提交」是最佳的隔离级别，这也是 PostgreSQL/Oracle 将其设为默认隔离级别的原因。 那么为什么 MySQL 这么特立独行，将默认隔离级别提高到了「可重复读」呢？为啥阿里这种大的互联网公司又会把 MySQL 默认的隔离级别改成「读已提交」？ 根据网上查到的资料，这是 MySQL 的历史问题导致的。MySQL 5.0 之前只支持 statement 这种 binlog 格式，此格式在「读已提交」的隔离级别下会出现诸多问题，最明显的就是可能会导致主从数据库的数据不一致。 除了设置默认的隔离级别外，MySQL 还禁止在使用 statement 格式的 binlog 时，使用 READ COMMITTED 作为事务隔离级别，尝试修改隔离级别会报错 Transaction level 'READ-COMMITTED' in InnoDB is not safe for binlog mode 'STATEMENT' 而互联网公司将隔离级别改为「读已提交」的原因也很好理解，正如前文所述「读已提交」是最佳的隔离级别，这样修改能够提升数据库的性能。 「隔离性」的本质其实就是事务的并发控制，不同的隔离级别代表了对并发事务的隔离程度，主要的实现手段是「多版本并发控制 MVCC」与「锁」。 锁机制前面已经简单介绍过了，而 MVCC 其实就是为每个事务创建一个特定隔离级别的快照，这样读写不会互相阻塞，性能就提升了。（MVCC 暂时也是超纲知识，后面再研究吧 emmmm） ANSI SQL-92 对异常现象的分析仍然太过简单了，1995 年新发布的论文 A Critique of ANSI SQL Isolation Levels 丰富和细化了 SQL-92 的内容，定义了六种隔离级别和八种异常现象（有大佬强烈建议通读此论文，重点是文中的快照隔离（Snapshot Isolation, SI）级别）。 ","date":"2022-08-07","objectID":"/posts/consistency-and-consensus-algorithm/:1:1","series":null,"tags":["共识","一致性","分布式","数据库","区块链"],"title":"分布式数据库的一致性问题与共识算法","uri":"/posts/consistency-and-consensus-algorithm/#事务与-acid-理论"},{"categories":["tech"],"content":" 1. 事务一致性 - Transactions Consistency「事务一致性」指的是数据库中事务的一致性，它是 ACID 理论中最不起眼的特性，也并不是本文的重点。 但是这里就写这么一句话也说不过去，所以下面就仔细介绍下事务与 ACID 理论。 事务与 ACID 理论事务是一种「要么全部完成，要么完全不做（All or Nothing）」的指令运行机制。 ACID 理论定义，拥有如下四个特性的「数据库指令序列」，就被称为事务： 原子性 Atomicity：事务是一个不可分割的工作单元，事务中的所有操作，要么全部完成，要么全部不完成，不可能停滞在某个中间状态。 比如 A 转账 100 元给 B，要么转账失败，要么转账成功，不可能卡在 A 被扣除了 100 元，而 B 还没收到 100 元的中间状态。 原子性在单机数据库上已得到妥善解决，但是在分布式数据库上它成为一项新的挑战。要在分布式架构下支持原子性并不容易，所以不少 NoSQL 产品都选择绕过这个问题，聚焦到那些对原子性不敏感的细分场景。 一致性 Consistency：也叫数据的「正确性 Correctness」或者完整性，指事务对数据库状态的变更必须满足所有预定义的规则，包括「约束 constraints」、「级联 cascades」、「触发器 triggers」以及这些规则的任何组合。 比如如果用户为某个字段设置了约束条件 unique，那么事务对该表的所有修改都必须保证此约束成立，否则它将会失败。 是存在感最低的特性 隔离性 Isolation：并发执行的多个事务之间是完全隔离的，它们的执行效果跟按事务的开始顺序串行执行完全一致。 事务中最复杂的特性 持久性 Durability：事务执行完毕后，结果就保存不变了。这个最好理解。 ACID 是传统的单机数据库的核心特性，比如 MySQL/PostgreSQL. ACID 中最复杂的特性 - 隔离性完全地实现 ACID 得到的数据库，性能是非常差的。 因此在关系数据库中，设计者通常会选择牺牲相对不重要的「隔离性」来获取更好的性能。 而一旦隔离不够彻底，就可能会遇到一些事务之间互相影响的异常情况，这些异常被分为如下几种： 脏写 Dirty writes：即事务 T1 跟事务 T2 同时在原数据的基础上更新同一个数据，导致结果不符合预期。 案例：两个事务同时尝试从账户中扣款 1000 元，但是它们读到的初始状态都是 5000 元，于是都尝试将账户修改为 4000 元，结果就是少扣了 1000 元。 最简单的解决方法：针对 UPDATE table SET field = field - 1000 WHERE id = 1 这类数据增删改的逻辑，需要对被更新的行加一把「行写锁」，使其他需要写此数据的事务等待。 脏读 Dirty reads：事务 T1 读取了事务 T2 未提交的数据。这个数据不一定准确，被称为脏数据，因为假如事务 T2 回滚了，T1 拿到的就是一个错误的数据 案例：假设小明小红在一个银行账户存了 5000 元，小明小红在用这同一个账户消费 1000 元，这中间小明付款的事务读取到账户已经被小红的事务修改为了 4000 元，于是它把余额修改为 3000 元，然后付款成功。但是在小明的付款事务成功后，小红的付款失败回滚了，余额又从 3000 被修改回 5000 元。小明就完成了 0 元购的壮举。 最简单的解决方法：事务 T2 写数据时对被修改的行加「行写锁」，T2 结束后再释放锁，这样事务 T1 的读取就会被阻塞，直到锁释放。 不可重复读 Non-repeatable reads：事务 T1 读取数据后，紧接着事务 T2 就更新了数据并提交，事务 T1 再次读取的时候发现数据不一致了 案例： 小明在京东上抢购商品，抢购事务启动时事务读到还剩 36 件商品，于是继续执行抢购逻辑，之后事务因为某种原因需要再读一次商品数量，结果发现商品数量已经变成 0 了，抢购失败。 更麻烦的是，不可重复读导致 SELECT 跟 UPDATE 之间也可能出现数据变更，如果你在事务中先通过 SELECT field INTO myvar FROM mytable WHERE uid = 1 读到余额，再在此基础上通过 UPDATE 去更新余额，很可能导致数据变得一团糟！ 正确的做法是使用 UPDATE mytable SET field = field - 1000 WHERE id = 1，因为每一条 SQL 命令本身都是原子的，这个 SQL 不会有问题。 最简单的解决办法：事务 T1 读数据时，也加一把「行」锁，直到不再需要读该数据了，再释放锁。 幻读 Phantom reads：事务 T1 在多次批量读数据时，事务 T2 往其中执行了插入/删除操作，导致 T1 读到的是旧数据的一个残影，而非当前真实的数据状态。 最简单的解决办法：事务 T1 在批量读数据时，先加一把范围锁，在事务 T1 结束读取之后，再释放这把锁。这能同时解决「幻读」跟「不可重复读」的问题。 根据隔离程度，ANSI SQL-92 标准中将「隔离性」细分为四个等级（避免「脏写」是数据库的必备要求，因此未记录在下面的四个等级中）： 串行化 Serializable：也就是完全的隔离，只要事务之间存在互相影响的可能，就（通过锁机制）强制它们串行执行。 可重复读 Repeatable read：可避免脏读、不可重复读的发生，但是解决不了幻读的问题。 读已提交 Read committed：只能避免脏读 读未提交 Read uncommitted：最低级别，完全放弃隔离性 MySQL 默认的隔离级别为「可重复读 Repeatable Read」，PostgreSQL 和 Oracle 默认隔离级别为「读已提交 Read committed」。 为什么 MySQL/PostgreSQL/Oracle 的默认隔离级别是这样设置的呢？该如何选择正确的隔离级别呢？ 我们针对普通的高并发业务场景做个简单分析： 首先，「脏读」是必须避免的，它会使事务读到错误的数据！最低的「读未提交」级别直接排除 「串行化」的性能太差，也直接排除 只要 SQL 用得对，「不可重复读」问题对业务逻辑的正确性通常并无影响，所以是可以容忍的。 因此一般「读已提交」是最佳的隔离级别，这也是 PostgreSQL/Oracle 将其设为默认隔离级别的原因。 那么为什么 MySQL 这么特立独行，将默认隔离级别提高到了「可重复读」呢？为啥阿里这种大的互联网公司又会把 MySQL 默认的隔离级别改成「读已提交」？ 根据网上查到的资料，这是 MySQL 的历史问题导致的。MySQL 5.0 之前只支持 statement 这种 binlog 格式，此格式在「读已提交」的隔离级别下会出现诸多问题，最明显的就是可能会导致主从数据库的数据不一致。 除了设置默认的隔离级别外，MySQL 还禁止在使用 statement 格式的 binlog 时，使用 READ COMMITTED 作为事务隔离级别，尝试修改隔离级别会报错 Transaction level 'READ-COMMITTED' in InnoDB is not safe for binlog mode 'STATEMENT' 而互联网公司将隔离级别改为「读已提交」的原因也很好理解，正如前文所述「读已提交」是最佳的隔离级别，这样修改能够提升数据库的性能。 「隔离性」的本质其实就是事务的并发控制，不同的隔离级别代表了对并发事务的隔离程度，主要的实现手段是「多版本并发控制 MVCC」与「锁」。 锁机制前面已经简单介绍过了，而 MVCC 其实就是为每个事务创建一个特定隔离级别的快照，这样读写不会互相阻塞，性能就提升了。（MVCC 暂时也是超纲知识，后面再研究吧 emmmm） ANSI SQL-92 对异常现象的分析仍然太过简单了，1995 年新发布的论文 A Critique of ANSI SQL Isolation Levels 丰富和细化了 SQL-92 的内容，定义了六种隔离级别和八种异常现象（有大佬强烈建议通读此论文，重点是文中的快照隔离（Snapshot Isolation, SI）级别）。 ","date":"2022-08-07","objectID":"/posts/consistency-and-consensus-algorithm/:1:1","series":null,"tags":["共识","一致性","分布式","数据库","区块链"],"title":"分布式数据库的一致性问题与共识算法","uri":"/posts/consistency-and-consensus-algorithm/#acid-中最复杂的特性---隔离性"},{"categories":["tech"],"content":" 2. 数据一致性 Data Consistency「数据一致性」是指对数据库的每一次读操作都应该读到最新写入的数据，或者直接报错。 对单机数据库而言「数据一致性」往往不是问题，因为它通常只有一份保存在磁盘或内存中的数据。 但是在分布式系统中，为了数据安全性或者为了性能，往往每一份数据都在多个节点上存有其副本，这就引出了数据副本们的一致性问题。 因此，我们通常谈论的「数据一致性」就是指分布式系统的「数据一致性」。 CAP 原则是分布式系统领域一个著名的理论，它告诉我们在分布式系统中如下三种属性不可能全部达成，因此也被称作「CAP 不可能三角」： 数据一致性 Data Consistency：客户端的每次读操作，不管访问系统的哪个节点，要么读到的都是同一份最新写入的数据，要么读取失败 强调数据完全正确 可用性 Availability：任何来自客户端的请求，不管访问哪个非故障节点，都能得到响应数据，但不保证是同一份最新数据 强调的是服务可用，但不保证数据正确 分区容错性 Partition Tolerance：即使节点之间出现了任意数量的消息丢失或者高延迟，系统仍能正常运行 就是说网络丢包或延迟会导致系统被分成多个 Partition，系统能够容忍这种情况 为了保证分区容错性 P，考虑当分布式系统因为网络问题被割裂成多个分区时，每个分区只有如下两种选择，A 跟 C 必须牺牲掉其中之一： 取消操作并拒绝提供服务，这降低了可用性，但是能确保数据一致性 继续处理请求，这确保了可用性，但是数据一致性就无法保证了 如果系统的多个分区都在同时提供服务，导致数据不一致并且存在冲突无法合并，这就被称为分布式系统的「脑裂」，显然任何分布式系统都不会希望发生「脑裂」。 因为分布式系统与单机系统不同，它涉及到多节点间的网络通讯和交互，但是只要有网络交互就一定会有延迟和数据丢失，节点间的分区故障是很有可能发生的。因此为了正常运行，P 是分布式系统必须保证的特性，在出现分区故障时，为了 P 只能牺牲掉 A 或者 C。 工程上是要 AP 还是 CP，得视情况而定： Etcd/Zookeeper/Consul: 它们通常被用于存储系统运行的关键元信息，每次读，都要能读取到最新数据。因此它们实现了 CP，牺牲了 A DynamoDB/Cassandra/MongoDB：不要求数据一致性，一段时间内用旧的缓存问题也不大，但是要求可用性，因此应该实现 AP，牺牲掉 C 数据一致性模型分布式系统中，多副本数据上的一组读写策略，被称为「（数据）一致性模型 Consistency Model」。 一致性模型数量很多，让人难以分辨。 为了便于理解，我们先从状态视角出发区分一下强一致与弱一致的概念，在这个的基础上再从操作视角去理解这众多的一致性模型。 1. 状态视角 - 强一致与弱一致我们首先把整个分布式系统看作一个白盒，从状态视角看，任何变更操作后，分布式系统的多个数据副本只有如下三种状态： 在某些条件下，各副本状态不一致的现象只是暂时的，后续还会转换到一致的状态，这被称为「弱一致」； 这通常是使用异步复制来同步各副本的状态。 相对的说，如果系统各副本不存在「不一致」这种状态，只要变更操作成功数据就一定完全一致，那它就被称为「强一致」。 这要求所有副本之间的数据更新必须完全同步，就必须使用全同步复制。 永远不会一致：这在分布式系统中就是 bug 了，也被称为「脑裂」。 上面描述的是整个系统的客观、实际状态，但对于绝大部分用户而言分布式系统更多的是一个黑盒，因此更流行的是基于「黑盒」的分类方式，它根据系统的对外状态将系统分成两种类型： 强一致：指对系统的任何节点/进程，写操作完成后，任何用户对任何节点的后续访问都能读到新的值。就好像系统只存在一个副本一样。 最常用算法是 Raft/Paxos，它们的写操作只要求超过半数节点写入成功，因此写入完成时，内部状态实际是不一致的，但是对它进行读写，效果跟「全同步复制」没有区别。 弱一致：指对系统的任何节点/进程，写操作完成后，后续的任何访问可能会拿到的值是不确定的，但经过一段时间后，后续的任何访问都能读到新的值。 弱一致是非常模糊的定义。如果我们把最终所有用户都能访问到新的值被称为「系统收敛」，系统收敛的用时可以有明确边界，也可以没有。系统收敛前的访问行为可以有明确规范，也可以不存在规范。一切都看具体系统的实现。 如果系统能够在有限时间内收敛，那它就是「最终一致」，否则可以认为它是「不一致」。 为了实际需要，数据库专家对系统收敛之前的读写效果进行各种限制，对系统的收敛时间进行各种限制，得到了许多一致性模型。 2. 操作视角 - 多种一致性模型从每个客户端的操作角度看，有四种一致性模型： 写后读一致性 Read after Write Consistency：也被称作「读自己所写一致性」，即自己写完数据版本 N 后，后续读到的版本一定不小于版本 N。 它解决的问题：A 发了个抖音视频，刷新页面后却莫名其妙消失了（旧版本），几分钟后才重新刷出来。 实现方式之一：为写入者单独添加一个读取规则，他的读都由已更新其写入数据的副本来处理。 单调读一致性 Monotonic Read Consistency：保证多个读操作的顺序，即客户端一旦读到某个数据版本 N，后续不会读到比 N 更低的版本。 它解决的问题是：A 删除了一个抖音视频，可多次刷新，偶尔刷不到视频，偶尔又能刷到被删除视频（旧版本），几分钟后才彻底被删除。 实现方式之一：为每个用户的读都创建一个副本映射，后续的读都由一个固定的副本处理，避免随机切换副本而读到更老的值。 单调写一致性 Monotonic Write Consistency：保证多个写操作的顺序，即客户端对同一数据的两次写入操作，一定按其被提交的顺序被执行。 读后写一致性 Write after Read Consistency：读后写一致性，保证一个客户端读到数据版本 N 后（可能是其他客户端写入的），随后对同一数据的写操作必须要在版本号大于等于 N 的副本上执行。 上述四个一致性模型都只从每个客户端自身的角度定义规则，比较片面，因此它们都是「弱一致模型」。 而不考虑客户端，直接从所有数据库用户的操作视角看，有如下几种一致性模型： 线性一致性 Linearizability：线性一致性利用了事件的提交顺序，它保证任何读操作得到的数据，其顺序跟读/写事件的提交顺序一致。 简单的说它要求整个系统表现得像只存在一个副本，所有操作的执行结果就跟这些事件按提交顺序完全串行执行一样。这实际也是在说所有并发事件都是原子的，一旦互相之间存在冲突，就一定得按顺序执行，因此也有人称它为「原子一致性」。 线性一致性，完全等价于系统对外状态的「强一致性」 线性一致性的系统是完全确定性的 实现方式：需要一个所有节点都一致的「全局时钟」，这样才可以对所有事件进行全局排序。 大多数分布式数据库如 TiDB/Etcd 都是通过 NTP 等协议进行单点授时与同步实现的全局时钟。 有全球化部署需要的 Google Spanner 是使用 GPS + 原子钟实现的全局时钟 TrueTime，全局误差可以控制在 7ms 以内。 局限性：根据爱因斯坦相对论，「时间是相对的」，实际上并不存在绝对的时间，因此线性一致性只在经典物理学范围内适用。 顺序一致性 Sequentially Consistent：顺序一致性最早是 Leslie Lamport 用来描述多核 CPU 的行为的，在分布式系统领域用得较少。 顺序一致性的要求有两点： 从单个进程（副本）的角度看，所有指令的执行顺序跟代码逻辑的顺序完全一致。 从所有的处理器（整个分布式系统）角度看，写操作不必立即对所有用户可见，但是所有副本必须以相同的顺序接收这些写操作。 顺序一致性和线性一致性都是要找到一个满足「写后读」的一组操作历史，差异在于线性一致性要求严格的时间序，而顺序一致性只要求满足代码的逻辑顺序，而其他代码逻辑未定义的事件顺序（比如多副本上各事件之间的顺序），具体是什么样的顺序无所谓，只要所有副本看到的事件顺序都相同就行。 顺序一致性并不能提供「确定性」，相同的两次操作仍然可能得到不同的事件顺序。 实现方式：因为不要求严格的全局时间序，它就不需要一个全局时钟了，但实际上为了满足全局的确定性，仍然需要一些复杂的操作。 因果一致性 Causal Consistency：线性一致性的全局时钟有其局限性，而因果一致性基于写事件的「偏序关系」提出了「逻辑时钟」的概念，并保证读顺序与逻辑时钟上的写事件顺序一致。 写事件的「偏序关系」关系是指，至少部分事件（比如一个节点内部的事件）是可以使用本地时钟直接排序的，而节点之间发生通讯时，接收方的事件一定晚于调用方的事件。基于这一点可以实现一个「逻辑时钟」，但逻辑时钟的缺点在于，如果某两个事件不存在相关性，那逻辑时钟给出的顺序就没有任何意义。 多数观点认为，因果一致性弱于线性一致性，但在并发性能上具有优势，也足以处理多数的异常现象，所以因果一致性也在工业界得到了应用。 CockroachDB 和 YugabyteDB 都在设计中采用了逻辑混合时钟（Hybrid Logical Clocks），这个方案源自 Lamport 的逻辑时钟，也取得了不错的效果 前缀一致性 Consistent Prefix：副本之间的同步过程中，会存在一些副本接收数据的顺序并不一致。「前缀一致性」是说所有用户读到的数据顺序的前缀永远是一致的。 「前缀」是指程序在执行写操作时，需要显式声明其「前缀」事件，这样每个事件就","date":"2022-08-07","objectID":"/posts/consistency-and-consensus-algorithm/:1:2","series":null,"tags":["共识","一致性","分布式","数据库","区块链"],"title":"分布式数据库的一致性问题与共识算法","uri":"/posts/consistency-and-consensus-algorithm/#2-数据一致性-data-consistency"},{"categories":["tech"],"content":" 2. 数据一致性 Data Consistency「数据一致性」是指对数据库的每一次读操作都应该读到最新写入的数据，或者直接报错。 对单机数据库而言「数据一致性」往往不是问题，因为它通常只有一份保存在磁盘或内存中的数据。 但是在分布式系统中，为了数据安全性或者为了性能，往往每一份数据都在多个节点上存有其副本，这就引出了数据副本们的一致性问题。 因此，我们通常谈论的「数据一致性」就是指分布式系统的「数据一致性」。 CAP 原则是分布式系统领域一个著名的理论，它告诉我们在分布式系统中如下三种属性不可能全部达成，因此也被称作「CAP 不可能三角」： 数据一致性 Data Consistency：客户端的每次读操作，不管访问系统的哪个节点，要么读到的都是同一份最新写入的数据，要么读取失败 强调数据完全正确 可用性 Availability：任何来自客户端的请求，不管访问哪个非故障节点，都能得到响应数据，但不保证是同一份最新数据 强调的是服务可用，但不保证数据正确 分区容错性 Partition Tolerance：即使节点之间出现了任意数量的消息丢失或者高延迟，系统仍能正常运行 就是说网络丢包或延迟会导致系统被分成多个 Partition，系统能够容忍这种情况 为了保证分区容错性 P，考虑当分布式系统因为网络问题被割裂成多个分区时，每个分区只有如下两种选择，A 跟 C 必须牺牲掉其中之一： 取消操作并拒绝提供服务，这降低了可用性，但是能确保数据一致性 继续处理请求，这确保了可用性，但是数据一致性就无法保证了 如果系统的多个分区都在同时提供服务，导致数据不一致并且存在冲突无法合并，这就被称为分布式系统的「脑裂」，显然任何分布式系统都不会希望发生「脑裂」。 因为分布式系统与单机系统不同，它涉及到多节点间的网络通讯和交互，但是只要有网络交互就一定会有延迟和数据丢失，节点间的分区故障是很有可能发生的。因此为了正常运行，P 是分布式系统必须保证的特性，在出现分区故障时，为了 P 只能牺牲掉 A 或者 C。 工程上是要 AP 还是 CP，得视情况而定： Etcd/Zookeeper/Consul: 它们通常被用于存储系统运行的关键元信息，每次读，都要能读取到最新数据。因此它们实现了 CP，牺牲了 A DynamoDB/Cassandra/MongoDB：不要求数据一致性，一段时间内用旧的缓存问题也不大，但是要求可用性，因此应该实现 AP，牺牲掉 C 数据一致性模型分布式系统中，多副本数据上的一组读写策略，被称为「（数据）一致性模型 Consistency Model」。 一致性模型数量很多，让人难以分辨。 为了便于理解，我们先从状态视角出发区分一下强一致与弱一致的概念，在这个的基础上再从操作视角去理解这众多的一致性模型。 1. 状态视角 - 强一致与弱一致我们首先把整个分布式系统看作一个白盒，从状态视角看，任何变更操作后，分布式系统的多个数据副本只有如下三种状态： 在某些条件下，各副本状态不一致的现象只是暂时的，后续还会转换到一致的状态，这被称为「弱一致」； 这通常是使用异步复制来同步各副本的状态。 相对的说，如果系统各副本不存在「不一致」这种状态，只要变更操作成功数据就一定完全一致，那它就被称为「强一致」。 这要求所有副本之间的数据更新必须完全同步，就必须使用全同步复制。 永远不会一致：这在分布式系统中就是 bug 了，也被称为「脑裂」。 上面描述的是整个系统的客观、实际状态，但对于绝大部分用户而言分布式系统更多的是一个黑盒，因此更流行的是基于「黑盒」的分类方式，它根据系统的对外状态将系统分成两种类型： 强一致：指对系统的任何节点/进程，写操作完成后，任何用户对任何节点的后续访问都能读到新的值。就好像系统只存在一个副本一样。 最常用算法是 Raft/Paxos，它们的写操作只要求超过半数节点写入成功，因此写入完成时，内部状态实际是不一致的，但是对它进行读写，效果跟「全同步复制」没有区别。 弱一致：指对系统的任何节点/进程，写操作完成后，后续的任何访问可能会拿到的值是不确定的，但经过一段时间后，后续的任何访问都能读到新的值。 弱一致是非常模糊的定义。如果我们把最终所有用户都能访问到新的值被称为「系统收敛」，系统收敛的用时可以有明确边界，也可以没有。系统收敛前的访问行为可以有明确规范，也可以不存在规范。一切都看具体系统的实现。 如果系统能够在有限时间内收敛，那它就是「最终一致」，否则可以认为它是「不一致」。 为了实际需要，数据库专家对系统收敛之前的读写效果进行各种限制，对系统的收敛时间进行各种限制，得到了许多一致性模型。 2. 操作视角 - 多种一致性模型从每个客户端的操作角度看，有四种一致性模型： 写后读一致性 Read after Write Consistency：也被称作「读自己所写一致性」，即自己写完数据版本 N 后，后续读到的版本一定不小于版本 N。 它解决的问题：A 发了个抖音视频，刷新页面后却莫名其妙消失了（旧版本），几分钟后才重新刷出来。 实现方式之一：为写入者单独添加一个读取规则，他的读都由已更新其写入数据的副本来处理。 单调读一致性 Monotonic Read Consistency：保证多个读操作的顺序，即客户端一旦读到某个数据版本 N，后续不会读到比 N 更低的版本。 它解决的问题是：A 删除了一个抖音视频，可多次刷新，偶尔刷不到视频，偶尔又能刷到被删除视频（旧版本），几分钟后才彻底被删除。 实现方式之一：为每个用户的读都创建一个副本映射，后续的读都由一个固定的副本处理，避免随机切换副本而读到更老的值。 单调写一致性 Monotonic Write Consistency：保证多个写操作的顺序，即客户端对同一数据的两次写入操作，一定按其被提交的顺序被执行。 读后写一致性 Write after Read Consistency：读后写一致性，保证一个客户端读到数据版本 N 后（可能是其他客户端写入的），随后对同一数据的写操作必须要在版本号大于等于 N 的副本上执行。 上述四个一致性模型都只从每个客户端自身的角度定义规则，比较片面，因此它们都是「弱一致模型」。 而不考虑客户端，直接从所有数据库用户的操作视角看，有如下几种一致性模型： 线性一致性 Linearizability：线性一致性利用了事件的提交顺序，它保证任何读操作得到的数据，其顺序跟读/写事件的提交顺序一致。 简单的说它要求整个系统表现得像只存在一个副本，所有操作的执行结果就跟这些事件按提交顺序完全串行执行一样。这实际也是在说所有并发事件都是原子的，一旦互相之间存在冲突，就一定得按顺序执行，因此也有人称它为「原子一致性」。 线性一致性，完全等价于系统对外状态的「强一致性」 线性一致性的系统是完全确定性的 实现方式：需要一个所有节点都一致的「全局时钟」，这样才可以对所有事件进行全局排序。 大多数分布式数据库如 TiDB/Etcd 都是通过 NTP 等协议进行单点授时与同步实现的全局时钟。 有全球化部署需要的 Google Spanner 是使用 GPS + 原子钟实现的全局时钟 TrueTime，全局误差可以控制在 7ms 以内。 局限性：根据爱因斯坦相对论，「时间是相对的」，实际上并不存在绝对的时间，因此线性一致性只在经典物理学范围内适用。 顺序一致性 Sequentially Consistent：顺序一致性最早是 Leslie Lamport 用来描述多核 CPU 的行为的，在分布式系统领域用得较少。 顺序一致性的要求有两点： 从单个进程（副本）的角度看，所有指令的执行顺序跟代码逻辑的顺序完全一致。 从所有的处理器（整个分布式系统）角度看，写操作不必立即对所有用户可见，但是所有副本必须以相同的顺序接收这些写操作。 顺序一致性和线性一致性都是要找到一个满足「写后读」的一组操作历史，差异在于线性一致性要求严格的时间序，而顺序一致性只要求满足代码的逻辑顺序，而其他代码逻辑未定义的事件顺序（比如多副本上各事件之间的顺序），具体是什么样的顺序无所谓，只要所有副本看到的事件顺序都相同就行。 顺序一致性并不能提供「确定性」，相同的两次操作仍然可能得到不同的事件顺序。 实现方式：因为不要求严格的全局时间序，它就不需要一个全局时钟了，但实际上为了满足全局的确定性，仍然需要一些复杂的操作。 因果一致性 Causal Consistency：线性一致性的全局时钟有其局限性，而因果一致性基于写事件的「偏序关系」提出了「逻辑时钟」的概念，并保证读顺序与逻辑时钟上的写事件顺序一致。 写事件的「偏序关系」关系是指，至少部分事件（比如一个节点内部的事件）是可以使用本地时钟直接排序的，而节点之间发生通讯时，接收方的事件一定晚于调用方的事件。基于这一点可以实现一个「逻辑时钟」，但逻辑时钟的缺点在于，如果某两个事件不存在相关性，那逻辑时钟给出的顺序就没有任何意义。 多数观点认为，因果一致性弱于线性一致性，但在并发性能上具有优势，也足以处理多数的异常现象，所以因果一致性也在工业界得到了应用。 CockroachDB 和 YugabyteDB 都在设计中采用了逻辑混合时钟（Hybrid Logical Clocks），这个方案源自 Lamport 的逻辑时钟，也取得了不错的效果 前缀一致性 Consistent Prefix：副本之间的同步过程中，会存在一些副本接收数据的顺序并不一致。「前缀一致性」是说所有用户读到的数据顺序的前缀永远是一致的。 「前缀」是指程序在执行写操作时，需要显式声明其「前缀」事件，这样每个事件就","date":"2022-08-07","objectID":"/posts/consistency-and-consensus-algorithm/:1:2","series":null,"tags":["共识","一致性","分布式","数据库","区块链"],"title":"分布式数据库的一致性问题与共识算法","uri":"/posts/consistency-and-consensus-algorithm/#数据一致性模型"},{"categories":["tech"],"content":" 2. 数据一致性 Data Consistency「数据一致性」是指对数据库的每一次读操作都应该读到最新写入的数据，或者直接报错。 对单机数据库而言「数据一致性」往往不是问题，因为它通常只有一份保存在磁盘或内存中的数据。 但是在分布式系统中，为了数据安全性或者为了性能，往往每一份数据都在多个节点上存有其副本，这就引出了数据副本们的一致性问题。 因此，我们通常谈论的「数据一致性」就是指分布式系统的「数据一致性」。 CAP 原则是分布式系统领域一个著名的理论，它告诉我们在分布式系统中如下三种属性不可能全部达成，因此也被称作「CAP 不可能三角」： 数据一致性 Data Consistency：客户端的每次读操作，不管访问系统的哪个节点，要么读到的都是同一份最新写入的数据，要么读取失败 强调数据完全正确 可用性 Availability：任何来自客户端的请求，不管访问哪个非故障节点，都能得到响应数据，但不保证是同一份最新数据 强调的是服务可用，但不保证数据正确 分区容错性 Partition Tolerance：即使节点之间出现了任意数量的消息丢失或者高延迟，系统仍能正常运行 就是说网络丢包或延迟会导致系统被分成多个 Partition，系统能够容忍这种情况 为了保证分区容错性 P，考虑当分布式系统因为网络问题被割裂成多个分区时，每个分区只有如下两种选择，A 跟 C 必须牺牲掉其中之一： 取消操作并拒绝提供服务，这降低了可用性，但是能确保数据一致性 继续处理请求，这确保了可用性，但是数据一致性就无法保证了 如果系统的多个分区都在同时提供服务，导致数据不一致并且存在冲突无法合并，这就被称为分布式系统的「脑裂」，显然任何分布式系统都不会希望发生「脑裂」。 因为分布式系统与单机系统不同，它涉及到多节点间的网络通讯和交互，但是只要有网络交互就一定会有延迟和数据丢失，节点间的分区故障是很有可能发生的。因此为了正常运行，P 是分布式系统必须保证的特性，在出现分区故障时，为了 P 只能牺牲掉 A 或者 C。 工程上是要 AP 还是 CP，得视情况而定： Etcd/Zookeeper/Consul: 它们通常被用于存储系统运行的关键元信息，每次读，都要能读取到最新数据。因此它们实现了 CP，牺牲了 A DynamoDB/Cassandra/MongoDB：不要求数据一致性，一段时间内用旧的缓存问题也不大，但是要求可用性，因此应该实现 AP，牺牲掉 C 数据一致性模型分布式系统中，多副本数据上的一组读写策略，被称为「（数据）一致性模型 Consistency Model」。 一致性模型数量很多，让人难以分辨。 为了便于理解，我们先从状态视角出发区分一下强一致与弱一致的概念，在这个的基础上再从操作视角去理解这众多的一致性模型。 1. 状态视角 - 强一致与弱一致我们首先把整个分布式系统看作一个白盒，从状态视角看，任何变更操作后，分布式系统的多个数据副本只有如下三种状态： 在某些条件下，各副本状态不一致的现象只是暂时的，后续还会转换到一致的状态，这被称为「弱一致」； 这通常是使用异步复制来同步各副本的状态。 相对的说，如果系统各副本不存在「不一致」这种状态，只要变更操作成功数据就一定完全一致，那它就被称为「强一致」。 这要求所有副本之间的数据更新必须完全同步，就必须使用全同步复制。 永远不会一致：这在分布式系统中就是 bug 了，也被称为「脑裂」。 上面描述的是整个系统的客观、实际状态，但对于绝大部分用户而言分布式系统更多的是一个黑盒，因此更流行的是基于「黑盒」的分类方式，它根据系统的对外状态将系统分成两种类型： 强一致：指对系统的任何节点/进程，写操作完成后，任何用户对任何节点的后续访问都能读到新的值。就好像系统只存在一个副本一样。 最常用算法是 Raft/Paxos，它们的写操作只要求超过半数节点写入成功，因此写入完成时，内部状态实际是不一致的，但是对它进行读写，效果跟「全同步复制」没有区别。 弱一致：指对系统的任何节点/进程，写操作完成后，后续的任何访问可能会拿到的值是不确定的，但经过一段时间后，后续的任何访问都能读到新的值。 弱一致是非常模糊的定义。如果我们把最终所有用户都能访问到新的值被称为「系统收敛」，系统收敛的用时可以有明确边界，也可以没有。系统收敛前的访问行为可以有明确规范，也可以不存在规范。一切都看具体系统的实现。 如果系统能够在有限时间内收敛，那它就是「最终一致」，否则可以认为它是「不一致」。 为了实际需要，数据库专家对系统收敛之前的读写效果进行各种限制，对系统的收敛时间进行各种限制，得到了许多一致性模型。 2. 操作视角 - 多种一致性模型从每个客户端的操作角度看，有四种一致性模型： 写后读一致性 Read after Write Consistency：也被称作「读自己所写一致性」，即自己写完数据版本 N 后，后续读到的版本一定不小于版本 N。 它解决的问题：A 发了个抖音视频，刷新页面后却莫名其妙消失了（旧版本），几分钟后才重新刷出来。 实现方式之一：为写入者单独添加一个读取规则，他的读都由已更新其写入数据的副本来处理。 单调读一致性 Monotonic Read Consistency：保证多个读操作的顺序，即客户端一旦读到某个数据版本 N，后续不会读到比 N 更低的版本。 它解决的问题是：A 删除了一个抖音视频，可多次刷新，偶尔刷不到视频，偶尔又能刷到被删除视频（旧版本），几分钟后才彻底被删除。 实现方式之一：为每个用户的读都创建一个副本映射，后续的读都由一个固定的副本处理，避免随机切换副本而读到更老的值。 单调写一致性 Monotonic Write Consistency：保证多个写操作的顺序，即客户端对同一数据的两次写入操作，一定按其被提交的顺序被执行。 读后写一致性 Write after Read Consistency：读后写一致性，保证一个客户端读到数据版本 N 后（可能是其他客户端写入的），随后对同一数据的写操作必须要在版本号大于等于 N 的副本上执行。 上述四个一致性模型都只从每个客户端自身的角度定义规则，比较片面，因此它们都是「弱一致模型」。 而不考虑客户端，直接从所有数据库用户的操作视角看，有如下几种一致性模型： 线性一致性 Linearizability：线性一致性利用了事件的提交顺序，它保证任何读操作得到的数据，其顺序跟读/写事件的提交顺序一致。 简单的说它要求整个系统表现得像只存在一个副本，所有操作的执行结果就跟这些事件按提交顺序完全串行执行一样。这实际也是在说所有并发事件都是原子的，一旦互相之间存在冲突，就一定得按顺序执行，因此也有人称它为「原子一致性」。 线性一致性，完全等价于系统对外状态的「强一致性」 线性一致性的系统是完全确定性的 实现方式：需要一个所有节点都一致的「全局时钟」，这样才可以对所有事件进行全局排序。 大多数分布式数据库如 TiDB/Etcd 都是通过 NTP 等协议进行单点授时与同步实现的全局时钟。 有全球化部署需要的 Google Spanner 是使用 GPS + 原子钟实现的全局时钟 TrueTime，全局误差可以控制在 7ms 以内。 局限性：根据爱因斯坦相对论，「时间是相对的」，实际上并不存在绝对的时间，因此线性一致性只在经典物理学范围内适用。 顺序一致性 Sequentially Consistent：顺序一致性最早是 Leslie Lamport 用来描述多核 CPU 的行为的，在分布式系统领域用得较少。 顺序一致性的要求有两点： 从单个进程（副本）的角度看，所有指令的执行顺序跟代码逻辑的顺序完全一致。 从所有的处理器（整个分布式系统）角度看，写操作不必立即对所有用户可见，但是所有副本必须以相同的顺序接收这些写操作。 顺序一致性和线性一致性都是要找到一个满足「写后读」的一组操作历史，差异在于线性一致性要求严格的时间序，而顺序一致性只要求满足代码的逻辑顺序，而其他代码逻辑未定义的事件顺序（比如多副本上各事件之间的顺序），具体是什么样的顺序无所谓，只要所有副本看到的事件顺序都相同就行。 顺序一致性并不能提供「确定性」，相同的两次操作仍然可能得到不同的事件顺序。 实现方式：因为不要求严格的全局时间序，它就不需要一个全局时钟了，但实际上为了满足全局的确定性，仍然需要一些复杂的操作。 因果一致性 Causal Consistency：线性一致性的全局时钟有其局限性，而因果一致性基于写事件的「偏序关系」提出了「逻辑时钟」的概念，并保证读顺序与逻辑时钟上的写事件顺序一致。 写事件的「偏序关系」关系是指，至少部分事件（比如一个节点内部的事件）是可以使用本地时钟直接排序的，而节点之间发生通讯时，接收方的事件一定晚于调用方的事件。基于这一点可以实现一个「逻辑时钟」，但逻辑时钟的缺点在于，如果某两个事件不存在相关性，那逻辑时钟给出的顺序就没有任何意义。 多数观点认为，因果一致性弱于线性一致性，但在并发性能上具有优势，也足以处理多数的异常现象，所以因果一致性也在工业界得到了应用。 CockroachDB 和 YugabyteDB 都在设计中采用了逻辑混合时钟（Hybrid Logical Clocks），这个方案源自 Lamport 的逻辑时钟，也取得了不错的效果 前缀一致性 Consistent Prefix：副本之间的同步过程中，会存在一些副本接收数据的顺序并不一致。「前缀一致性」是说所有用户读到的数据顺序的前缀永远是一致的。 「前缀」是指程序在执行写操作时，需要显式声明其「前缀」事件，这样每个事件就","date":"2022-08-07","objectID":"/posts/consistency-and-consensus-algorithm/:1:2","series":null,"tags":["共识","一致性","分布式","数据库","区块链"],"title":"分布式数据库的一致性问题与共识算法","uri":"/posts/consistency-and-consensus-algorithm/#1-状态视角---强一致与弱一致"},{"categories":["tech"],"content":" 2. 数据一致性 Data Consistency「数据一致性」是指对数据库的每一次读操作都应该读到最新写入的数据，或者直接报错。 对单机数据库而言「数据一致性」往往不是问题，因为它通常只有一份保存在磁盘或内存中的数据。 但是在分布式系统中，为了数据安全性或者为了性能，往往每一份数据都在多个节点上存有其副本，这就引出了数据副本们的一致性问题。 因此，我们通常谈论的「数据一致性」就是指分布式系统的「数据一致性」。 CAP 原则是分布式系统领域一个著名的理论，它告诉我们在分布式系统中如下三种属性不可能全部达成，因此也被称作「CAP 不可能三角」： 数据一致性 Data Consistency：客户端的每次读操作，不管访问系统的哪个节点，要么读到的都是同一份最新写入的数据，要么读取失败 强调数据完全正确 可用性 Availability：任何来自客户端的请求，不管访问哪个非故障节点，都能得到响应数据，但不保证是同一份最新数据 强调的是服务可用，但不保证数据正确 分区容错性 Partition Tolerance：即使节点之间出现了任意数量的消息丢失或者高延迟，系统仍能正常运行 就是说网络丢包或延迟会导致系统被分成多个 Partition，系统能够容忍这种情况 为了保证分区容错性 P，考虑当分布式系统因为网络问题被割裂成多个分区时，每个分区只有如下两种选择，A 跟 C 必须牺牲掉其中之一： 取消操作并拒绝提供服务，这降低了可用性，但是能确保数据一致性 继续处理请求，这确保了可用性，但是数据一致性就无法保证了 如果系统的多个分区都在同时提供服务，导致数据不一致并且存在冲突无法合并，这就被称为分布式系统的「脑裂」，显然任何分布式系统都不会希望发生「脑裂」。 因为分布式系统与单机系统不同，它涉及到多节点间的网络通讯和交互，但是只要有网络交互就一定会有延迟和数据丢失，节点间的分区故障是很有可能发生的。因此为了正常运行，P 是分布式系统必须保证的特性，在出现分区故障时，为了 P 只能牺牲掉 A 或者 C。 工程上是要 AP 还是 CP，得视情况而定： Etcd/Zookeeper/Consul: 它们通常被用于存储系统运行的关键元信息，每次读，都要能读取到最新数据。因此它们实现了 CP，牺牲了 A DynamoDB/Cassandra/MongoDB：不要求数据一致性，一段时间内用旧的缓存问题也不大，但是要求可用性，因此应该实现 AP，牺牲掉 C 数据一致性模型分布式系统中，多副本数据上的一组读写策略，被称为「（数据）一致性模型 Consistency Model」。 一致性模型数量很多，让人难以分辨。 为了便于理解，我们先从状态视角出发区分一下强一致与弱一致的概念，在这个的基础上再从操作视角去理解这众多的一致性模型。 1. 状态视角 - 强一致与弱一致我们首先把整个分布式系统看作一个白盒，从状态视角看，任何变更操作后，分布式系统的多个数据副本只有如下三种状态： 在某些条件下，各副本状态不一致的现象只是暂时的，后续还会转换到一致的状态，这被称为「弱一致」； 这通常是使用异步复制来同步各副本的状态。 相对的说，如果系统各副本不存在「不一致」这种状态，只要变更操作成功数据就一定完全一致，那它就被称为「强一致」。 这要求所有副本之间的数据更新必须完全同步，就必须使用全同步复制。 永远不会一致：这在分布式系统中就是 bug 了，也被称为「脑裂」。 上面描述的是整个系统的客观、实际状态，但对于绝大部分用户而言分布式系统更多的是一个黑盒，因此更流行的是基于「黑盒」的分类方式，它根据系统的对外状态将系统分成两种类型： 强一致：指对系统的任何节点/进程，写操作完成后，任何用户对任何节点的后续访问都能读到新的值。就好像系统只存在一个副本一样。 最常用算法是 Raft/Paxos，它们的写操作只要求超过半数节点写入成功，因此写入完成时，内部状态实际是不一致的，但是对它进行读写，效果跟「全同步复制」没有区别。 弱一致：指对系统的任何节点/进程，写操作完成后，后续的任何访问可能会拿到的值是不确定的，但经过一段时间后，后续的任何访问都能读到新的值。 弱一致是非常模糊的定义。如果我们把最终所有用户都能访问到新的值被称为「系统收敛」，系统收敛的用时可以有明确边界，也可以没有。系统收敛前的访问行为可以有明确规范，也可以不存在规范。一切都看具体系统的实现。 如果系统能够在有限时间内收敛，那它就是「最终一致」，否则可以认为它是「不一致」。 为了实际需要，数据库专家对系统收敛之前的读写效果进行各种限制，对系统的收敛时间进行各种限制，得到了许多一致性模型。 2. 操作视角 - 多种一致性模型从每个客户端的操作角度看，有四种一致性模型： 写后读一致性 Read after Write Consistency：也被称作「读自己所写一致性」，即自己写完数据版本 N 后，后续读到的版本一定不小于版本 N。 它解决的问题：A 发了个抖音视频，刷新页面后却莫名其妙消失了（旧版本），几分钟后才重新刷出来。 实现方式之一：为写入者单独添加一个读取规则，他的读都由已更新其写入数据的副本来处理。 单调读一致性 Monotonic Read Consistency：保证多个读操作的顺序，即客户端一旦读到某个数据版本 N，后续不会读到比 N 更低的版本。 它解决的问题是：A 删除了一个抖音视频，可多次刷新，偶尔刷不到视频，偶尔又能刷到被删除视频（旧版本），几分钟后才彻底被删除。 实现方式之一：为每个用户的读都创建一个副本映射，后续的读都由一个固定的副本处理，避免随机切换副本而读到更老的值。 单调写一致性 Monotonic Write Consistency：保证多个写操作的顺序，即客户端对同一数据的两次写入操作，一定按其被提交的顺序被执行。 读后写一致性 Write after Read Consistency：读后写一致性，保证一个客户端读到数据版本 N 后（可能是其他客户端写入的），随后对同一数据的写操作必须要在版本号大于等于 N 的副本上执行。 上述四个一致性模型都只从每个客户端自身的角度定义规则，比较片面，因此它们都是「弱一致模型」。 而不考虑客户端，直接从所有数据库用户的操作视角看，有如下几种一致性模型： 线性一致性 Linearizability：线性一致性利用了事件的提交顺序，它保证任何读操作得到的数据，其顺序跟读/写事件的提交顺序一致。 简单的说它要求整个系统表现得像只存在一个副本，所有操作的执行结果就跟这些事件按提交顺序完全串行执行一样。这实际也是在说所有并发事件都是原子的，一旦互相之间存在冲突，就一定得按顺序执行，因此也有人称它为「原子一致性」。 线性一致性，完全等价于系统对外状态的「强一致性」 线性一致性的系统是完全确定性的 实现方式：需要一个所有节点都一致的「全局时钟」，这样才可以对所有事件进行全局排序。 大多数分布式数据库如 TiDB/Etcd 都是通过 NTP 等协议进行单点授时与同步实现的全局时钟。 有全球化部署需要的 Google Spanner 是使用 GPS + 原子钟实现的全局时钟 TrueTime，全局误差可以控制在 7ms 以内。 局限性：根据爱因斯坦相对论，「时间是相对的」，实际上并不存在绝对的时间，因此线性一致性只在经典物理学范围内适用。 顺序一致性 Sequentially Consistent：顺序一致性最早是 Leslie Lamport 用来描述多核 CPU 的行为的，在分布式系统领域用得较少。 顺序一致性的要求有两点： 从单个进程（副本）的角度看，所有指令的执行顺序跟代码逻辑的顺序完全一致。 从所有的处理器（整个分布式系统）角度看，写操作不必立即对所有用户可见，但是所有副本必须以相同的顺序接收这些写操作。 顺序一致性和线性一致性都是要找到一个满足「写后读」的一组操作历史，差异在于线性一致性要求严格的时间序，而顺序一致性只要求满足代码的逻辑顺序，而其他代码逻辑未定义的事件顺序（比如多副本上各事件之间的顺序），具体是什么样的顺序无所谓，只要所有副本看到的事件顺序都相同就行。 顺序一致性并不能提供「确定性」，相同的两次操作仍然可能得到不同的事件顺序。 实现方式：因为不要求严格的全局时间序，它就不需要一个全局时钟了，但实际上为了满足全局的确定性，仍然需要一些复杂的操作。 因果一致性 Causal Consistency：线性一致性的全局时钟有其局限性，而因果一致性基于写事件的「偏序关系」提出了「逻辑时钟」的概念，并保证读顺序与逻辑时钟上的写事件顺序一致。 写事件的「偏序关系」关系是指，至少部分事件（比如一个节点内部的事件）是可以使用本地时钟直接排序的，而节点之间发生通讯时，接收方的事件一定晚于调用方的事件。基于这一点可以实现一个「逻辑时钟」，但逻辑时钟的缺点在于，如果某两个事件不存在相关性，那逻辑时钟给出的顺序就没有任何意义。 多数观点认为，因果一致性弱于线性一致性，但在并发性能上具有优势，也足以处理多数的异常现象，所以因果一致性也在工业界得到了应用。 CockroachDB 和 YugabyteDB 都在设计中采用了逻辑混合时钟（Hybrid Logical Clocks），这个方案源自 Lamport 的逻辑时钟，也取得了不错的效果 前缀一致性 Consistent Prefix：副本之间的同步过程中，会存在一些副本接收数据的顺序并不一致。「前缀一致性」是说所有用户读到的数据顺序的前缀永远是一致的。 「前缀」是指程序在执行写操作时，需要显式声明其「前缀」事件，这样每个事件就","date":"2022-08-07","objectID":"/posts/consistency-and-consensus-algorithm/:1:2","series":null,"tags":["共识","一致性","分布式","数据库","区块链"],"title":"分布式数据库的一致性问题与共识算法","uri":"/posts/consistency-and-consensus-algorithm/#2-操作视角---多种一致性模型"},{"categories":["tech"],"content":" 二、分布式系统的 BASE 与最终一致性BASE 理论： 基本可用 Basically Available：当分布式系统在出现不可预知的故障时，允许损失部分功能的可用性，保障核心功能的可用性 四种实现基本可用的手段：流量削峰、延迟响应、体验降级、过载保护 软状态 Soft state：在柔性事务中，允许系统存在中间状态，且这个中间状态不会影响系统整体可用性。比如，数据库读写分离，写库同步到读库（主库同步到从库）会有一个延时，其实就是一种柔性状态。 最终一致性 Eventually consistent：前面已经说得很详细了，它指对系统的任何节点/进程，写操作完成后，后续的任何访问可能会拿到的值是不确定的，但经过有限的一段时间后，后续的任何访问都能读到新的值。 ACID 与 BASE 实质上是分布式系统实现中的的两个极端： ACID 理论就如它的含义「酸」一样，是 CAP 原则中一致性的边界——最强的一致性，是牺牲掉 A 后达到 CP 的极致。 BASE 翻译过来就是「碱」，它是 CAP 原则中可用性的边界——最高的可用性，最弱的一致性，通过牺牲掉 C 来达到 AP 的极致。 根据 CAP 理论，如果在分布式系统中实现了一致性，可用性必然受到影响。 比如，如果出现一个节点故障，则整个分布式事务的执行都是失败的。实际上，绝大部分场景对一致性要求没那么高，短暂的不一致是能接受的，另外，也基于可用性和并发性能的考虑，建议在开发实现分布式系统，如果不是必须，尽量不要实现事务，可以考虑采用最终一致性。 最终一致性的实现手段： 读时修复：在读取数据时，检测数据的不一致，进行修复 写时修复：在写入数据时，检测数据的不一致，进行修复 异步修复：这个是最常用的方式，通过定时对账，检测副本数据的一致性并修复 在实现最终一致性的时候，还推荐同时实现自定义写一致性级别（比如 All、Quorum、One、Any），许多分布式数据库的最终一致性级别都是可调的。 但是随着 TiDB 等分布式关系数据库的兴起，分布式领域的 BASE 理论实际上正在被 ACID 赶超，ACID 焕发又一春了。 ","date":"2022-08-07","objectID":"/posts/consistency-and-consensus-algorithm/:2:0","series":null,"tags":["共识","一致性","分布式","数据库","区块链"],"title":"分布式数据库的一致性问题与共识算法","uri":"/posts/consistency-and-consensus-algorithm/#二分布式系统的-base-与最终一致性"},{"categories":["tech"],"content":" 三、共识算法共识算法，也被称为一致性协议，是指在分布式系统中多个节点之间对某个提案 Proposal（例如多个事务请求，先执行谁？）达成一致看法的一套流程。 提案的含义在分布式系统中十分宽泛，如多个事件发生的顺序、某个键对应的值、谁是主节点……等等。可以认为任何可以达成一致的信息都是一个提案。 对于分布式系统来讲，各个节点通常都是相同的确定性状态机模型（又称为状态机复制问题，State-Machine Replication），从相同初始状态开始接收相同顺序的指令，则可以保证相同的结果状态。因此，系统中多个节点最关键的是对多个事件的顺序进行共识，即排序。 共识算法是达成数据一致性的一种手段，而且是数据强一致性的必要非充分条件。比如直接使用 Raft 算法，但是允许读取集群的任何节点，只能得到数据的最终一致性，还需要其他手段才能确保强一致性。 ","date":"2022-08-07","objectID":"/posts/consistency-and-consensus-algorithm/:3:0","series":null,"tags":["共识","一致性","分布式","数据库","区块链"],"title":"分布式数据库的一致性问题与共识算法","uri":"/posts/consistency-and-consensus-algorithm/#三共识算法"},{"categories":["tech"],"content":" 拜占庭将军问题与拜占庭容错拜占庭错误是 1982 年兰伯特在《拜占庭将军问题》中提出的一个错误模型，描述了在少数节点不仅存在故障，还存在恶意行为的场景下，能否达成共识这样一个问题，论文描述如下： 9 位拜占庭将军分别率领一支军队要共同围困一座城市，因为这座城市很强大，如果不协调统一将军们的行动策略，部分军队进攻、部分军队撤退会造成围困失败，因此各位将军必须通过投票来达成一致策略，要么一起进攻，要么一起撤退。 因为各位将军分别占据城市的一角，他们只能通过信使互相联系。在协调过程中每位将军都将自己投票“进攻”还是“撤退”的消息通过信使分别通知其他所有将军，这样一来每位将军根据自己的投票和其他将军送过来的投票，就可以知道投票结果，从而决定是进攻还是撤退。 而问题的复杂性就在于：将军中可能出现叛徒，他们不仅可以投票给错误的决策，还可能会选择性地发送投票。假设 9 位将军中有 1 名叛徒，8 位忠诚的将军中出现了 4 人投“进攻”，4 人投“撤退”，这时候叛徒可能故意给 4 名投“进攻”的将军投“进攻”，而给另外 4 名投“撤退”的将军投“撤退”。这样在 4 名投“进攻”的将军看来，投票是 5 人投“进攻”，从而发动进攻；而另外 4 名将军看来是 5 人投“撤退”，从而撤退。这样，一致性就遭到了破坏。 还有一种情况，因为将军之间需要通过信使交流，即便所有的将军都是忠诚的，派出去的信使也可能被敌军截杀，甚至被间谍替换，也就是说将军之间进行交流的信息通道是不能保证可靠性的。所以在没有收到对应将军消息的时候，将军们会默认投一个票，例如“进攻”。 更一般地，在已知有 N 个将军谋反的情况下，其余 M 个忠诚的将军在不受叛徒的影响下能否达成共识？有什么样的前提条件，该如何达成共识？这就是拜占庭将军问题。 如果一个共识算法在一定条件下能够解决拜占庭将军问题，那我们就称这个算法是「拜占庭容错 Byzantine Fault Tolerance（BFT）」算法。 反之如果一个共识算法无法接受任何一个节点作恶，那它就被称为「非拜占庭容错 Crash Fault Tolerance (CFT)」算法。 可以通过简单穷举发现，二忠一叛是无法达成共识的，这个结论结合反证法可证明，拜占庭容错算法要求叛徒的比例必须低于 1/3。 ","date":"2022-08-07","objectID":"/posts/consistency-and-consensus-algorithm/:3:1","series":null,"tags":["共识","一致性","分布式","数据库","区块链"],"title":"分布式数据库的一致性问题与共识算法","uri":"/posts/consistency-and-consensus-algorithm/#拜占庭将军问题与拜占庭容错"},{"categories":["tech"],"content":" 常用共识算法对于「非拜占庭容错 Crash Fault Tolerance (CFT)」的情况，已经存在不少经典的算法，包括 Paxos（1990 年）、Raft（2014 年）及其变种等。这类容错算法往往性能比较好，处理较快，容忍不超过一半的故障节点。 对于「拜占庭容错 Byzantine Fault Tolerance（BFT）」的情况，目前有 PBFT（Practical Byzantine Fault Tolerance，1999 年）为代表的确定性系列算法、PoW（1999 年）为代表的概率算法等算法可选。 确定性算法一旦达成共识就不可逆转，即共识是最终结果； 而概率类算法的共识结果则是临时的，随着时间推移或某种强化，共识结果被推翻的概率越来越小，最终成为事实上结果。 拜占庭类容错算法往往性能较差，容忍不超过 1/3 的故障节点。 此外，XFT（Cross Fault Tolerance，2015 年）等最近提出的改进算法可以提供类似 CFT 的处理响应速度，并能在大多数节点正常工作时提供 BFT 保障。 Algorand 算法（2017 年）基于 PBFT 进行改进，通过引入可验证随机函数解决了提案选择的问题，理论上可以在容忍拜占庭错误的前提下实现更好的性能（1000+ TPS）。 注：实践中，对客户端来说要拿到共识结果需要自行验证，典型地，可访问足够多个服务节点来比对结果，确保获取结果的准确性。 常见共识算法列举如下： 拜占庭容错 一致性 性能 可用性（能容忍多大比例的节点出现故障） 两阶段提交 2PC 否 强一致性 低 低 TCC(try-confirm-cancel) 否 最终一致性 低 低 Paxos 否 强一致性 中 中 ZAB 否 最终一致性 中 中 Raft 否 强一致性 中 中 Gossip 否 最终一致性 高 高 Quorum NWR 否 强一致性 中 中 PBFT 是 N/A 低 中 PoW 是 N/A 低 中 PoS 是 N/A 低 中 PoH 是 N/A 中 中 注：这里虽然列出了 PoW/PoS/PoH 等应用在区块链中的一致性算法，但是它们跟 PBFT 等其他拜占庭容错算法存在很大的区别，后面会给出介绍。 ","date":"2022-08-07","objectID":"/posts/consistency-and-consensus-algorithm/:3:2","series":null,"tags":["共识","一致性","分布式","数据库","区块链"],"title":"分布式数据库的一致性问题与共识算法","uri":"/posts/consistency-and-consensus-algorithm/#常用共识算法"},{"categories":["tech"],"content":" 不同共识算法的应用场景在不可信环境中，因为可能存在恶意行为，就需要使用支持拜占庭容错的共识算法如 PoW/PoS，使系统在存在部分节点作恶的情况下仍然能达成共识。这就是区块链使用 PoW/PoS 算法而不是 Paxos/Raft 算法的原因。 而在企业内网等场景下，可以认为是可信环境，基本不会出现恶意节点或者可以通过 mTLS 等手段进行节点身份认证，这种场景下系统具有故障容错能力就够了，就没必要做到拜占庭容错，因此常用 Raft/Paxos 等算法。 ","date":"2022-08-07","objectID":"/posts/consistency-and-consensus-algorithm/:3:3","series":null,"tags":["共识","一致性","分布式","数据库","区块链"],"title":"分布式数据库的一致性问题与共识算法","uri":"/posts/consistency-and-consensus-algorithm/#不同共识算法的应用场景"},{"categories":["tech"],"content":" 非拜占庭错误共识算法 Paxos 与 Raft受限于篇幅与笔者精力，这部分暂时跳过…后面可能会写篇新的文章专门介绍 Paxos/Raft 算法。 ","date":"2022-08-07","objectID":"/posts/consistency-and-consensus-algorithm/:3:4","series":null,"tags":["共识","一致性","分布式","数据库","区块链"],"title":"分布式数据库的一致性问题与共识算法","uri":"/posts/consistency-and-consensus-algorithm/#非拜占庭错误共识算法-paxos-与-raft"},{"categories":["tech"],"content":" 能容忍拜占庭错误的 PoW 概率共识算法PoW 即 Proof of Work 工作量证明，指系统为达到某一目标而设置的度量方法。简单理解就是一份证明，用来确认你做过一定量的工作。 监测工作的整个过程通常是极为低效的，而通过对工作的结果进行认证来证明完成了相应的工作量，这个认证流程是非常简单高效的，这就是 PoW 的优势所在。 在 1993 年，Cynthia Dwork 和 Moni Naor 设计了一个系统用于反垃圾邮件、避免资源被滥用，这是 PoW 算法的雏形。其核心思想如下： The main idea is to require a user to compute a moderately hard but not intractable function in order to gain access to the resource, thus preventing frivolous use. 翻译成中文： 其主要思想是要求用户计算一个中等难度但不难处理的函数，以获得对资源的访问，从而防止（系统资源被）滥用。 在 1999 年，Markus Jakobsson 与 Ari Juels 第一次从各种协议中提炼出 Proofs of Work 这个概念。 POW 系统中一定有两个角色，工作者和验证者，他们需要具有以下特点： 工作者需要完成的工作必须有一定的量，这个量由工作验证者给出。 验证者可以迅速的检验工作量是否达标。 工作者无法自己\"创造工作\"，必须由验证者发布工作。 工作者无法找到很快完成工作的办法。 说到这里，我们对 PoW 应该有足够的理解了，它就是让工作者消耗一定的资源作为使用系统的成本。 对于正常的用户而言这部分被消耗的资源是完全可以接受的，但是对于恶意攻击者而言，它如果想滥用系统的资源或者发送海量的垃圾邮件，就需要消耗海量的计算资源作为成本，这样就极大地提升了攻击成本。 再总结下，PoW 算法的核心是它为信息发送加入了成本，降低了信息传递的速率。 把比特币区块链转换成拜占庭将军问题来看，它的思路是这样的： 限制一段时间内提案的个数，只有拥有对应权限的节点（将军）可以发起提案。 这是通过 PoW 工作量证明实现的，比特币区块链要求节点进行海量的哈希计算作为获得提案权限的代价，算法难度每隔两周调整一次以保证整个系统找到正确 Hash 值的平均用时大约为 10 分钟。 由强一致性放宽至最终一致性。 对应一次提案的结果不需要全部的节点马上跟进，只需要在节点能搜寻到的全网络中的所有链条中，选取最长的链条进行后续拓展就可以。 使用非对称加密算法对节点间的消息传递提供签名技术支持，每个节点（将军）都有属于自己的秘钥（公钥私钥），唯一标识节点身份。 使用非对称加密算法传递消息，能够保证消息传递的私密性。而且消息签名不可篡改，这避免了消息被恶意节点伪造。 我们前面有给出一个结论：拜占庭容错算法要求叛徒的比例必须低于 1/3。 但是区块链与拜占庭将军问题的区别很大，举例如下： 区块链允许任何节点随时加入或离开区块链，而拜占庭将军问题是预设了节点数，而且不考虑节点的添加或删除。 比特币区块链的 PoW 算法只能保证整个系统找到正确 Hash 值的平均用时大约为 10 分钟，那肯定就存在性能更好的节点用时更短，性能更差的节点用时更长，甚至某些节点运气好几秒钟就算出了结果，这都是完全可能的。而越早算出这个 Hash 值的节点，它的提案（区块）成为最长链条的概率就越大。 PoW 由强一致性放宽至最终一致性，系统总会选取最长的链进行后续拓展，那如果某个链条一开始不长，但是它的拓展速度足够快，它就能成为最长的链条。而拜占庭将军问题不允许任何分支，只存在一个结果！ 只是受限于算力，随着时间的推移，短的链条追上最长链条的概率会越来越小。 总之因为区块链这样的特点，它会产生一些跟拜占庭容错算法不同的结果： 攻击者拥有的节点数量占比是毫无意义的，核心是算力，也就对应着区块链中的提案权。 即使攻击者拥有了 99% 的节点，但是它的总体算力很弱的话，它的提案（区块）成为最长链条的概率也会很低。 区块链的 51% 攻击：因为「系统总是选取最长链条进行后续拓展」这个原则，只有某个攻击者拥有了超过 50% 算力的情况下，它才拥有绝对性的优势，使它的区块在一定时间后一定能成为最长的链条，并且始终维持这样一个优势，从而达成攻击目的。 至于 PoW 算法的具体实现，以及它的替代算法 PoS/PoH 等新兴算法的原理与实现，将在后续的区块链系列文章中详细介绍，尽请期待… ","date":"2022-08-07","objectID":"/posts/consistency-and-consensus-algorithm/:3:5","series":null,"tags":["共识","一致性","分布式","数据库","区块链"],"title":"分布式数据库的一致性问题与共识算法","uri":"/posts/consistency-and-consensus-algorithm/#能容忍拜占庭错误的-pow-概率共识算法"},{"categories":["tech"],"content":" 参考 《Designing Data-Intensive Applications - The Big Ideas Behind Reliable, Scalable, and Maintainable Systems (Martin Kleppmann)》 极客时间《分布式数据库 30 讲》 极客时间《分布式协议与算法实战》 分布式存储系统的一致性是什么？- OceanBase 线性一致性和 Raft - PingCAP 一致性模型笔记 条分缕析分布式：浅析强弱一致性 - 张铁蕾 Why you should pick strong consistency, whenever possible - Google Spanner 拜占庭将军问题与区块链 区块链协议安全系列— —当拜占庭将军犯错时，区块链共识还安全么？（上集） Eventually Consistent - Revisited ","date":"2022-08-07","objectID":"/posts/consistency-and-consensus-algorithm/:4:0","series":null,"tags":["共识","一致性","分布式","数据库","区块链"],"title":"分布式数据库的一致性问题与共识算法","uri":"/posts/consistency-and-consensus-algorithm/#参考"},{"categories":["tech"],"content":"我在之前的文章 写给开发人员的实用密码学（八）—— 数字证书与 TLS 协议 中，介绍了如何使用 openssl 生成与管理各种用途的数字证书，也简单介绍了如何通过 certbot 等工具与 ACME 证书申请与管理协议，进行数字证书的申请与自动更新（autorenew）。 这篇文章要介绍的 cert-mangager，跟 certbot 这类工具有点类似，区别在于它是工作在 Kubernetes 中的。 cert-manager 是一个证书的自动化管理工具，用于在 Kubernetes 集群中自动化地颁发与管理各种来源、各种用途的数字证书。它将确保证书有效，并在合适的时间自动更新证书。 多的就不说了，证书相关的内容请参见我的 写给开发人员的实用密码学（八）—— 数字证书与 TLS 协议 或者其他资料，现在直接进入正题。 注：cert-manager 的管理对象是「证书」，如果你仅需要使用非对称加密的公私钥对进行 JWT 签名、数据加解密，可以考虑直接使用 secrets 管理工具 Vault. ","date":"2022-07-31","objectID":"/posts/kubernetes-cert-management/:0:0","series":null,"tags":["数字证书","证书","TLS","Kubernetes","cert-manager"],"title":"Kubernetes 中的证书管理工具 - cert-manager","uri":"/posts/kubernetes-cert-management/#"},{"categories":["tech"],"content":" 一、部署 https://cert-manager.io/docs/installation/helm/ 官方提供了多种部署方式，使用 helm3 安装的方法如下： # 查看版本号 helm search repo jetstack/cert-manager -l | head # 下载并解压 chart，目的是方便 gitops 版本管理 helm pull jetstack/cert-manager --untar --version 1.8.2 helm install \\ cert-manager ./cert-manager \\ --namespace cert-manager \\ --create-namespace \\ # 这会导致使用 helm 卸载的时候会删除所有 CRDs，可能导致所有 CRDs 资源全部丢失！要格外注意 --set installCRDs=true ","date":"2022-07-31","objectID":"/posts/kubernetes-cert-management/:1:0","series":null,"tags":["数字证书","证书","TLS","Kubernetes","cert-manager"],"title":"Kubernetes 中的证书管理工具 - cert-manager","uri":"/posts/kubernetes-cert-management/#一部署"},{"categories":["tech"],"content":" 二、创建 Issuercert-manager 支持多种 issuer，你甚至可以通过它的标准 API 创建自己的 Issuer。 但是总的来说不外乎三种： 由权威 CA 机构签名的「公网受信任证书」: 这类证书会被浏览器、小程序等第三方应用/服务商信任 本地签名证书: 即由本地 CA 证书签名的数字证书 自签名证书: 即使用证书的私钥为证书自己签名 下面介绍下如何申请公网证书以及本地签名证书。 ","date":"2022-07-31","objectID":"/posts/kubernetes-cert-management/:2:0","series":null,"tags":["数字证书","证书","TLS","Kubernetes","cert-manager"],"title":"Kubernetes 中的证书管理工具 - cert-manager","uri":"/posts/kubernetes-cert-management/#二创建-issuer"},{"categories":["tech"],"content":" 1. 通过权威机构创建公网受信证书通过权威机构创建的公网受信证书，可以直接应用在边界网关上，用于给公网用户提供 TLS 加密访问服务，比如各种 HTTPS 站点、API。 这是需求最广的一类数字证书服务。 cert-manager 支持两种申请公网受信证书的方式： ACME（Automated Certificate Management Environment (ACME) Certificate Authority server）证书自动化申请与管理协议。 venafi-as-a-service: venafi 是一个证书的集中化管理平台，它也提供了 cert-manager 插件，可用于自动化申请 DigiCert/Entrust/GlobalSign/Let’s Encrypt 四种类型的公网受信证书。 这里主要介绍使用 ACMEv2 协议申请公网证书，支持使用此开放协议申请证书的权威机构有： 免费服务 Let’s Encrypt: 众所周知，它提供三个月有效期的免费证书。 ZeroSSL: 貌似也是一个比较有名的 SSL 证书服务 通过 ACME 协议支持不限数量的 90 天证书，也支持多域名证书与泛域名证书。 它提供了一个额外的 Dashboard 查看与管理所有申请的证书，这是比较方便的地方。 付费服务 DigiCert: 这个非常有名（但也是相当贵），官方文档 Digicert - Third-party ACME client automation Google Public Authority(Google Trust Services): Google 推出的公网证书服务，也是三个月有效期，其根证书交叉验证了 GlobalSign，OCSP 服务器在国内速度也很快。 详见 acme.sh/wiki/Google-Public-CA 此功能目前（2022-08-10）仍处于 beta 状态，需要提表单申请才能获得使用 官方地址：https://pki.goog/ Entrust: 官方文档 Entrust’s ACME implementation GlobalSign: 官方文档 GlobalSign ACME Service 这里也顺便介绍下收费证书服务对证书的分级，以及该如何选用： Domain Validated（DV）证书 仅验证域名所有权，验证步骤最少，价格最低，仅需要数分钟即可签发。 优点就是易于签发，很适合做自动化。 各云厂商（AWS/GCP/Cloudflare，以及 Vercel/Github 的站点服务）给自家服务提供的免费证书都是 DV 证书，Let’s Encrypt 的证书也是这个类型。 很明显这些证书的签发都非常方便，而且仅验证域名所有权。 但是 AWS/GCP/Cloudflare/Vercel/Github 提供的 DV 证书都仅能在它们的云服务上使用，不提供私钥功能！ Organization Validated (OV) 证书 是企业 SSL 证书的首选，通过企业认证确保企业 SSL 证书的真实性。 除域名所有权外，CA 机构还会审核组织及企业的真实性，包括注册状况、联系方式、恶意软件等内容。 如果要做合规化，可能至少也得用 OV 这个级别的证书。 Extended Validation（EV）证书 最严格的认证方式，CA 机构会深度审核组织及企业各方面的信息。 被认为适合用于大型企业、金融机构等组织或企业。 而且仅支持签发单域名、多域名证书，不支持签发泛域名证书，安全性杠杠的。 ACME 支持 HTTP01 跟 DNS01 两种域名验证方式，其中 DNS01 是最简便的方法。 下面以 AWS Route53 为例介绍如何申请一个 Let’s Encrypt 证书。（其他 DNS 提供商的配置方式请直接看官方文档） https://cert-manager.io/docs/configuration/acme/dns01/route53/ 1.1 AWS IAM 授权首先需要为 EKS 集群创建 OIDC provider，参见 aws-iam-and-kubernetes，这里不再赘述。 cert-manager 需要查询与更新 Route53 记录的权限，因此需要使用如下配置创建一个 IAM Policy，可以命名为 \u003cClusterName\u003eCertManagerRoute53Access（注意替换掉 \u003cClusterName\u003e）： { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Action\": \"route53:GetChange\", \"Resource\": \"arn:aws:route53:::change/*\" }, { \"Effect\": \"Allow\", \"Action\": [ \"route53:ChangeResourceRecordSets\", \"route53:ListResourceRecordSets\" ], \"Resource\": \"arn:aws:route53:::hostedzone/*\" }, { \"Effect\": \"Allow\", \"Action\": \"route53:ListHostedZonesByName\", \"Resource\": \"*\" } ] } 比如使用 awscli 创建此 policy： aws iam create-policy \\ --policy-name XxxCertManagerRoute53Access \\ --policy-document file://cert-manager-route53-access.json 然后通过上述配置创建一个 IAM Role 并自动给 cert-manager 所在的 EKS 集群添加信任关系： export CLUSTER_NAME=\"xxx\" export AWS_ACCOUNT_ID=\"112233445566\" # 使用 eksctl 自动创建对应的 role 并添加信任关系 # 需要先安装好 eksctl eksctl create iamserviceaccount \\ --cluster \"${CLUSTER_NAME}\" --name cert-manager --namespace cert-manager \\ --role-name \"${CLUSTER_NAME}-cert-manager-route53-role\" \\ --attach-policy-arn \"arn:aws:iam::${AWS_ACCOUNT_ID}:policy/\u003cClusterName\u003eCertManagerRoute53Access\" \\ --role-only \\ --approve 之后需要为 cert-manager 的 ServiceAccount 添加注解来绑定上面刚创建好的 IAM Role，首先创建如下 helm values 文件 cert-manager-values.yaml: # 如果把这个改成 false，也会导致 cert-manager 的所有 CRDs 及相关资源被删除！ installCRDs: true serviceAccount: annotations: # 注意修改这里的 ${AWS_ACCOUNT_ID} 以及 ${CLUSTER_NAME} eks.amazonaws.com/role-arn: \u003e- arn:aws:iam::${AWS_ACCOUNT_ID}:role/${CLUSTER_NAME}-cert-manager-route53-role securityContext: enabled: true # 根据官方文档，还得修改下这个，允许 cert-manager 读取 ServiceAccount Token，从而获得授权 fsGroup: 1001 然后重新部署 cert-manager: helm upgrade -i cert-manager ./cert-manager -n cert-manager -f cert-manager-values.yaml 这样就完成了授权。 1.2 创建 ACME Issuer在 xxx 名字空间创建一个 Iusser： apiVersion: cert-manager.io/v1 kind: Issuer metadata: name: letsencrypt-prod namespace: xxx spec: acme: # 用于接受域名过期提醒的邮件地址 email: user@example.com # ACME 服务器，比如 let's encrypt、Digicert 等 # let's encrypt 的测试 URL，可用于测试配置正确性 server: https://acme-staging-v02.api.letsencrypt.org/directory # let's encrypt 的正式 URL，有速率限制 # server: https://acme-v02.api.letsencrypt.org/directory # 用于存放 ACME 账号私钥的 Secret 名称，Issuer 创建时会自动生成此 secret privateKeySecretRef: name: letsencrypt-staging # DNS 验证设置 solvers: - sele","date":"2022-07-31","objectID":"/posts/kubernetes-cert-management/:2:1","series":null,"tags":["数字证书","证书","TLS","Kubernetes","cert-manager"],"title":"Kubernetes 中的证书管理工具 - cert-manager","uri":"/posts/kubernetes-cert-management/#1-通过权威机构创建公网受信证书"},{"categories":["tech"],"content":" 1. 通过权威机构创建公网受信证书通过权威机构创建的公网受信证书，可以直接应用在边界网关上，用于给公网用户提供 TLS 加密访问服务，比如各种 HTTPS 站点、API。 这是需求最广的一类数字证书服务。 cert-manager 支持两种申请公网受信证书的方式： ACME（Automated Certificate Management Environment (ACME) Certificate Authority server）证书自动化申请与管理协议。 venafi-as-a-service: venafi 是一个证书的集中化管理平台，它也提供了 cert-manager 插件，可用于自动化申请 DigiCert/Entrust/GlobalSign/Let’s Encrypt 四种类型的公网受信证书。 这里主要介绍使用 ACMEv2 协议申请公网证书，支持使用此开放协议申请证书的权威机构有： 免费服务 Let’s Encrypt: 众所周知，它提供三个月有效期的免费证书。 ZeroSSL: 貌似也是一个比较有名的 SSL 证书服务 通过 ACME 协议支持不限数量的 90 天证书，也支持多域名证书与泛域名证书。 它提供了一个额外的 Dashboard 查看与管理所有申请的证书，这是比较方便的地方。 付费服务 DigiCert: 这个非常有名（但也是相当贵），官方文档 Digicert - Third-party ACME client automation Google Public Authority(Google Trust Services): Google 推出的公网证书服务，也是三个月有效期，其根证书交叉验证了 GlobalSign，OCSP 服务器在国内速度也很快。 详见 acme.sh/wiki/Google-Public-CA 此功能目前（2022-08-10）仍处于 beta 状态，需要提表单申请才能获得使用 官方地址：https://pki.goog/ Entrust: 官方文档 Entrust’s ACME implementation GlobalSign: 官方文档 GlobalSign ACME Service 这里也顺便介绍下收费证书服务对证书的分级，以及该如何选用： Domain Validated（DV）证书 仅验证域名所有权，验证步骤最少，价格最低，仅需要数分钟即可签发。 优点就是易于签发，很适合做自动化。 各云厂商（AWS/GCP/Cloudflare，以及 Vercel/Github 的站点服务）给自家服务提供的免费证书都是 DV 证书，Let’s Encrypt 的证书也是这个类型。 很明显这些证书的签发都非常方便，而且仅验证域名所有权。 但是 AWS/GCP/Cloudflare/Vercel/Github 提供的 DV 证书都仅能在它们的云服务上使用，不提供私钥功能！ Organization Validated (OV) 证书 是企业 SSL 证书的首选，通过企业认证确保企业 SSL 证书的真实性。 除域名所有权外，CA 机构还会审核组织及企业的真实性，包括注册状况、联系方式、恶意软件等内容。 如果要做合规化，可能至少也得用 OV 这个级别的证书。 Extended Validation（EV）证书 最严格的认证方式，CA 机构会深度审核组织及企业各方面的信息。 被认为适合用于大型企业、金融机构等组织或企业。 而且仅支持签发单域名、多域名证书，不支持签发泛域名证书，安全性杠杠的。 ACME 支持 HTTP01 跟 DNS01 两种域名验证方式，其中 DNS01 是最简便的方法。 下面以 AWS Route53 为例介绍如何申请一个 Let’s Encrypt 证书。（其他 DNS 提供商的配置方式请直接看官方文档） https://cert-manager.io/docs/configuration/acme/dns01/route53/ 1.1 AWS IAM 授权首先需要为 EKS 集群创建 OIDC provider，参见 aws-iam-and-kubernetes，这里不再赘述。 cert-manager 需要查询与更新 Route53 记录的权限，因此需要使用如下配置创建一个 IAM Policy，可以命名为 CertManagerRoute53Access（注意替换掉 ）： { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Action\": \"route53:GetChange\", \"Resource\": \"arn:aws:route53:::change/*\" }, { \"Effect\": \"Allow\", \"Action\": [ \"route53:ChangeResourceRecordSets\", \"route53:ListResourceRecordSets\" ], \"Resource\": \"arn:aws:route53:::hostedzone/*\" }, { \"Effect\": \"Allow\", \"Action\": \"route53:ListHostedZonesByName\", \"Resource\": \"*\" } ] } 比如使用 awscli 创建此 policy： aws iam create-policy \\ --policy-name XxxCertManagerRoute53Access \\ --policy-document file://cert-manager-route53-access.json 然后通过上述配置创建一个 IAM Role 并自动给 cert-manager 所在的 EKS 集群添加信任关系： export CLUSTER_NAME=\"xxx\" export AWS_ACCOUNT_ID=\"112233445566\" # 使用 eksctl 自动创建对应的 role 并添加信任关系 # 需要先安装好 eksctl eksctl create iamserviceaccount \\ --cluster \"${CLUSTER_NAME}\" --name cert-manager --namespace cert-manager \\ --role-name \"${CLUSTER_NAME}-cert-manager-route53-role\" \\ --attach-policy-arn \"arn:aws:iam::${AWS_ACCOUNT_ID}:policy/CertManagerRoute53Access\" \\ --role-only \\ --approve 之后需要为 cert-manager 的 ServiceAccount 添加注解来绑定上面刚创建好的 IAM Role，首先创建如下 helm values 文件 cert-manager-values.yaml: # 如果把这个改成 false，也会导致 cert-manager 的所有 CRDs 及相关资源被删除！ installCRDs: true serviceAccount: annotations: # 注意修改这里的 ${AWS_ACCOUNT_ID} 以及 ${CLUSTER_NAME} eks.amazonaws.com/role-arn: \u003e- arn:aws:iam::${AWS_ACCOUNT_ID}:role/${CLUSTER_NAME}-cert-manager-route53-role securityContext: enabled: true # 根据官方文档，还得修改下这个，允许 cert-manager 读取 ServiceAccount Token，从而获得授权 fsGroup: 1001 然后重新部署 cert-manager: helm upgrade -i cert-manager ./cert-manager -n cert-manager -f cert-manager-values.yaml 这样就完成了授权。 1.2 创建 ACME Issuer在 xxx 名字空间创建一个 Iusser： apiVersion: cert-manager.io/v1 kind: Issuer metadata: name: letsencrypt-prod namespace: xxx spec: acme: # 用于接受域名过期提醒的邮件地址 email: user@example.com # ACME 服务器，比如 let's encrypt、Digicert 等 # let's encrypt 的测试 URL，可用于测试配置正确性 server: https://acme-staging-v02.api.letsencrypt.org/directory # let's encrypt 的正式 URL，有速率限制 # server: https://acme-v02.api.letsencrypt.org/directory # 用于存放 ACME 账号私钥的 Secret 名称，Issuer 创建时会自动生成此 secret privateKeySecretRef: name: letsencrypt-staging # DNS 验证设置 solvers: - sele","date":"2022-07-31","objectID":"/posts/kubernetes-cert-management/:2:1","series":null,"tags":["数字证书","证书","TLS","Kubernetes","cert-manager"],"title":"Kubernetes 中的证书管理工具 - cert-manager","uri":"/posts/kubernetes-cert-management/#11-aws-iam-授权"},{"categories":["tech"],"content":" 1. 通过权威机构创建公网受信证书通过权威机构创建的公网受信证书，可以直接应用在边界网关上，用于给公网用户提供 TLS 加密访问服务，比如各种 HTTPS 站点、API。 这是需求最广的一类数字证书服务。 cert-manager 支持两种申请公网受信证书的方式： ACME（Automated Certificate Management Environment (ACME) Certificate Authority server）证书自动化申请与管理协议。 venafi-as-a-service: venafi 是一个证书的集中化管理平台，它也提供了 cert-manager 插件，可用于自动化申请 DigiCert/Entrust/GlobalSign/Let’s Encrypt 四种类型的公网受信证书。 这里主要介绍使用 ACMEv2 协议申请公网证书，支持使用此开放协议申请证书的权威机构有： 免费服务 Let’s Encrypt: 众所周知，它提供三个月有效期的免费证书。 ZeroSSL: 貌似也是一个比较有名的 SSL 证书服务 通过 ACME 协议支持不限数量的 90 天证书，也支持多域名证书与泛域名证书。 它提供了一个额外的 Dashboard 查看与管理所有申请的证书，这是比较方便的地方。 付费服务 DigiCert: 这个非常有名（但也是相当贵），官方文档 Digicert - Third-party ACME client automation Google Public Authority(Google Trust Services): Google 推出的公网证书服务，也是三个月有效期，其根证书交叉验证了 GlobalSign，OCSP 服务器在国内速度也很快。 详见 acme.sh/wiki/Google-Public-CA 此功能目前（2022-08-10）仍处于 beta 状态，需要提表单申请才能获得使用 官方地址：https://pki.goog/ Entrust: 官方文档 Entrust’s ACME implementation GlobalSign: 官方文档 GlobalSign ACME Service 这里也顺便介绍下收费证书服务对证书的分级，以及该如何选用： Domain Validated（DV）证书 仅验证域名所有权，验证步骤最少，价格最低，仅需要数分钟即可签发。 优点就是易于签发，很适合做自动化。 各云厂商（AWS/GCP/Cloudflare，以及 Vercel/Github 的站点服务）给自家服务提供的免费证书都是 DV 证书，Let’s Encrypt 的证书也是这个类型。 很明显这些证书的签发都非常方便，而且仅验证域名所有权。 但是 AWS/GCP/Cloudflare/Vercel/Github 提供的 DV 证书都仅能在它们的云服务上使用，不提供私钥功能！ Organization Validated (OV) 证书 是企业 SSL 证书的首选，通过企业认证确保企业 SSL 证书的真实性。 除域名所有权外，CA 机构还会审核组织及企业的真实性，包括注册状况、联系方式、恶意软件等内容。 如果要做合规化，可能至少也得用 OV 这个级别的证书。 Extended Validation（EV）证书 最严格的认证方式，CA 机构会深度审核组织及企业各方面的信息。 被认为适合用于大型企业、金融机构等组织或企业。 而且仅支持签发单域名、多域名证书，不支持签发泛域名证书，安全性杠杠的。 ACME 支持 HTTP01 跟 DNS01 两种域名验证方式，其中 DNS01 是最简便的方法。 下面以 AWS Route53 为例介绍如何申请一个 Let’s Encrypt 证书。（其他 DNS 提供商的配置方式请直接看官方文档） https://cert-manager.io/docs/configuration/acme/dns01/route53/ 1.1 AWS IAM 授权首先需要为 EKS 集群创建 OIDC provider，参见 aws-iam-and-kubernetes，这里不再赘述。 cert-manager 需要查询与更新 Route53 记录的权限，因此需要使用如下配置创建一个 IAM Policy，可以命名为 CertManagerRoute53Access（注意替换掉 ）： { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Action\": \"route53:GetChange\", \"Resource\": \"arn:aws:route53:::change/*\" }, { \"Effect\": \"Allow\", \"Action\": [ \"route53:ChangeResourceRecordSets\", \"route53:ListResourceRecordSets\" ], \"Resource\": \"arn:aws:route53:::hostedzone/*\" }, { \"Effect\": \"Allow\", \"Action\": \"route53:ListHostedZonesByName\", \"Resource\": \"*\" } ] } 比如使用 awscli 创建此 policy： aws iam create-policy \\ --policy-name XxxCertManagerRoute53Access \\ --policy-document file://cert-manager-route53-access.json 然后通过上述配置创建一个 IAM Role 并自动给 cert-manager 所在的 EKS 集群添加信任关系： export CLUSTER_NAME=\"xxx\" export AWS_ACCOUNT_ID=\"112233445566\" # 使用 eksctl 自动创建对应的 role 并添加信任关系 # 需要先安装好 eksctl eksctl create iamserviceaccount \\ --cluster \"${CLUSTER_NAME}\" --name cert-manager --namespace cert-manager \\ --role-name \"${CLUSTER_NAME}-cert-manager-route53-role\" \\ --attach-policy-arn \"arn:aws:iam::${AWS_ACCOUNT_ID}:policy/CertManagerRoute53Access\" \\ --role-only \\ --approve 之后需要为 cert-manager 的 ServiceAccount 添加注解来绑定上面刚创建好的 IAM Role，首先创建如下 helm values 文件 cert-manager-values.yaml: # 如果把这个改成 false，也会导致 cert-manager 的所有 CRDs 及相关资源被删除！ installCRDs: true serviceAccount: annotations: # 注意修改这里的 ${AWS_ACCOUNT_ID} 以及 ${CLUSTER_NAME} eks.amazonaws.com/role-arn: \u003e- arn:aws:iam::${AWS_ACCOUNT_ID}:role/${CLUSTER_NAME}-cert-manager-route53-role securityContext: enabled: true # 根据官方文档，还得修改下这个，允许 cert-manager 读取 ServiceAccount Token，从而获得授权 fsGroup: 1001 然后重新部署 cert-manager: helm upgrade -i cert-manager ./cert-manager -n cert-manager -f cert-manager-values.yaml 这样就完成了授权。 1.2 创建 ACME Issuer在 xxx 名字空间创建一个 Iusser： apiVersion: cert-manager.io/v1 kind: Issuer metadata: name: letsencrypt-prod namespace: xxx spec: acme: # 用于接受域名过期提醒的邮件地址 email: user@example.com # ACME 服务器，比如 let's encrypt、Digicert 等 # let's encrypt 的测试 URL，可用于测试配置正确性 server: https://acme-staging-v02.api.letsencrypt.org/directory # let's encrypt 的正式 URL，有速率限制 # server: https://acme-v02.api.letsencrypt.org/directory # 用于存放 ACME 账号私钥的 Secret 名称，Issuer 创建时会自动生成此 secret privateKeySecretRef: name: letsencrypt-staging # DNS 验证设置 solvers: - sele","date":"2022-07-31","objectID":"/posts/kubernetes-cert-management/:2:1","series":null,"tags":["数字证书","证书","TLS","Kubernetes","cert-manager"],"title":"Kubernetes 中的证书管理工具 - cert-manager","uri":"/posts/kubernetes-cert-management/#12-创建-acme-issuer"},{"categories":["tech"],"content":" 1. 通过权威机构创建公网受信证书通过权威机构创建的公网受信证书，可以直接应用在边界网关上，用于给公网用户提供 TLS 加密访问服务，比如各种 HTTPS 站点、API。 这是需求最广的一类数字证书服务。 cert-manager 支持两种申请公网受信证书的方式： ACME（Automated Certificate Management Environment (ACME) Certificate Authority server）证书自动化申请与管理协议。 venafi-as-a-service: venafi 是一个证书的集中化管理平台，它也提供了 cert-manager 插件，可用于自动化申请 DigiCert/Entrust/GlobalSign/Let’s Encrypt 四种类型的公网受信证书。 这里主要介绍使用 ACMEv2 协议申请公网证书，支持使用此开放协议申请证书的权威机构有： 免费服务 Let’s Encrypt: 众所周知，它提供三个月有效期的免费证书。 ZeroSSL: 貌似也是一个比较有名的 SSL 证书服务 通过 ACME 协议支持不限数量的 90 天证书，也支持多域名证书与泛域名证书。 它提供了一个额外的 Dashboard 查看与管理所有申请的证书，这是比较方便的地方。 付费服务 DigiCert: 这个非常有名（但也是相当贵），官方文档 Digicert - Third-party ACME client automation Google Public Authority(Google Trust Services): Google 推出的公网证书服务，也是三个月有效期，其根证书交叉验证了 GlobalSign，OCSP 服务器在国内速度也很快。 详见 acme.sh/wiki/Google-Public-CA 此功能目前（2022-08-10）仍处于 beta 状态，需要提表单申请才能获得使用 官方地址：https://pki.goog/ Entrust: 官方文档 Entrust’s ACME implementation GlobalSign: 官方文档 GlobalSign ACME Service 这里也顺便介绍下收费证书服务对证书的分级，以及该如何选用： Domain Validated（DV）证书 仅验证域名所有权，验证步骤最少，价格最低，仅需要数分钟即可签发。 优点就是易于签发，很适合做自动化。 各云厂商（AWS/GCP/Cloudflare，以及 Vercel/Github 的站点服务）给自家服务提供的免费证书都是 DV 证书，Let’s Encrypt 的证书也是这个类型。 很明显这些证书的签发都非常方便，而且仅验证域名所有权。 但是 AWS/GCP/Cloudflare/Vercel/Github 提供的 DV 证书都仅能在它们的云服务上使用，不提供私钥功能！ Organization Validated (OV) 证书 是企业 SSL 证书的首选，通过企业认证确保企业 SSL 证书的真实性。 除域名所有权外，CA 机构还会审核组织及企业的真实性，包括注册状况、联系方式、恶意软件等内容。 如果要做合规化，可能至少也得用 OV 这个级别的证书。 Extended Validation（EV）证书 最严格的认证方式，CA 机构会深度审核组织及企业各方面的信息。 被认为适合用于大型企业、金融机构等组织或企业。 而且仅支持签发单域名、多域名证书，不支持签发泛域名证书，安全性杠杠的。 ACME 支持 HTTP01 跟 DNS01 两种域名验证方式，其中 DNS01 是最简便的方法。 下面以 AWS Route53 为例介绍如何申请一个 Let’s Encrypt 证书。（其他 DNS 提供商的配置方式请直接看官方文档） https://cert-manager.io/docs/configuration/acme/dns01/route53/ 1.1 AWS IAM 授权首先需要为 EKS 集群创建 OIDC provider，参见 aws-iam-and-kubernetes，这里不再赘述。 cert-manager 需要查询与更新 Route53 记录的权限，因此需要使用如下配置创建一个 IAM Policy，可以命名为 CertManagerRoute53Access（注意替换掉 ）： { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Action\": \"route53:GetChange\", \"Resource\": \"arn:aws:route53:::change/*\" }, { \"Effect\": \"Allow\", \"Action\": [ \"route53:ChangeResourceRecordSets\", \"route53:ListResourceRecordSets\" ], \"Resource\": \"arn:aws:route53:::hostedzone/*\" }, { \"Effect\": \"Allow\", \"Action\": \"route53:ListHostedZonesByName\", \"Resource\": \"*\" } ] } 比如使用 awscli 创建此 policy： aws iam create-policy \\ --policy-name XxxCertManagerRoute53Access \\ --policy-document file://cert-manager-route53-access.json 然后通过上述配置创建一个 IAM Role 并自动给 cert-manager 所在的 EKS 集群添加信任关系： export CLUSTER_NAME=\"xxx\" export AWS_ACCOUNT_ID=\"112233445566\" # 使用 eksctl 自动创建对应的 role 并添加信任关系 # 需要先安装好 eksctl eksctl create iamserviceaccount \\ --cluster \"${CLUSTER_NAME}\" --name cert-manager --namespace cert-manager \\ --role-name \"${CLUSTER_NAME}-cert-manager-route53-role\" \\ --attach-policy-arn \"arn:aws:iam::${AWS_ACCOUNT_ID}:policy/CertManagerRoute53Access\" \\ --role-only \\ --approve 之后需要为 cert-manager 的 ServiceAccount 添加注解来绑定上面刚创建好的 IAM Role，首先创建如下 helm values 文件 cert-manager-values.yaml: # 如果把这个改成 false，也会导致 cert-manager 的所有 CRDs 及相关资源被删除！ installCRDs: true serviceAccount: annotations: # 注意修改这里的 ${AWS_ACCOUNT_ID} 以及 ${CLUSTER_NAME} eks.amazonaws.com/role-arn: \u003e- arn:aws:iam::${AWS_ACCOUNT_ID}:role/${CLUSTER_NAME}-cert-manager-route53-role securityContext: enabled: true # 根据官方文档，还得修改下这个，允许 cert-manager 读取 ServiceAccount Token，从而获得授权 fsGroup: 1001 然后重新部署 cert-manager: helm upgrade -i cert-manager ./cert-manager -n cert-manager -f cert-manager-values.yaml 这样就完成了授权。 1.2 创建 ACME Issuer在 xxx 名字空间创建一个 Iusser： apiVersion: cert-manager.io/v1 kind: Issuer metadata: name: letsencrypt-prod namespace: xxx spec: acme: # 用于接受域名过期提醒的邮件地址 email: user@example.com # ACME 服务器，比如 let's encrypt、Digicert 等 # let's encrypt 的测试 URL，可用于测试配置正确性 server: https://acme-staging-v02.api.letsencrypt.org/directory # let's encrypt 的正式 URL，有速率限制 # server: https://acme-v02.api.letsencrypt.org/directory # 用于存放 ACME 账号私钥的 Secret 名称，Issuer 创建时会自动生成此 secret privateKeySecretRef: name: letsencrypt-staging # DNS 验证设置 solvers: - sele","date":"2022-07-31","objectID":"/posts/kubernetes-cert-management/:2:1","series":null,"tags":["数字证书","证书","TLS","Kubernetes","cert-manager"],"title":"Kubernetes 中的证书管理工具 - cert-manager","uri":"/posts/kubernetes-cert-management/#13-通过-acme-创建证书以及问题排查"},{"categories":["tech"],"content":" 1. 通过权威机构创建公网受信证书通过权威机构创建的公网受信证书，可以直接应用在边界网关上，用于给公网用户提供 TLS 加密访问服务，比如各种 HTTPS 站点、API。 这是需求最广的一类数字证书服务。 cert-manager 支持两种申请公网受信证书的方式： ACME（Automated Certificate Management Environment (ACME) Certificate Authority server）证书自动化申请与管理协议。 venafi-as-a-service: venafi 是一个证书的集中化管理平台，它也提供了 cert-manager 插件，可用于自动化申请 DigiCert/Entrust/GlobalSign/Let’s Encrypt 四种类型的公网受信证书。 这里主要介绍使用 ACMEv2 协议申请公网证书，支持使用此开放协议申请证书的权威机构有： 免费服务 Let’s Encrypt: 众所周知，它提供三个月有效期的免费证书。 ZeroSSL: 貌似也是一个比较有名的 SSL 证书服务 通过 ACME 协议支持不限数量的 90 天证书，也支持多域名证书与泛域名证书。 它提供了一个额外的 Dashboard 查看与管理所有申请的证书，这是比较方便的地方。 付费服务 DigiCert: 这个非常有名（但也是相当贵），官方文档 Digicert - Third-party ACME client automation Google Public Authority(Google Trust Services): Google 推出的公网证书服务，也是三个月有效期，其根证书交叉验证了 GlobalSign，OCSP 服务器在国内速度也很快。 详见 acme.sh/wiki/Google-Public-CA 此功能目前（2022-08-10）仍处于 beta 状态，需要提表单申请才能获得使用 官方地址：https://pki.goog/ Entrust: 官方文档 Entrust’s ACME implementation GlobalSign: 官方文档 GlobalSign ACME Service 这里也顺便介绍下收费证书服务对证书的分级，以及该如何选用： Domain Validated（DV）证书 仅验证域名所有权，验证步骤最少，价格最低，仅需要数分钟即可签发。 优点就是易于签发，很适合做自动化。 各云厂商（AWS/GCP/Cloudflare，以及 Vercel/Github 的站点服务）给自家服务提供的免费证书都是 DV 证书，Let’s Encrypt 的证书也是这个类型。 很明显这些证书的签发都非常方便，而且仅验证域名所有权。 但是 AWS/GCP/Cloudflare/Vercel/Github 提供的 DV 证书都仅能在它们的云服务上使用，不提供私钥功能！ Organization Validated (OV) 证书 是企业 SSL 证书的首选，通过企业认证确保企业 SSL 证书的真实性。 除域名所有权外，CA 机构还会审核组织及企业的真实性，包括注册状况、联系方式、恶意软件等内容。 如果要做合规化，可能至少也得用 OV 这个级别的证书。 Extended Validation（EV）证书 最严格的认证方式，CA 机构会深度审核组织及企业各方面的信息。 被认为适合用于大型企业、金融机构等组织或企业。 而且仅支持签发单域名、多域名证书，不支持签发泛域名证书，安全性杠杠的。 ACME 支持 HTTP01 跟 DNS01 两种域名验证方式，其中 DNS01 是最简便的方法。 下面以 AWS Route53 为例介绍如何申请一个 Let’s Encrypt 证书。（其他 DNS 提供商的配置方式请直接看官方文档） https://cert-manager.io/docs/configuration/acme/dns01/route53/ 1.1 AWS IAM 授权首先需要为 EKS 集群创建 OIDC provider，参见 aws-iam-and-kubernetes，这里不再赘述。 cert-manager 需要查询与更新 Route53 记录的权限，因此需要使用如下配置创建一个 IAM Policy，可以命名为 CertManagerRoute53Access（注意替换掉 ）： { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Action\": \"route53:GetChange\", \"Resource\": \"arn:aws:route53:::change/*\" }, { \"Effect\": \"Allow\", \"Action\": [ \"route53:ChangeResourceRecordSets\", \"route53:ListResourceRecordSets\" ], \"Resource\": \"arn:aws:route53:::hostedzone/*\" }, { \"Effect\": \"Allow\", \"Action\": \"route53:ListHostedZonesByName\", \"Resource\": \"*\" } ] } 比如使用 awscli 创建此 policy： aws iam create-policy \\ --policy-name XxxCertManagerRoute53Access \\ --policy-document file://cert-manager-route53-access.json 然后通过上述配置创建一个 IAM Role 并自动给 cert-manager 所在的 EKS 集群添加信任关系： export CLUSTER_NAME=\"xxx\" export AWS_ACCOUNT_ID=\"112233445566\" # 使用 eksctl 自动创建对应的 role 并添加信任关系 # 需要先安装好 eksctl eksctl create iamserviceaccount \\ --cluster \"${CLUSTER_NAME}\" --name cert-manager --namespace cert-manager \\ --role-name \"${CLUSTER_NAME}-cert-manager-route53-role\" \\ --attach-policy-arn \"arn:aws:iam::${AWS_ACCOUNT_ID}:policy/CertManagerRoute53Access\" \\ --role-only \\ --approve 之后需要为 cert-manager 的 ServiceAccount 添加注解来绑定上面刚创建好的 IAM Role，首先创建如下 helm values 文件 cert-manager-values.yaml: # 如果把这个改成 false，也会导致 cert-manager 的所有 CRDs 及相关资源被删除！ installCRDs: true serviceAccount: annotations: # 注意修改这里的 ${AWS_ACCOUNT_ID} 以及 ${CLUSTER_NAME} eks.amazonaws.com/role-arn: \u003e- arn:aws:iam::${AWS_ACCOUNT_ID}:role/${CLUSTER_NAME}-cert-manager-route53-role securityContext: enabled: true # 根据官方文档，还得修改下这个，允许 cert-manager 读取 ServiceAccount Token，从而获得授权 fsGroup: 1001 然后重新部署 cert-manager: helm upgrade -i cert-manager ./cert-manager -n cert-manager -f cert-manager-values.yaml 这样就完成了授权。 1.2 创建 ACME Issuer在 xxx 名字空间创建一个 Iusser： apiVersion: cert-manager.io/v1 kind: Issuer metadata: name: letsencrypt-prod namespace: xxx spec: acme: # 用于接受域名过期提醒的邮件地址 email: user@example.com # ACME 服务器，比如 let's encrypt、Digicert 等 # let's encrypt 的测试 URL，可用于测试配置正确性 server: https://acme-staging-v02.api.letsencrypt.org/directory # let's encrypt 的正式 URL，有速率限制 # server: https://acme-v02.api.letsencrypt.org/directory # 用于存放 ACME 账号私钥的 Secret 名称，Issuer 创建时会自动生成此 secret privateKeySecretRef: name: letsencrypt-staging # DNS 验证设置 solvers: - sele","date":"2022-07-31","objectID":"/posts/kubernetes-cert-management/:2:1","series":null,"tags":["数字证书","证书","TLS","Kubernetes","cert-manager"],"title":"Kubernetes 中的证书管理工具 - cert-manager","uri":"/posts/kubernetes-cert-management/#14-通过-csi-driver-创建证书"},{"categories":["tech"],"content":" 2. 通过私有 CA 颁发证书Private CA 是一种企业自己生成的 CA 证书，通常企业用它来构建自己的 PKI 基础设施。 在 TLS 协议这个应用场景下，Private CA 颁发的证书仅适合在企业内部使用，必须在客户端安装上这个 CA 证书，才能正常访问由它签名的数字证书加密的 Web API 或者站点。Private CA 签名的数字证书在公网上是不被信任的！ cert-manager 提供的 Private CA 服务有： Vault: 鼎鼎大名了，Vault 是一个密码即服务工具，可以部署在 K8s 集群中，提供许多密码、证书相关的功能。 开源免费 AWS Certificate Manager Private CA: 跟 Vault 的 CA 功能是一致的，区别是它是托管的，由 AWS 负责维护。 每个 Private CA 证书：$400/month 每个签发的证书（仅读取了私钥及证书内容后才会收费）：按梯度一次性收费，0-1000 个以内是 $0.75 每个 其他的自己看文档… 这个因为暂时用不上，所以还没研究，之后有研究再给补上。 TO BE DONE. ","date":"2022-07-31","objectID":"/posts/kubernetes-cert-management/:2:2","series":null,"tags":["数字证书","证书","TLS","Kubernetes","cert-manager"],"title":"Kubernetes 中的证书管理工具 - cert-manager","uri":"/posts/kubernetes-cert-management/#2-通过私有-ca-颁发证书"},{"categories":["tech"],"content":" 三、cert-manager 与 istio/ingress 等网关集成cert-manager 提供的 Certificate 资源，会将生成好的公私钥存放在 Secret 中，而 Istio/Ingress 都支持这种格式的 Secret，所以使用还是挺简单的。 以 Istio Gateway 为例，直接在 Gateway 资源上指定 Secret 名称即可： apiVersion: networking.istio.io/v1alpha3 kind: Gateway metadata: name: example-gateway spec: selector: istio: ingressgateway servers: - port: number: 8080 name: http protocol: HTTP hosts: - product.example.com tls: httpsRedirect: true # sends 301 redirect for http requests - port: number: 8443 name: https protocol: HTTPS tls: mode: SIMPLE # enables HTTPS on this port credentialName: tls-example.com # This should match the Certificate secretName hosts: - product.example.com # This should match a DNS name in the Certificate --- apiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: product spec: hosts: - product.example.com gateways: - example-gateway http: - route: - destination: host: product port: number: 8080 --- apiVersion: v1 kind: Service metadata: labels: app: product name: product namespace: prod spec: ports: - name: grpc port: 9090 protocol: TCP targetPort: 9090 - name: http port: 8080 protocol: TCP targetPort: 8080 selector: app: product sessionAffinity: None type: ClusterIP --- apiVersion: networking.istio.io/v1alpha3 kind: DestinationRule metadata: name: product spec: host: product # 定义了两个 subset subsets: - labels: version: v1 name: v1 - labels: version: v2 name: v2 --- # 其他 deployment 等配置 之后再配合 VirtualService 等资源，就可以将 Istio 跟 cert-manager 结合起来啦。 ","date":"2022-07-31","objectID":"/posts/kubernetes-cert-management/:3:0","series":null,"tags":["数字证书","证书","TLS","Kubernetes","cert-manager"],"title":"Kubernetes 中的证书管理工具 - cert-manager","uri":"/posts/kubernetes-cert-management/#三cert-manager-与-istioingress-等网关集成"},{"categories":["tech"],"content":" 四、将 cert-manager 证书挂载到自定义网关中 注意，千万别使用 subPath 挂载，根据官方文档，这种方式挂载的 Secret 文件不会自动更新！ 既然证书被存放在 Secret 中，自然可以直接当成数据卷挂载到 Pods 中，示例如下： apiVersion: v1 kind: Pod metadata: name: nginx spec: containers: - name: nginx image: nginx:latest volumeMounts: - name: tls-example.com mountPath: \"/certs/example.com\" readOnly: true volumes: - name: tls-example.com secret: secretName: tls-example.com optional: false # default setting; \"mysecret\" must exist 对于 nginx 而言，可以简单地搞个 sidecar 监控下，有配置变更就 reload 下 nginx，实现证书自动更新。 或者可以考虑直接写个 k8s informer 监控 secret 的变更，有变更就直接 reload 所有 nginx 实例，总之实现的方式有很多种。 ","date":"2022-07-31","objectID":"/posts/kubernetes-cert-management/:4:0","series":null,"tags":["数字证书","证书","TLS","Kubernetes","cert-manager"],"title":"Kubernetes 中的证书管理工具 - cert-manager","uri":"/posts/kubernetes-cert-management/#四将-cert-manager-证书挂载到自定义网关中"},{"categories":["tech"],"content":" 五、注意事项服务端 TLS 协议的配置有许多的优化点，有些配置对性能的提升是很明显的，建议自行网上搜索相关资料，这里仅列出部分相关信息。 ","date":"2022-07-31","objectID":"/posts/kubernetes-cert-management/:5:0","series":null,"tags":["数字证书","证书","TLS","Kubernetes","cert-manager"],"title":"Kubernetes 中的证书管理工具 - cert-manager","uri":"/posts/kubernetes-cert-management/#五注意事项"},{"categories":["tech"],"content":" OCSP 证书验证协议会大幅拖慢 HTTPS 协议的响应速度 https://www.ssl.com/blogs/how-do-browsers-handle-revoked-ssl-tls-certificates/ https://imququ.com/post/why-can-not-turn-on-ocsp-stapling.html https://www.digicert.com/help/ 前面提到除了数字证书自带的有效期外，为了在私钥泄漏的情况下，能够吊销对应的证书，PKI 公钥基础设施还提供了 OCSP（Online Certificate Status Protocol）证书状态查询协议。 可以使用如下命令测试，确认站点是否启用了 ocsp stapling: $ openssl s_client -connect www.digicert.com:443 -servername www.digicert.com -status -tlsextdebug \u003c /dev/null 2\u003e\u00261 | grep -i \"OCSP response\" 如果输出包含 OCSP Response Status: successful 就说明站点支持 ocsp stapling， 如果输出内容为 OCSP response: no response sent 则说明站点不支持ocsp stapling。 实际上 Google/AWS 等大多数站点都不会启用也不需要启用 ocsp stapling，一是因为它们自己就是证书颁发机构，OCSP 服务器也归它们自己管，不存在隐私的问题。二是它们的 OCSP 服务器遍布全球，也不存在性能问题。 这种情况下开个 OCSP Stapling 反而是浪费流量，因为每次 TLS 握手都得发送一个 OCSP 状态信息。 我测试发现只有 www.digicert.com/www.douban.com 等少数站点启用了 ocsp stapling，www.baidu.com/www.google.com/www.zhihu.com 都未启用 ocsp stapling. 这导致了一些问题： Chrome/Firefox 等浏览器都会定期通过 OCSP 协议去请求 CA 机构的 OCSP 服务器验证证书状态，这可能会拖慢 HTTPS 协议的响应速度。 所谓的定期是指超过上一个 OCSP 响应的 nextUpdate 时间（一般为 7 天），或者如果该值为空的话，Firefox 默认 24h 后会重新查询 OCSP 状态。 因为客户端直接去请求 CA 机构的 OCSP 地址获取证书状态，这就导致 CA 机构可以获取到一些对应站点的用户信息（IP 地址、网络状态等）。 为了解决这两个问题，rfc6066 定义了 OCSP stapling 功能，它使服务器可以提前访问 OCSP 获取证书状态信息并缓存到本地，基本 Nginx/Caddy 等各大 Web 服务器或网关，都支持 OCSP stapling 协议。 在客户端使用 TLS 协议访问 HTTPS 服务时，服务端会直接在握手阶段将缓存的 OCSP 信息发送给客户端。 因为 OCSP 信息会带有 CA 证书的签名及有效期，客户端可以直接通过签名验证 OCSP 信息的真实性与有效性，这样就避免了客户端访问 OCSP 服务器带来的开销。 而另一个方法，就是选用 ocsp 服务器在目标用户区域速度快的 CA 机构签发证书。 ","date":"2022-07-31","objectID":"/posts/kubernetes-cert-management/:5:1","series":null,"tags":["数字证书","证书","TLS","Kubernetes","cert-manager"],"title":"Kubernetes 中的证书管理工具 - cert-manager","uri":"/posts/kubernetes-cert-management/#ocsp-证书验证协议会大幅拖慢-https-协议的响应速度"},{"categories":["life"],"content":" 我并不知道何时才是死期， 却日日问自己， 会不会有幽灵来牵我的手， 引我一路向西？ 可他们看到的光明又在哪里？ 会不会也为我亮起？ 这一切快来吧，我已等不及！ ——《红色地带的沉思》 by 临终患者 Patricia 我最近在看一本书，《在生命的尽头拥抱你-临终关怀医生手记》，它的英文原名即本文的标题。 李白在《春夜宴从弟桃花园序》中写「而浮生若梦，为欢几何？」，仿照下文法，本文的标题「Death Is But a Dream」大概可以直译成「临终若梦」，跟此书的中文版名称有些不同的韵味在。 这书讲的是死亡的过程——临终梦境。 随着年龄渐长，老一辈们慢慢老去，我们都不可避免地会越来越多地接触到死亡。 这世间轮替更迭，爱恨情仇是不变的主题，但是死亡始终是配角，实在是因为每人一生都只有唯一一次机会去真正体味死亡，难有实感。 而且死亡往往代表着终结，我们做为生者，当然更向往书写生者的世界。 救人一命胜造七级浮屠，珍爱生命是我们从小到大被教育的思想。 但是这样的思想却也造成了许多悲剧，全世界有许多的患者痛苦不堪地活在世上，求死不能，最终在病魔的折磨下凄惨离世，如果我们能谨慎承认「死亡」的价值，从这个角度看也是拯救了许多的患者与家庭。 而关于死亡，在我亲身经历的几次长辈葬礼中，我发现父辈们对死亡大都看得很开，他们说「人总有一死，老人家过世了我们当子女的肯定要送最后一程，但是不需要想太多，魂归天地罢了。」，我佩服这种豁达。 但是说到临终梦境，我是真的没什么了解。 我从来没跟长辈交流过生命末期的梦境，脑海中也挖掘不出相关的记忆。 送走我爷爷的时候，看着爷爷因为呼吸困难而大口喘气，堂哥跪在我前面，双眼泛红隐含泪光，但我完全没有实感——一切都显得那么不真实。 听着爷爷被痰堵塞气管、艰难的呼吸声，我甚至感到害怕，想要逃离。 爷爷过世后，奶奶就是一个人生活了，一个人起居、一个人给菜苗松土，然后在菜地里不小心跌了一跤，就随着爷爷去了。 我后来在爷爷奶奶房里一个壁橱上，找到三四枚铜钱，还沾着泥土，有些锈蚀痕迹。 我把其中一枚通宝红线串好，贴身带了好几年，心情不好的时候就凝视着这枚铜钱黯然神伤，心情好的时候也要捂着它入睡。 我还喜欢上了戏曲，缠着同学读了她的《中国戏剧史》。 大学的时候又喜欢上越剧，吴侬软语。 又因为初中时学过点竹笛，喜欢上了传统乐曲，我对一些经典老歌也情有独钟。 在很多同学跟同事的眼中，我的音乐品味是很「独特」的，这或许都源自爷爷奶奶的熏陶。 实际上我小的时候并不喜欢戏剧，我跟爷爷奶奶去看庙戏的目的，通常都只是为了吃一碗凉粉，或者为了去玩耍、看个热闹。偶尔去爷爷奶奶家玩，也只是觉得他们太孤单了，跟他们随便聊聊天，实际上这么多年，跟爷爷奶奶看过的戏曲，我就没听懂过几句台词。 那是多少年前了呢？只知道是很多年前了，不仔细回忆回忆、掐指算算，都搞不清具体过了多少年月。 这么多个日日夜夜里，我幼稚过、热血过，也迷茫过、颓废过，倒也不算庸庸碌碌，我还是知足的，这种心态貌似是被称作现充 emmm 高三时曾经看过一本超级喜欢的励志书，这么多年来我一直带在身边，名字叫《这一生再也不会有的奇遇》。书的扉页只有一句话：「当明天再也不是无限，你还会像今天一样度过你的人生吗？」 《这一生再也不会有的奇遇》已陪我度过了七八个春秋 当明天再也不是无限 写到这儿，我又想起我高中时还看过一本书《刺猬的优雅》，它同样陪伴我度过了四年大学岁月，后来又辗转到了深圳，但现在倒是不在身边。书中有几句话我印象深刻，放在这篇文章里也挺应景的： 话又说回来，不能因为有想死的心，往后就要像烂菜帮一样的过日子，甚至应该完全相反。 重要的不是死亡，而是在死亡的那一刻我们在做什么。 我在做什么呢？ 我曾遇到一个人，而且我正准备爱上他。 随意写下这些文字，脑子里各种想法恣意流淌，我打算提前写写我的临终遗言，把一切都准备好。 但在死亡到来之前，我仍要精彩的活！ ","date":"2022-05-24","objectID":"/posts/death-is-but-a-dream/:0:0","series":null,"tags":["临终体验","临终关怀","死亡","临终","梦境","哲学"],"title":"Death Is But a Dream","uri":"/posts/death-is-but-a-dream/#"},{"categories":["life"],"content":" 参考 《在生命的尽头拥抱你-临终关怀医生手记》 Death Is But A Dream (2021) Official trailer I See Dead People: Dreams and Visions of the Dying | Dr. Christopher Kerr | TEDxBuffalo End-of-Life Experiences - Dr. Christopher Kerr 有谁看过《刺猬的优雅》这本书吗？如何？ - 於清樂 有什么人生中值得一读的小说或名著？（最好现代作家的书）？ - 於清樂 ","date":"2022-05-24","objectID":"/posts/death-is-but-a-dream/:1:0","series":null,"tags":["临终体验","临终关怀","死亡","临终","梦境","哲学"],"title":"Death Is But a Dream","uri":"/posts/death-is-but-a-dream/#参考"},{"categories":["tech"],"content":" 个人笔记，不一定正确… 当前文章完成度 - 70% ","date":"2022-05-13","objectID":"/posts/about-nat/:0:0","series":null,"tags":["NAT","网络","NAT 穿越","内网穿透","虚拟网络","P2P","WebRTC"],"title":"NAT 网关、NAT 穿越以及虚拟网络","uri":"/posts/about-nat/#"},{"categories":["tech"],"content":" 前言NAT，即 Network Address Translation，是 IPv4 网络中非常重要的一个功能，用于执行 IP 地址与端口的转换。 IPv4 的设计者没预料到因特网技术的发展会如此之快，在设计时只使用了 32bits 的地址空间，随着因特网的飞速发展，它很快就变得不够用了。 后来虽然设计了新的 IPv6 协议，但是它与 IPv4 不兼容，需要新的硬件设备以及各种网络程序支持，无法快速普及。 NAT 就是在 IPv6 普及前，临时解决 IPv4 地址空间不够用而开发的技术，通俗地讲 NAT 就是用来给 IPv4 续命的。它解决 IPv4 地址短缺问题的方法是： 每个家庭、组织、企业，在内部都使用局域网通讯，不占用公网 IPv4 资源 在局域网与上层网络的交界处（路由器），使用 NAT 技术进行 IP/port 转换，使用户能正常访问上层网络 在曾经 IPv4 地址还不是特别短缺的时候，普通家庭的网络架构通常是：「家庭局域网」=\u003e「NAT 网关（家庭路由器）」=\u003e「因特网」。 但是互联网主要发展于欧美，因此许多欧美的组织与机构在初期被分配了大量的 IPv4 资源，而后入场的中国分配到的 IPv4 地址就不太能匹配上我们的人口。 因此相比欧美，中国的 IPv4 地址是非常短缺的，即使使用上述这样的网络架构——也就是给每个家庭（或组织）分配一个 IPv4 地址——都有点捉襟见肘了。 于是中国电信等运营商不得不再加一层 NAT，让多个家庭共用同一个 IP 地址，这时网络架构会变成这样：「家庭局域网」=\u003e「NAT 网关（家庭路由器）」=\u003e「广域网（由 ISP 管理）」=\u003e「NAT 网关」=\u003e「因特网」。 总的来说，NAT 是一项非常成功的技术，它成功帮 IPv4 续命了几十年，甚至到如今 2022 年，全球网络仍然是 IPv4 的天下。 ","date":"2022-05-13","objectID":"/posts/about-nat/:1:0","series":null,"tags":["NAT","网络","NAT 穿越","内网穿透","虚拟网络","P2P","WebRTC"],"title":"NAT 网关、NAT 穿越以及虚拟网络","uri":"/posts/about-nat/#前言"},{"categories":["tech"],"content":" NAT 如何工作NAT 的工作方式，使用图例描述是这样的： NAT 示例 从外部网络看一个 NAT 网关（一个启用了 NAT 的路由器），它只是拥有一个 IPv4 地址的普通设备，所有从局域网发送到公网的流量，其 IP 地址都是这个路由器的 WAN IP 地址，在上图中，这个 IP 地址是 138.76.29.7. 本质上，NAT 网关隐藏了家庭网络的细节，从外部网络上看，整个家庭网络就像一台普通的网络设备。 下面我们会学习到，上述这个 NAT 工作方式实际上是 NAPT，它同时使用 L3/L4 的信息进行地址转换工作。 ","date":"2022-05-13","objectID":"/posts/about-nat/:2:0","series":null,"tags":["NAT","网络","NAT 穿越","内网穿透","虚拟网络","P2P","WebRTC"],"title":"NAT 网关、NAT 穿越以及虚拟网络","uri":"/posts/about-nat/#nat-如何工作"},{"categories":["tech"],"content":" NAT 的地址映射方式NAT 的具体实现有许多的变种，不存在统一的规范，但是大体上能分为两种模型：「一对一 NAT」与「一对多 NAT」，下面分别进行介绍。 ","date":"2022-05-13","objectID":"/posts/about-nat/:3:0","series":null,"tags":["NAT","网络","NAT 穿越","内网穿透","虚拟网络","P2P","WebRTC"],"title":"NAT 网关、NAT 穿越以及虚拟网络","uri":"/posts/about-nat/#nat-的地址映射方式"},{"categories":["tech"],"content":" 1. 一对一 NAT一对一 NAT，这种类型的 NAT 在 RFC2663 中被称为 Basic NAT。 它在技术上比较简单，只利用网络层的信息，对 IP 地址进行转换。 简单的说，Basic NAT 要求每个内网 IP 都需要绑定一个唯一的公网 IP，才能连通外部网络。 其主要应用场景是，公网用户需要访问到内网主机。 Basic NAT 有三种类型：「静态 NAT」、「动态 NAT」以及「NAT Server」。 现在的很多家庭路由器都自带一个被称为 DMZ 主机的功能，它是「Demilitarized Zone」的缩写，意为隔离区。 它允许将某一台内网主机设置为 DMZ 主机（或者叫隔离区主机，仅此主机可供外网访问），所有从外部进来的流量，都会被通过 Basic NAT 修改为对应的内网 IP 地址，然后直接发送到该主机。 路由器的这种 DMZ 技术就是「静态 NAT」，因为 DMZ 主机对应的内网 IP 需要手动配置，不会动态变化。 DMZ 主机拓扑结构 而「动态 NAT」则需要一个公网 IP 地址池，每次用户需要访问公网时，动态 NAT 会给它分配一个动态公网 IP 并自动配置相应的 NAT 规则，使用完再回收。 第三种是「NAT Server」，云服务商提供的「公网 IP」就是通过「NAT Server」实现的，在云服务器中使用 ip addr ls 查看你会发现，该主机上实际只配了局域网 IP 地址，但是它却能正常使用公网 IP 通信，原因就是云服务商在「NAT Server」上为这些服务器配置了 IP 转发规则。 为一台云服务器绑定一个公网 IP，实际上就是请求「NAT Server」从公网 IP 地址池中取出一个，并配置对应的 NAT 规则到这台云服务器的局域网 IP。 示例如下，其中的 Internet Gateway 实际上就是个一对一 NAT Server： AWS VPC 中的 NAT 网关以及 Internet 网关 云服务 VPC 中的公有子网，实际上就是一个 DMZ(Demilitarized Zone) 隔离区，是不安全的。而私有子网则是安全区，公网无法直接访问到其中的主机。 而「动态 NAT」则需要路由器维护一个公网 IP 地址池，内网服务器需要访问公网时，动态 NAT 就从地址池中拿出一个公网 IP 给它使用，用完再回收。 这种场景需要一个公网 IP 地址池，每当内部有服务需要请求外网时，就动态地为它分配一个公网 IP 地址，使用完再回收。 Basic NAT 的好处是，它仅工作在 L3 网络层，网络层上的协议都可以正常使用（比如 P2P），不需要啥「内网穿越」技术。 ","date":"2022-05-13","objectID":"/posts/about-nat/:3:1","series":null,"tags":["NAT","网络","NAT 穿越","内网穿透","虚拟网络","P2P","WebRTC"],"title":"NAT 网关、NAT 穿越以及虚拟网络","uri":"/posts/about-nat/#1-一对一-nat"},{"categories":["tech"],"content":" 2. 一对多 NAT - NAPT一对多 NAT，也被称为 NAPT（network address and port translation），同样在 RFC2663 中被定义。Easy IP 是 NAPT 的一个特殊形式。 NAPT 的主要应用场景是，内网用户需要访问到公网主机。绝大多数的家庭网络、办公网络都是 NAPT 类型的。 原因应该很好理解——家庭网络或办公网络都包含许多联网设备，但是这类网络通常只有一个或数个公网 IP，使用一对一 NAT 的话公网 IP 显然是不够用的，所以需要使用一对多 NAT. NAPT 通过同时利用 L3 的 IP 信息，以及 L4 传输层的端口信息，来为局域网设备提供透明的、配置方便的、支持超高并发连接的外部网络通信，示意图如下： NAPT 的端口分配与转换规则（Mapping Behavior）以及对外来流量的过滤规则（Filtering Behavior）都存在许多不同的实现，没有统一的规范与标准，但是存在两种分类规范，这种分类方法主要用在 NAT 穿越技术中。 RFC3489 定义的 NAT 类型（四种）在 RFC3489 中将 NAPT 分为四种类型，这也是应用最为广泛的 NAT 分类方法，如下图： 下面我们逐一介绍这四种不同的 NAPT 类型。 从这里开始，下文中的 NAT 特指 NAPT，如果涉及「一对一 NAT」会使用它的全名。 1. Full-cone NATFull-cone NAT 的特点如下： 数据包流出：一旦内部地址（iAddr:iPort）映射到外部地址（eAddr:ePort），所有发自 iAddr:iPort 的数据包都经由 eAddr:ePort 向外发送。 数据包流入：任意主机发送到 eAddr:ePort 的数据包，都能通过 NAT 到达 iAddr:iPort. 也就是不对外部进来的数据做任何限制，全部放行。 cone 圆锥，个人理解是一个比喻，任意发送进来的数据（多），都能通过 NAT 到达这个内部地址（一），就像一个圆锥。 允许任意主机发送到 eAddr:ePort 的数据到达内部地址是很危险的行为，因为内部主机不一定配置了合适的安全策略。 因此 Full-cone NAT 比较少见，就算路由器等 NAT 设备支持 Full-cone NAT，通常也不会是默认选项。我们会在后面更详细地介绍它。 2. Restricted cone NAT 数据包流出：（跟 Full-cone NAT 完全一致）一旦内部地址（iAddr:iPort）映射到外部地址（eAddr:ePort），所有发自 iAddr:iPort 的数据包都经由 eAddr:ePort 向外发送。 数据包流入：只有内部地址（iAddr:iPort）主动连接过的外部主机（nAddr:any），发送到 eAddr:ePort 的数据包，才能通过 NAT 到达 iAddr:iPort. 跟 Full-cone NAT 的区别在于，它限制了外部主机的 IP 地址。只有主动连接过的主机，才能发送数据到 NAT 内部。这提升了一些安全性。 3. Port-Restricted cone NAT 数据包流出：（跟 Full-cone NAT 完全一致）一旦内部地址（iAddr:iPort）映射到外部地址（eAddr:ePort），所有发自 iAddr:iPort 的数据包都经由 eAddr:ePort 向外发送。 数据包流入：只有内部地址（iAddr:iPort）主动连接过的外部程序（nAddr:nPort），发送到 eAddr:ePort 的数据包，才能通过 NAT 到达 iAddr:iPort. 与 Address-Restricted cone NAT 的区别在于，它同时限制了外部主机的 IP 与端口，可以说是更进一步地提升了安全性。 4. Symmetric NATPort-Restricted cone NAT 的数据流入流出规则导致一个问题：同一个内部地址只能映射到唯一的一个 NAT 外部地址，也就只能与唯一的一个外部程序通讯，不能并发请求多个外部地址！这实际上限制了内部主机的最高并发连接数。 而 Symmetric NAT 解决了这个问题，它的数据流入流出规则如下： 数据包流出：同一个内部地址（iAddr:iPort）与不同外部主机（nAddr:nPort）的通信，会使用不同的 NAT 外部端口（eAddr:randomPort）。也就是说内部地址与 NAT 外部地址的关系也是一对多！ 通过允许端口的一对多映射，实际上提升了每个内部地址（iAddr:iPort）的并发连接数上限（从 1 直接扩到了可用端口数上限）。这也是 NAT 穿越最大的难点，它导致 Symmetric NAT 的端口难以预测！ 数据包流入：只有内部地址（iAddr:iPort）主动连接过的外部程序（nAddr:nPort），发送到 eAddr:ePort 的数据包，才能通过 NAT 到达 iAddr:iPort. 这个数据流入规则，与 Port-Restricted cone NAT 是完全一致的。 对称 NAT 是最安全的一种 NAT 结构，限制最为严格，应该也是应用最广泛的 NAT 结构。 但是它导致所有的 TCP 连接都只能由从内部主动发起，外部发起的 TCP 连接请求会直接被 NAT 拒绝，因此它也是 P2P 玩家最头疼的一种 NAT 类型。 解决方案是通过 UDP 迂回实现连接的建立，我们会在后面讨论这个问题。 5. Linux 中的 NAPTLinux 的网络栈中，可通过 iptables/netfilter 的 SNAT/MASQUERADE 实现 NAPT 网关，这种方式只能实现一个 Symmetric NAT. 也就是说绝大多数基于 Linux 实现的家庭局域网、Docker 虚拟网络、Kubernetes 虚拟网络、云服务的虚拟网络，都是 Symmetric NAT. 只有一些有 Full-cone NAT 需求的网吧、ISP 的 LSN(Large Scale NAT) 网关等组织，会使用非 Linux 内核的企业级路由器提供 Full-cone NAT 能力，这些设备可能是基于 FPGA 等专用芯片设计的。 想要将 Symmetric NAT 内的主机提供给外部访问，只能通过端口映射、一对一 NAT 等方式实现，后面会详细介绍这些方法。 RFC5389 定义的 NAT 类型（九种）RFC3489 这个早期 RFC 存在一些问题，问题之一就是它对 NAT 归类过于笼统，很多 NAPT 网关都无法很好的匹配上其中某个类别。 于是后来，RFC3489 被废弃并由 RFC5389 来替代，在 RFC5389 中，将 Mapping Behavior（映射规则）和 Filtering Behavior（过滤规则）分开来，定义了 3 种 Mapping Behavior（映射规则）和 3 种 Filtering Behavior（过滤规则），一共有 9 种组合。 1. 映射规则三种映射规则如图所示，假设一个内网主机 HostX 的内网 IP 地址为 X，端口号为 x，经 NAT 映射后的外网 IP 地址为 M，端口号为 m。为方便描述，将内网的 Endpoint 记为 Endpoint(X,x)，映射后外网的 Endpoint 记为 Endpoint(M,m)。内网 Endpoint(X,x) 发往外网 HostD1 的 IP 地址和端口号记为目的 Endpoint(D1,d1)；发往外网 HostD2 的 IP 地址和端口号记为目的 Endpoint(D2,d2)。 NAT 映射规则 EIM(Endpoint-Independent Mapping) 外部地址无关映射 对于一个内网 Endpoint(X,x)，其映射的外网 Endpoint(M,m) 是固定的。即从相同的 Endpoint(X,x) 发送到任何外部 IP 地址和任何外部端口的报文在 NAT 设备上使用相同的映射。 ADM(Address-Dependent Mapping) 外部地址相关映射：对于一个内网 Endpoint(X,x)，发往目的 Endpoint(D1,d1) 的报文，Endpoint(X,x) 被映射成 Endpoint(M1,m1)；发往目的 Endpoint(D2,d2) 的报文，Endpoint(X,x) 被映射成 Endpoint(M2,m2)。只要D1=D2，不管d1和d2是多少，都有 Endpoint(M1,m1)=Endpoint(M2,m2)。即从相同的 Endpoint(X,x) 发送到相同外部 IP 地址和任何外部端口的报文在 NAT 设备上使用相同的映射。 APDM（Address and Port-Dependent Mapping）外部地址和端口相关映射：对于一个内网 Endpoint(X,x)，发往目的 Endpoint(D1,d1) 的报文，Endpoint(X,x) 被映射成 Endpoint(M1,m1)；发往目的 Endpoint(D2,d2) 的报文， Endpoint(X,x) 被映射成 Endpoint(M2,m2)。只有当D1=D2，且d1=d2，才有 Endpoint(M1,m1)=Endpoint(M2,m2)。即从相同的 Endpoint(X,x) 发送到相同外部IP地址和相同外部端口的报文在NAT设备上使用相同的映射。 2. 过滤规则 NAT 过滤规则 EIF（Endpoint-Indep","date":"2022-05-13","objectID":"/posts/about-nat/:3:2","series":null,"tags":["NAT","网络","NAT 穿越","内网穿透","虚拟网络","P2P","WebRTC"],"title":"NAT 网关、NAT 穿越以及虚拟网络","uri":"/posts/about-nat/#2-一对多-nat---napt"},{"categories":["tech"],"content":" 2. 一对多 NAT - NAPT一对多 NAT，也被称为 NAPT（network address and port translation），同样在 RFC2663 中被定义。Easy IP 是 NAPT 的一个特殊形式。 NAPT 的主要应用场景是，内网用户需要访问到公网主机。绝大多数的家庭网络、办公网络都是 NAPT 类型的。 原因应该很好理解——家庭网络或办公网络都包含许多联网设备，但是这类网络通常只有一个或数个公网 IP，使用一对一 NAT 的话公网 IP 显然是不够用的，所以需要使用一对多 NAT. NAPT 通过同时利用 L3 的 IP 信息，以及 L4 传输层的端口信息，来为局域网设备提供透明的、配置方便的、支持超高并发连接的外部网络通信，示意图如下： NAPT 的端口分配与转换规则（Mapping Behavior）以及对外来流量的过滤规则（Filtering Behavior）都存在许多不同的实现，没有统一的规范与标准，但是存在两种分类规范，这种分类方法主要用在 NAT 穿越技术中。 RFC3489 定义的 NAT 类型（四种）在 RFC3489 中将 NAPT 分为四种类型，这也是应用最为广泛的 NAT 分类方法，如下图： 下面我们逐一介绍这四种不同的 NAPT 类型。 从这里开始，下文中的 NAT 特指 NAPT，如果涉及「一对一 NAT」会使用它的全名。 1. Full-cone NATFull-cone NAT 的特点如下： 数据包流出：一旦内部地址（iAddr:iPort）映射到外部地址（eAddr:ePort），所有发自 iAddr:iPort 的数据包都经由 eAddr:ePort 向外发送。 数据包流入：任意主机发送到 eAddr:ePort 的数据包，都能通过 NAT 到达 iAddr:iPort. 也就是不对外部进来的数据做任何限制，全部放行。 cone 圆锥，个人理解是一个比喻，任意发送进来的数据（多），都能通过 NAT 到达这个内部地址（一），就像一个圆锥。 允许任意主机发送到 eAddr:ePort 的数据到达内部地址是很危险的行为，因为内部主机不一定配置了合适的安全策略。 因此 Full-cone NAT 比较少见，就算路由器等 NAT 设备支持 Full-cone NAT，通常也不会是默认选项。我们会在后面更详细地介绍它。 2. Restricted cone NAT 数据包流出：（跟 Full-cone NAT 完全一致）一旦内部地址（iAddr:iPort）映射到外部地址（eAddr:ePort），所有发自 iAddr:iPort 的数据包都经由 eAddr:ePort 向外发送。 数据包流入：只有内部地址（iAddr:iPort）主动连接过的外部主机（nAddr:any），发送到 eAddr:ePort 的数据包，才能通过 NAT 到达 iAddr:iPort. 跟 Full-cone NAT 的区别在于，它限制了外部主机的 IP 地址。只有主动连接过的主机，才能发送数据到 NAT 内部。这提升了一些安全性。 3. Port-Restricted cone NAT 数据包流出：（跟 Full-cone NAT 完全一致）一旦内部地址（iAddr:iPort）映射到外部地址（eAddr:ePort），所有发自 iAddr:iPort 的数据包都经由 eAddr:ePort 向外发送。 数据包流入：只有内部地址（iAddr:iPort）主动连接过的外部程序（nAddr:nPort），发送到 eAddr:ePort 的数据包，才能通过 NAT 到达 iAddr:iPort. 与 Address-Restricted cone NAT 的区别在于，它同时限制了外部主机的 IP 与端口，可以说是更进一步地提升了安全性。 4. Symmetric NATPort-Restricted cone NAT 的数据流入流出规则导致一个问题：同一个内部地址只能映射到唯一的一个 NAT 外部地址，也就只能与唯一的一个外部程序通讯，不能并发请求多个外部地址！这实际上限制了内部主机的最高并发连接数。 而 Symmetric NAT 解决了这个问题，它的数据流入流出规则如下： 数据包流出：同一个内部地址（iAddr:iPort）与不同外部主机（nAddr:nPort）的通信，会使用不同的 NAT 外部端口（eAddr:randomPort）。也就是说内部地址与 NAT 外部地址的关系也是一对多！ 通过允许端口的一对多映射，实际上提升了每个内部地址（iAddr:iPort）的并发连接数上限（从 1 直接扩到了可用端口数上限）。这也是 NAT 穿越最大的难点，它导致 Symmetric NAT 的端口难以预测！ 数据包流入：只有内部地址（iAddr:iPort）主动连接过的外部程序（nAddr:nPort），发送到 eAddr:ePort 的数据包，才能通过 NAT 到达 iAddr:iPort. 这个数据流入规则，与 Port-Restricted cone NAT 是完全一致的。 对称 NAT 是最安全的一种 NAT 结构，限制最为严格，应该也是应用最广泛的 NAT 结构。 但是它导致所有的 TCP 连接都只能由从内部主动发起，外部发起的 TCP 连接请求会直接被 NAT 拒绝，因此它也是 P2P 玩家最头疼的一种 NAT 类型。 解决方案是通过 UDP 迂回实现连接的建立，我们会在后面讨论这个问题。 5. Linux 中的 NAPTLinux 的网络栈中，可通过 iptables/netfilter 的 SNAT/MASQUERADE 实现 NAPT 网关，这种方式只能实现一个 Symmetric NAT. 也就是说绝大多数基于 Linux 实现的家庭局域网、Docker 虚拟网络、Kubernetes 虚拟网络、云服务的虚拟网络，都是 Symmetric NAT. 只有一些有 Full-cone NAT 需求的网吧、ISP 的 LSN(Large Scale NAT) 网关等组织，会使用非 Linux 内核的企业级路由器提供 Full-cone NAT 能力，这些设备可能是基于 FPGA 等专用芯片设计的。 想要将 Symmetric NAT 内的主机提供给外部访问，只能通过端口映射、一对一 NAT 等方式实现，后面会详细介绍这些方法。 RFC5389 定义的 NAT 类型（九种）RFC3489 这个早期 RFC 存在一些问题，问题之一就是它对 NAT 归类过于笼统，很多 NAPT 网关都无法很好的匹配上其中某个类别。 于是后来，RFC3489 被废弃并由 RFC5389 来替代，在 RFC5389 中，将 Mapping Behavior（映射规则）和 Filtering Behavior（过滤规则）分开来，定义了 3 种 Mapping Behavior（映射规则）和 3 种 Filtering Behavior（过滤规则），一共有 9 种组合。 1. 映射规则三种映射规则如图所示，假设一个内网主机 HostX 的内网 IP 地址为 X，端口号为 x，经 NAT 映射后的外网 IP 地址为 M，端口号为 m。为方便描述，将内网的 Endpoint 记为 Endpoint(X,x)，映射后外网的 Endpoint 记为 Endpoint(M,m)。内网 Endpoint(X,x) 发往外网 HostD1 的 IP 地址和端口号记为目的 Endpoint(D1,d1)；发往外网 HostD2 的 IP 地址和端口号记为目的 Endpoint(D2,d2)。 NAT 映射规则 EIM(Endpoint-Independent Mapping) 外部地址无关映射 对于一个内网 Endpoint(X,x)，其映射的外网 Endpoint(M,m) 是固定的。即从相同的 Endpoint(X,x) 发送到任何外部 IP 地址和任何外部端口的报文在 NAT 设备上使用相同的映射。 ADM(Address-Dependent Mapping) 外部地址相关映射：对于一个内网 Endpoint(X,x)，发往目的 Endpoint(D1,d1) 的报文，Endpoint(X,x) 被映射成 Endpoint(M1,m1)；发往目的 Endpoint(D2,d2) 的报文，Endpoint(X,x) 被映射成 Endpoint(M2,m2)。只要D1=D2，不管d1和d2是多少，都有 Endpoint(M1,m1)=Endpoint(M2,m2)。即从相同的 Endpoint(X,x) 发送到相同外部 IP 地址和任何外部端口的报文在 NAT 设备上使用相同的映射。 APDM（Address and Port-Dependent Mapping）外部地址和端口相关映射：对于一个内网 Endpoint(X,x)，发往目的 Endpoint(D1,d1) 的报文，Endpoint(X,x) 被映射成 Endpoint(M1,m1)；发往目的 Endpoint(D2,d2) 的报文， Endpoint(X,x) 被映射成 Endpoint(M2,m2)。只有当D1=D2，且d1=d2，才有 Endpoint(M1,m1)=Endpoint(M2,m2)。即从相同的 Endpoint(X,x) 发送到相同外部IP地址和相同外部端口的报文在NAT设备上使用相同的映射。 2. 过滤规则 NAT 过滤规则 EIF（Endpoint-Indep","date":"2022-05-13","objectID":"/posts/about-nat/:3:2","series":null,"tags":["NAT","网络","NAT 穿越","内网穿透","虚拟网络","P2P","WebRTC"],"title":"NAT 网关、NAT 穿越以及虚拟网络","uri":"/posts/about-nat/#rfc3489-定义的-nat-类型四种"},{"categories":["tech"],"content":" 2. 一对多 NAT - NAPT一对多 NAT，也被称为 NAPT（network address and port translation），同样在 RFC2663 中被定义。Easy IP 是 NAPT 的一个特殊形式。 NAPT 的主要应用场景是，内网用户需要访问到公网主机。绝大多数的家庭网络、办公网络都是 NAPT 类型的。 原因应该很好理解——家庭网络或办公网络都包含许多联网设备，但是这类网络通常只有一个或数个公网 IP，使用一对一 NAT 的话公网 IP 显然是不够用的，所以需要使用一对多 NAT. NAPT 通过同时利用 L3 的 IP 信息，以及 L4 传输层的端口信息，来为局域网设备提供透明的、配置方便的、支持超高并发连接的外部网络通信，示意图如下： NAPT 的端口分配与转换规则（Mapping Behavior）以及对外来流量的过滤规则（Filtering Behavior）都存在许多不同的实现，没有统一的规范与标准，但是存在两种分类规范，这种分类方法主要用在 NAT 穿越技术中。 RFC3489 定义的 NAT 类型（四种）在 RFC3489 中将 NAPT 分为四种类型，这也是应用最为广泛的 NAT 分类方法，如下图： 下面我们逐一介绍这四种不同的 NAPT 类型。 从这里开始，下文中的 NAT 特指 NAPT，如果涉及「一对一 NAT」会使用它的全名。 1. Full-cone NATFull-cone NAT 的特点如下： 数据包流出：一旦内部地址（iAddr:iPort）映射到外部地址（eAddr:ePort），所有发自 iAddr:iPort 的数据包都经由 eAddr:ePort 向外发送。 数据包流入：任意主机发送到 eAddr:ePort 的数据包，都能通过 NAT 到达 iAddr:iPort. 也就是不对外部进来的数据做任何限制，全部放行。 cone 圆锥，个人理解是一个比喻，任意发送进来的数据（多），都能通过 NAT 到达这个内部地址（一），就像一个圆锥。 允许任意主机发送到 eAddr:ePort 的数据到达内部地址是很危险的行为，因为内部主机不一定配置了合适的安全策略。 因此 Full-cone NAT 比较少见，就算路由器等 NAT 设备支持 Full-cone NAT，通常也不会是默认选项。我们会在后面更详细地介绍它。 2. Restricted cone NAT 数据包流出：（跟 Full-cone NAT 完全一致）一旦内部地址（iAddr:iPort）映射到外部地址（eAddr:ePort），所有发自 iAddr:iPort 的数据包都经由 eAddr:ePort 向外发送。 数据包流入：只有内部地址（iAddr:iPort）主动连接过的外部主机（nAddr:any），发送到 eAddr:ePort 的数据包，才能通过 NAT 到达 iAddr:iPort. 跟 Full-cone NAT 的区别在于，它限制了外部主机的 IP 地址。只有主动连接过的主机，才能发送数据到 NAT 内部。这提升了一些安全性。 3. Port-Restricted cone NAT 数据包流出：（跟 Full-cone NAT 完全一致）一旦内部地址（iAddr:iPort）映射到外部地址（eAddr:ePort），所有发自 iAddr:iPort 的数据包都经由 eAddr:ePort 向外发送。 数据包流入：只有内部地址（iAddr:iPort）主动连接过的外部程序（nAddr:nPort），发送到 eAddr:ePort 的数据包，才能通过 NAT 到达 iAddr:iPort. 与 Address-Restricted cone NAT 的区别在于，它同时限制了外部主机的 IP 与端口，可以说是更进一步地提升了安全性。 4. Symmetric NATPort-Restricted cone NAT 的数据流入流出规则导致一个问题：同一个内部地址只能映射到唯一的一个 NAT 外部地址，也就只能与唯一的一个外部程序通讯，不能并发请求多个外部地址！这实际上限制了内部主机的最高并发连接数。 而 Symmetric NAT 解决了这个问题，它的数据流入流出规则如下： 数据包流出：同一个内部地址（iAddr:iPort）与不同外部主机（nAddr:nPort）的通信，会使用不同的 NAT 外部端口（eAddr:randomPort）。也就是说内部地址与 NAT 外部地址的关系也是一对多！ 通过允许端口的一对多映射，实际上提升了每个内部地址（iAddr:iPort）的并发连接数上限（从 1 直接扩到了可用端口数上限）。这也是 NAT 穿越最大的难点，它导致 Symmetric NAT 的端口难以预测！ 数据包流入：只有内部地址（iAddr:iPort）主动连接过的外部程序（nAddr:nPort），发送到 eAddr:ePort 的数据包，才能通过 NAT 到达 iAddr:iPort. 这个数据流入规则，与 Port-Restricted cone NAT 是完全一致的。 对称 NAT 是最安全的一种 NAT 结构，限制最为严格，应该也是应用最广泛的 NAT 结构。 但是它导致所有的 TCP 连接都只能由从内部主动发起，外部发起的 TCP 连接请求会直接被 NAT 拒绝，因此它也是 P2P 玩家最头疼的一种 NAT 类型。 解决方案是通过 UDP 迂回实现连接的建立，我们会在后面讨论这个问题。 5. Linux 中的 NAPTLinux 的网络栈中，可通过 iptables/netfilter 的 SNAT/MASQUERADE 实现 NAPT 网关，这种方式只能实现一个 Symmetric NAT. 也就是说绝大多数基于 Linux 实现的家庭局域网、Docker 虚拟网络、Kubernetes 虚拟网络、云服务的虚拟网络，都是 Symmetric NAT. 只有一些有 Full-cone NAT 需求的网吧、ISP 的 LSN(Large Scale NAT) 网关等组织，会使用非 Linux 内核的企业级路由器提供 Full-cone NAT 能力，这些设备可能是基于 FPGA 等专用芯片设计的。 想要将 Symmetric NAT 内的主机提供给外部访问，只能通过端口映射、一对一 NAT 等方式实现，后面会详细介绍这些方法。 RFC5389 定义的 NAT 类型（九种）RFC3489 这个早期 RFC 存在一些问题，问题之一就是它对 NAT 归类过于笼统，很多 NAPT 网关都无法很好的匹配上其中某个类别。 于是后来，RFC3489 被废弃并由 RFC5389 来替代，在 RFC5389 中，将 Mapping Behavior（映射规则）和 Filtering Behavior（过滤规则）分开来，定义了 3 种 Mapping Behavior（映射规则）和 3 种 Filtering Behavior（过滤规则），一共有 9 种组合。 1. 映射规则三种映射规则如图所示，假设一个内网主机 HostX 的内网 IP 地址为 X，端口号为 x，经 NAT 映射后的外网 IP 地址为 M，端口号为 m。为方便描述，将内网的 Endpoint 记为 Endpoint(X,x)，映射后外网的 Endpoint 记为 Endpoint(M,m)。内网 Endpoint(X,x) 发往外网 HostD1 的 IP 地址和端口号记为目的 Endpoint(D1,d1)；发往外网 HostD2 的 IP 地址和端口号记为目的 Endpoint(D2,d2)。 NAT 映射规则 EIM(Endpoint-Independent Mapping) 外部地址无关映射 对于一个内网 Endpoint(X,x)，其映射的外网 Endpoint(M,m) 是固定的。即从相同的 Endpoint(X,x) 发送到任何外部 IP 地址和任何外部端口的报文在 NAT 设备上使用相同的映射。 ADM(Address-Dependent Mapping) 外部地址相关映射：对于一个内网 Endpoint(X,x)，发往目的 Endpoint(D1,d1) 的报文，Endpoint(X,x) 被映射成 Endpoint(M1,m1)；发往目的 Endpoint(D2,d2) 的报文，Endpoint(X,x) 被映射成 Endpoint(M2,m2)。只要D1=D2，不管d1和d2是多少，都有 Endpoint(M1,m1)=Endpoint(M2,m2)。即从相同的 Endpoint(X,x) 发送到相同外部 IP 地址和任何外部端口的报文在 NAT 设备上使用相同的映射。 APDM（Address and Port-Dependent Mapping）外部地址和端口相关映射：对于一个内网 Endpoint(X,x)，发往目的 Endpoint(D1,d1) 的报文，Endpoint(X,x) 被映射成 Endpoint(M1,m1)；发往目的 Endpoint(D2,d2) 的报文， Endpoint(X,x) 被映射成 Endpoint(M2,m2)。只有当D1=D2，且d1=d2，才有 Endpoint(M1,m1)=Endpoint(M2,m2)。即从相同的 Endpoint(X,x) 发送到相同外部IP地址和相同外部端口的报文在NAT设备上使用相同的映射。 2. 过滤规则 NAT 过滤规则 EIF（Endpoint-Indep","date":"2022-05-13","objectID":"/posts/about-nat/:3:2","series":null,"tags":["NAT","网络","NAT 穿越","内网穿透","虚拟网络","P2P","WebRTC"],"title":"NAT 网关、NAT 穿越以及虚拟网络","uri":"/posts/about-nat/#1-full-cone-nat"},{"categories":["tech"],"content":" 2. 一对多 NAT - NAPT一对多 NAT，也被称为 NAPT（network address and port translation），同样在 RFC2663 中被定义。Easy IP 是 NAPT 的一个特殊形式。 NAPT 的主要应用场景是，内网用户需要访问到公网主机。绝大多数的家庭网络、办公网络都是 NAPT 类型的。 原因应该很好理解——家庭网络或办公网络都包含许多联网设备，但是这类网络通常只有一个或数个公网 IP，使用一对一 NAT 的话公网 IP 显然是不够用的，所以需要使用一对多 NAT. NAPT 通过同时利用 L3 的 IP 信息，以及 L4 传输层的端口信息，来为局域网设备提供透明的、配置方便的、支持超高并发连接的外部网络通信，示意图如下： NAPT 的端口分配与转换规则（Mapping Behavior）以及对外来流量的过滤规则（Filtering Behavior）都存在许多不同的实现，没有统一的规范与标准，但是存在两种分类规范，这种分类方法主要用在 NAT 穿越技术中。 RFC3489 定义的 NAT 类型（四种）在 RFC3489 中将 NAPT 分为四种类型，这也是应用最为广泛的 NAT 分类方法，如下图： 下面我们逐一介绍这四种不同的 NAPT 类型。 从这里开始，下文中的 NAT 特指 NAPT，如果涉及「一对一 NAT」会使用它的全名。 1. Full-cone NATFull-cone NAT 的特点如下： 数据包流出：一旦内部地址（iAddr:iPort）映射到外部地址（eAddr:ePort），所有发自 iAddr:iPort 的数据包都经由 eAddr:ePort 向外发送。 数据包流入：任意主机发送到 eAddr:ePort 的数据包，都能通过 NAT 到达 iAddr:iPort. 也就是不对外部进来的数据做任何限制，全部放行。 cone 圆锥，个人理解是一个比喻，任意发送进来的数据（多），都能通过 NAT 到达这个内部地址（一），就像一个圆锥。 允许任意主机发送到 eAddr:ePort 的数据到达内部地址是很危险的行为，因为内部主机不一定配置了合适的安全策略。 因此 Full-cone NAT 比较少见，就算路由器等 NAT 设备支持 Full-cone NAT，通常也不会是默认选项。我们会在后面更详细地介绍它。 2. Restricted cone NAT 数据包流出：（跟 Full-cone NAT 完全一致）一旦内部地址（iAddr:iPort）映射到外部地址（eAddr:ePort），所有发自 iAddr:iPort 的数据包都经由 eAddr:ePort 向外发送。 数据包流入：只有内部地址（iAddr:iPort）主动连接过的外部主机（nAddr:any），发送到 eAddr:ePort 的数据包，才能通过 NAT 到达 iAddr:iPort. 跟 Full-cone NAT 的区别在于，它限制了外部主机的 IP 地址。只有主动连接过的主机，才能发送数据到 NAT 内部。这提升了一些安全性。 3. Port-Restricted cone NAT 数据包流出：（跟 Full-cone NAT 完全一致）一旦内部地址（iAddr:iPort）映射到外部地址（eAddr:ePort），所有发自 iAddr:iPort 的数据包都经由 eAddr:ePort 向外发送。 数据包流入：只有内部地址（iAddr:iPort）主动连接过的外部程序（nAddr:nPort），发送到 eAddr:ePort 的数据包，才能通过 NAT 到达 iAddr:iPort. 与 Address-Restricted cone NAT 的区别在于，它同时限制了外部主机的 IP 与端口，可以说是更进一步地提升了安全性。 4. Symmetric NATPort-Restricted cone NAT 的数据流入流出规则导致一个问题：同一个内部地址只能映射到唯一的一个 NAT 外部地址，也就只能与唯一的一个外部程序通讯，不能并发请求多个外部地址！这实际上限制了内部主机的最高并发连接数。 而 Symmetric NAT 解决了这个问题，它的数据流入流出规则如下： 数据包流出：同一个内部地址（iAddr:iPort）与不同外部主机（nAddr:nPort）的通信，会使用不同的 NAT 外部端口（eAddr:randomPort）。也就是说内部地址与 NAT 外部地址的关系也是一对多！ 通过允许端口的一对多映射，实际上提升了每个内部地址（iAddr:iPort）的并发连接数上限（从 1 直接扩到了可用端口数上限）。这也是 NAT 穿越最大的难点，它导致 Symmetric NAT 的端口难以预测！ 数据包流入：只有内部地址（iAddr:iPort）主动连接过的外部程序（nAddr:nPort），发送到 eAddr:ePort 的数据包，才能通过 NAT 到达 iAddr:iPort. 这个数据流入规则，与 Port-Restricted cone NAT 是完全一致的。 对称 NAT 是最安全的一种 NAT 结构，限制最为严格，应该也是应用最广泛的 NAT 结构。 但是它导致所有的 TCP 连接都只能由从内部主动发起，外部发起的 TCP 连接请求会直接被 NAT 拒绝，因此它也是 P2P 玩家最头疼的一种 NAT 类型。 解决方案是通过 UDP 迂回实现连接的建立，我们会在后面讨论这个问题。 5. Linux 中的 NAPTLinux 的网络栈中，可通过 iptables/netfilter 的 SNAT/MASQUERADE 实现 NAPT 网关，这种方式只能实现一个 Symmetric NAT. 也就是说绝大多数基于 Linux 实现的家庭局域网、Docker 虚拟网络、Kubernetes 虚拟网络、云服务的虚拟网络，都是 Symmetric NAT. 只有一些有 Full-cone NAT 需求的网吧、ISP 的 LSN(Large Scale NAT) 网关等组织，会使用非 Linux 内核的企业级路由器提供 Full-cone NAT 能力，这些设备可能是基于 FPGA 等专用芯片设计的。 想要将 Symmetric NAT 内的主机提供给外部访问，只能通过端口映射、一对一 NAT 等方式实现，后面会详细介绍这些方法。 RFC5389 定义的 NAT 类型（九种）RFC3489 这个早期 RFC 存在一些问题，问题之一就是它对 NAT 归类过于笼统，很多 NAPT 网关都无法很好的匹配上其中某个类别。 于是后来，RFC3489 被废弃并由 RFC5389 来替代，在 RFC5389 中，将 Mapping Behavior（映射规则）和 Filtering Behavior（过滤规则）分开来，定义了 3 种 Mapping Behavior（映射规则）和 3 种 Filtering Behavior（过滤规则），一共有 9 种组合。 1. 映射规则三种映射规则如图所示，假设一个内网主机 HostX 的内网 IP 地址为 X，端口号为 x，经 NAT 映射后的外网 IP 地址为 M，端口号为 m。为方便描述，将内网的 Endpoint 记为 Endpoint(X,x)，映射后外网的 Endpoint 记为 Endpoint(M,m)。内网 Endpoint(X,x) 发往外网 HostD1 的 IP 地址和端口号记为目的 Endpoint(D1,d1)；发往外网 HostD2 的 IP 地址和端口号记为目的 Endpoint(D2,d2)。 NAT 映射规则 EIM(Endpoint-Independent Mapping) 外部地址无关映射 对于一个内网 Endpoint(X,x)，其映射的外网 Endpoint(M,m) 是固定的。即从相同的 Endpoint(X,x) 发送到任何外部 IP 地址和任何外部端口的报文在 NAT 设备上使用相同的映射。 ADM(Address-Dependent Mapping) 外部地址相关映射：对于一个内网 Endpoint(X,x)，发往目的 Endpoint(D1,d1) 的报文，Endpoint(X,x) 被映射成 Endpoint(M1,m1)；发往目的 Endpoint(D2,d2) 的报文，Endpoint(X,x) 被映射成 Endpoint(M2,m2)。只要D1=D2，不管d1和d2是多少，都有 Endpoint(M1,m1)=Endpoint(M2,m2)。即从相同的 Endpoint(X,x) 发送到相同外部 IP 地址和任何外部端口的报文在 NAT 设备上使用相同的映射。 APDM（Address and Port-Dependent Mapping）外部地址和端口相关映射：对于一个内网 Endpoint(X,x)，发往目的 Endpoint(D1,d1) 的报文，Endpoint(X,x) 被映射成 Endpoint(M1,m1)；发往目的 Endpoint(D2,d2) 的报文， Endpoint(X,x) 被映射成 Endpoint(M2,m2)。只有当D1=D2，且d1=d2，才有 Endpoint(M1,m1)=Endpoint(M2,m2)。即从相同的 Endpoint(X,x) 发送到相同外部IP地址和相同外部端口的报文在NAT设备上使用相同的映射。 2. 过滤规则 NAT 过滤规则 EIF（Endpoint-Indep","date":"2022-05-13","objectID":"/posts/about-nat/:3:2","series":null,"tags":["NAT","网络","NAT 穿越","内网穿透","虚拟网络","P2P","WebRTC"],"title":"NAT 网关、NAT 穿越以及虚拟网络","uri":"/posts/about-nat/#2-restricted-cone-nat"},{"categories":["tech"],"content":" 2. 一对多 NAT - NAPT一对多 NAT，也被称为 NAPT（network address and port translation），同样在 RFC2663 中被定义。Easy IP 是 NAPT 的一个特殊形式。 NAPT 的主要应用场景是，内网用户需要访问到公网主机。绝大多数的家庭网络、办公网络都是 NAPT 类型的。 原因应该很好理解——家庭网络或办公网络都包含许多联网设备，但是这类网络通常只有一个或数个公网 IP，使用一对一 NAT 的话公网 IP 显然是不够用的，所以需要使用一对多 NAT. NAPT 通过同时利用 L3 的 IP 信息，以及 L4 传输层的端口信息，来为局域网设备提供透明的、配置方便的、支持超高并发连接的外部网络通信，示意图如下： NAPT 的端口分配与转换规则（Mapping Behavior）以及对外来流量的过滤规则（Filtering Behavior）都存在许多不同的实现，没有统一的规范与标准，但是存在两种分类规范，这种分类方法主要用在 NAT 穿越技术中。 RFC3489 定义的 NAT 类型（四种）在 RFC3489 中将 NAPT 分为四种类型，这也是应用最为广泛的 NAT 分类方法，如下图： 下面我们逐一介绍这四种不同的 NAPT 类型。 从这里开始，下文中的 NAT 特指 NAPT，如果涉及「一对一 NAT」会使用它的全名。 1. Full-cone NATFull-cone NAT 的特点如下： 数据包流出：一旦内部地址（iAddr:iPort）映射到外部地址（eAddr:ePort），所有发自 iAddr:iPort 的数据包都经由 eAddr:ePort 向外发送。 数据包流入：任意主机发送到 eAddr:ePort 的数据包，都能通过 NAT 到达 iAddr:iPort. 也就是不对外部进来的数据做任何限制，全部放行。 cone 圆锥，个人理解是一个比喻，任意发送进来的数据（多），都能通过 NAT 到达这个内部地址（一），就像一个圆锥。 允许任意主机发送到 eAddr:ePort 的数据到达内部地址是很危险的行为，因为内部主机不一定配置了合适的安全策略。 因此 Full-cone NAT 比较少见，就算路由器等 NAT 设备支持 Full-cone NAT，通常也不会是默认选项。我们会在后面更详细地介绍它。 2. Restricted cone NAT 数据包流出：（跟 Full-cone NAT 完全一致）一旦内部地址（iAddr:iPort）映射到外部地址（eAddr:ePort），所有发自 iAddr:iPort 的数据包都经由 eAddr:ePort 向外发送。 数据包流入：只有内部地址（iAddr:iPort）主动连接过的外部主机（nAddr:any），发送到 eAddr:ePort 的数据包，才能通过 NAT 到达 iAddr:iPort. 跟 Full-cone NAT 的区别在于，它限制了外部主机的 IP 地址。只有主动连接过的主机，才能发送数据到 NAT 内部。这提升了一些安全性。 3. Port-Restricted cone NAT 数据包流出：（跟 Full-cone NAT 完全一致）一旦内部地址（iAddr:iPort）映射到外部地址（eAddr:ePort），所有发自 iAddr:iPort 的数据包都经由 eAddr:ePort 向外发送。 数据包流入：只有内部地址（iAddr:iPort）主动连接过的外部程序（nAddr:nPort），发送到 eAddr:ePort 的数据包，才能通过 NAT 到达 iAddr:iPort. 与 Address-Restricted cone NAT 的区别在于，它同时限制了外部主机的 IP 与端口，可以说是更进一步地提升了安全性。 4. Symmetric NATPort-Restricted cone NAT 的数据流入流出规则导致一个问题：同一个内部地址只能映射到唯一的一个 NAT 外部地址，也就只能与唯一的一个外部程序通讯，不能并发请求多个外部地址！这实际上限制了内部主机的最高并发连接数。 而 Symmetric NAT 解决了这个问题，它的数据流入流出规则如下： 数据包流出：同一个内部地址（iAddr:iPort）与不同外部主机（nAddr:nPort）的通信，会使用不同的 NAT 外部端口（eAddr:randomPort）。也就是说内部地址与 NAT 外部地址的关系也是一对多！ 通过允许端口的一对多映射，实际上提升了每个内部地址（iAddr:iPort）的并发连接数上限（从 1 直接扩到了可用端口数上限）。这也是 NAT 穿越最大的难点，它导致 Symmetric NAT 的端口难以预测！ 数据包流入：只有内部地址（iAddr:iPort）主动连接过的外部程序（nAddr:nPort），发送到 eAddr:ePort 的数据包，才能通过 NAT 到达 iAddr:iPort. 这个数据流入规则，与 Port-Restricted cone NAT 是完全一致的。 对称 NAT 是最安全的一种 NAT 结构，限制最为严格，应该也是应用最广泛的 NAT 结构。 但是它导致所有的 TCP 连接都只能由从内部主动发起，外部发起的 TCP 连接请求会直接被 NAT 拒绝，因此它也是 P2P 玩家最头疼的一种 NAT 类型。 解决方案是通过 UDP 迂回实现连接的建立，我们会在后面讨论这个问题。 5. Linux 中的 NAPTLinux 的网络栈中，可通过 iptables/netfilter 的 SNAT/MASQUERADE 实现 NAPT 网关，这种方式只能实现一个 Symmetric NAT. 也就是说绝大多数基于 Linux 实现的家庭局域网、Docker 虚拟网络、Kubernetes 虚拟网络、云服务的虚拟网络，都是 Symmetric NAT. 只有一些有 Full-cone NAT 需求的网吧、ISP 的 LSN(Large Scale NAT) 网关等组织，会使用非 Linux 内核的企业级路由器提供 Full-cone NAT 能力，这些设备可能是基于 FPGA 等专用芯片设计的。 想要将 Symmetric NAT 内的主机提供给外部访问，只能通过端口映射、一对一 NAT 等方式实现，后面会详细介绍这些方法。 RFC5389 定义的 NAT 类型（九种）RFC3489 这个早期 RFC 存在一些问题，问题之一就是它对 NAT 归类过于笼统，很多 NAPT 网关都无法很好的匹配上其中某个类别。 于是后来，RFC3489 被废弃并由 RFC5389 来替代，在 RFC5389 中，将 Mapping Behavior（映射规则）和 Filtering Behavior（过滤规则）分开来，定义了 3 种 Mapping Behavior（映射规则）和 3 种 Filtering Behavior（过滤规则），一共有 9 种组合。 1. 映射规则三种映射规则如图所示，假设一个内网主机 HostX 的内网 IP 地址为 X，端口号为 x，经 NAT 映射后的外网 IP 地址为 M，端口号为 m。为方便描述，将内网的 Endpoint 记为 Endpoint(X,x)，映射后外网的 Endpoint 记为 Endpoint(M,m)。内网 Endpoint(X,x) 发往外网 HostD1 的 IP 地址和端口号记为目的 Endpoint(D1,d1)；发往外网 HostD2 的 IP 地址和端口号记为目的 Endpoint(D2,d2)。 NAT 映射规则 EIM(Endpoint-Independent Mapping) 外部地址无关映射 对于一个内网 Endpoint(X,x)，其映射的外网 Endpoint(M,m) 是固定的。即从相同的 Endpoint(X,x) 发送到任何外部 IP 地址和任何外部端口的报文在 NAT 设备上使用相同的映射。 ADM(Address-Dependent Mapping) 外部地址相关映射：对于一个内网 Endpoint(X,x)，发往目的 Endpoint(D1,d1) 的报文，Endpoint(X,x) 被映射成 Endpoint(M1,m1)；发往目的 Endpoint(D2,d2) 的报文，Endpoint(X,x) 被映射成 Endpoint(M2,m2)。只要D1=D2，不管d1和d2是多少，都有 Endpoint(M1,m1)=Endpoint(M2,m2)。即从相同的 Endpoint(X,x) 发送到相同外部 IP 地址和任何外部端口的报文在 NAT 设备上使用相同的映射。 APDM（Address and Port-Dependent Mapping）外部地址和端口相关映射：对于一个内网 Endpoint(X,x)，发往目的 Endpoint(D1,d1) 的报文，Endpoint(X,x) 被映射成 Endpoint(M1,m1)；发往目的 Endpoint(D2,d2) 的报文， Endpoint(X,x) 被映射成 Endpoint(M2,m2)。只有当D1=D2，且d1=d2，才有 Endpoint(M1,m1)=Endpoint(M2,m2)。即从相同的 Endpoint(X,x) 发送到相同外部IP地址和相同外部端口的报文在NAT设备上使用相同的映射。 2. 过滤规则 NAT 过滤规则 EIF（Endpoint-Indep","date":"2022-05-13","objectID":"/posts/about-nat/:3:2","series":null,"tags":["NAT","网络","NAT 穿越","内网穿透","虚拟网络","P2P","WebRTC"],"title":"NAT 网关、NAT 穿越以及虚拟网络","uri":"/posts/about-nat/#3-port-restricted-cone-nat"},{"categories":["tech"],"content":" 2. 一对多 NAT - NAPT一对多 NAT，也被称为 NAPT（network address and port translation），同样在 RFC2663 中被定义。Easy IP 是 NAPT 的一个特殊形式。 NAPT 的主要应用场景是，内网用户需要访问到公网主机。绝大多数的家庭网络、办公网络都是 NAPT 类型的。 原因应该很好理解——家庭网络或办公网络都包含许多联网设备，但是这类网络通常只有一个或数个公网 IP，使用一对一 NAT 的话公网 IP 显然是不够用的，所以需要使用一对多 NAT. NAPT 通过同时利用 L3 的 IP 信息，以及 L4 传输层的端口信息，来为局域网设备提供透明的、配置方便的、支持超高并发连接的外部网络通信，示意图如下： NAPT 的端口分配与转换规则（Mapping Behavior）以及对外来流量的过滤规则（Filtering Behavior）都存在许多不同的实现，没有统一的规范与标准，但是存在两种分类规范，这种分类方法主要用在 NAT 穿越技术中。 RFC3489 定义的 NAT 类型（四种）在 RFC3489 中将 NAPT 分为四种类型，这也是应用最为广泛的 NAT 分类方法，如下图： 下面我们逐一介绍这四种不同的 NAPT 类型。 从这里开始，下文中的 NAT 特指 NAPT，如果涉及「一对一 NAT」会使用它的全名。 1. Full-cone NATFull-cone NAT 的特点如下： 数据包流出：一旦内部地址（iAddr:iPort）映射到外部地址（eAddr:ePort），所有发自 iAddr:iPort 的数据包都经由 eAddr:ePort 向外发送。 数据包流入：任意主机发送到 eAddr:ePort 的数据包，都能通过 NAT 到达 iAddr:iPort. 也就是不对外部进来的数据做任何限制，全部放行。 cone 圆锥，个人理解是一个比喻，任意发送进来的数据（多），都能通过 NAT 到达这个内部地址（一），就像一个圆锥。 允许任意主机发送到 eAddr:ePort 的数据到达内部地址是很危险的行为，因为内部主机不一定配置了合适的安全策略。 因此 Full-cone NAT 比较少见，就算路由器等 NAT 设备支持 Full-cone NAT，通常也不会是默认选项。我们会在后面更详细地介绍它。 2. Restricted cone NAT 数据包流出：（跟 Full-cone NAT 完全一致）一旦内部地址（iAddr:iPort）映射到外部地址（eAddr:ePort），所有发自 iAddr:iPort 的数据包都经由 eAddr:ePort 向外发送。 数据包流入：只有内部地址（iAddr:iPort）主动连接过的外部主机（nAddr:any），发送到 eAddr:ePort 的数据包，才能通过 NAT 到达 iAddr:iPort. 跟 Full-cone NAT 的区别在于，它限制了外部主机的 IP 地址。只有主动连接过的主机，才能发送数据到 NAT 内部。这提升了一些安全性。 3. Port-Restricted cone NAT 数据包流出：（跟 Full-cone NAT 完全一致）一旦内部地址（iAddr:iPort）映射到外部地址（eAddr:ePort），所有发自 iAddr:iPort 的数据包都经由 eAddr:ePort 向外发送。 数据包流入：只有内部地址（iAddr:iPort）主动连接过的外部程序（nAddr:nPort），发送到 eAddr:ePort 的数据包，才能通过 NAT 到达 iAddr:iPort. 与 Address-Restricted cone NAT 的区别在于，它同时限制了外部主机的 IP 与端口，可以说是更进一步地提升了安全性。 4. Symmetric NATPort-Restricted cone NAT 的数据流入流出规则导致一个问题：同一个内部地址只能映射到唯一的一个 NAT 外部地址，也就只能与唯一的一个外部程序通讯，不能并发请求多个外部地址！这实际上限制了内部主机的最高并发连接数。 而 Symmetric NAT 解决了这个问题，它的数据流入流出规则如下： 数据包流出：同一个内部地址（iAddr:iPort）与不同外部主机（nAddr:nPort）的通信，会使用不同的 NAT 外部端口（eAddr:randomPort）。也就是说内部地址与 NAT 外部地址的关系也是一对多！ 通过允许端口的一对多映射，实际上提升了每个内部地址（iAddr:iPort）的并发连接数上限（从 1 直接扩到了可用端口数上限）。这也是 NAT 穿越最大的难点，它导致 Symmetric NAT 的端口难以预测！ 数据包流入：只有内部地址（iAddr:iPort）主动连接过的外部程序（nAddr:nPort），发送到 eAddr:ePort 的数据包，才能通过 NAT 到达 iAddr:iPort. 这个数据流入规则，与 Port-Restricted cone NAT 是完全一致的。 对称 NAT 是最安全的一种 NAT 结构，限制最为严格，应该也是应用最广泛的 NAT 结构。 但是它导致所有的 TCP 连接都只能由从内部主动发起，外部发起的 TCP 连接请求会直接被 NAT 拒绝，因此它也是 P2P 玩家最头疼的一种 NAT 类型。 解决方案是通过 UDP 迂回实现连接的建立，我们会在后面讨论这个问题。 5. Linux 中的 NAPTLinux 的网络栈中，可通过 iptables/netfilter 的 SNAT/MASQUERADE 实现 NAPT 网关，这种方式只能实现一个 Symmetric NAT. 也就是说绝大多数基于 Linux 实现的家庭局域网、Docker 虚拟网络、Kubernetes 虚拟网络、云服务的虚拟网络，都是 Symmetric NAT. 只有一些有 Full-cone NAT 需求的网吧、ISP 的 LSN(Large Scale NAT) 网关等组织，会使用非 Linux 内核的企业级路由器提供 Full-cone NAT 能力，这些设备可能是基于 FPGA 等专用芯片设计的。 想要将 Symmetric NAT 内的主机提供给外部访问，只能通过端口映射、一对一 NAT 等方式实现，后面会详细介绍这些方法。 RFC5389 定义的 NAT 类型（九种）RFC3489 这个早期 RFC 存在一些问题，问题之一就是它对 NAT 归类过于笼统，很多 NAPT 网关都无法很好的匹配上其中某个类别。 于是后来，RFC3489 被废弃并由 RFC5389 来替代，在 RFC5389 中，将 Mapping Behavior（映射规则）和 Filtering Behavior（过滤规则）分开来，定义了 3 种 Mapping Behavior（映射规则）和 3 种 Filtering Behavior（过滤规则），一共有 9 种组合。 1. 映射规则三种映射规则如图所示，假设一个内网主机 HostX 的内网 IP 地址为 X，端口号为 x，经 NAT 映射后的外网 IP 地址为 M，端口号为 m。为方便描述，将内网的 Endpoint 记为 Endpoint(X,x)，映射后外网的 Endpoint 记为 Endpoint(M,m)。内网 Endpoint(X,x) 发往外网 HostD1 的 IP 地址和端口号记为目的 Endpoint(D1,d1)；发往外网 HostD2 的 IP 地址和端口号记为目的 Endpoint(D2,d2)。 NAT 映射规则 EIM(Endpoint-Independent Mapping) 外部地址无关映射 对于一个内网 Endpoint(X,x)，其映射的外网 Endpoint(M,m) 是固定的。即从相同的 Endpoint(X,x) 发送到任何外部 IP 地址和任何外部端口的报文在 NAT 设备上使用相同的映射。 ADM(Address-Dependent Mapping) 外部地址相关映射：对于一个内网 Endpoint(X,x)，发往目的 Endpoint(D1,d1) 的报文，Endpoint(X,x) 被映射成 Endpoint(M1,m1)；发往目的 Endpoint(D2,d2) 的报文，Endpoint(X,x) 被映射成 Endpoint(M2,m2)。只要D1=D2，不管d1和d2是多少，都有 Endpoint(M1,m1)=Endpoint(M2,m2)。即从相同的 Endpoint(X,x) 发送到相同外部 IP 地址和任何外部端口的报文在 NAT 设备上使用相同的映射。 APDM（Address and Port-Dependent Mapping）外部地址和端口相关映射：对于一个内网 Endpoint(X,x)，发往目的 Endpoint(D1,d1) 的报文，Endpoint(X,x) 被映射成 Endpoint(M1,m1)；发往目的 Endpoint(D2,d2) 的报文， Endpoint(X,x) 被映射成 Endpoint(M2,m2)。只有当D1=D2，且d1=d2，才有 Endpoint(M1,m1)=Endpoint(M2,m2)。即从相同的 Endpoint(X,x) 发送到相同外部IP地址和相同外部端口的报文在NAT设备上使用相同的映射。 2. 过滤规则 NAT 过滤规则 EIF（Endpoint-Indep","date":"2022-05-13","objectID":"/posts/about-nat/:3:2","series":null,"tags":["NAT","网络","NAT 穿越","内网穿透","虚拟网络","P2P","WebRTC"],"title":"NAT 网关、NAT 穿越以及虚拟网络","uri":"/posts/about-nat/#4-symmetric-nat"},{"categories":["tech"],"content":" 2. 一对多 NAT - NAPT一对多 NAT，也被称为 NAPT（network address and port translation），同样在 RFC2663 中被定义。Easy IP 是 NAPT 的一个特殊形式。 NAPT 的主要应用场景是，内网用户需要访问到公网主机。绝大多数的家庭网络、办公网络都是 NAPT 类型的。 原因应该很好理解——家庭网络或办公网络都包含许多联网设备，但是这类网络通常只有一个或数个公网 IP，使用一对一 NAT 的话公网 IP 显然是不够用的，所以需要使用一对多 NAT. NAPT 通过同时利用 L3 的 IP 信息，以及 L4 传输层的端口信息，来为局域网设备提供透明的、配置方便的、支持超高并发连接的外部网络通信，示意图如下： NAPT 的端口分配与转换规则（Mapping Behavior）以及对外来流量的过滤规则（Filtering Behavior）都存在许多不同的实现，没有统一的规范与标准，但是存在两种分类规范，这种分类方法主要用在 NAT 穿越技术中。 RFC3489 定义的 NAT 类型（四种）在 RFC3489 中将 NAPT 分为四种类型，这也是应用最为广泛的 NAT 分类方法，如下图： 下面我们逐一介绍这四种不同的 NAPT 类型。 从这里开始，下文中的 NAT 特指 NAPT，如果涉及「一对一 NAT」会使用它的全名。 1. Full-cone NATFull-cone NAT 的特点如下： 数据包流出：一旦内部地址（iAddr:iPort）映射到外部地址（eAddr:ePort），所有发自 iAddr:iPort 的数据包都经由 eAddr:ePort 向外发送。 数据包流入：任意主机发送到 eAddr:ePort 的数据包，都能通过 NAT 到达 iAddr:iPort. 也就是不对外部进来的数据做任何限制，全部放行。 cone 圆锥，个人理解是一个比喻，任意发送进来的数据（多），都能通过 NAT 到达这个内部地址（一），就像一个圆锥。 允许任意主机发送到 eAddr:ePort 的数据到达内部地址是很危险的行为，因为内部主机不一定配置了合适的安全策略。 因此 Full-cone NAT 比较少见，就算路由器等 NAT 设备支持 Full-cone NAT，通常也不会是默认选项。我们会在后面更详细地介绍它。 2. Restricted cone NAT 数据包流出：（跟 Full-cone NAT 完全一致）一旦内部地址（iAddr:iPort）映射到外部地址（eAddr:ePort），所有发自 iAddr:iPort 的数据包都经由 eAddr:ePort 向外发送。 数据包流入：只有内部地址（iAddr:iPort）主动连接过的外部主机（nAddr:any），发送到 eAddr:ePort 的数据包，才能通过 NAT 到达 iAddr:iPort. 跟 Full-cone NAT 的区别在于，它限制了外部主机的 IP 地址。只有主动连接过的主机，才能发送数据到 NAT 内部。这提升了一些安全性。 3. Port-Restricted cone NAT 数据包流出：（跟 Full-cone NAT 完全一致）一旦内部地址（iAddr:iPort）映射到外部地址（eAddr:ePort），所有发自 iAddr:iPort 的数据包都经由 eAddr:ePort 向外发送。 数据包流入：只有内部地址（iAddr:iPort）主动连接过的外部程序（nAddr:nPort），发送到 eAddr:ePort 的数据包，才能通过 NAT 到达 iAddr:iPort. 与 Address-Restricted cone NAT 的区别在于，它同时限制了外部主机的 IP 与端口，可以说是更进一步地提升了安全性。 4. Symmetric NATPort-Restricted cone NAT 的数据流入流出规则导致一个问题：同一个内部地址只能映射到唯一的一个 NAT 外部地址，也就只能与唯一的一个外部程序通讯，不能并发请求多个外部地址！这实际上限制了内部主机的最高并发连接数。 而 Symmetric NAT 解决了这个问题，它的数据流入流出规则如下： 数据包流出：同一个内部地址（iAddr:iPort）与不同外部主机（nAddr:nPort）的通信，会使用不同的 NAT 外部端口（eAddr:randomPort）。也就是说内部地址与 NAT 外部地址的关系也是一对多！ 通过允许端口的一对多映射，实际上提升了每个内部地址（iAddr:iPort）的并发连接数上限（从 1 直接扩到了可用端口数上限）。这也是 NAT 穿越最大的难点，它导致 Symmetric NAT 的端口难以预测！ 数据包流入：只有内部地址（iAddr:iPort）主动连接过的外部程序（nAddr:nPort），发送到 eAddr:ePort 的数据包，才能通过 NAT 到达 iAddr:iPort. 这个数据流入规则，与 Port-Restricted cone NAT 是完全一致的。 对称 NAT 是最安全的一种 NAT 结构，限制最为严格，应该也是应用最广泛的 NAT 结构。 但是它导致所有的 TCP 连接都只能由从内部主动发起，外部发起的 TCP 连接请求会直接被 NAT 拒绝，因此它也是 P2P 玩家最头疼的一种 NAT 类型。 解决方案是通过 UDP 迂回实现连接的建立，我们会在后面讨论这个问题。 5. Linux 中的 NAPTLinux 的网络栈中，可通过 iptables/netfilter 的 SNAT/MASQUERADE 实现 NAPT 网关，这种方式只能实现一个 Symmetric NAT. 也就是说绝大多数基于 Linux 实现的家庭局域网、Docker 虚拟网络、Kubernetes 虚拟网络、云服务的虚拟网络，都是 Symmetric NAT. 只有一些有 Full-cone NAT 需求的网吧、ISP 的 LSN(Large Scale NAT) 网关等组织，会使用非 Linux 内核的企业级路由器提供 Full-cone NAT 能力，这些设备可能是基于 FPGA 等专用芯片设计的。 想要将 Symmetric NAT 内的主机提供给外部访问，只能通过端口映射、一对一 NAT 等方式实现，后面会详细介绍这些方法。 RFC5389 定义的 NAT 类型（九种）RFC3489 这个早期 RFC 存在一些问题，问题之一就是它对 NAT 归类过于笼统，很多 NAPT 网关都无法很好的匹配上其中某个类别。 于是后来，RFC3489 被废弃并由 RFC5389 来替代，在 RFC5389 中，将 Mapping Behavior（映射规则）和 Filtering Behavior（过滤规则）分开来，定义了 3 种 Mapping Behavior（映射规则）和 3 种 Filtering Behavior（过滤规则），一共有 9 种组合。 1. 映射规则三种映射规则如图所示，假设一个内网主机 HostX 的内网 IP 地址为 X，端口号为 x，经 NAT 映射后的外网 IP 地址为 M，端口号为 m。为方便描述，将内网的 Endpoint 记为 Endpoint(X,x)，映射后外网的 Endpoint 记为 Endpoint(M,m)。内网 Endpoint(X,x) 发往外网 HostD1 的 IP 地址和端口号记为目的 Endpoint(D1,d1)；发往外网 HostD2 的 IP 地址和端口号记为目的 Endpoint(D2,d2)。 NAT 映射规则 EIM(Endpoint-Independent Mapping) 外部地址无关映射 对于一个内网 Endpoint(X,x)，其映射的外网 Endpoint(M,m) 是固定的。即从相同的 Endpoint(X,x) 发送到任何外部 IP 地址和任何外部端口的报文在 NAT 设备上使用相同的映射。 ADM(Address-Dependent Mapping) 外部地址相关映射：对于一个内网 Endpoint(X,x)，发往目的 Endpoint(D1,d1) 的报文，Endpoint(X,x) 被映射成 Endpoint(M1,m1)；发往目的 Endpoint(D2,d2) 的报文，Endpoint(X,x) 被映射成 Endpoint(M2,m2)。只要D1=D2，不管d1和d2是多少，都有 Endpoint(M1,m1)=Endpoint(M2,m2)。即从相同的 Endpoint(X,x) 发送到相同外部 IP 地址和任何外部端口的报文在 NAT 设备上使用相同的映射。 APDM（Address and Port-Dependent Mapping）外部地址和端口相关映射：对于一个内网 Endpoint(X,x)，发往目的 Endpoint(D1,d1) 的报文，Endpoint(X,x) 被映射成 Endpoint(M1,m1)；发往目的 Endpoint(D2,d2) 的报文， Endpoint(X,x) 被映射成 Endpoint(M2,m2)。只有当D1=D2，且d1=d2，才有 Endpoint(M1,m1)=Endpoint(M2,m2)。即从相同的 Endpoint(X,x) 发送到相同外部IP地址和相同外部端口的报文在NAT设备上使用相同的映射。 2. 过滤规则 NAT 过滤规则 EIF（Endpoint-Indep","date":"2022-05-13","objectID":"/posts/about-nat/:3:2","series":null,"tags":["NAT","网络","NAT 穿越","内网穿透","虚拟网络","P2P","WebRTC"],"title":"NAT 网关、NAT 穿越以及虚拟网络","uri":"/posts/about-nat/#5-linux-中的-napt"},{"categories":["tech"],"content":" 2. 一对多 NAT - NAPT一对多 NAT，也被称为 NAPT（network address and port translation），同样在 RFC2663 中被定义。Easy IP 是 NAPT 的一个特殊形式。 NAPT 的主要应用场景是，内网用户需要访问到公网主机。绝大多数的家庭网络、办公网络都是 NAPT 类型的。 原因应该很好理解——家庭网络或办公网络都包含许多联网设备，但是这类网络通常只有一个或数个公网 IP，使用一对一 NAT 的话公网 IP 显然是不够用的，所以需要使用一对多 NAT. NAPT 通过同时利用 L3 的 IP 信息，以及 L4 传输层的端口信息，来为局域网设备提供透明的、配置方便的、支持超高并发连接的外部网络通信，示意图如下： NAPT 的端口分配与转换规则（Mapping Behavior）以及对外来流量的过滤规则（Filtering Behavior）都存在许多不同的实现，没有统一的规范与标准，但是存在两种分类规范，这种分类方法主要用在 NAT 穿越技术中。 RFC3489 定义的 NAT 类型（四种）在 RFC3489 中将 NAPT 分为四种类型，这也是应用最为广泛的 NAT 分类方法，如下图： 下面我们逐一介绍这四种不同的 NAPT 类型。 从这里开始，下文中的 NAT 特指 NAPT，如果涉及「一对一 NAT」会使用它的全名。 1. Full-cone NATFull-cone NAT 的特点如下： 数据包流出：一旦内部地址（iAddr:iPort）映射到外部地址（eAddr:ePort），所有发自 iAddr:iPort 的数据包都经由 eAddr:ePort 向外发送。 数据包流入：任意主机发送到 eAddr:ePort 的数据包，都能通过 NAT 到达 iAddr:iPort. 也就是不对外部进来的数据做任何限制，全部放行。 cone 圆锥，个人理解是一个比喻，任意发送进来的数据（多），都能通过 NAT 到达这个内部地址（一），就像一个圆锥。 允许任意主机发送到 eAddr:ePort 的数据到达内部地址是很危险的行为，因为内部主机不一定配置了合适的安全策略。 因此 Full-cone NAT 比较少见，就算路由器等 NAT 设备支持 Full-cone NAT，通常也不会是默认选项。我们会在后面更详细地介绍它。 2. Restricted cone NAT 数据包流出：（跟 Full-cone NAT 完全一致）一旦内部地址（iAddr:iPort）映射到外部地址（eAddr:ePort），所有发自 iAddr:iPort 的数据包都经由 eAddr:ePort 向外发送。 数据包流入：只有内部地址（iAddr:iPort）主动连接过的外部主机（nAddr:any），发送到 eAddr:ePort 的数据包，才能通过 NAT 到达 iAddr:iPort. 跟 Full-cone NAT 的区别在于，它限制了外部主机的 IP 地址。只有主动连接过的主机，才能发送数据到 NAT 内部。这提升了一些安全性。 3. Port-Restricted cone NAT 数据包流出：（跟 Full-cone NAT 完全一致）一旦内部地址（iAddr:iPort）映射到外部地址（eAddr:ePort），所有发自 iAddr:iPort 的数据包都经由 eAddr:ePort 向外发送。 数据包流入：只有内部地址（iAddr:iPort）主动连接过的外部程序（nAddr:nPort），发送到 eAddr:ePort 的数据包，才能通过 NAT 到达 iAddr:iPort. 与 Address-Restricted cone NAT 的区别在于，它同时限制了外部主机的 IP 与端口，可以说是更进一步地提升了安全性。 4. Symmetric NATPort-Restricted cone NAT 的数据流入流出规则导致一个问题：同一个内部地址只能映射到唯一的一个 NAT 外部地址，也就只能与唯一的一个外部程序通讯，不能并发请求多个外部地址！这实际上限制了内部主机的最高并发连接数。 而 Symmetric NAT 解决了这个问题，它的数据流入流出规则如下： 数据包流出：同一个内部地址（iAddr:iPort）与不同外部主机（nAddr:nPort）的通信，会使用不同的 NAT 外部端口（eAddr:randomPort）。也就是说内部地址与 NAT 外部地址的关系也是一对多！ 通过允许端口的一对多映射，实际上提升了每个内部地址（iAddr:iPort）的并发连接数上限（从 1 直接扩到了可用端口数上限）。这也是 NAT 穿越最大的难点，它导致 Symmetric NAT 的端口难以预测！ 数据包流入：只有内部地址（iAddr:iPort）主动连接过的外部程序（nAddr:nPort），发送到 eAddr:ePort 的数据包，才能通过 NAT 到达 iAddr:iPort. 这个数据流入规则，与 Port-Restricted cone NAT 是完全一致的。 对称 NAT 是最安全的一种 NAT 结构，限制最为严格，应该也是应用最广泛的 NAT 结构。 但是它导致所有的 TCP 连接都只能由从内部主动发起，外部发起的 TCP 连接请求会直接被 NAT 拒绝，因此它也是 P2P 玩家最头疼的一种 NAT 类型。 解决方案是通过 UDP 迂回实现连接的建立，我们会在后面讨论这个问题。 5. Linux 中的 NAPTLinux 的网络栈中，可通过 iptables/netfilter 的 SNAT/MASQUERADE 实现 NAPT 网关，这种方式只能实现一个 Symmetric NAT. 也就是说绝大多数基于 Linux 实现的家庭局域网、Docker 虚拟网络、Kubernetes 虚拟网络、云服务的虚拟网络，都是 Symmetric NAT. 只有一些有 Full-cone NAT 需求的网吧、ISP 的 LSN(Large Scale NAT) 网关等组织，会使用非 Linux 内核的企业级路由器提供 Full-cone NAT 能力，这些设备可能是基于 FPGA 等专用芯片设计的。 想要将 Symmetric NAT 内的主机提供给外部访问，只能通过端口映射、一对一 NAT 等方式实现，后面会详细介绍这些方法。 RFC5389 定义的 NAT 类型（九种）RFC3489 这个早期 RFC 存在一些问题，问题之一就是它对 NAT 归类过于笼统，很多 NAPT 网关都无法很好的匹配上其中某个类别。 于是后来，RFC3489 被废弃并由 RFC5389 来替代，在 RFC5389 中，将 Mapping Behavior（映射规则）和 Filtering Behavior（过滤规则）分开来，定义了 3 种 Mapping Behavior（映射规则）和 3 种 Filtering Behavior（过滤规则），一共有 9 种组合。 1. 映射规则三种映射规则如图所示，假设一个内网主机 HostX 的内网 IP 地址为 X，端口号为 x，经 NAT 映射后的外网 IP 地址为 M，端口号为 m。为方便描述，将内网的 Endpoint 记为 Endpoint(X,x)，映射后外网的 Endpoint 记为 Endpoint(M,m)。内网 Endpoint(X,x) 发往外网 HostD1 的 IP 地址和端口号记为目的 Endpoint(D1,d1)；发往外网 HostD2 的 IP 地址和端口号记为目的 Endpoint(D2,d2)。 NAT 映射规则 EIM(Endpoint-Independent Mapping) 外部地址无关映射 对于一个内网 Endpoint(X,x)，其映射的外网 Endpoint(M,m) 是固定的。即从相同的 Endpoint(X,x) 发送到任何外部 IP 地址和任何外部端口的报文在 NAT 设备上使用相同的映射。 ADM(Address-Dependent Mapping) 外部地址相关映射：对于一个内网 Endpoint(X,x)，发往目的 Endpoint(D1,d1) 的报文，Endpoint(X,x) 被映射成 Endpoint(M1,m1)；发往目的 Endpoint(D2,d2) 的报文，Endpoint(X,x) 被映射成 Endpoint(M2,m2)。只要D1=D2，不管d1和d2是多少，都有 Endpoint(M1,m1)=Endpoint(M2,m2)。即从相同的 Endpoint(X,x) 发送到相同外部 IP 地址和任何外部端口的报文在 NAT 设备上使用相同的映射。 APDM（Address and Port-Dependent Mapping）外部地址和端口相关映射：对于一个内网 Endpoint(X,x)，发往目的 Endpoint(D1,d1) 的报文，Endpoint(X,x) 被映射成 Endpoint(M1,m1)；发往目的 Endpoint(D2,d2) 的报文， Endpoint(X,x) 被映射成 Endpoint(M2,m2)。只有当D1=D2，且d1=d2，才有 Endpoint(M1,m1)=Endpoint(M2,m2)。即从相同的 Endpoint(X,x) 发送到相同外部IP地址和相同外部端口的报文在NAT设备上使用相同的映射。 2. 过滤规则 NAT 过滤规则 EIF（Endpoint-Indep","date":"2022-05-13","objectID":"/posts/about-nat/:3:2","series":null,"tags":["NAT","网络","NAT 穿越","内网穿透","虚拟网络","P2P","WebRTC"],"title":"NAT 网关、NAT 穿越以及虚拟网络","uri":"/posts/about-nat/#rfc5389-定义的-nat-类型九种"},{"categories":["tech"],"content":" 2. 一对多 NAT - NAPT一对多 NAT，也被称为 NAPT（network address and port translation），同样在 RFC2663 中被定义。Easy IP 是 NAPT 的一个特殊形式。 NAPT 的主要应用场景是，内网用户需要访问到公网主机。绝大多数的家庭网络、办公网络都是 NAPT 类型的。 原因应该很好理解——家庭网络或办公网络都包含许多联网设备，但是这类网络通常只有一个或数个公网 IP，使用一对一 NAT 的话公网 IP 显然是不够用的，所以需要使用一对多 NAT. NAPT 通过同时利用 L3 的 IP 信息，以及 L4 传输层的端口信息，来为局域网设备提供透明的、配置方便的、支持超高并发连接的外部网络通信，示意图如下： NAPT 的端口分配与转换规则（Mapping Behavior）以及对外来流量的过滤规则（Filtering Behavior）都存在许多不同的实现，没有统一的规范与标准，但是存在两种分类规范，这种分类方法主要用在 NAT 穿越技术中。 RFC3489 定义的 NAT 类型（四种）在 RFC3489 中将 NAPT 分为四种类型，这也是应用最为广泛的 NAT 分类方法，如下图： 下面我们逐一介绍这四种不同的 NAPT 类型。 从这里开始，下文中的 NAT 特指 NAPT，如果涉及「一对一 NAT」会使用它的全名。 1. Full-cone NATFull-cone NAT 的特点如下： 数据包流出：一旦内部地址（iAddr:iPort）映射到外部地址（eAddr:ePort），所有发自 iAddr:iPort 的数据包都经由 eAddr:ePort 向外发送。 数据包流入：任意主机发送到 eAddr:ePort 的数据包，都能通过 NAT 到达 iAddr:iPort. 也就是不对外部进来的数据做任何限制，全部放行。 cone 圆锥，个人理解是一个比喻，任意发送进来的数据（多），都能通过 NAT 到达这个内部地址（一），就像一个圆锥。 允许任意主机发送到 eAddr:ePort 的数据到达内部地址是很危险的行为，因为内部主机不一定配置了合适的安全策略。 因此 Full-cone NAT 比较少见，就算路由器等 NAT 设备支持 Full-cone NAT，通常也不会是默认选项。我们会在后面更详细地介绍它。 2. Restricted cone NAT 数据包流出：（跟 Full-cone NAT 完全一致）一旦内部地址（iAddr:iPort）映射到外部地址（eAddr:ePort），所有发自 iAddr:iPort 的数据包都经由 eAddr:ePort 向外发送。 数据包流入：只有内部地址（iAddr:iPort）主动连接过的外部主机（nAddr:any），发送到 eAddr:ePort 的数据包，才能通过 NAT 到达 iAddr:iPort. 跟 Full-cone NAT 的区别在于，它限制了外部主机的 IP 地址。只有主动连接过的主机，才能发送数据到 NAT 内部。这提升了一些安全性。 3. Port-Restricted cone NAT 数据包流出：（跟 Full-cone NAT 完全一致）一旦内部地址（iAddr:iPort）映射到外部地址（eAddr:ePort），所有发自 iAddr:iPort 的数据包都经由 eAddr:ePort 向外发送。 数据包流入：只有内部地址（iAddr:iPort）主动连接过的外部程序（nAddr:nPort），发送到 eAddr:ePort 的数据包，才能通过 NAT 到达 iAddr:iPort. 与 Address-Restricted cone NAT 的区别在于，它同时限制了外部主机的 IP 与端口，可以说是更进一步地提升了安全性。 4. Symmetric NATPort-Restricted cone NAT 的数据流入流出规则导致一个问题：同一个内部地址只能映射到唯一的一个 NAT 外部地址，也就只能与唯一的一个外部程序通讯，不能并发请求多个外部地址！这实际上限制了内部主机的最高并发连接数。 而 Symmetric NAT 解决了这个问题，它的数据流入流出规则如下： 数据包流出：同一个内部地址（iAddr:iPort）与不同外部主机（nAddr:nPort）的通信，会使用不同的 NAT 外部端口（eAddr:randomPort）。也就是说内部地址与 NAT 外部地址的关系也是一对多！ 通过允许端口的一对多映射，实际上提升了每个内部地址（iAddr:iPort）的并发连接数上限（从 1 直接扩到了可用端口数上限）。这也是 NAT 穿越最大的难点，它导致 Symmetric NAT 的端口难以预测！ 数据包流入：只有内部地址（iAddr:iPort）主动连接过的外部程序（nAddr:nPort），发送到 eAddr:ePort 的数据包，才能通过 NAT 到达 iAddr:iPort. 这个数据流入规则，与 Port-Restricted cone NAT 是完全一致的。 对称 NAT 是最安全的一种 NAT 结构，限制最为严格，应该也是应用最广泛的 NAT 结构。 但是它导致所有的 TCP 连接都只能由从内部主动发起，外部发起的 TCP 连接请求会直接被 NAT 拒绝，因此它也是 P2P 玩家最头疼的一种 NAT 类型。 解决方案是通过 UDP 迂回实现连接的建立，我们会在后面讨论这个问题。 5. Linux 中的 NAPTLinux 的网络栈中，可通过 iptables/netfilter 的 SNAT/MASQUERADE 实现 NAPT 网关，这种方式只能实现一个 Symmetric NAT. 也就是说绝大多数基于 Linux 实现的家庭局域网、Docker 虚拟网络、Kubernetes 虚拟网络、云服务的虚拟网络，都是 Symmetric NAT. 只有一些有 Full-cone NAT 需求的网吧、ISP 的 LSN(Large Scale NAT) 网关等组织，会使用非 Linux 内核的企业级路由器提供 Full-cone NAT 能力，这些设备可能是基于 FPGA 等专用芯片设计的。 想要将 Symmetric NAT 内的主机提供给外部访问，只能通过端口映射、一对一 NAT 等方式实现，后面会详细介绍这些方法。 RFC5389 定义的 NAT 类型（九种）RFC3489 这个早期 RFC 存在一些问题，问题之一就是它对 NAT 归类过于笼统，很多 NAPT 网关都无法很好的匹配上其中某个类别。 于是后来，RFC3489 被废弃并由 RFC5389 来替代，在 RFC5389 中，将 Mapping Behavior（映射规则）和 Filtering Behavior（过滤规则）分开来，定义了 3 种 Mapping Behavior（映射规则）和 3 种 Filtering Behavior（过滤规则），一共有 9 种组合。 1. 映射规则三种映射规则如图所示，假设一个内网主机 HostX 的内网 IP 地址为 X，端口号为 x，经 NAT 映射后的外网 IP 地址为 M，端口号为 m。为方便描述，将内网的 Endpoint 记为 Endpoint(X,x)，映射后外网的 Endpoint 记为 Endpoint(M,m)。内网 Endpoint(X,x) 发往外网 HostD1 的 IP 地址和端口号记为目的 Endpoint(D1,d1)；发往外网 HostD2 的 IP 地址和端口号记为目的 Endpoint(D2,d2)。 NAT 映射规则 EIM(Endpoint-Independent Mapping) 外部地址无关映射 对于一个内网 Endpoint(X,x)，其映射的外网 Endpoint(M,m) 是固定的。即从相同的 Endpoint(X,x) 发送到任何外部 IP 地址和任何外部端口的报文在 NAT 设备上使用相同的映射。 ADM(Address-Dependent Mapping) 外部地址相关映射：对于一个内网 Endpoint(X,x)，发往目的 Endpoint(D1,d1) 的报文，Endpoint(X,x) 被映射成 Endpoint(M1,m1)；发往目的 Endpoint(D2,d2) 的报文，Endpoint(X,x) 被映射成 Endpoint(M2,m2)。只要D1=D2，不管d1和d2是多少，都有 Endpoint(M1,m1)=Endpoint(M2,m2)。即从相同的 Endpoint(X,x) 发送到相同外部 IP 地址和任何外部端口的报文在 NAT 设备上使用相同的映射。 APDM（Address and Port-Dependent Mapping）外部地址和端口相关映射：对于一个内网 Endpoint(X,x)，发往目的 Endpoint(D1,d1) 的报文，Endpoint(X,x) 被映射成 Endpoint(M1,m1)；发往目的 Endpoint(D2,d2) 的报文， Endpoint(X,x) 被映射成 Endpoint(M2,m2)。只有当D1=D2，且d1=d2，才有 Endpoint(M1,m1)=Endpoint(M2,m2)。即从相同的 Endpoint(X,x) 发送到相同外部IP地址和相同外部端口的报文在NAT设备上使用相同的映射。 2. 过滤规则 NAT 过滤规则 EIF（Endpoint-Indep","date":"2022-05-13","objectID":"/posts/about-nat/:3:2","series":null,"tags":["NAT","网络","NAT 穿越","内网穿透","虚拟网络","P2P","WebRTC"],"title":"NAT 网关、NAT 穿越以及虚拟网络","uri":"/posts/about-nat/#1-映射规则"},{"categories":["tech"],"content":" 2. 一对多 NAT - NAPT一对多 NAT，也被称为 NAPT（network address and port translation），同样在 RFC2663 中被定义。Easy IP 是 NAPT 的一个特殊形式。 NAPT 的主要应用场景是，内网用户需要访问到公网主机。绝大多数的家庭网络、办公网络都是 NAPT 类型的。 原因应该很好理解——家庭网络或办公网络都包含许多联网设备，但是这类网络通常只有一个或数个公网 IP，使用一对一 NAT 的话公网 IP 显然是不够用的，所以需要使用一对多 NAT. NAPT 通过同时利用 L3 的 IP 信息，以及 L4 传输层的端口信息，来为局域网设备提供透明的、配置方便的、支持超高并发连接的外部网络通信，示意图如下： NAPT 的端口分配与转换规则（Mapping Behavior）以及对外来流量的过滤规则（Filtering Behavior）都存在许多不同的实现，没有统一的规范与标准，但是存在两种分类规范，这种分类方法主要用在 NAT 穿越技术中。 RFC3489 定义的 NAT 类型（四种）在 RFC3489 中将 NAPT 分为四种类型，这也是应用最为广泛的 NAT 分类方法，如下图： 下面我们逐一介绍这四种不同的 NAPT 类型。 从这里开始，下文中的 NAT 特指 NAPT，如果涉及「一对一 NAT」会使用它的全名。 1. Full-cone NATFull-cone NAT 的特点如下： 数据包流出：一旦内部地址（iAddr:iPort）映射到外部地址（eAddr:ePort），所有发自 iAddr:iPort 的数据包都经由 eAddr:ePort 向外发送。 数据包流入：任意主机发送到 eAddr:ePort 的数据包，都能通过 NAT 到达 iAddr:iPort. 也就是不对外部进来的数据做任何限制，全部放行。 cone 圆锥，个人理解是一个比喻，任意发送进来的数据（多），都能通过 NAT 到达这个内部地址（一），就像一个圆锥。 允许任意主机发送到 eAddr:ePort 的数据到达内部地址是很危险的行为，因为内部主机不一定配置了合适的安全策略。 因此 Full-cone NAT 比较少见，就算路由器等 NAT 设备支持 Full-cone NAT，通常也不会是默认选项。我们会在后面更详细地介绍它。 2. Restricted cone NAT 数据包流出：（跟 Full-cone NAT 完全一致）一旦内部地址（iAddr:iPort）映射到外部地址（eAddr:ePort），所有发自 iAddr:iPort 的数据包都经由 eAddr:ePort 向外发送。 数据包流入：只有内部地址（iAddr:iPort）主动连接过的外部主机（nAddr:any），发送到 eAddr:ePort 的数据包，才能通过 NAT 到达 iAddr:iPort. 跟 Full-cone NAT 的区别在于，它限制了外部主机的 IP 地址。只有主动连接过的主机，才能发送数据到 NAT 内部。这提升了一些安全性。 3. Port-Restricted cone NAT 数据包流出：（跟 Full-cone NAT 完全一致）一旦内部地址（iAddr:iPort）映射到外部地址（eAddr:ePort），所有发自 iAddr:iPort 的数据包都经由 eAddr:ePort 向外发送。 数据包流入：只有内部地址（iAddr:iPort）主动连接过的外部程序（nAddr:nPort），发送到 eAddr:ePort 的数据包，才能通过 NAT 到达 iAddr:iPort. 与 Address-Restricted cone NAT 的区别在于，它同时限制了外部主机的 IP 与端口，可以说是更进一步地提升了安全性。 4. Symmetric NATPort-Restricted cone NAT 的数据流入流出规则导致一个问题：同一个内部地址只能映射到唯一的一个 NAT 外部地址，也就只能与唯一的一个外部程序通讯，不能并发请求多个外部地址！这实际上限制了内部主机的最高并发连接数。 而 Symmetric NAT 解决了这个问题，它的数据流入流出规则如下： 数据包流出：同一个内部地址（iAddr:iPort）与不同外部主机（nAddr:nPort）的通信，会使用不同的 NAT 外部端口（eAddr:randomPort）。也就是说内部地址与 NAT 外部地址的关系也是一对多！ 通过允许端口的一对多映射，实际上提升了每个内部地址（iAddr:iPort）的并发连接数上限（从 1 直接扩到了可用端口数上限）。这也是 NAT 穿越最大的难点，它导致 Symmetric NAT 的端口难以预测！ 数据包流入：只有内部地址（iAddr:iPort）主动连接过的外部程序（nAddr:nPort），发送到 eAddr:ePort 的数据包，才能通过 NAT 到达 iAddr:iPort. 这个数据流入规则，与 Port-Restricted cone NAT 是完全一致的。 对称 NAT 是最安全的一种 NAT 结构，限制最为严格，应该也是应用最广泛的 NAT 结构。 但是它导致所有的 TCP 连接都只能由从内部主动发起，外部发起的 TCP 连接请求会直接被 NAT 拒绝，因此它也是 P2P 玩家最头疼的一种 NAT 类型。 解决方案是通过 UDP 迂回实现连接的建立，我们会在后面讨论这个问题。 5. Linux 中的 NAPTLinux 的网络栈中，可通过 iptables/netfilter 的 SNAT/MASQUERADE 实现 NAPT 网关，这种方式只能实现一个 Symmetric NAT. 也就是说绝大多数基于 Linux 实现的家庭局域网、Docker 虚拟网络、Kubernetes 虚拟网络、云服务的虚拟网络，都是 Symmetric NAT. 只有一些有 Full-cone NAT 需求的网吧、ISP 的 LSN(Large Scale NAT) 网关等组织，会使用非 Linux 内核的企业级路由器提供 Full-cone NAT 能力，这些设备可能是基于 FPGA 等专用芯片设计的。 想要将 Symmetric NAT 内的主机提供给外部访问，只能通过端口映射、一对一 NAT 等方式实现，后面会详细介绍这些方法。 RFC5389 定义的 NAT 类型（九种）RFC3489 这个早期 RFC 存在一些问题，问题之一就是它对 NAT 归类过于笼统，很多 NAPT 网关都无法很好的匹配上其中某个类别。 于是后来，RFC3489 被废弃并由 RFC5389 来替代，在 RFC5389 中，将 Mapping Behavior（映射规则）和 Filtering Behavior（过滤规则）分开来，定义了 3 种 Mapping Behavior（映射规则）和 3 种 Filtering Behavior（过滤规则），一共有 9 种组合。 1. 映射规则三种映射规则如图所示，假设一个内网主机 HostX 的内网 IP 地址为 X，端口号为 x，经 NAT 映射后的外网 IP 地址为 M，端口号为 m。为方便描述，将内网的 Endpoint 记为 Endpoint(X,x)，映射后外网的 Endpoint 记为 Endpoint(M,m)。内网 Endpoint(X,x) 发往外网 HostD1 的 IP 地址和端口号记为目的 Endpoint(D1,d1)；发往外网 HostD2 的 IP 地址和端口号记为目的 Endpoint(D2,d2)。 NAT 映射规则 EIM(Endpoint-Independent Mapping) 外部地址无关映射 对于一个内网 Endpoint(X,x)，其映射的外网 Endpoint(M,m) 是固定的。即从相同的 Endpoint(X,x) 发送到任何外部 IP 地址和任何外部端口的报文在 NAT 设备上使用相同的映射。 ADM(Address-Dependent Mapping) 外部地址相关映射：对于一个内网 Endpoint(X,x)，发往目的 Endpoint(D1,d1) 的报文，Endpoint(X,x) 被映射成 Endpoint(M1,m1)；发往目的 Endpoint(D2,d2) 的报文，Endpoint(X,x) 被映射成 Endpoint(M2,m2)。只要D1=D2，不管d1和d2是多少，都有 Endpoint(M1,m1)=Endpoint(M2,m2)。即从相同的 Endpoint(X,x) 发送到相同外部 IP 地址和任何外部端口的报文在 NAT 设备上使用相同的映射。 APDM（Address and Port-Dependent Mapping）外部地址和端口相关映射：对于一个内网 Endpoint(X,x)，发往目的 Endpoint(D1,d1) 的报文，Endpoint(X,x) 被映射成 Endpoint(M1,m1)；发往目的 Endpoint(D2,d2) 的报文， Endpoint(X,x) 被映射成 Endpoint(M2,m2)。只有当D1=D2，且d1=d2，才有 Endpoint(M1,m1)=Endpoint(M2,m2)。即从相同的 Endpoint(X,x) 发送到相同外部IP地址和相同外部端口的报文在NAT设备上使用相同的映射。 2. 过滤规则 NAT 过滤规则 EIF（Endpoint-Indep","date":"2022-05-13","objectID":"/posts/about-nat/:3:2","series":null,"tags":["NAT","网络","NAT 穿越","内网穿透","虚拟网络","P2P","WebRTC"],"title":"NAT 网关、NAT 穿越以及虚拟网络","uri":"/posts/about-nat/#2-过滤规则"},{"categories":["tech"],"content":" 2. 一对多 NAT - NAPT一对多 NAT，也被称为 NAPT（network address and port translation），同样在 RFC2663 中被定义。Easy IP 是 NAPT 的一个特殊形式。 NAPT 的主要应用场景是，内网用户需要访问到公网主机。绝大多数的家庭网络、办公网络都是 NAPT 类型的。 原因应该很好理解——家庭网络或办公网络都包含许多联网设备，但是这类网络通常只有一个或数个公网 IP，使用一对一 NAT 的话公网 IP 显然是不够用的，所以需要使用一对多 NAT. NAPT 通过同时利用 L3 的 IP 信息，以及 L4 传输层的端口信息，来为局域网设备提供透明的、配置方便的、支持超高并发连接的外部网络通信，示意图如下： NAPT 的端口分配与转换规则（Mapping Behavior）以及对外来流量的过滤规则（Filtering Behavior）都存在许多不同的实现，没有统一的规范与标准，但是存在两种分类规范，这种分类方法主要用在 NAT 穿越技术中。 RFC3489 定义的 NAT 类型（四种）在 RFC3489 中将 NAPT 分为四种类型，这也是应用最为广泛的 NAT 分类方法，如下图： 下面我们逐一介绍这四种不同的 NAPT 类型。 从这里开始，下文中的 NAT 特指 NAPT，如果涉及「一对一 NAT」会使用它的全名。 1. Full-cone NATFull-cone NAT 的特点如下： 数据包流出：一旦内部地址（iAddr:iPort）映射到外部地址（eAddr:ePort），所有发自 iAddr:iPort 的数据包都经由 eAddr:ePort 向外发送。 数据包流入：任意主机发送到 eAddr:ePort 的数据包，都能通过 NAT 到达 iAddr:iPort. 也就是不对外部进来的数据做任何限制，全部放行。 cone 圆锥，个人理解是一个比喻，任意发送进来的数据（多），都能通过 NAT 到达这个内部地址（一），就像一个圆锥。 允许任意主机发送到 eAddr:ePort 的数据到达内部地址是很危险的行为，因为内部主机不一定配置了合适的安全策略。 因此 Full-cone NAT 比较少见，就算路由器等 NAT 设备支持 Full-cone NAT，通常也不会是默认选项。我们会在后面更详细地介绍它。 2. Restricted cone NAT 数据包流出：（跟 Full-cone NAT 完全一致）一旦内部地址（iAddr:iPort）映射到外部地址（eAddr:ePort），所有发自 iAddr:iPort 的数据包都经由 eAddr:ePort 向外发送。 数据包流入：只有内部地址（iAddr:iPort）主动连接过的外部主机（nAddr:any），发送到 eAddr:ePort 的数据包，才能通过 NAT 到达 iAddr:iPort. 跟 Full-cone NAT 的区别在于，它限制了外部主机的 IP 地址。只有主动连接过的主机，才能发送数据到 NAT 内部。这提升了一些安全性。 3. Port-Restricted cone NAT 数据包流出：（跟 Full-cone NAT 完全一致）一旦内部地址（iAddr:iPort）映射到外部地址（eAddr:ePort），所有发自 iAddr:iPort 的数据包都经由 eAddr:ePort 向外发送。 数据包流入：只有内部地址（iAddr:iPort）主动连接过的外部程序（nAddr:nPort），发送到 eAddr:ePort 的数据包，才能通过 NAT 到达 iAddr:iPort. 与 Address-Restricted cone NAT 的区别在于，它同时限制了外部主机的 IP 与端口，可以说是更进一步地提升了安全性。 4. Symmetric NATPort-Restricted cone NAT 的数据流入流出规则导致一个问题：同一个内部地址只能映射到唯一的一个 NAT 外部地址，也就只能与唯一的一个外部程序通讯，不能并发请求多个外部地址！这实际上限制了内部主机的最高并发连接数。 而 Symmetric NAT 解决了这个问题，它的数据流入流出规则如下： 数据包流出：同一个内部地址（iAddr:iPort）与不同外部主机（nAddr:nPort）的通信，会使用不同的 NAT 外部端口（eAddr:randomPort）。也就是说内部地址与 NAT 外部地址的关系也是一对多！ 通过允许端口的一对多映射，实际上提升了每个内部地址（iAddr:iPort）的并发连接数上限（从 1 直接扩到了可用端口数上限）。这也是 NAT 穿越最大的难点，它导致 Symmetric NAT 的端口难以预测！ 数据包流入：只有内部地址（iAddr:iPort）主动连接过的外部程序（nAddr:nPort），发送到 eAddr:ePort 的数据包，才能通过 NAT 到达 iAddr:iPort. 这个数据流入规则，与 Port-Restricted cone NAT 是完全一致的。 对称 NAT 是最安全的一种 NAT 结构，限制最为严格，应该也是应用最广泛的 NAT 结构。 但是它导致所有的 TCP 连接都只能由从内部主动发起，外部发起的 TCP 连接请求会直接被 NAT 拒绝，因此它也是 P2P 玩家最头疼的一种 NAT 类型。 解决方案是通过 UDP 迂回实现连接的建立，我们会在后面讨论这个问题。 5. Linux 中的 NAPTLinux 的网络栈中，可通过 iptables/netfilter 的 SNAT/MASQUERADE 实现 NAPT 网关，这种方式只能实现一个 Symmetric NAT. 也就是说绝大多数基于 Linux 实现的家庭局域网、Docker 虚拟网络、Kubernetes 虚拟网络、云服务的虚拟网络，都是 Symmetric NAT. 只有一些有 Full-cone NAT 需求的网吧、ISP 的 LSN(Large Scale NAT) 网关等组织，会使用非 Linux 内核的企业级路由器提供 Full-cone NAT 能力，这些设备可能是基于 FPGA 等专用芯片设计的。 想要将 Symmetric NAT 内的主机提供给外部访问，只能通过端口映射、一对一 NAT 等方式实现，后面会详细介绍这些方法。 RFC5389 定义的 NAT 类型（九种）RFC3489 这个早期 RFC 存在一些问题，问题之一就是它对 NAT 归类过于笼统，很多 NAPT 网关都无法很好的匹配上其中某个类别。 于是后来，RFC3489 被废弃并由 RFC5389 来替代，在 RFC5389 中，将 Mapping Behavior（映射规则）和 Filtering Behavior（过滤规则）分开来，定义了 3 种 Mapping Behavior（映射规则）和 3 种 Filtering Behavior（过滤规则），一共有 9 种组合。 1. 映射规则三种映射规则如图所示，假设一个内网主机 HostX 的内网 IP 地址为 X，端口号为 x，经 NAT 映射后的外网 IP 地址为 M，端口号为 m。为方便描述，将内网的 Endpoint 记为 Endpoint(X,x)，映射后外网的 Endpoint 记为 Endpoint(M,m)。内网 Endpoint(X,x) 发往外网 HostD1 的 IP 地址和端口号记为目的 Endpoint(D1,d1)；发往外网 HostD2 的 IP 地址和端口号记为目的 Endpoint(D2,d2)。 NAT 映射规则 EIM(Endpoint-Independent Mapping) 外部地址无关映射 对于一个内网 Endpoint(X,x)，其映射的外网 Endpoint(M,m) 是固定的。即从相同的 Endpoint(X,x) 发送到任何外部 IP 地址和任何外部端口的报文在 NAT 设备上使用相同的映射。 ADM(Address-Dependent Mapping) 外部地址相关映射：对于一个内网 Endpoint(X,x)，发往目的 Endpoint(D1,d1) 的报文，Endpoint(X,x) 被映射成 Endpoint(M1,m1)；发往目的 Endpoint(D2,d2) 的报文，Endpoint(X,x) 被映射成 Endpoint(M2,m2)。只要D1=D2，不管d1和d2是多少，都有 Endpoint(M1,m1)=Endpoint(M2,m2)。即从相同的 Endpoint(X,x) 发送到相同外部 IP 地址和任何外部端口的报文在 NAT 设备上使用相同的映射。 APDM（Address and Port-Dependent Mapping）外部地址和端口相关映射：对于一个内网 Endpoint(X,x)，发往目的 Endpoint(D1,d1) 的报文，Endpoint(X,x) 被映射成 Endpoint(M1,m1)；发往目的 Endpoint(D2,d2) 的报文， Endpoint(X,x) 被映射成 Endpoint(M2,m2)。只有当D1=D2，且d1=d2，才有 Endpoint(M1,m1)=Endpoint(M2,m2)。即从相同的 Endpoint(X,x) 发送到相同外部IP地址和相同外部端口的报文在NAT设备上使用相同的映射。 2. 过滤规则 NAT 过滤规则 EIF（Endpoint-Indep","date":"2022-05-13","objectID":"/posts/about-nat/:3:2","series":null,"tags":["NAT","网络","NAT 穿越","内网穿透","虚拟网络","P2P","WebRTC"],"title":"NAT 网关、NAT 穿越以及虚拟网络","uri":"/posts/about-nat/#3-rfc3489-与-rfc5389-的-nat-类型定义关系"},{"categories":["tech"],"content":" NAT 的弊端 IP 会话的保持时效变短：NAT 需要维护一个会话列表，如果会话静默时间超过一个阈值，将会被从列表中移除。 为了避免这种情况，就需要定期发送心跳包来维持 NAT 会话。俗称心跳保活 IP 跟踪机制失效：一对多 NAT 使得多个局域网主机共用一个公网 IP，这导致基于公网 IP 进行流量分析的逻辑失去意义。 比如很多站点都加了基于 IP 的访问频率限制，这会造成局域网内多个用户之间的服务抢占与排队。 NAT 的工作机制依赖于修改IP包头的信息，这会妨碍一些安全协议的工作。 因为 NAT 篡改了 IP 地址、传输层端口号和校验和，这会导致 IP 层的认证协议彻底不能工作，因为认证目的就是要保证这些信息在传输过程中没有变化。 对于一些隧道协议，NAT 的存在也导致了额外的问题，因为隧道协议通常用外层地址标识隧道实体，穿过 NAT 的隧道会有 IP 复用关系，在另一端需要小心处理。 ICMP 是一种网络控制协议，它的工作原理也是在两个主机之间传递差错和控制消息，因为IP的对应关系被重新映射，ICMP 也要进行复用和解复用处理，很多情况下因为 ICMP 报文载荷无法提供足够的信息，解复用会失败。 IP 分片机制是在信息源端或网络路径上，需要发送的 IP 报文尺寸大于路径实际能承载最大尺寸时，IP 协议层会将一个报文分成多个片断发送，然后在接收端重组这些片断恢复原始报文。IP 这样的分片机制会导致传输层的信息只包括在第一个分片中，NAT难以识别后续分片与关联表的对应关系，因此需要特殊处理。 ","date":"2022-05-13","objectID":"/posts/about-nat/:4:0","series":null,"tags":["NAT","网络","NAT 穿越","内网穿透","虚拟网络","P2P","WebRTC"],"title":"NAT 网关、NAT 穿越以及虚拟网络","uri":"/posts/about-nat/#nat-的弊端"},{"categories":["tech"],"content":" NAT 穿越 - NAT Traversal天下苦 NAT 久矣，尤其是对各种 P2P 玩家，如 NAS 玩家、P2P 游戏玩家，以及需要搭建 VPN 虚拟私有网络的网络管理员而言。 在常见的联机游戏、BitTorrent 文件共享协议、P2P 聊天等点对点通讯场景中，通讯双方客户端通常都运行在家庭局域网中，也就是说中间隔着两层家庭路由器的 NAT，路由器的默认配置都是安全优先的，存在很多安全限制，直接进行 P2P 通讯大概率会失败。 为了穿越这些 NAT 网关进行 P2P 通讯，就需要借助 NAT 穿越技术。 这里讨论的前提是，你的网络只有单层 NAT，如果外部还存在公寓 NAT、ISP 广域网 NAT，那下面介绍的 NAT 提升技术实际上就没啥意义了。 ","date":"2022-05-13","objectID":"/posts/about-nat/:5:0","series":null,"tags":["NAT","网络","NAT 穿越","内网穿透","虚拟网络","P2P","WebRTC"],"title":"NAT 网关、NAT 穿越以及虚拟网络","uri":"/posts/about-nat/#nat-穿越---nat-traversal"},{"categories":["tech"],"content":" 1. 「DMZ 主机」或者「定向 DNAT 转发」最简单的方法是 DMZ 主机功能，前面已经介绍过了，DMZ 可以直接给内网服务器绑定路由器的外部 IP，从该 IP 进来的所有流量都会直接被发送给这台内网服务器。 被指定的 DMZ 主机，其 NAT 类型将从 NAPT 变成一对一 NAT，而一对一 NAT 对 P2P 通讯而言是透明的，这样就可以愉快地玩耍了。 在 Linux 路由器上实现类似 DMZ 的功能，只需要两行 iptables 命令，这可以称作「定向 DNAT 转发」： iptables -t nat -A POSTROUTING -o eth0 -j MASQUERADE # 普通的SNAT iptables -t nat -A PREROUTING -i eth0 -j DNAT --to-destination 192.168.1.3 # 将入站流量DNAT转发到内网主机192.168.1.3 这两项技术的缺点是只能将一台主机提供给外网访问，而且将整台主机开放到公网实际上是很危险的，如果不懂网络很容易被黑客入侵。 ","date":"2022-05-13","objectID":"/posts/about-nat/:5:1","series":null,"tags":["NAT","网络","NAT 穿越","内网穿透","虚拟网络","P2P","WebRTC"],"title":"NAT 网关、NAT 穿越以及虚拟网络","uri":"/posts/about-nat/#1-dmz-主机或者定向-dnat-转发"},{"categories":["tech"],"content":" 2. 静态端口转发退一步，可以直接用静态端口转发功能，就是在路由器上手动设置某个端口号的所有 TCP/UDP 流量，都直接 NAT 转发到到内网的指定地址。也就是往 NAT 的转发表中手动添加内容，示意图： NAPT tables 设置好端口转发后，只要使用的是被设定的端口，NAT 对 P2P 通信而言将完全透明。 绝大多数路由器都支持这项功能，NAS 发烧友们想玩 P2P 下载分享，基本都是这么搞的。 ","date":"2022-05-13","objectID":"/posts/about-nat/:5:2","series":null,"tags":["NAT","网络","NAT 穿越","内网穿透","虚拟网络","P2P","WebRTC"],"title":"NAT 网关、NAT 穿越以及虚拟网络","uri":"/posts/about-nat/#2-静态端口转发"},{"categories":["tech"],"content":" 3. UPnP 动态端口转发 最流行的 UPnP 实现是 https://github.com/miniupnp/miniupnp 静态端口转发对用户的技术要求较高，我作为一个网络小白，希望有一个傻瓜式的开关能让我愉快地玩耍 Xbox/PS5 联机游戏，该怎么办呢？ 你需要的只是在路由器上启用 UPnP(Universal Plug and Play) 协议，启用它后，内网游戏设备就可以通过 UPnP 向路由器动态申请一个端口号供其使用，UPnP 会自动配置对应的端口转发规则。 现在新出的路由器基本都支持 UPnP 功能，它是最简单有效的 NAT 提升方式。 UPnP 解决了「静态端口转发」需要手动配置的问题，在启用了 UPnP 后，对所有支持 UPnP 的内网程序而言，NAT 类型将提升到 Full-cone NAT. ","date":"2022-05-13","objectID":"/posts/about-nat/:5:3","series":null,"tags":["NAT","网络","NAT 穿越","内网穿透","虚拟网络","P2P","WebRTC"],"title":"NAT 网关、NAT 穿越以及虚拟网络","uri":"/posts/about-nat/#3-upnp-动态端口转发"},{"categories":["tech"],"content":" 4. NAT 穿越协议 - STUN/TURN/ICE如果很不幸前面提到的「DMZ 主机」/「静态端口转发」/「UPnP」 三项技术，你的路由器都不支持，那你就只能借助 NAT 穿越协议了。 目前有如下几个 NAT 穿越协议标准： RFC3489 Classic STUN Classic STUN 是一个早期的 STUN 规范，它定义了一整套完整的 NAT 穿越方案，但是因为存在许多问题，已经被废弃。 RFC5389 - Simple Traversal of UDP Through NATs (STUN) RFC5389 所定义的 STUN 协议是对 Classic STUN 的改进，它的定位不再是一个完整的 NAT 穿越解决方案，而是作为其他协议（例如SIP、FTP、DNS）处理 NAT 穿越问题的一个工具。 其可以用于检查网络中NAT设备的存在，并确定两个通信端点被NAT设备分配的IP地址和端口号。然后，通过ICE（Interactive Connectivity Establishment），自动创建一条能够进行NAT穿越的数据通道。 STUN 支持除 Symmetric NAT 之外的另外三种 NAT 类型 RFC5766 - Traversal Using Relays around NAT (TURN) TURN 在 STUN 协议之上添加了一个中继，以确保在无法实现 NAT 穿越的情况下，可以 fallback 到直接使用中继服务器进行通信。 这个中继的原理类似反向代理，单纯负责数据的转发 在美国有一项数据表示在进行 P2P 穿越的时候，穿越成功的概率为 70%，但是在国内这个成功率 50% 可能都到不了。因此就有必要使用 TURN 协议，这样才能保证在穿越失败的情况下，用户仍然能正常通信。 RFC8445 - Interactive Connectivity Establishment (ICE) 一个 NAT 穿越的协商协议，它统一了 STUN 与 TURN 两种协议，会尝试遍历所有可能的连接方案。 总的来说，标准的 NAT 穿越协议优先使用打洞（NAT Hole Pounching）技术，如果打洞失败，就使用中继服务器技术兜底，确保能成功穿越。 STUN/TURN/ICE 的 NAT 类型检测RFC5389 定义了对 NAT 映射类型以及过滤类型的检测方法。 TBD STUN/TURN/ICE 协议如何实现 NAT 打洞首先 P2P 双方如果只隔着 0-1 层 NAT，那是不需要使用 NAT 打洞技术的，可以直连或者反向连接。 下面就讨论下 P2P 双方隔着 2 层及以上 NAT 的场景下，如何利用 UDP 协议实现 NAT 打洞。 一个完整的 NAT 打洞方案，需要包含如下功能： A 跟 B 需要知道对方的公网 IP 以及监听的端口号 解决方法：需要一个公网中介来介绍双方认识（交换 IP/port） NAT 连通性测试，需要借助公网主机，检测双方中间网络的类型 针对不同的 NAT 类型，存在哪些穿越手段？以何种顺序进行穿越尝试？ NAT 打洞可以使用 UDP/TCP 两种 L4 协议，但是 TCP 面向连接的特性使它在这个场景中限制性更大（具体限制见参考文章，我有空再补充），因此各种 NAT 穿越协议通常都基于 UDP 实现。 此外，因为 NAT 的具体行为是非标准化的，路由器的防火墙策略也存在很大变动空间，再有就是 RF3489 的这种 NAT 分类方法不够精确，这些因素导致 NAT 穿透能否成功通常都是谈概率。 1. A 与 B 在同一局域网中这是最简单的情况，最佳方案是直接走内网通讯，不经过 NAT. 第二个方案是，这两个同一局域网内的客户端不走内网，仍然通过 NAT 通讯。这种通讯方式被称作「回环 NAT(Loopback NAT)」或者「发夹 NAT(Hairpin NAT)」。 对于不支持或未启用「Hairpin NAT」的网关设备而言，这样的通讯尝试将会失败！ 2. A 与 B 分别在不同的局域网中这样实际上 A 与 B 中间就隔了两个 NAT 网关，这是最普遍的一种情况。 STUN/TURN 的 NAT 穿透流程大致如下： 首先，A 跟 B 两个程序启动时，需要把自己的内外网 IP 及端口信息上报到一台中介服务器 S 现在假设 A 想要跟 B 建立一个 P2P 连接，首先他们需要从 S 获得对方的 ID A 将 B 的 ID 发送给中介服务器 S，请求与 B 建立 P2P 连接 中介服务器将 B 的内外网 IP 及端口信息发送给 A，同时将 A 的网络信息发送给 B A 尝试请求 B 的公网地址 B_public_ip:B_public_port 这肯定会失败，但是会在 A 的 NAT 网关上留下记录：A 曾经请求过这个地址，那之后这个地址发到 A 的 NAT 网关的流量就可以进来了。 B 尝试请求 A 的公网地址 A_public_ip:A_public_port 同样这肯定也会失败，但是会在 B 的 NAT 网关上流量记录：B 曾经请求过这个地址，那之后这个地址发到 B 的 NAT 网关的流量就可以进来了 中间的两层 NAT 网关均形成 NAT 穿越记录，穿越完成。 现在 A 尝试请求 B 的公网地址 B_public_ip:B_public_port，由于 B 的 NAT 已有记录，流量顺利通过 NAT 到达程序 B B 发送给 A 的数据也同样，可以顺利到达 A 上述流程中的关键点在于，如何查出内网服务器被 NAT 分配的外部 IP 及端口，只要有了这两个信息，就可以通过 STUN 中介服务器交换这个信息，然后完成连接的建立了。 家庭服务器通常都只有一个公网 IP，所以基本可以认为 IP 是固定的，因此最关键的问题就是「如何知道 NAT 为会话分配的端口地址」。 对端口的限制严格程度跟 NAPT 的类型有关，Full-cone 跟 Restricted cone 对端口都没有任何限制，所以上述流程肯定可以成功； TBD 一个穿越 Symmetric NATs 的 STUN 草案：Symmetric NAT Traversal using STUN 在使用 STUN/TURN 进行 NAT 穿越时，支持的的 NAT 类型如下表。行与列分别代表双方的 NAT 类型，✅ 表示支持 UDP 穿越，❌ 表示 TURN 无法进行 UDP 穿越： NAT 类型 Full Cone Restricted Port-Restricted Symmetric Full Cone ✅ ✅ ✅ ✅ Restricted ✅ ✅ ✅ ✅ Port-Restricted ✅ ✅ ✅ ❌ Symmetric ✅ ✅ ❌ ❌ 这种场景下 TURN 协议给出的解决方案是，fallback 到中继服务器策略作为兜底方案，保证连接能成功，但是这会给中继服务器带来很大压力，延迟等参数将不可避免地变差。 3. A 与 B 之间隔着三层以上的 NAT这种情况较为常见的有： ISP 为了节约使用公网 IP，给用户分配了个广域网 IP，中间就多了个广域网 NAT 大城市的各种租房公寓通常只会从 ISP 购买一两根宽带，二次分销给整栋楼的租客共用，这就造成中间多了一层公寓的 NAT 这是最复杂的一种情况，基本上就没什么 NAT 穿透的希望了，只能走下面介绍的兜底策略——服务器中继。 TBD 待续 4. 特殊穿越方案 - 服务器中继Relay 服务器中继是兼容性最佳，但是性能最差的方案，因为这个方案下，所有的 P2P 连接都需要经过中继服务器转发，在使用人数众多时这会给中继服务器造成很大的压力。 因此这个方案通常是用于兜底的。 ","date":"2022-05-13","objectID":"/posts/about-nat/:5:4","series":null,"tags":["NAT","网络","NAT 穿越","内网穿透","虚拟网络","P2P","WebRTC"],"title":"NAT 网关、NAT 穿越以及虚拟网络","uri":"/posts/about-nat/#4-nat-穿越协议---stunturnice"},{"categories":["tech"],"content":" 4. NAT 穿越协议 - STUN/TURN/ICE如果很不幸前面提到的「DMZ 主机」/「静态端口转发」/「UPnP」 三项技术，你的路由器都不支持，那你就只能借助 NAT 穿越协议了。 目前有如下几个 NAT 穿越协议标准： RFC3489 Classic STUN Classic STUN 是一个早期的 STUN 规范，它定义了一整套完整的 NAT 穿越方案，但是因为存在许多问题，已经被废弃。 RFC5389 - Simple Traversal of UDP Through NATs (STUN) RFC5389 所定义的 STUN 协议是对 Classic STUN 的改进，它的定位不再是一个完整的 NAT 穿越解决方案，而是作为其他协议（例如SIP、FTP、DNS）处理 NAT 穿越问题的一个工具。 其可以用于检查网络中NAT设备的存在，并确定两个通信端点被NAT设备分配的IP地址和端口号。然后，通过ICE（Interactive Connectivity Establishment），自动创建一条能够进行NAT穿越的数据通道。 STUN 支持除 Symmetric NAT 之外的另外三种 NAT 类型 RFC5766 - Traversal Using Relays around NAT (TURN) TURN 在 STUN 协议之上添加了一个中继，以确保在无法实现 NAT 穿越的情况下，可以 fallback 到直接使用中继服务器进行通信。 这个中继的原理类似反向代理，单纯负责数据的转发 在美国有一项数据表示在进行 P2P 穿越的时候，穿越成功的概率为 70%，但是在国内这个成功率 50% 可能都到不了。因此就有必要使用 TURN 协议，这样才能保证在穿越失败的情况下，用户仍然能正常通信。 RFC8445 - Interactive Connectivity Establishment (ICE) 一个 NAT 穿越的协商协议，它统一了 STUN 与 TURN 两种协议，会尝试遍历所有可能的连接方案。 总的来说，标准的 NAT 穿越协议优先使用打洞（NAT Hole Pounching）技术，如果打洞失败，就使用中继服务器技术兜底，确保能成功穿越。 STUN/TURN/ICE 的 NAT 类型检测RFC5389 定义了对 NAT 映射类型以及过滤类型的检测方法。 TBD STUN/TURN/ICE 协议如何实现 NAT 打洞首先 P2P 双方如果只隔着 0-1 层 NAT，那是不需要使用 NAT 打洞技术的，可以直连或者反向连接。 下面就讨论下 P2P 双方隔着 2 层及以上 NAT 的场景下，如何利用 UDP 协议实现 NAT 打洞。 一个完整的 NAT 打洞方案，需要包含如下功能： A 跟 B 需要知道对方的公网 IP 以及监听的端口号 解决方法：需要一个公网中介来介绍双方认识（交换 IP/port） NAT 连通性测试，需要借助公网主机，检测双方中间网络的类型 针对不同的 NAT 类型，存在哪些穿越手段？以何种顺序进行穿越尝试？ NAT 打洞可以使用 UDP/TCP 两种 L4 协议，但是 TCP 面向连接的特性使它在这个场景中限制性更大（具体限制见参考文章，我有空再补充），因此各种 NAT 穿越协议通常都基于 UDP 实现。 此外，因为 NAT 的具体行为是非标准化的，路由器的防火墙策略也存在很大变动空间，再有就是 RF3489 的这种 NAT 分类方法不够精确，这些因素导致 NAT 穿透能否成功通常都是谈概率。 1. A 与 B 在同一局域网中这是最简单的情况，最佳方案是直接走内网通讯，不经过 NAT. 第二个方案是，这两个同一局域网内的客户端不走内网，仍然通过 NAT 通讯。这种通讯方式被称作「回环 NAT(Loopback NAT)」或者「发夹 NAT(Hairpin NAT)」。 对于不支持或未启用「Hairpin NAT」的网关设备而言，这样的通讯尝试将会失败！ 2. A 与 B 分别在不同的局域网中这样实际上 A 与 B 中间就隔了两个 NAT 网关，这是最普遍的一种情况。 STUN/TURN 的 NAT 穿透流程大致如下： 首先，A 跟 B 两个程序启动时，需要把自己的内外网 IP 及端口信息上报到一台中介服务器 S 现在假设 A 想要跟 B 建立一个 P2P 连接，首先他们需要从 S 获得对方的 ID A 将 B 的 ID 发送给中介服务器 S，请求与 B 建立 P2P 连接 中介服务器将 B 的内外网 IP 及端口信息发送给 A，同时将 A 的网络信息发送给 B A 尝试请求 B 的公网地址 B_public_ip:B_public_port 这肯定会失败，但是会在 A 的 NAT 网关上留下记录：A 曾经请求过这个地址，那之后这个地址发到 A 的 NAT 网关的流量就可以进来了。 B 尝试请求 A 的公网地址 A_public_ip:A_public_port 同样这肯定也会失败，但是会在 B 的 NAT 网关上流量记录：B 曾经请求过这个地址，那之后这个地址发到 B 的 NAT 网关的流量就可以进来了 中间的两层 NAT 网关均形成 NAT 穿越记录，穿越完成。 现在 A 尝试请求 B 的公网地址 B_public_ip:B_public_port，由于 B 的 NAT 已有记录，流量顺利通过 NAT 到达程序 B B 发送给 A 的数据也同样，可以顺利到达 A 上述流程中的关键点在于，如何查出内网服务器被 NAT 分配的外部 IP 及端口，只要有了这两个信息，就可以通过 STUN 中介服务器交换这个信息，然后完成连接的建立了。 家庭服务器通常都只有一个公网 IP，所以基本可以认为 IP 是固定的，因此最关键的问题就是「如何知道 NAT 为会话分配的端口地址」。 对端口的限制严格程度跟 NAPT 的类型有关，Full-cone 跟 Restricted cone 对端口都没有任何限制，所以上述流程肯定可以成功； TBD 一个穿越 Symmetric NATs 的 STUN 草案：Symmetric NAT Traversal using STUN 在使用 STUN/TURN 进行 NAT 穿越时，支持的的 NAT 类型如下表。行与列分别代表双方的 NAT 类型，✅ 表示支持 UDP 穿越，❌ 表示 TURN 无法进行 UDP 穿越： NAT 类型 Full Cone Restricted Port-Restricted Symmetric Full Cone ✅ ✅ ✅ ✅ Restricted ✅ ✅ ✅ ✅ Port-Restricted ✅ ✅ ✅ ❌ Symmetric ✅ ✅ ❌ ❌ 这种场景下 TURN 协议给出的解决方案是，fallback 到中继服务器策略作为兜底方案，保证连接能成功，但是这会给中继服务器带来很大压力，延迟等参数将不可避免地变差。 3. A 与 B 之间隔着三层以上的 NAT这种情况较为常见的有： ISP 为了节约使用公网 IP，给用户分配了个广域网 IP，中间就多了个广域网 NAT 大城市的各种租房公寓通常只会从 ISP 购买一两根宽带，二次分销给整栋楼的租客共用，这就造成中间多了一层公寓的 NAT 这是最复杂的一种情况，基本上就没什么 NAT 穿透的希望了，只能走下面介绍的兜底策略——服务器中继。 TBD 待续 4. 特殊穿越方案 - 服务器中继Relay 服务器中继是兼容性最佳，但是性能最差的方案，因为这个方案下，所有的 P2P 连接都需要经过中继服务器转发，在使用人数众多时这会给中继服务器造成很大的压力。 因此这个方案通常是用于兜底的。 ","date":"2022-05-13","objectID":"/posts/about-nat/:5:4","series":null,"tags":["NAT","网络","NAT 穿越","内网穿透","虚拟网络","P2P","WebRTC"],"title":"NAT 网关、NAT 穿越以及虚拟网络","uri":"/posts/about-nat/#stunturnice-的-nat-类型检测"},{"categories":["tech"],"content":" 4. NAT 穿越协议 - STUN/TURN/ICE如果很不幸前面提到的「DMZ 主机」/「静态端口转发」/「UPnP」 三项技术，你的路由器都不支持，那你就只能借助 NAT 穿越协议了。 目前有如下几个 NAT 穿越协议标准： RFC3489 Classic STUN Classic STUN 是一个早期的 STUN 规范，它定义了一整套完整的 NAT 穿越方案，但是因为存在许多问题，已经被废弃。 RFC5389 - Simple Traversal of UDP Through NATs (STUN) RFC5389 所定义的 STUN 协议是对 Classic STUN 的改进，它的定位不再是一个完整的 NAT 穿越解决方案，而是作为其他协议（例如SIP、FTP、DNS）处理 NAT 穿越问题的一个工具。 其可以用于检查网络中NAT设备的存在，并确定两个通信端点被NAT设备分配的IP地址和端口号。然后，通过ICE（Interactive Connectivity Establishment），自动创建一条能够进行NAT穿越的数据通道。 STUN 支持除 Symmetric NAT 之外的另外三种 NAT 类型 RFC5766 - Traversal Using Relays around NAT (TURN) TURN 在 STUN 协议之上添加了一个中继，以确保在无法实现 NAT 穿越的情况下，可以 fallback 到直接使用中继服务器进行通信。 这个中继的原理类似反向代理，单纯负责数据的转发 在美国有一项数据表示在进行 P2P 穿越的时候，穿越成功的概率为 70%，但是在国内这个成功率 50% 可能都到不了。因此就有必要使用 TURN 协议，这样才能保证在穿越失败的情况下，用户仍然能正常通信。 RFC8445 - Interactive Connectivity Establishment (ICE) 一个 NAT 穿越的协商协议，它统一了 STUN 与 TURN 两种协议，会尝试遍历所有可能的连接方案。 总的来说，标准的 NAT 穿越协议优先使用打洞（NAT Hole Pounching）技术，如果打洞失败，就使用中继服务器技术兜底，确保能成功穿越。 STUN/TURN/ICE 的 NAT 类型检测RFC5389 定义了对 NAT 映射类型以及过滤类型的检测方法。 TBD STUN/TURN/ICE 协议如何实现 NAT 打洞首先 P2P 双方如果只隔着 0-1 层 NAT，那是不需要使用 NAT 打洞技术的，可以直连或者反向连接。 下面就讨论下 P2P 双方隔着 2 层及以上 NAT 的场景下，如何利用 UDP 协议实现 NAT 打洞。 一个完整的 NAT 打洞方案，需要包含如下功能： A 跟 B 需要知道对方的公网 IP 以及监听的端口号 解决方法：需要一个公网中介来介绍双方认识（交换 IP/port） NAT 连通性测试，需要借助公网主机，检测双方中间网络的类型 针对不同的 NAT 类型，存在哪些穿越手段？以何种顺序进行穿越尝试？ NAT 打洞可以使用 UDP/TCP 两种 L4 协议，但是 TCP 面向连接的特性使它在这个场景中限制性更大（具体限制见参考文章，我有空再补充），因此各种 NAT 穿越协议通常都基于 UDP 实现。 此外，因为 NAT 的具体行为是非标准化的，路由器的防火墙策略也存在很大变动空间，再有就是 RF3489 的这种 NAT 分类方法不够精确，这些因素导致 NAT 穿透能否成功通常都是谈概率。 1. A 与 B 在同一局域网中这是最简单的情况，最佳方案是直接走内网通讯，不经过 NAT. 第二个方案是，这两个同一局域网内的客户端不走内网，仍然通过 NAT 通讯。这种通讯方式被称作「回环 NAT(Loopback NAT)」或者「发夹 NAT(Hairpin NAT)」。 对于不支持或未启用「Hairpin NAT」的网关设备而言，这样的通讯尝试将会失败！ 2. A 与 B 分别在不同的局域网中这样实际上 A 与 B 中间就隔了两个 NAT 网关，这是最普遍的一种情况。 STUN/TURN 的 NAT 穿透流程大致如下： 首先，A 跟 B 两个程序启动时，需要把自己的内外网 IP 及端口信息上报到一台中介服务器 S 现在假设 A 想要跟 B 建立一个 P2P 连接，首先他们需要从 S 获得对方的 ID A 将 B 的 ID 发送给中介服务器 S，请求与 B 建立 P2P 连接 中介服务器将 B 的内外网 IP 及端口信息发送给 A，同时将 A 的网络信息发送给 B A 尝试请求 B 的公网地址 B_public_ip:B_public_port 这肯定会失败，但是会在 A 的 NAT 网关上留下记录：A 曾经请求过这个地址，那之后这个地址发到 A 的 NAT 网关的流量就可以进来了。 B 尝试请求 A 的公网地址 A_public_ip:A_public_port 同样这肯定也会失败，但是会在 B 的 NAT 网关上流量记录：B 曾经请求过这个地址，那之后这个地址发到 B 的 NAT 网关的流量就可以进来了 中间的两层 NAT 网关均形成 NAT 穿越记录，穿越完成。 现在 A 尝试请求 B 的公网地址 B_public_ip:B_public_port，由于 B 的 NAT 已有记录，流量顺利通过 NAT 到达程序 B B 发送给 A 的数据也同样，可以顺利到达 A 上述流程中的关键点在于，如何查出内网服务器被 NAT 分配的外部 IP 及端口，只要有了这两个信息，就可以通过 STUN 中介服务器交换这个信息，然后完成连接的建立了。 家庭服务器通常都只有一个公网 IP，所以基本可以认为 IP 是固定的，因此最关键的问题就是「如何知道 NAT 为会话分配的端口地址」。 对端口的限制严格程度跟 NAPT 的类型有关，Full-cone 跟 Restricted cone 对端口都没有任何限制，所以上述流程肯定可以成功； TBD 一个穿越 Symmetric NATs 的 STUN 草案：Symmetric NAT Traversal using STUN 在使用 STUN/TURN 进行 NAT 穿越时，支持的的 NAT 类型如下表。行与列分别代表双方的 NAT 类型，✅ 表示支持 UDP 穿越，❌ 表示 TURN 无法进行 UDP 穿越： NAT 类型 Full Cone Restricted Port-Restricted Symmetric Full Cone ✅ ✅ ✅ ✅ Restricted ✅ ✅ ✅ ✅ Port-Restricted ✅ ✅ ✅ ❌ Symmetric ✅ ✅ ❌ ❌ 这种场景下 TURN 协议给出的解决方案是，fallback 到中继服务器策略作为兜底方案，保证连接能成功，但是这会给中继服务器带来很大压力，延迟等参数将不可避免地变差。 3. A 与 B 之间隔着三层以上的 NAT这种情况较为常见的有： ISP 为了节约使用公网 IP，给用户分配了个广域网 IP，中间就多了个广域网 NAT 大城市的各种租房公寓通常只会从 ISP 购买一两根宽带，二次分销给整栋楼的租客共用，这就造成中间多了一层公寓的 NAT 这是最复杂的一种情况，基本上就没什么 NAT 穿透的希望了，只能走下面介绍的兜底策略——服务器中继。 TBD 待续 4. 特殊穿越方案 - 服务器中继Relay 服务器中继是兼容性最佳，但是性能最差的方案，因为这个方案下，所有的 P2P 连接都需要经过中继服务器转发，在使用人数众多时这会给中继服务器造成很大的压力。 因此这个方案通常是用于兜底的。 ","date":"2022-05-13","objectID":"/posts/about-nat/:5:4","series":null,"tags":["NAT","网络","NAT 穿越","内网穿透","虚拟网络","P2P","WebRTC"],"title":"NAT 网关、NAT 穿越以及虚拟网络","uri":"/posts/about-nat/#stunturnice-协议如何实现-nat-打洞"},{"categories":["tech"],"content":" 4. NAT 穿越协议 - STUN/TURN/ICE如果很不幸前面提到的「DMZ 主机」/「静态端口转发」/「UPnP」 三项技术，你的路由器都不支持，那你就只能借助 NAT 穿越协议了。 目前有如下几个 NAT 穿越协议标准： RFC3489 Classic STUN Classic STUN 是一个早期的 STUN 规范，它定义了一整套完整的 NAT 穿越方案，但是因为存在许多问题，已经被废弃。 RFC5389 - Simple Traversal of UDP Through NATs (STUN) RFC5389 所定义的 STUN 协议是对 Classic STUN 的改进，它的定位不再是一个完整的 NAT 穿越解决方案，而是作为其他协议（例如SIP、FTP、DNS）处理 NAT 穿越问题的一个工具。 其可以用于检查网络中NAT设备的存在，并确定两个通信端点被NAT设备分配的IP地址和端口号。然后，通过ICE（Interactive Connectivity Establishment），自动创建一条能够进行NAT穿越的数据通道。 STUN 支持除 Symmetric NAT 之外的另外三种 NAT 类型 RFC5766 - Traversal Using Relays around NAT (TURN) TURN 在 STUN 协议之上添加了一个中继，以确保在无法实现 NAT 穿越的情况下，可以 fallback 到直接使用中继服务器进行通信。 这个中继的原理类似反向代理，单纯负责数据的转发 在美国有一项数据表示在进行 P2P 穿越的时候，穿越成功的概率为 70%，但是在国内这个成功率 50% 可能都到不了。因此就有必要使用 TURN 协议，这样才能保证在穿越失败的情况下，用户仍然能正常通信。 RFC8445 - Interactive Connectivity Establishment (ICE) 一个 NAT 穿越的协商协议，它统一了 STUN 与 TURN 两种协议，会尝试遍历所有可能的连接方案。 总的来说，标准的 NAT 穿越协议优先使用打洞（NAT Hole Pounching）技术，如果打洞失败，就使用中继服务器技术兜底，确保能成功穿越。 STUN/TURN/ICE 的 NAT 类型检测RFC5389 定义了对 NAT 映射类型以及过滤类型的检测方法。 TBD STUN/TURN/ICE 协议如何实现 NAT 打洞首先 P2P 双方如果只隔着 0-1 层 NAT，那是不需要使用 NAT 打洞技术的，可以直连或者反向连接。 下面就讨论下 P2P 双方隔着 2 层及以上 NAT 的场景下，如何利用 UDP 协议实现 NAT 打洞。 一个完整的 NAT 打洞方案，需要包含如下功能： A 跟 B 需要知道对方的公网 IP 以及监听的端口号 解决方法：需要一个公网中介来介绍双方认识（交换 IP/port） NAT 连通性测试，需要借助公网主机，检测双方中间网络的类型 针对不同的 NAT 类型，存在哪些穿越手段？以何种顺序进行穿越尝试？ NAT 打洞可以使用 UDP/TCP 两种 L4 协议，但是 TCP 面向连接的特性使它在这个场景中限制性更大（具体限制见参考文章，我有空再补充），因此各种 NAT 穿越协议通常都基于 UDP 实现。 此外，因为 NAT 的具体行为是非标准化的，路由器的防火墙策略也存在很大变动空间，再有就是 RF3489 的这种 NAT 分类方法不够精确，这些因素导致 NAT 穿透能否成功通常都是谈概率。 1. A 与 B 在同一局域网中这是最简单的情况，最佳方案是直接走内网通讯，不经过 NAT. 第二个方案是，这两个同一局域网内的客户端不走内网，仍然通过 NAT 通讯。这种通讯方式被称作「回环 NAT(Loopback NAT)」或者「发夹 NAT(Hairpin NAT)」。 对于不支持或未启用「Hairpin NAT」的网关设备而言，这样的通讯尝试将会失败！ 2. A 与 B 分别在不同的局域网中这样实际上 A 与 B 中间就隔了两个 NAT 网关，这是最普遍的一种情况。 STUN/TURN 的 NAT 穿透流程大致如下： 首先，A 跟 B 两个程序启动时，需要把自己的内外网 IP 及端口信息上报到一台中介服务器 S 现在假设 A 想要跟 B 建立一个 P2P 连接，首先他们需要从 S 获得对方的 ID A 将 B 的 ID 发送给中介服务器 S，请求与 B 建立 P2P 连接 中介服务器将 B 的内外网 IP 及端口信息发送给 A，同时将 A 的网络信息发送给 B A 尝试请求 B 的公网地址 B_public_ip:B_public_port 这肯定会失败，但是会在 A 的 NAT 网关上留下记录：A 曾经请求过这个地址，那之后这个地址发到 A 的 NAT 网关的流量就可以进来了。 B 尝试请求 A 的公网地址 A_public_ip:A_public_port 同样这肯定也会失败，但是会在 B 的 NAT 网关上流量记录：B 曾经请求过这个地址，那之后这个地址发到 B 的 NAT 网关的流量就可以进来了 中间的两层 NAT 网关均形成 NAT 穿越记录，穿越完成。 现在 A 尝试请求 B 的公网地址 B_public_ip:B_public_port，由于 B 的 NAT 已有记录，流量顺利通过 NAT 到达程序 B B 发送给 A 的数据也同样，可以顺利到达 A 上述流程中的关键点在于，如何查出内网服务器被 NAT 分配的外部 IP 及端口，只要有了这两个信息，就可以通过 STUN 中介服务器交换这个信息，然后完成连接的建立了。 家庭服务器通常都只有一个公网 IP，所以基本可以认为 IP 是固定的，因此最关键的问题就是「如何知道 NAT 为会话分配的端口地址」。 对端口的限制严格程度跟 NAPT 的类型有关，Full-cone 跟 Restricted cone 对端口都没有任何限制，所以上述流程肯定可以成功； TBD 一个穿越 Symmetric NATs 的 STUN 草案：Symmetric NAT Traversal using STUN 在使用 STUN/TURN 进行 NAT 穿越时，支持的的 NAT 类型如下表。行与列分别代表双方的 NAT 类型，✅ 表示支持 UDP 穿越，❌ 表示 TURN 无法进行 UDP 穿越： NAT 类型 Full Cone Restricted Port-Restricted Symmetric Full Cone ✅ ✅ ✅ ✅ Restricted ✅ ✅ ✅ ✅ Port-Restricted ✅ ✅ ✅ ❌ Symmetric ✅ ✅ ❌ ❌ 这种场景下 TURN 协议给出的解决方案是，fallback 到中继服务器策略作为兜底方案，保证连接能成功，但是这会给中继服务器带来很大压力，延迟等参数将不可避免地变差。 3. A 与 B 之间隔着三层以上的 NAT这种情况较为常见的有： ISP 为了节约使用公网 IP，给用户分配了个广域网 IP，中间就多了个广域网 NAT 大城市的各种租房公寓通常只会从 ISP 购买一两根宽带，二次分销给整栋楼的租客共用，这就造成中间多了一层公寓的 NAT 这是最复杂的一种情况，基本上就没什么 NAT 穿透的希望了，只能走下面介绍的兜底策略——服务器中继。 TBD 待续 4. 特殊穿越方案 - 服务器中继Relay 服务器中继是兼容性最佳，但是性能最差的方案，因为这个方案下，所有的 P2P 连接都需要经过中继服务器转发，在使用人数众多时这会给中继服务器造成很大的压力。 因此这个方案通常是用于兜底的。 ","date":"2022-05-13","objectID":"/posts/about-nat/:5:4","series":null,"tags":["NAT","网络","NAT 穿越","内网穿透","虚拟网络","P2P","WebRTC"],"title":"NAT 网关、NAT 穿越以及虚拟网络","uri":"/posts/about-nat/#1-a-与-b-在同一局域网中"},{"categories":["tech"],"content":" 4. NAT 穿越协议 - STUN/TURN/ICE如果很不幸前面提到的「DMZ 主机」/「静态端口转发」/「UPnP」 三项技术，你的路由器都不支持，那你就只能借助 NAT 穿越协议了。 目前有如下几个 NAT 穿越协议标准： RFC3489 Classic STUN Classic STUN 是一个早期的 STUN 规范，它定义了一整套完整的 NAT 穿越方案，但是因为存在许多问题，已经被废弃。 RFC5389 - Simple Traversal of UDP Through NATs (STUN) RFC5389 所定义的 STUN 协议是对 Classic STUN 的改进，它的定位不再是一个完整的 NAT 穿越解决方案，而是作为其他协议（例如SIP、FTP、DNS）处理 NAT 穿越问题的一个工具。 其可以用于检查网络中NAT设备的存在，并确定两个通信端点被NAT设备分配的IP地址和端口号。然后，通过ICE（Interactive Connectivity Establishment），自动创建一条能够进行NAT穿越的数据通道。 STUN 支持除 Symmetric NAT 之外的另外三种 NAT 类型 RFC5766 - Traversal Using Relays around NAT (TURN) TURN 在 STUN 协议之上添加了一个中继，以确保在无法实现 NAT 穿越的情况下，可以 fallback 到直接使用中继服务器进行通信。 这个中继的原理类似反向代理，单纯负责数据的转发 在美国有一项数据表示在进行 P2P 穿越的时候，穿越成功的概率为 70%，但是在国内这个成功率 50% 可能都到不了。因此就有必要使用 TURN 协议，这样才能保证在穿越失败的情况下，用户仍然能正常通信。 RFC8445 - Interactive Connectivity Establishment (ICE) 一个 NAT 穿越的协商协议，它统一了 STUN 与 TURN 两种协议，会尝试遍历所有可能的连接方案。 总的来说，标准的 NAT 穿越协议优先使用打洞（NAT Hole Pounching）技术，如果打洞失败，就使用中继服务器技术兜底，确保能成功穿越。 STUN/TURN/ICE 的 NAT 类型检测RFC5389 定义了对 NAT 映射类型以及过滤类型的检测方法。 TBD STUN/TURN/ICE 协议如何实现 NAT 打洞首先 P2P 双方如果只隔着 0-1 层 NAT，那是不需要使用 NAT 打洞技术的，可以直连或者反向连接。 下面就讨论下 P2P 双方隔着 2 层及以上 NAT 的场景下，如何利用 UDP 协议实现 NAT 打洞。 一个完整的 NAT 打洞方案，需要包含如下功能： A 跟 B 需要知道对方的公网 IP 以及监听的端口号 解决方法：需要一个公网中介来介绍双方认识（交换 IP/port） NAT 连通性测试，需要借助公网主机，检测双方中间网络的类型 针对不同的 NAT 类型，存在哪些穿越手段？以何种顺序进行穿越尝试？ NAT 打洞可以使用 UDP/TCP 两种 L4 协议，但是 TCP 面向连接的特性使它在这个场景中限制性更大（具体限制见参考文章，我有空再补充），因此各种 NAT 穿越协议通常都基于 UDP 实现。 此外，因为 NAT 的具体行为是非标准化的，路由器的防火墙策略也存在很大变动空间，再有就是 RF3489 的这种 NAT 分类方法不够精确，这些因素导致 NAT 穿透能否成功通常都是谈概率。 1. A 与 B 在同一局域网中这是最简单的情况，最佳方案是直接走内网通讯，不经过 NAT. 第二个方案是，这两个同一局域网内的客户端不走内网，仍然通过 NAT 通讯。这种通讯方式被称作「回环 NAT(Loopback NAT)」或者「发夹 NAT(Hairpin NAT)」。 对于不支持或未启用「Hairpin NAT」的网关设备而言，这样的通讯尝试将会失败！ 2. A 与 B 分别在不同的局域网中这样实际上 A 与 B 中间就隔了两个 NAT 网关，这是最普遍的一种情况。 STUN/TURN 的 NAT 穿透流程大致如下： 首先，A 跟 B 两个程序启动时，需要把自己的内外网 IP 及端口信息上报到一台中介服务器 S 现在假设 A 想要跟 B 建立一个 P2P 连接，首先他们需要从 S 获得对方的 ID A 将 B 的 ID 发送给中介服务器 S，请求与 B 建立 P2P 连接 中介服务器将 B 的内外网 IP 及端口信息发送给 A，同时将 A 的网络信息发送给 B A 尝试请求 B 的公网地址 B_public_ip:B_public_port 这肯定会失败，但是会在 A 的 NAT 网关上留下记录：A 曾经请求过这个地址，那之后这个地址发到 A 的 NAT 网关的流量就可以进来了。 B 尝试请求 A 的公网地址 A_public_ip:A_public_port 同样这肯定也会失败，但是会在 B 的 NAT 网关上流量记录：B 曾经请求过这个地址，那之后这个地址发到 B 的 NAT 网关的流量就可以进来了 中间的两层 NAT 网关均形成 NAT 穿越记录，穿越完成。 现在 A 尝试请求 B 的公网地址 B_public_ip:B_public_port，由于 B 的 NAT 已有记录，流量顺利通过 NAT 到达程序 B B 发送给 A 的数据也同样，可以顺利到达 A 上述流程中的关键点在于，如何查出内网服务器被 NAT 分配的外部 IP 及端口，只要有了这两个信息，就可以通过 STUN 中介服务器交换这个信息，然后完成连接的建立了。 家庭服务器通常都只有一个公网 IP，所以基本可以认为 IP 是固定的，因此最关键的问题就是「如何知道 NAT 为会话分配的端口地址」。 对端口的限制严格程度跟 NAPT 的类型有关，Full-cone 跟 Restricted cone 对端口都没有任何限制，所以上述流程肯定可以成功； TBD 一个穿越 Symmetric NATs 的 STUN 草案：Symmetric NAT Traversal using STUN 在使用 STUN/TURN 进行 NAT 穿越时，支持的的 NAT 类型如下表。行与列分别代表双方的 NAT 类型，✅ 表示支持 UDP 穿越，❌ 表示 TURN 无法进行 UDP 穿越： NAT 类型 Full Cone Restricted Port-Restricted Symmetric Full Cone ✅ ✅ ✅ ✅ Restricted ✅ ✅ ✅ ✅ Port-Restricted ✅ ✅ ✅ ❌ Symmetric ✅ ✅ ❌ ❌ 这种场景下 TURN 协议给出的解决方案是，fallback 到中继服务器策略作为兜底方案，保证连接能成功，但是这会给中继服务器带来很大压力，延迟等参数将不可避免地变差。 3. A 与 B 之间隔着三层以上的 NAT这种情况较为常见的有： ISP 为了节约使用公网 IP，给用户分配了个广域网 IP，中间就多了个广域网 NAT 大城市的各种租房公寓通常只会从 ISP 购买一两根宽带，二次分销给整栋楼的租客共用，这就造成中间多了一层公寓的 NAT 这是最复杂的一种情况，基本上就没什么 NAT 穿透的希望了，只能走下面介绍的兜底策略——服务器中继。 TBD 待续 4. 特殊穿越方案 - 服务器中继Relay 服务器中继是兼容性最佳，但是性能最差的方案，因为这个方案下，所有的 P2P 连接都需要经过中继服务器转发，在使用人数众多时这会给中继服务器造成很大的压力。 因此这个方案通常是用于兜底的。 ","date":"2022-05-13","objectID":"/posts/about-nat/:5:4","series":null,"tags":["NAT","网络","NAT 穿越","内网穿透","虚拟网络","P2P","WebRTC"],"title":"NAT 网关、NAT 穿越以及虚拟网络","uri":"/posts/about-nat/#2-a-与-b-分别在不同的局域网中"},{"categories":["tech"],"content":" 4. NAT 穿越协议 - STUN/TURN/ICE如果很不幸前面提到的「DMZ 主机」/「静态端口转发」/「UPnP」 三项技术，你的路由器都不支持，那你就只能借助 NAT 穿越协议了。 目前有如下几个 NAT 穿越协议标准： RFC3489 Classic STUN Classic STUN 是一个早期的 STUN 规范，它定义了一整套完整的 NAT 穿越方案，但是因为存在许多问题，已经被废弃。 RFC5389 - Simple Traversal of UDP Through NATs (STUN) RFC5389 所定义的 STUN 协议是对 Classic STUN 的改进，它的定位不再是一个完整的 NAT 穿越解决方案，而是作为其他协议（例如SIP、FTP、DNS）处理 NAT 穿越问题的一个工具。 其可以用于检查网络中NAT设备的存在，并确定两个通信端点被NAT设备分配的IP地址和端口号。然后，通过ICE（Interactive Connectivity Establishment），自动创建一条能够进行NAT穿越的数据通道。 STUN 支持除 Symmetric NAT 之外的另外三种 NAT 类型 RFC5766 - Traversal Using Relays around NAT (TURN) TURN 在 STUN 协议之上添加了一个中继，以确保在无法实现 NAT 穿越的情况下，可以 fallback 到直接使用中继服务器进行通信。 这个中继的原理类似反向代理，单纯负责数据的转发 在美国有一项数据表示在进行 P2P 穿越的时候，穿越成功的概率为 70%，但是在国内这个成功率 50% 可能都到不了。因此就有必要使用 TURN 协议，这样才能保证在穿越失败的情况下，用户仍然能正常通信。 RFC8445 - Interactive Connectivity Establishment (ICE) 一个 NAT 穿越的协商协议，它统一了 STUN 与 TURN 两种协议，会尝试遍历所有可能的连接方案。 总的来说，标准的 NAT 穿越协议优先使用打洞（NAT Hole Pounching）技术，如果打洞失败，就使用中继服务器技术兜底，确保能成功穿越。 STUN/TURN/ICE 的 NAT 类型检测RFC5389 定义了对 NAT 映射类型以及过滤类型的检测方法。 TBD STUN/TURN/ICE 协议如何实现 NAT 打洞首先 P2P 双方如果只隔着 0-1 层 NAT，那是不需要使用 NAT 打洞技术的，可以直连或者反向连接。 下面就讨论下 P2P 双方隔着 2 层及以上 NAT 的场景下，如何利用 UDP 协议实现 NAT 打洞。 一个完整的 NAT 打洞方案，需要包含如下功能： A 跟 B 需要知道对方的公网 IP 以及监听的端口号 解决方法：需要一个公网中介来介绍双方认识（交换 IP/port） NAT 连通性测试，需要借助公网主机，检测双方中间网络的类型 针对不同的 NAT 类型，存在哪些穿越手段？以何种顺序进行穿越尝试？ NAT 打洞可以使用 UDP/TCP 两种 L4 协议，但是 TCP 面向连接的特性使它在这个场景中限制性更大（具体限制见参考文章，我有空再补充），因此各种 NAT 穿越协议通常都基于 UDP 实现。 此外，因为 NAT 的具体行为是非标准化的，路由器的防火墙策略也存在很大变动空间，再有就是 RF3489 的这种 NAT 分类方法不够精确，这些因素导致 NAT 穿透能否成功通常都是谈概率。 1. A 与 B 在同一局域网中这是最简单的情况，最佳方案是直接走内网通讯，不经过 NAT. 第二个方案是，这两个同一局域网内的客户端不走内网，仍然通过 NAT 通讯。这种通讯方式被称作「回环 NAT(Loopback NAT)」或者「发夹 NAT(Hairpin NAT)」。 对于不支持或未启用「Hairpin NAT」的网关设备而言，这样的通讯尝试将会失败！ 2. A 与 B 分别在不同的局域网中这样实际上 A 与 B 中间就隔了两个 NAT 网关，这是最普遍的一种情况。 STUN/TURN 的 NAT 穿透流程大致如下： 首先，A 跟 B 两个程序启动时，需要把自己的内外网 IP 及端口信息上报到一台中介服务器 S 现在假设 A 想要跟 B 建立一个 P2P 连接，首先他们需要从 S 获得对方的 ID A 将 B 的 ID 发送给中介服务器 S，请求与 B 建立 P2P 连接 中介服务器将 B 的内外网 IP 及端口信息发送给 A，同时将 A 的网络信息发送给 B A 尝试请求 B 的公网地址 B_public_ip:B_public_port 这肯定会失败，但是会在 A 的 NAT 网关上留下记录：A 曾经请求过这个地址，那之后这个地址发到 A 的 NAT 网关的流量就可以进来了。 B 尝试请求 A 的公网地址 A_public_ip:A_public_port 同样这肯定也会失败，但是会在 B 的 NAT 网关上流量记录：B 曾经请求过这个地址，那之后这个地址发到 B 的 NAT 网关的流量就可以进来了 中间的两层 NAT 网关均形成 NAT 穿越记录，穿越完成。 现在 A 尝试请求 B 的公网地址 B_public_ip:B_public_port，由于 B 的 NAT 已有记录，流量顺利通过 NAT 到达程序 B B 发送给 A 的数据也同样，可以顺利到达 A 上述流程中的关键点在于，如何查出内网服务器被 NAT 分配的外部 IP 及端口，只要有了这两个信息，就可以通过 STUN 中介服务器交换这个信息，然后完成连接的建立了。 家庭服务器通常都只有一个公网 IP，所以基本可以认为 IP 是固定的，因此最关键的问题就是「如何知道 NAT 为会话分配的端口地址」。 对端口的限制严格程度跟 NAPT 的类型有关，Full-cone 跟 Restricted cone 对端口都没有任何限制，所以上述流程肯定可以成功； TBD 一个穿越 Symmetric NATs 的 STUN 草案：Symmetric NAT Traversal using STUN 在使用 STUN/TURN 进行 NAT 穿越时，支持的的 NAT 类型如下表。行与列分别代表双方的 NAT 类型，✅ 表示支持 UDP 穿越，❌ 表示 TURN 无法进行 UDP 穿越： NAT 类型 Full Cone Restricted Port-Restricted Symmetric Full Cone ✅ ✅ ✅ ✅ Restricted ✅ ✅ ✅ ✅ Port-Restricted ✅ ✅ ✅ ❌ Symmetric ✅ ✅ ❌ ❌ 这种场景下 TURN 协议给出的解决方案是，fallback 到中继服务器策略作为兜底方案，保证连接能成功，但是这会给中继服务器带来很大压力，延迟等参数将不可避免地变差。 3. A 与 B 之间隔着三层以上的 NAT这种情况较为常见的有： ISP 为了节约使用公网 IP，给用户分配了个广域网 IP，中间就多了个广域网 NAT 大城市的各种租房公寓通常只会从 ISP 购买一两根宽带，二次分销给整栋楼的租客共用，这就造成中间多了一层公寓的 NAT 这是最复杂的一种情况，基本上就没什么 NAT 穿透的希望了，只能走下面介绍的兜底策略——服务器中继。 TBD 待续 4. 特殊穿越方案 - 服务器中继Relay 服务器中继是兼容性最佳，但是性能最差的方案，因为这个方案下，所有的 P2P 连接都需要经过中继服务器转发，在使用人数众多时这会给中继服务器造成很大的压力。 因此这个方案通常是用于兜底的。 ","date":"2022-05-13","objectID":"/posts/about-nat/:5:4","series":null,"tags":["NAT","网络","NAT 穿越","内网穿透","虚拟网络","P2P","WebRTC"],"title":"NAT 网关、NAT 穿越以及虚拟网络","uri":"/posts/about-nat/#3-a-与-b-之间隔着三层以上的-nat"},{"categories":["tech"],"content":" 4. NAT 穿越协议 - STUN/TURN/ICE如果很不幸前面提到的「DMZ 主机」/「静态端口转发」/「UPnP」 三项技术，你的路由器都不支持，那你就只能借助 NAT 穿越协议了。 目前有如下几个 NAT 穿越协议标准： RFC3489 Classic STUN Classic STUN 是一个早期的 STUN 规范，它定义了一整套完整的 NAT 穿越方案，但是因为存在许多问题，已经被废弃。 RFC5389 - Simple Traversal of UDP Through NATs (STUN) RFC5389 所定义的 STUN 协议是对 Classic STUN 的改进，它的定位不再是一个完整的 NAT 穿越解决方案，而是作为其他协议（例如SIP、FTP、DNS）处理 NAT 穿越问题的一个工具。 其可以用于检查网络中NAT设备的存在，并确定两个通信端点被NAT设备分配的IP地址和端口号。然后，通过ICE（Interactive Connectivity Establishment），自动创建一条能够进行NAT穿越的数据通道。 STUN 支持除 Symmetric NAT 之外的另外三种 NAT 类型 RFC5766 - Traversal Using Relays around NAT (TURN) TURN 在 STUN 协议之上添加了一个中继，以确保在无法实现 NAT 穿越的情况下，可以 fallback 到直接使用中继服务器进行通信。 这个中继的原理类似反向代理，单纯负责数据的转发 在美国有一项数据表示在进行 P2P 穿越的时候，穿越成功的概率为 70%，但是在国内这个成功率 50% 可能都到不了。因此就有必要使用 TURN 协议，这样才能保证在穿越失败的情况下，用户仍然能正常通信。 RFC8445 - Interactive Connectivity Establishment (ICE) 一个 NAT 穿越的协商协议，它统一了 STUN 与 TURN 两种协议，会尝试遍历所有可能的连接方案。 总的来说，标准的 NAT 穿越协议优先使用打洞（NAT Hole Pounching）技术，如果打洞失败，就使用中继服务器技术兜底，确保能成功穿越。 STUN/TURN/ICE 的 NAT 类型检测RFC5389 定义了对 NAT 映射类型以及过滤类型的检测方法。 TBD STUN/TURN/ICE 协议如何实现 NAT 打洞首先 P2P 双方如果只隔着 0-1 层 NAT，那是不需要使用 NAT 打洞技术的，可以直连或者反向连接。 下面就讨论下 P2P 双方隔着 2 层及以上 NAT 的场景下，如何利用 UDP 协议实现 NAT 打洞。 一个完整的 NAT 打洞方案，需要包含如下功能： A 跟 B 需要知道对方的公网 IP 以及监听的端口号 解决方法：需要一个公网中介来介绍双方认识（交换 IP/port） NAT 连通性测试，需要借助公网主机，检测双方中间网络的类型 针对不同的 NAT 类型，存在哪些穿越手段？以何种顺序进行穿越尝试？ NAT 打洞可以使用 UDP/TCP 两种 L4 协议，但是 TCP 面向连接的特性使它在这个场景中限制性更大（具体限制见参考文章，我有空再补充），因此各种 NAT 穿越协议通常都基于 UDP 实现。 此外，因为 NAT 的具体行为是非标准化的，路由器的防火墙策略也存在很大变动空间，再有就是 RF3489 的这种 NAT 分类方法不够精确，这些因素导致 NAT 穿透能否成功通常都是谈概率。 1. A 与 B 在同一局域网中这是最简单的情况，最佳方案是直接走内网通讯，不经过 NAT. 第二个方案是，这两个同一局域网内的客户端不走内网，仍然通过 NAT 通讯。这种通讯方式被称作「回环 NAT(Loopback NAT)」或者「发夹 NAT(Hairpin NAT)」。 对于不支持或未启用「Hairpin NAT」的网关设备而言，这样的通讯尝试将会失败！ 2. A 与 B 分别在不同的局域网中这样实际上 A 与 B 中间就隔了两个 NAT 网关，这是最普遍的一种情况。 STUN/TURN 的 NAT 穿透流程大致如下： 首先，A 跟 B 两个程序启动时，需要把自己的内外网 IP 及端口信息上报到一台中介服务器 S 现在假设 A 想要跟 B 建立一个 P2P 连接，首先他们需要从 S 获得对方的 ID A 将 B 的 ID 发送给中介服务器 S，请求与 B 建立 P2P 连接 中介服务器将 B 的内外网 IP 及端口信息发送给 A，同时将 A 的网络信息发送给 B A 尝试请求 B 的公网地址 B_public_ip:B_public_port 这肯定会失败，但是会在 A 的 NAT 网关上留下记录：A 曾经请求过这个地址，那之后这个地址发到 A 的 NAT 网关的流量就可以进来了。 B 尝试请求 A 的公网地址 A_public_ip:A_public_port 同样这肯定也会失败，但是会在 B 的 NAT 网关上流量记录：B 曾经请求过这个地址，那之后这个地址发到 B 的 NAT 网关的流量就可以进来了 中间的两层 NAT 网关均形成 NAT 穿越记录，穿越完成。 现在 A 尝试请求 B 的公网地址 B_public_ip:B_public_port，由于 B 的 NAT 已有记录，流量顺利通过 NAT 到达程序 B B 发送给 A 的数据也同样，可以顺利到达 A 上述流程中的关键点在于，如何查出内网服务器被 NAT 分配的外部 IP 及端口，只要有了这两个信息，就可以通过 STUN 中介服务器交换这个信息，然后完成连接的建立了。 家庭服务器通常都只有一个公网 IP，所以基本可以认为 IP 是固定的，因此最关键的问题就是「如何知道 NAT 为会话分配的端口地址」。 对端口的限制严格程度跟 NAPT 的类型有关，Full-cone 跟 Restricted cone 对端口都没有任何限制，所以上述流程肯定可以成功； TBD 一个穿越 Symmetric NATs 的 STUN 草案：Symmetric NAT Traversal using STUN 在使用 STUN/TURN 进行 NAT 穿越时，支持的的 NAT 类型如下表。行与列分别代表双方的 NAT 类型，✅ 表示支持 UDP 穿越，❌ 表示 TURN 无法进行 UDP 穿越： NAT 类型 Full Cone Restricted Port-Restricted Symmetric Full Cone ✅ ✅ ✅ ✅ Restricted ✅ ✅ ✅ ✅ Port-Restricted ✅ ✅ ✅ ❌ Symmetric ✅ ✅ ❌ ❌ 这种场景下 TURN 协议给出的解决方案是，fallback 到中继服务器策略作为兜底方案，保证连接能成功，但是这会给中继服务器带来很大压力，延迟等参数将不可避免地变差。 3. A 与 B 之间隔着三层以上的 NAT这种情况较为常见的有： ISP 为了节约使用公网 IP，给用户分配了个广域网 IP，中间就多了个广域网 NAT 大城市的各种租房公寓通常只会从 ISP 购买一两根宽带，二次分销给整栋楼的租客共用，这就造成中间多了一层公寓的 NAT 这是最复杂的一种情况，基本上就没什么 NAT 穿透的希望了，只能走下面介绍的兜底策略——服务器中继。 TBD 待续 4. 特殊穿越方案 - 服务器中继Relay 服务器中继是兼容性最佳，但是性能最差的方案，因为这个方案下，所有的 P2P 连接都需要经过中继服务器转发，在使用人数众多时这会给中继服务器造成很大的压力。 因此这个方案通常是用于兜底的。 ","date":"2022-05-13","objectID":"/posts/about-nat/:5:4","series":null,"tags":["NAT","网络","NAT 穿越","内网穿透","虚拟网络","P2P","WebRTC"],"title":"NAT 网关、NAT 穿越以及虚拟网络","uri":"/posts/about-nat/#4-特殊穿越方案---服务器中继"},{"categories":["tech"],"content":" 特定协议的自穿越技术在所有方法中最复杂也最可靠的就是自己解决自己的问题。比如 IKE 和 IPsec 技术，在设计时就考虑了到如何穿越 NAT 的问题。因为这个协议是一个自加密的协议并且具有报文防修改的鉴别能力，其他通用方法爱莫能助。因为实际应用的 NAT 网关基本都是 NAPT 方式，所有通过传输层协议承载的报文可以顺利通过 NAT。IKE 和 IPsec 采用的方案就是用 UDP 在报文外面再加一层封装，而内部的报文就不再受到影响。IKE 中还专门增加了 NAT 网关是否存在的检查能力以及绕开 NAT 网关检测 IKE 协议的方法。 ","date":"2022-05-13","objectID":"/posts/about-nat/:5:5","series":null,"tags":["NAT","网络","NAT 穿越","内网穿透","虚拟网络","P2P","WebRTC"],"title":"NAT 网关、NAT 穿越以及虚拟网络","uri":"/posts/about-nat/#特定协议的自穿越技术"},{"categories":["tech"],"content":" NAT ALG(Application Level Gateway)NAT ALG 是一种解决应用层协议（例如DNS、FTP）报文穿越 NAT 的技术，已经被 NAT 设备产商广泛采用，是 NAT 设备的必备功能。 TLDR 一句话介绍：NAT ALG 通过识别协议，直接修改报文数据部分（payload）的 IP 地址和端口信息，解决某些应用协议的报文穿越 NAT 问题。NAT ALG 工作在 L3-L7 层。 NAT ALG 的原理是利用带有 ALG 功能的 NAT 设备对特定应用层协议的支持，当设备检测到新的连接请求时，先根据传输层端口信息判断是否为已知应用类型。如果判断为已知应用，则调用该应用协议的 ALG 功能对报文的深层内容进行检查。若发现任何形式表达的 IP 地址和端口信息，NAT 都会将这些信息同步进行转换，并为这个新的连接建立一个附加的转换表项。当报文到达外网侧的目的主机时，应用层协议中携带的信息就是 NAT 设备转换后的IP地址和端口号，这样，就可以解决某些应用协议的报文穿越 NAT 问题。 目前支持NAT ALG功能的协议包括：DNS、FTP、SIP、PPTP和RTSP。NAT ALG 在对这些特定应用层协议进行 NAT 转换时，通过 NAT 的状态信息来改变封装在 IP 报文数据部分的特定数据，最终使应用层协议可以跨越不同范围运行。 ","date":"2022-05-13","objectID":"/posts/about-nat/:5:6","series":null,"tags":["NAT","网络","NAT 穿越","内网穿透","虚拟网络","P2P","WebRTC"],"title":"NAT 网关、NAT 穿越以及虚拟网络","uri":"/posts/about-nat/#nat-algapplication-level-gateway"},{"categories":["tech"],"content":" 使用 Go 实验 NAT 穿透Go 可用的 NAT 穿越库有： coturn: 貌似是最流行的 STUN/TURN/ICE server go-stun: 一个简洁的 stun client 实现，大概适合用于学习？ pion/turn: 一个 STUN/TURN/ICE client/client 实现 pion/ice: 一个 ice 实现 TBD 待完善 ","date":"2022-05-13","objectID":"/posts/about-nat/:6:0","series":null,"tags":["NAT","网络","NAT 穿越","内网穿透","虚拟网络","P2P","WebRTC"],"title":"NAT 网关、NAT 穿越以及虚拟网络","uri":"/posts/about-nat/#使用-go-实验-nat-穿透"},{"categories":["tech"],"content":" 虚拟网络、Overlay 与 Underlay虚拟网络就是在物理网络之上，构建的逻辑网络，也被称作 overlay 网络。 比如 AWS VPC、Docker 容器网络、QEMU 的默认网络，都是虚拟网络。 而 underlay 网络，则是指承载 overlay 网络的底层网络。 我个人理解，它是一个相对的概念，物理网络一定是 underlay 网络，但是虚拟网络之上如果还承载了其他虚拟网络（套娃），那它相对上层网络而言，也是一个 underlay 网络。 overlay 本质上就是一种隧道技术，将原生态的二层数据帧报文进行封装后通过隧道进行传输。常见的 overlay 网络协议主要是 vxlan 以及新一代的 geneve，它俩都是使用 UDP 包来封装链路层的以太网帧。 vxlan 在 2014 年标准化，而 geneve 在 2020 年底才通过草案阶段，目前尚未形成最终标准。但是目前 linux/cilium 都已经支持了 geneve. geneve 相对 vxlan 最大的变化，是它更灵活——它的 header 长度是可变的。 目前所有 overlay 的跨主机容器网络方案，几乎都是基于 vxlan 实现的（例外：cilium 也支持 geneve）。 vxlan/geneve 的详细介绍，参见 Linux 中的虚拟网络接口 - vxlan/geneve 顺带再提一嘴，cilium/calico/kube-ovn 等 overlay 容器网络，都是 SDN 软件定义网络。 ","date":"2022-05-13","objectID":"/posts/about-nat/:7:0","series":null,"tags":["NAT","网络","NAT 穿越","内网穿透","虚拟网络","P2P","WebRTC"],"title":"NAT 网关、NAT 穿越以及虚拟网络","uri":"/posts/about-nat/#虚拟网络overlay-与-underlay"},{"categories":["tech"],"content":" 相关工具有一些专门用于跨网搭建私有虚拟网络的工具，由于家庭网络设备前面通常都有至少一层 NAT（家庭路由器），因此这些工具都重度依赖 NAT 穿越技术。 如果 NAT 层数太多无法穿越，它们会 fallback 到代理模式，也就是由一台公网服务器进行流量中继，但是这会对中继服务器造成很大压力，延迟跟带宽通常都会差很多。 如下是两个比较流行的 VPN 搭建工具： zerotier: 在 P2P 网络之上搭建的 SDN overlay 网络，使用自定义协议。 tailscales: 基于 wireguard 协议快速搭建私有虚拟网络 VPN ","date":"2022-05-13","objectID":"/posts/about-nat/:7:1","series":null,"tags":["NAT","网络","NAT 穿越","内网穿透","虚拟网络","P2P","WebRTC"],"title":"NAT 网关、NAT 穿越以及虚拟网络","uri":"/posts/about-nat/#相关工具"},{"categories":["tech"],"content":" VPN 协议主流的 VPN 协议有：PPTP、L2TP、IPSec、OpenVPN、SSTP，以及最新潮的 Wireguard. TBD ","date":"2022-05-13","objectID":"/posts/about-nat/:7:2","series":null,"tags":["NAT","网络","NAT 穿越","内网穿透","虚拟网络","P2P","WebRTC"],"title":"NAT 网关、NAT 穿越以及虚拟网络","uri":"/posts/about-nat/#vpn-协议"},{"categories":["tech"],"content":" 拓展知识","date":"2022-05-13","objectID":"/posts/about-nat/:8:0","series":null,"tags":["NAT","网络","NAT 穿越","内网穿透","虚拟网络","P2P","WebRTC"],"title":"NAT 网关、NAT 穿越以及虚拟网络","uri":"/posts/about-nat/#拓展知识"},{"categories":["tech"],"content":" Symmetric NAT 允许的最大并发 TCP 连接数是多少？TCP 并发连接数受许多参数的限制，以 Linux 服务器为例，默认参数无法满足需要，通常都会手动修改它的参数，放宽文件描述符限制以及 TCP 连接队列、缓存相关的限制。 单纯从网络协议层面分析，对于一个仅有一个公网 IP 的 Symmetric NAT 网关，它与某个外部站点 http://x.x.x.x:xx 要建立连接。 考虑到 TCP 连接的定义实际上是一个四元组 (srcIP, srcPort, dstIP, dstPort)，其中就只有 NAT 网关自己的 srcPort 是唯一的变量了，而端口号是一个 16bits 数字，取值范围为 0 - 65535。 此外低于 1024 的数字是操作系统的保留端口，因此 NAT 一般只会使用 1024-65535 这个区间的端口号，也就是说这个 NAT 网关最多只能与该站点建立 64512 个连接。 那么对于不同的协议 NAT 是如何处理的呢？NAT 肯定可以通过协议特征区分不同协议的流量，因此不同协议通过 NAT 建立的并发连接不会相互影响。 对于家庭网络而言 64512 个连接已经完全够用了，但是对于数据中心或者云上的 VPC 而言，就不一定够用了。 举个例子，在 AWS NAT 网关的文档中就有提到，AWS NAT 网关最高支持与每个不同的地址建立 55000 个并发连接。destination 的 IP 地址、端口号、(TCP/UDP/ICMP) 任一个发生改变，都可以再建立其他 55000 个并发连接。 如果超过这个限制，就会发生「ErrorPortAllocation」错误。如果在 AWS 上遇到这个错误，那就说明你们的云上网络规划有问题了。 当然除了端口限制外，受限于 NAT 硬件、以太网协议以及其他影响，NAT 网关肯定还有包处理速率、带宽的限制，这个就略过不提了。 ","date":"2022-05-13","objectID":"/posts/about-nat/:8:1","series":null,"tags":["NAT","网络","NAT 穿越","内网穿透","虚拟网络","P2P","WebRTC"],"title":"NAT 网关、NAT 穿越以及虚拟网络","uri":"/posts/about-nat/#symmetric-nat-允许的最大并发-tcp-连接数是多少"},{"categories":["tech"],"content":" AWS VPC 与 NATAWS VPC(virtual private cloud) 是一个逻辑隔离的虚拟私有网络，云服务架构的最佳实践之一就是通过 VPC 搭建云上私有网络，提升网络安全性。 AWS VPC 提供两种网关类型： NAT 网关 支持三种协议：TCP, UDP, ICMP 支持 IPv4 与 IPv6 两种 IP 协议 支持 5 Gbps 带宽，可自动扩展到 45 Gbps 可通过划分子网并在多个子网中添加 NAT 网关的方式，获得超过 45Gbps 的带宽 最高支持与每个不同的地址建立 55000 个并发连接 NAT 网关从属于 VPC 的子网 每个 NAT 网关只能绑定一个 IP 可通过划分子网并在多个子网中添加 NAT 网关的方式获得多个 IP 可达到 100w packets 每秒的处理速度，并能自动扩展到 400w packets 每秒 同样，需要更高的处理速度，请添加更多 NAT 网关 按处理数据量收费 默认路由到 NAT 子网，被称为「私有子网」（或者没默认路由，那就是无法访问公网的私有子网），连接只能由内网程序主动发起。 NAT 网关为流量执行「Symmetric NAT」 IGW 因特网网关 IGW 是一个高度可用的逻辑组件，不会限制 VPC 的总带宽、处理能力。 IGW 实例直接关联 VPC，不从属于任何可用区或子网 IGW 实质上是一个 NAT 设备，为绑定了公网 IP 地址的 ENI/EC2 实例，执行「一对一 NAT」 默认路由到 IGW 的子网，被称为「公有子网」 ","date":"2022-05-13","objectID":"/posts/about-nat/:8:2","series":null,"tags":["NAT","网络","NAT 穿越","内网穿透","虚拟网络","P2P","WebRTC"],"title":"NAT 网关、NAT 穿越以及虚拟网络","uri":"/posts/about-nat/#aws-vpc-与-nat"},{"categories":["tech"],"content":" 参考 What Is Network Address Translation (NAT)? - Huawei Docs What Is STUN? - Huawei Docs NetEngine AR V300R019 配置指南-IP业务 - NAT 穿越 - 华为文档 P2P技术详解(一)：NAT详解——详细原理、P2P简介 P2P技术详解(二)：P2P中的NAT穿越(打洞)方案详解 P2P技术详解(三)：P2P中的NAT穿越(打洞)方案详解(进阶分析篇) Connect to the internet using an internet gateway - AWS VPC Internet Gateway 从DNAT到netfilter内核子系统，浅谈Linux的Full Cone NAT实现 Network address translation - wikipedia WebRTC NAT Traversal Methods: A Case for Embedded TURN WireGuard 教程：使用 DNS-SD 进行 NAT-to-NAT 穿透 - 云原生实验室 ","date":"2022-05-13","objectID":"/posts/about-nat/:9:0","series":null,"tags":["NAT","网络","NAT 穿越","内网穿透","虚拟网络","P2P","WebRTC"],"title":"NAT 网关、NAT 穿越以及虚拟网络","uri":"/posts/about-nat/#参考"},{"categories":["tech"],"content":" FinOps 是一种不断发展的云财务管理学科和文化实践，通过帮助工程、财务、技术和业务团队在数据驱动的预算分配上进行协作，使成本预算能够产生最大的业务价值。 ","date":"2022-05-04","objectID":"/posts/finops-for-kubernetes/:0:0","series":null,"tags":["云原生","Kubernetes","FinOps","成本分析","Kubecost","MultiCloud","多云","多云财务管控"],"title":"FinOps for Kubernetes - 如何拆分 Kubernetes 成本","uri":"/posts/finops-for-kubernetes/#"},{"categories":["tech"],"content":" 云计算成本管控随着越来越多的企业上云，云计算的成本管控也越来越受关注。在讨论 Kubernetes 成本之前，先简单聊下如何管控云计算成本，有一个新名词被用于形容这项工作——FinOps. 传统的数据中心的成本是比较固定的，所有的成本变动通常都伴随着硬件更替。 而在云上环境就很不一样了，由于云服务的按量收费特性，以及五花八门的计费规则，开发人员稍有不慎，云成本就可能会出现意料之外的变化。另一方面由于计费的复杂性，业务扩容对成本的影响也变得难以预测。 目前的主流云服务商（AWS/GCP/Alicloud/…）基本都提供基于资源标签的成本查询方法，也支持将成本导出并使用 SQL 进行细致分析。 因此其实要做到快速高效的云成本分析与管控，主要就涉及到如下几个点： 契合需求的标签规范: 从公司业务侧需求出发，制定出合理的、多维度的（Department/Team/Product/…）、有扩展空间的标签规范，这样才能按业务侧需要进行成本分析。 资源标签的准确率: 随着公司业务的发展，标签规范的迭代，标签的准确率总是会上下波动。而标签准确率越高，我们对云计算成本的管控能力就越强。 但是也存在许多特殊的云上资源，云服务商目前并未提供良好的成本分析手段，Kubernetes 集群成本就是其中之一。 ","date":"2022-05-04","objectID":"/posts/finops-for-kubernetes/:1:0","series":null,"tags":["云原生","Kubernetes","FinOps","成本分析","Kubecost","MultiCloud","多云","多云财务管控"],"title":"FinOps for Kubernetes - 如何拆分 Kubernetes 成本","uri":"/posts/finops-for-kubernetes/#云计算成本管控"},{"categories":["tech"],"content":" Kubernetes 成本分析的难点目前许多企业应该都面临着这样的场景：所有的服务都运行在一或多个 Kubernetes 集群上，其中包含多条业务线、多个产品、多个业务团队的服务，甚至除了业务服务，可能还包含 CICD、数据分析、机器学习等多种其他工作负载。而这些 Kubernetes 集群通常都由一个独立的 SRE 部门管理。 但是 Kubernetes 集群本身并不提供成本拆分的能力，我们只能查到集群的整体成本、每个节点组的成本等这样粗粒度的成本信息，缺乏细粒度的成本分析能力。 此外，Kubernetes 集群是一个非常动态的运行环境，其节点的数量、节点规格、Pod 所在的节点/Zone/Region，都可能会随着时间动态变动，这为成本分析带来了更大的困难。 这就导致我们很难回答这些问题：每条业务线、每个产品、每个业务团队、或者每个服务分别花了多少钱？是否存在资源浪费？有何优化手段？ 而 FinOps for Kubernetes，就是通过工程化分析、可视化成本分析等手段，来回答这些成本问题，分析与管控 Kubernetes 的成本。 接下来我会先介绍下云上 Kubernetes 成本分析的思路与手段，最后再介绍如何使用 Kubecost 分析 Kubernetes 集群的成本。 要做好 Kubernetes 成本工作，有如下三个要点： 理解 Kubernetes 成本的构成，搞懂准确分析 Kubernetes 成本有哪些难点 寻找优化 Kubernetes 集群、业务服务的手段 确定 Kubernetes 集群的成本拆分手段，建立能快速高效地分析与管控集群成本的流程 ","date":"2022-05-04","objectID":"/posts/finops-for-kubernetes/:2:0","series":null,"tags":["云原生","Kubernetes","FinOps","成本分析","Kubecost","MultiCloud","多云","多云财务管控"],"title":"FinOps for Kubernetes - 如何拆分 Kubernetes 成本","uri":"/posts/finops-for-kubernetes/#kubernetes-成本分析的难点"},{"categories":["tech"],"content":" Kubernetes 成本的构成以 AWS EKS 为例，它的成本有这些组成部分： AWS EKS 本身有 $0.1 per hour 的固定费用，这个很低 EKS 的所有节点会收对应的 EC2 实例运行费用、EBS 数据卷费用 EKS 中使用的 PV 会带来 EBS 数据卷的费用 跨区流量传输费用 所有节点之间的通讯（主要是服务之间的互相访问），如果跨了可用区，会收跨区流量传输费用 EKS 中的服务访问其他 AWS 服务如 RDS/ElastiCache，如果是跨可用区，会收取跨区流量费用 如果使用了 Istio IngressGateway 或 traefik 等网关层代理 Pod，那这些 Pod 与服务实例之间，有可能会产生跨区流量 NAT 网关费用 EKS 中的容器如果要访问因特网，就需要通过 NAT 网关，产生 NAT 费用 如果 VPC 未配置 endpoints 使访问 AWS 服务（dynamodb/s3 等）时直接走 AWS 内部网络，这些流量会经过 VPC 的 NAT 网关，从而产生 NAT 网关费用 服务如果要对外提供访问，最佳实践是通过 aws-load-balancer-controller 绑定 AWS ALB, 这里会产生 ALB 费用 监控系统成本 Kubernetes 的监控系统是不可或缺的 如果你使用的是 Datadog/NewRelic 等云服务，会造成云服务的成本；如果是自建 Prometheus，会造成 Prometheus 的运行成本，以及 Pull 指标造成的跨区流量成本 总结下，其实就是三部分成本：计算、存储、网络。其中计算与存储成本是相对固定的，而网络成本就比较动态，跟是否跨区、是否通过 NAT 等诸多因素有关。 ","date":"2022-05-04","objectID":"/posts/finops-for-kubernetes/:3:0","series":null,"tags":["云原生","Kubernetes","FinOps","成本分析","Kubecost","MultiCloud","多云","多云财务管控"],"title":"FinOps for Kubernetes - 如何拆分 Kubernetes 成本","uri":"/posts/finops-for-kubernetes/#kubernetes-成本的构成"},{"categories":["tech"],"content":" Kubernetes 资源分配的方式Kubernetes 提供了三种资源分配的方式，即服务质量 QoS，不同的分配方式，成本的计算难度也有区别： Guaranteed resource allocation(保证资源分配): 即将 requests 与 limits 设置为相等，确保预留所有所需资源 最保守的策略，服务性能最可靠，但是成本也最高 这种方式分配的资源，拆分起来是最方便的，因为它的计算成本是静态的 Burstable resource allocation(突发性能): 将 requests 设置得比 limits 低，这样相差的这一部分就是服务的可 Burst 资源量。 最佳实践，选择合适的 requests 与 limits，可达成性能与可靠性之间的平衡 这种资源，它 requests 的计算成本是静态的，Burstable 部分的计算成本是动态的 Best effort resource allocation(尽力而为): 只设置 limits，不设置 requests，让 Pod 可以调度到任何可调度的节点上 下策，这个选项会导致服务的性能无法保证，通常只在开发测试等资源受限的环境使用 这种方式分配的资源，完全依赖监控指标进行成本拆分 ","date":"2022-05-04","objectID":"/posts/finops-for-kubernetes/:4:0","series":null,"tags":["云原生","Kubernetes","FinOps","成本分析","Kubecost","MultiCloud","多云","多云财务管控"],"title":"FinOps for Kubernetes - 如何拆分 Kubernetes 成本","uri":"/posts/finops-for-kubernetes/#kubernetes-资源分配的方式"},{"categories":["tech"],"content":" 最佳实践要做到统一分析、拆分 Kubernetes 与其他云资源的成本，如下是一些最佳实践： 按产品或者业务线来划分名字空间，不允许跨名字空间互相访问。 如果存在多个产品或业务线共用的服务，可以在每个产品的名字空间分别部署一个副本，并把它们当成不同的服务来处理。 这样名字空间就是成本划分的一个维度，我们还可以在名字空间上为每个产品设置资源上限与预警。 按产品或业务线来划分节点组，通过节点组的标签来进行成本划分 这是第二个维度，但是节点组划分得太细，可能会导致资源利用不够充分。 这个方案仅供参考，不一定好用 为 Kubernetes 服务设计与其他云资源一致的成本标签，添加到 Pod 的 label 中，通过 kubecost 等手段，基于 label 进行更细致的成本分析 标签一致的好处是可以统一分析 Kubernetes 与其他云资源的成本 定期（比如每周一） check 云成本变化，定位并解决成本异常 建立自动化的成本异常检测与告警机制（部分云服务有提供类似的服务，也可自建），收到告警即触发成本异常分析任务 始终将资源标签准确率维持在较高数值，准确率低于一定数值即自动告警，触发标签修正任务 将成本上升的压力与成本下降的效益覆盖到开发人员，授权他们跟踪服务的 Kubernetes 利用率与成本，以激励开发人员与 SRE 合作管控服务成本。 成本优化实践： 多种工作负载混合部署，提升资源利用率。但是需要合理规划避免资源竞争 调节集群伸缩组件，在保障 SLA 的前提下提升资源利用率 比如 aws 就可以考虑在一些场景下用 karpenter 来做扩缩容、引入 AWS Node Termination Handler 提升 Spot 实例的 SLA 尽量使用竞价实例，AWS 的竞价实例单价平均优惠超过 50% 合理地购买 Saving Plans 与 Reserved Instances，达成成本节约。 ","date":"2022-05-04","objectID":"/posts/finops-for-kubernetes/:5:0","series":null,"tags":["云原生","Kubernetes","FinOps","成本分析","Kubecost","MultiCloud","多云","多云财务管控"],"title":"FinOps for Kubernetes - 如何拆分 Kubernetes 成本","uri":"/posts/finops-for-kubernetes/#最佳实践"},{"categories":["tech"],"content":" 多云环境上述讨论的绝大部分策略，都适用于多云环境。在这种涉及多个云服务提供商的场景，最重要的一点是：搭建平台无关的成本分析与管控平台。而其核心仍然是文章最前面提到的两点，只需要补充两个字 一致： 一致的资源标签规范: 从公司业务侧需求出发，制定出跨平台一致的标签规范，这样才能统一分析多云成本。 资源标签的准确率: 随着公司业务的发展，标签规范的迭代，标签的准确率总是会上下波动。而标签准确率越高，我们对云计算成本的管控能力就越强。 这样就可以把不同云服务商的数据转换成统一的格式，然后在自有的成本平台上进行统一的分析了。 搭建一个这样的成本分析平台其实并不难，许多大公司都是这么干的，小公司也可以从一个最小的平台开始做起，再慢慢完善功能。 以我现有的经验看，其实主要就包含这么几个部分： 成本数据转换模块：将来自不同云的成本数据，转换成与云服务无关的格式，方便统一处理 折扣模块：处理不同资源的折扣 比如 CDN 在用量高的时候通常会有很高的折扣比例 还有 SavingPlans/CommitmentDiscounts 也需要特殊的处理 标签修整模块 随着标签体系的发展，总会有些标签的变更，不方便直接在资源上执行，就需要在成本计算这里进行修正、增补或者删除 成本拆分模块 有些资源的成本是共用的，就需要结合其他来源的数据进行成本拆分，比如 Kubernetes 集群的成本 成本报表：将最终的数据制作成符合各类人员需求的可视化图表，按需求还可以考虑添加交互式特征 可使用 Grafana/Google DataStudio 等报表工具 此外这样一个跨云的成本管控平台也不一定需要完全自己来做，已经有很多公司看到了这块的前景，做出了现成的方案，可以看看 Gartner 的如下报告： Cloud Management Tooling Reviews and Ratings - Gartner 多云场景下其实要考虑的还有很多，目前多云网络（multicloud networking）、多云财务（multicloud finops）、多云应用管理（multicloud application management）领域的需求越来越强劲，相关产品也越来越多，有需要可以自行研究。 ","date":"2022-05-04","objectID":"/posts/finops-for-kubernetes/:6:0","series":null,"tags":["云原生","Kubernetes","FinOps","成本分析","Kubecost","MultiCloud","多云","多云财务管控"],"title":"FinOps for Kubernetes - 如何拆分 Kubernetes 成本","uri":"/posts/finops-for-kubernetes/#多云环境"},{"categories":["tech"],"content":" Kubernetes 成本分析前面讨论的内容都很「虚」，下面来点更「务实」的：Kubernetes 成本分析实战。 目前据我所知，主要有如下两个相关的开源工具： Kubecost: kubecost 应该是目前最优秀的开源成本分析工具了，self-hosted 是免费的，支持按 deployment/service/label 等多个维度进行成本拆分，而且支持拆分网络成本。收费版提供更丰富的功能以及更长的数据存储时间。 crane: 腾讯开源的一款 Kubernetes 成本优化工具，支持成本报表以及 EHPA 两个功能，才刚开源几个月，目前还比较简陋。 腾讯推出国内首个云原生成本优化开源项目 Crane 腾讯云在国内上线了 crane 的闭源版本「容器服务成本大师」，如果你使用的是腾讯云，可以体验看看（感觉跟 kubecost 很像） 其中 kubecost 是最成熟的一个，我们接下来以 kubecost 为例介绍下如何分析 Kubernetes 成本。 ","date":"2022-05-04","objectID":"/posts/finops-for-kubernetes/:7:0","series":null,"tags":["云原生","Kubernetes","FinOps","成本分析","Kubecost","MultiCloud","多云","多云财务管控"],"title":"FinOps for Kubernetes - 如何拆分 Kubernetes 成本","uri":"/posts/finops-for-kubernetes/#kubernetes-成本分析"},{"categories":["tech"],"content":" 安装 kubecostkubecost 有两种推荐的安装方法： 使用 helm 安装免费版 包含如下组件： frontend 前端 UI 面板 cost-model 核心组件，提供基础的成本拆分能力 postgres 长期存储，仅企业版支持 kubecost-network-costs 一个 daemonset，提供网络指标用于计算网络成本（貌似未开源） cluster-controller 提供集群「大小调整（RightSizing）」以及「定时关闭集群」的能力 只保留 15 天的指标，无 SSO/SAML 登录支持，无 alerts/notification, 不可保存 reportes 报表 每个 kubecost 只可管理一个集群 只安装 Apache License 开源的 cost-model，它仅提供基础的成本拆分功能以及 API，无 UI 面板、长期存储、网络成本拆分、SAML 接入及其他商业功能。 开源的 cost-model 直接使用此配置文件即可部署：https://github.com/kubecost/cost-model/blob/master/kubernetes/exporter/exporter.yaml 而如果要部署带 UI 的商业版，需要首先访问 https://www.kubecost.com/install#show-instructions 获取到 kubecostToken，然后使用 helm 进行部署。 首先下载并编辑 values.yaml 配置文件：https://github.com/kubecost/cost-analyzer-helm-chart/blob/develop/cost-analyzer/values.yaml，示例如下： # kubecost-values.yaml # 通过 http://kubecost.com/install 获取 token，用于跟踪商业授权状态 kubecostToken: \"xxx\" global: # 自动部署 prometheus + nodeExporter，也可以直接对接外部 prometheus prometheus: enabled: true # 如果 enable=false，则使用如下地址连接外部 prometheus fqdn: http://cost-analyzer-prometheus-server.default.svc # 自动部署 grafana，也可对接外部 grafana 面板 grafana: enabled: true # 如果 enable=false，则使用如下地址连接外部 grafana domainName: cost-analyzer-grafana.default.svc scheme: \"http\" # http or https, for the domain name above. proxy: true # If true, the kubecost frontend will route to your grafana through its service endpoint # grafana 子 chart 的配置 ## 更好的选择是单独部署 grafana，不使用 kubecost 的 subchart grafana: image: repository: grafana/grafana # 建议替换成私有镜像仓库地址 tag: 8.3.2 # prometheus 子 chart 的配置 ## 更好的选择是单独部署 prometheus，不使用 kubecost 的 subchart prometheus: server: persistentVolume: enabled: true size: 32Gi # 这个大小得视情况调整，集群较大的话 32Gi 肯定不够 retention: 15d # p8s 指标保留时长 nodeExporter: enabled: true ## If true, node-exporter pods share the host network namespace hostNetwork: true ## If true, node-exporter pods share the host PID namespace hostPID: true ## node-exporter container name name: node-exporter ## node-exporter container image image: repository: quay.io/prometheus/node-exporter # 替换成 quay 仓库避免 docker 仓库拉取限制 tag: v0.18.1 pullPolicy: IfNotPresent ## Monitors ConfigMap changes and POSTs to a URL ## Ref: https://github.com/jimmidyson/configmap-reload ## configmapReload: prometheus: ## If false, the configmap-reload container will not be deployed enabled: true ## configmap-reload container name name: configmap-reload ## configmap-reload container image image: repository: jimmidyson/configmap-reload # 建议替换成私有仓库避免 docker 仓库拉取限制 tag: v0.7.1 persistentVolume: enabled: true size: 32Gi # 同前所述 # storageClass: \"-\" # # 配置 ingress 入口，供外部访问 ingress: enabled: false # className: nginx annotations: # kubernetes.io/ingress.class: nginx # kubernetes.io/tls-acme: \"true\" paths: [\"/\"] # There's no need to route specifically to the pods-- we have an nginx deployed that handles routing pathType: ImplementationSpecific hosts: - cost-analyzer.local nodeSelector: {} # 提升网络安全性的配置 networkPolicy: enabled: false denyEgress: true # create a network policy that denies egress from kubecost sameNamespace: true # Set to true if cost analyser and prometheus are on the same namespace # namespace: kubecost # Namespace where prometheus is installed # 分析网络成本，需要额外部署一个 daemonset networkCosts: enabled: false config: {} # 详见 values.yaml 内容 serviceAccount: create: true annotations: # 如果是 aws 上的集群，可以通过 serviceAccount 授权访问 ec2 pricing API 及 cur 数据 # 也可以直接为服务提供 AccessKeyID/Secret 进行授权 # 与 AWS 的集成会在后面详细介绍 eks.amazonaws.com/role-arn: arn:aws:iam:112233445566:role/KubecostRole # 注意替换这个 role-arn # 如下配置也可通过 Kubecost product UI 调整 # 但是此处的配置优先级更高，如果在这里配置了默认值，容器重启后就会使用此默认值，UI 上的修改将失效 kubecostProductConfigs: {} 然后部署： # 添加 repo helm repo add kubecost https://kubecost.github.io/cost-analyzer/ # 查看版本号 helm search repo kubecost/cost-analyzer -l | head # 下载并解压某个 chart helm pull kubecost/cost-analyzer --untar --version 1.92.0 # 使用自定义 values 配置安装或更新本地的 chart helm upgrade --create-namespace --install kubecost ./cost-analyzer -n kubecost -f kubecost-values.yaml 通过 port-forward 访问： kubectl port-forward --names","date":"2022-05-04","objectID":"/posts/finops-for-kubernetes/:7:1","series":null,"tags":["云原生","Kubernetes","FinOps","成本分析","Kubecost","MultiCloud","多云","多云财务管控"],"title":"FinOps for Kubernetes - 如何拆分 Kubernetes 成本","uri":"/posts/finops-for-kubernetes/#安装-kubecost"},{"categories":["tech"],"content":" kubecost 的成本统计原理 1. CPU/RAM/GPU/Storage 成本分析Kubecost 通过 AWS/GCP 等云服务商 API 动态获取各 region/zone 的上述四项资源的每小时成本：CPU-hour, GPU-hour, Storage Gb-hour 与 RAM Gb-hour，或者通过 json 文件静态配置这几项资源的成本。 OD 按需实例的资源价格通常比较固定，而 AWS Spot 实例的成本波动会比较大，可以通过 SpotCPU/SpotRAM 这两个参数来设置 spot 的默认价格，也可以为 kubecost 提供权限使它动态获取这两项资源的价格。 kubecost 根据每个容器的资源请求 requests 以及资源用量监控进行成本分配，对于未配置 requests 的资源将仅按实际用量监控进行成本分配。 kubecost 的成本统计粒度为 container，而 deployment/service/namespace/label 只是按不同的维度进行成本聚合而已。 2. 网络成本的分析 https://github.com/kubecost/docs/blob/b7e9d25994ce3df6b3936a06023588f2249554e5/network-allocation.md 对提供线上服务的云上 Kubernetes 集群而言，网络成本很可能等于甚至超过计算成本。这里面最贵的，是跨区/跨域传输的流量成本，以及 NAT 网关成本。 使用单个可用区风险比较高，资源池也可能不够用，因此我们通常会使用多个可用区，这就导致跨区流量成本激增。 kubecost 也支持使用 Pod network 监控指标对整个集群的流量成本进行拆分，kubecost 会部署一个绑定 hostNetwork 的 daemonset 来采集需要的网络指标，提供给 prometheus 拉取，再进行进一步的分析。 kubecost 将网络流量分成如下几类： in-zone: 免费流量 in-region: 跨区流量，国外的云服务商基本都会对跨区流量收费 cross-region: 跨域流量 更多的待研究，看 kubecost 官方文档吧。 另外还看到 kubecost 有忽略 s3 流量（因为不收费）的 issue: https://github.com/kubecost/cost-model/issues/517 ","date":"2022-05-04","objectID":"/posts/finops-for-kubernetes/:7:2","series":null,"tags":["云原生","Kubernetes","FinOps","成本分析","Kubecost","MultiCloud","多云","多云财务管控"],"title":"FinOps for Kubernetes - 如何拆分 Kubernetes 成本","uri":"/posts/finops-for-kubernetes/#kubecost-的成本统计原理"},{"categories":["tech"],"content":" kubecost 的成本统计原理 1. CPU/RAM/GPU/Storage 成本分析Kubecost 通过 AWS/GCP 等云服务商 API 动态获取各 region/zone 的上述四项资源的每小时成本：CPU-hour, GPU-hour, Storage Gb-hour 与 RAM Gb-hour，或者通过 json 文件静态配置这几项资源的成本。 OD 按需实例的资源价格通常比较固定，而 AWS Spot 实例的成本波动会比较大，可以通过 SpotCPU/SpotRAM 这两个参数来设置 spot 的默认价格，也可以为 kubecost 提供权限使它动态获取这两项资源的价格。 kubecost 根据每个容器的资源请求 requests 以及资源用量监控进行成本分配，对于未配置 requests 的资源将仅按实际用量监控进行成本分配。 kubecost 的成本统计粒度为 container，而 deployment/service/namespace/label 只是按不同的维度进行成本聚合而已。 2. 网络成本的分析 https://github.com/kubecost/docs/blob/b7e9d25994ce3df6b3936a06023588f2249554e5/network-allocation.md 对提供线上服务的云上 Kubernetes 集群而言，网络成本很可能等于甚至超过计算成本。这里面最贵的，是跨区/跨域传输的流量成本，以及 NAT 网关成本。 使用单个可用区风险比较高，资源池也可能不够用，因此我们通常会使用多个可用区，这就导致跨区流量成本激增。 kubecost 也支持使用 Pod network 监控指标对整个集群的流量成本进行拆分，kubecost 会部署一个绑定 hostNetwork 的 daemonset 来采集需要的网络指标，提供给 prometheus 拉取，再进行进一步的分析。 kubecost 将网络流量分成如下几类： in-zone: 免费流量 in-region: 跨区流量，国外的云服务商基本都会对跨区流量收费 cross-region: 跨域流量 更多的待研究，看 kubecost 官方文档吧。 另外还看到 kubecost 有忽略 s3 流量（因为不收费）的 issue: https://github.com/kubecost/cost-model/issues/517 ","date":"2022-05-04","objectID":"/posts/finops-for-kubernetes/:7:2","series":null,"tags":["云原生","Kubernetes","FinOps","成本分析","Kubecost","MultiCloud","多云","多云财务管控"],"title":"FinOps for Kubernetes - 如何拆分 Kubernetes 成本","uri":"/posts/finops-for-kubernetes/#1-cpuramgpustorage-成本分析"},{"categories":["tech"],"content":" kubecost 的成本统计原理 1. CPU/RAM/GPU/Storage 成本分析Kubecost 通过 AWS/GCP 等云服务商 API 动态获取各 region/zone 的上述四项资源的每小时成本：CPU-hour, GPU-hour, Storage Gb-hour 与 RAM Gb-hour，或者通过 json 文件静态配置这几项资源的成本。 OD 按需实例的资源价格通常比较固定，而 AWS Spot 实例的成本波动会比较大，可以通过 SpotCPU/SpotRAM 这两个参数来设置 spot 的默认价格，也可以为 kubecost 提供权限使它动态获取这两项资源的价格。 kubecost 根据每个容器的资源请求 requests 以及资源用量监控进行成本分配，对于未配置 requests 的资源将仅按实际用量监控进行成本分配。 kubecost 的成本统计粒度为 container，而 deployment/service/namespace/label 只是按不同的维度进行成本聚合而已。 2. 网络成本的分析 https://github.com/kubecost/docs/blob/b7e9d25994ce3df6b3936a06023588f2249554e5/network-allocation.md 对提供线上服务的云上 Kubernetes 集群而言，网络成本很可能等于甚至超过计算成本。这里面最贵的，是跨区/跨域传输的流量成本，以及 NAT 网关成本。 使用单个可用区风险比较高，资源池也可能不够用，因此我们通常会使用多个可用区，这就导致跨区流量成本激增。 kubecost 也支持使用 Pod network 监控指标对整个集群的流量成本进行拆分，kubecost 会部署一个绑定 hostNetwork 的 daemonset 来采集需要的网络指标，提供给 prometheus 拉取，再进行进一步的分析。 kubecost 将网络流量分成如下几类： in-zone: 免费流量 in-region: 跨区流量，国外的云服务商基本都会对跨区流量收费 cross-region: 跨域流量 更多的待研究，看 kubecost 官方文档吧。 另外还看到 kubecost 有忽略 s3 流量（因为不收费）的 issue: https://github.com/kubecost/cost-model/issues/517 ","date":"2022-05-04","objectID":"/posts/finops-for-kubernetes/:7:2","series":null,"tags":["云原生","Kubernetes","FinOps","成本分析","Kubecost","MultiCloud","多云","多云财务管控"],"title":"FinOps for Kubernetes - 如何拆分 Kubernetes 成本","uri":"/posts/finops-for-kubernetes/#2-网络成本的分析"},{"categories":["tech"],"content":" kubecost API https://github.com/kubecost/docs/blob/b7e9d25994ce3df6b3936a06023588f2249554e5/apis.md 成本拆分文档：https://github.com/kubecost/docs/blob/b7e9d25994ce3df6b3936a06023588f2249554e5/cost-allocation.md 成本拆分 API 文档：https://github.com/kubecost/docs/blob/b7e9d25994ce3df6b3936a06023588f2249554e5/allocation.md 查询成本拆分结果的 API 示例： import requests resp = requests.get(\"http://localhost:9090/model/allocation\", params={ \"window\": \"2022-05-05T00:00:00Z,2022-05-06T00:00:00Z\", \"aggregate\": \"namespace,label:app\", # 以这几个纬度进行成本聚合 \"external\": True, # 拆分集群外部的成本（比如 s3/rds/es 等），需要通过其他手段提供外部资源的成本 \"accumulate\": True, # 累加指定 window 的所有成本 \"shareIdle\": False, # 将空闲成本拆分到所有资源上 \"idleByNode\": False, # 基于节点进行空闲资源的统计 \"shareTenancyCosts\": True, # 在集群的多个租户之间共享集群管理成本、节点数据卷成本。这部分成本将被添加到 `sharedCost` 字段中 \"shareNamespaces\": \"kube-system,kubecost,istio-system,monitoring\", # 将这些名字空间的成本设为共享成本 \"shareLabels\": \"\", \"shareCost\": None, \"shareSplit\": \"weighted\", # 共享成本的拆分方法，weight 加权拆分，even 均分 }) resp_json = resp.json() print(resp_json['code']) result = resp_json['data'] print(result[0]) 查询结果中有这几种特殊成本类别： __idle__: 未被占用的空闲资源消耗的成本 __unallocated_: 不含有 aggregate 对应维度的成本，比如按 label:app 进行聚合，不含有 app 这个 label 的 pod 成本就会被分类到此标签 __unmounted__: 未挂载 PV 的成本 此外如果使用 kubecost 可视化面板，可能还会看到一个 other 类别，这是为了方便可视化，把成本太低的一些指标聚合展示了。 ","date":"2022-05-04","objectID":"/posts/finops-for-kubernetes/:7:3","series":null,"tags":["云原生","Kubernetes","FinOps","成本分析","Kubecost","MultiCloud","多云","多云财务管控"],"title":"FinOps for Kubernetes - 如何拆分 Kubernetes 成本","uri":"/posts/finops-for-kubernetes/#kubecost-api"},{"categories":["tech"],"content":" Kubecost 与 AWS 集成 https://github.com/kubecost/docs/blob/b7e9d25994ce3df6b3936a06023588f2249554e5/aws-cloud-integrations.md https://github.com/kubecost/docs/blob/main/aws-node-price-reconcilitation-methodology.md TBD ","date":"2022-05-04","objectID":"/posts/finops-for-kubernetes/:7:4","series":null,"tags":["云原生","Kubernetes","FinOps","成本分析","Kubecost","MultiCloud","多云","多云财务管控"],"title":"FinOps for Kubernetes - 如何拆分 Kubernetes 成本","uri":"/posts/finops-for-kubernetes/#kubecost-与-aws-集成"},{"categories":["tech"],"content":" 参考 kubecost: kubecost 应该是目前最优秀的开源成本分析工具了，self-hosted 是免费的，也提供收费的云上版本，值得研究。 文档：https://github.com/kubecost/docs crane: 腾讯开源的一款 Kubernetes 成本优化工具，支持成本报表以及 EHPA 两个功能，才刚开源几个月，目前还比较简陋。 Calculating Container Costs - FinOps CPU利用率从10%提升至60%：中型企业云原生成本优化实战指南 - 星汉未来(Galaxy-Future) 资源利用率分析和优化建议 - 腾讯云容器服务 ","date":"2022-05-04","objectID":"/posts/finops-for-kubernetes/:8:0","series":null,"tags":["云原生","Kubernetes","FinOps","成本分析","Kubecost","MultiCloud","多云","多云财务管控"],"title":"FinOps for Kubernetes - 如何拆分 Kubernetes 成本","uri":"/posts/finops-for-kubernetes/#参考"},{"categories":["life","tech"],"content":"最近在学区块链技术，跟群友讨论时，一位群友抛出了他的观点：「所以智能合约，本质上依然是一个特殊的协议吧，只是套上了一个看起来高大上的词语而本质依然属于一种通信协议的东西，这么一想如果拆解开来实际上也应该没什么特别的。」 是啊，这样说的话，区块链技术确实挺简单的，没啥新的东西。底层就是各种现代密码学算法跟通讯协议而已，这些都是经过了几十年发展，已经很成熟的技术或者概念了。 但是中本聪把这些旧技术组合到一起，搞了个比特币，没几年就引发了加密货币狂潮，就连 GPU 都因为加密货币的发展价格一路狂飙。 2015 年以太坊往区块链上加了个功能：可以运行任何图灵完备的计算机程序（合约），编译成 EVM 字节码即可丢到以太坊区块链上运行。运行程序这样一件事本身有什么特殊的么？是台计算机都可以跑程序，但是以太坊第一个提出在区块链上跑图灵完备的程序，这导致以太坊成为了目前世界上第二大区块链系统，并且形成了一个庞大的开发者社区，目前网络上有多不胜数的以太坊开发教程及资料。 区块链技术仅仅只是以新的方式，组合使用了一系列成熟的工具而已，但是却引发了世界性的金融变革，甚至以太坊还在这之上研究出了更多的妙用，提出了 Web3.0 的概念。 现有技术的新用法，也完全可以形成一场革命性的变革，甚至这个新用法可以非常简单，关键在于你能否发现这样一种用法，并且意识到它可能存在的价值。 这个是非常非常难的，微软曾经没看懂开源跟云计算的威力，诺基亚曾经觉得安卓就是垃圾不足为惧，很多搞金融的曾经觉得加密货币就是个笑话。 恰如很多现在丢了工作的传统运维，可能以前也是觉得容器跟云计算没啥特别的，影响不到自己吧。 在没被新技术骑脸之前，一般人是很难感知到它对自己的影响的。 所以作为一名普通的技术人，我们也只能时时关注自己核心领域内各项新技术的发展，评估它们的潜力与价值，尽力看清它们的本质。 甚至偶尔也要拓展自己的视野去了解下其他关联领域的变化，这样才能降低自己被时代抛弃的概率。 每个新兴领域都不缺乏时代的弄潮儿，而我们普通人，只能未雨绸缪，尽量打造好自己的技术小船，不致被时代的大潮倾翻。 最后回到正题，我这个周末才刚学了两天区块链，还没搞明白智能合约是个啥，更遑论看清「智能合约的潜在价值」了，但是既然区块链是目前的一个风口，这么多人鼓吹，我觉得是值得花点时间搞清楚它导致是个啥的。 区块链、Web3.0 DAO NFT DeFi 这样的新概念，单单执着眼于别人的鼓吹或者贬低，只会是雾里看花一知半解。只有自己搞懂它，才有机会把握住它的本质。 路途漫漫，诸君共勉。 ","date":"2022-03-28","objectID":"/posts/revolution-and-innovation/:0:0","series":null,"tags":["创新","变革"],"title":"变革与创新","uri":"/posts/revolution-and-innovation/#"},{"categories":["tech"],"content":" 本文基本上是一篇原创文章，但是行文有点生硬，仍然在优化中，不太适合初学者阅读。 《写给开发人员的实用密码学》系列文章目录: 写给开发人员的实用密码学（一）—— 概览 写给开发人员的实用密码学（二）—— 哈希函数 写给开发人员的实用密码学（三）—— MAC 与密钥派生函数 KDF 写给开发人员的实用密码学（四）—— 安全随机数生成器 CSPRNG 写给开发人员的实用密码学（五）—— 密钥交换 DHKE 与完美前向保密 PFS 写给开发人员的实用密码学（六）—— 对称密钥加密算法 写给开发人员的实用密码学（七）—— 非对称密钥加密算法 RSA/ECC 写给开发人员的实用密码学（八）—— 数字证书与 TLS 协议 待续 ","date":"2022-03-14","objectID":"/posts/about-tls-cert/:0:0","series":null,"tags":["Cryptography","密码学","HTTPS","TLS","SSL","OpenSSL","PKI","数字证书","证书","SSH","QUIC","HTTP/3","安全"],"title":"写给开发人员的实用密码学（八）—— 数字证书与 TLS 协议","uri":"/posts/about-tls-cert/#"},{"categories":["tech"],"content":" 更新记录 2021-01-17: 完成 TLS 协议简介、数字证书介绍、数字证书的申请或生成方法、mTLS 介绍、TLS 协议的破解手段 2022-03-13 ~ 2022-03-14: 重新整理补充，改写为《写给开发人员的实用密码学（八）—— 数字证书与 TLS 协议》，整合进我的实用密码学系列文章中 补充 PKI 公钥基础架构及 X509 证书标准介绍 TODO: 补充 TLS 协议的逆向手段 基于 cfssl 详细介绍 PKI 的各项组件 基于 PKI 的应用服务间身份识别技术：SPIFF ID SPIFF ID 是云原生领域的标准，服务网格项目 Istio 就使用了 SPIFF ID 作为安全命名 ","date":"2022-03-14","objectID":"/posts/about-tls-cert/:1:0","series":null,"tags":["Cryptography","密码学","HTTPS","TLS","SSL","OpenSSL","PKI","数字证书","证书","SSH","QUIC","HTTP/3","安全"],"title":"写给开发人员的实用密码学（八）—— 数字证书与 TLS 协议","uri":"/posts/about-tls-cert/#更新记录"},{"categories":["tech"],"content":" 零、前言现代人的日常生活中，HTTPS 协议几乎无处不在，我们每天浏览网页时、用手机刷京东淘宝时、甚至每天秀自己绿色的健康码时，都在使用 HTTPS 协议。 作为一个开发人员，我想你应该多多少少有了解一点 HTTPS 协议。 你可能知道 HTTPS 是一种加密传输协议，能保证数据传输的保密性。 如果你拥有部署 HTTPS 服务的经验，那你或许还懂如何申请权威 HTTPS 证书，并配置在 Nginx 等 Web 程序上。 但是你是否清楚 HTTPS 是由 HTTP + TLS 两种协议组合而成的呢？ 更进一步你是否有抓包了解过 TLS 协议的完整流程？是否清楚它加解密的底层原理？是否清楚 Nginx 的 HTTPS 配置中一堆密码学参数的真正含义？是否知道 TLS 协议有哪些弱点、存在哪些攻击手段、如何防范？ 我们在《写给开发人员的实用密码学》的前七篇文章中已经学习了许多的密码学概念与算法，接下来我们就利用这些知识，深度剖析下 HTTPS 协议中的数字证书以及 TLS 协议。 ","date":"2022-03-14","objectID":"/posts/about-tls-cert/:2:0","series":null,"tags":["Cryptography","密码学","HTTPS","TLS","SSL","OpenSSL","PKI","数字证书","证书","SSH","QUIC","HTTP/3","安全"],"title":"写给开发人员的实用密码学（八）—— 数字证书与 TLS 协议","uri":"/posts/about-tls-cert/#零前言"},{"categories":["tech"],"content":" 一、数字证书与 PKI 公钥基础架构我们在前面已经学习了「对称密码算法」与「非对称密码算法」两个密码学体系，这里做个简单的总结。 对称密码算法（如 AES/ChaCha20）: 计算速度快、安全强度高，但是缺乏安全交换密钥的手段、密钥的保存和管理也很困难。 非对称密码算法（如 RSA/ECC）: 计算速度慢，但是它解决了上述对称密码算法最大的两个缺陷，一是给出了安全的密钥交换算法 DHE/ECDHE，二呢它的公钥是可公开的，这降低了密钥的保存与管理难度。 但是非对称密码算法仍然存在一些问题: 公钥该如何分发？比如 Alice 跟 Bob 交换公钥时，如何确定收到的确实是对方的公钥，也就是说如何确认公钥的真实性、完整性、认证其来源身份？ 前面我们已经学习过，DH/ECDH 密钥交换协议可以防范嗅探攻击（窃听），但是无法抵挡中间人攻击（中继）。 如果 Alice 的私钥泄漏了，她该如何作废自己旧的公钥？ 数字证书与公钥基础架构就是为了解决上述问题而设计的。 首先简单介绍下公钥基础架构（Public Key Infrastructure），它是一组由硬件、软件、参与者、管理政策与流程组成的基础架构，其目的在于创造、管理、分配、使用、存储以及撤销数字证书。 PKI 是一个总称，而并非指单独的某一个规范或标准，因此显然数字证书的规范（X.509）、存储格式（PKCS系列标准、DER、PEM）、TLS 协议等都是 PKI 的一部分。 我们下面从公钥证书开始逐步介绍 PKI 中的各种概念及架构。 ","date":"2022-03-14","objectID":"/posts/about-tls-cert/:3:0","series":null,"tags":["Cryptography","密码学","HTTPS","TLS","SSL","OpenSSL","PKI","数字证书","证书","SSH","QUIC","HTTP/3","安全"],"title":"写给开发人员的实用密码学（八）—— 数字证书与 TLS 协议","uri":"/posts/about-tls-cert/#一数字证书与-pki-公钥基础架构"},{"categories":["tech"],"content":" 1. 公钥证书前面我们介绍了公钥密码系统存在的一个问题是「在分发公钥时，难以确认公钥的真实性、完整性及其来源身份」。 PKI 通过「数字证书」+「证书认证机构」来解决这个问题，下面先简单介绍下「数字证书」。 数字证书指的其实就是公钥证书（也可直接简称为证书）。 在现代网络通讯中通行的公钥证书标准名为 X.509 v3, 由 RFC5280 定义。 X.509 v3 格式被广泛应用在 TLS/SSL 等众多加密通讯协议中，它规定公钥证书应该包含如下内容: 证书 序列号（Serial Number）: 用以识别每一张证书，在作废证书的时候会用到它 版本: 证书的规范版本 公钥（Public Key）: 我们的核心目的就是分发公钥，因此显然要把公钥放在证书里面 公钥指纹: 即公钥的 Hash 值，当前大部分证书都使用 SHA256 计算此指纹 公钥用途（Key Usage + Extended Key Usage）: 记录了此证书可用于哪些用途——数字签名、身份认证等 主体（Subject）: 即姓名、组织、邮箱、地址等证书拥有者的个人信息。 有了这个我们就能确认证书的拥有者了 证书有效期的开始时间、结束时间（Not Before + Not After）: 为了确保安全性，每个证书都会记录一个自身的有效期 证书一旦签发并公开，随着科技的发展、时间的推移，其公钥的安全性会慢慢减弱 因此每个证书都应该包含一个合理的有效期，证书的拥有者应该在有效期截止前更换自身的证书以确保安全性 签发者（Issuer）: 签发此证书的「签发者」信息 其他拓展信息 数字签名（Signature）: 我们还需要对上面整个证书计算一个数字签名，来确保这些数据的真实性、完整性，确保证书未被恶意篡改/伪造 此数字签名由「证书签发者（Issuer）」使用其私钥+证书内容计算得出 数字签名算法（Signature Algorithm）: 证书所使用的签名算法，常用的有 RSA-SHA-256 与 ECDSA-SHA-256 每个证书都有唯一的 ID，这样在私钥泄漏的情况下，我们可以通过公钥基础设施的 OCSP（Online Certificate Status Protocol）协议吊销某个证书。 吊销证书的操作还是比较罕见的，毕竟私钥泄漏并不容易遇到，因此这里就略过不提了，有需要的可以自行搜索。 使用 Firefox 查看网站 https://www.google.com 的证书信息如下： Google 证书内容 ","date":"2022-03-14","objectID":"/posts/about-tls-cert/:3:1","series":null,"tags":["Cryptography","密码学","HTTPS","TLS","SSL","OpenSSL","PKI","数字证书","证书","SSH","QUIC","HTTP/3","安全"],"title":"写给开发人员的实用密码学（八）—— 数字证书与 TLS 协议","uri":"/posts/about-tls-cert/#1-公钥证书"},{"categories":["tech"],"content":" 2. 证书链前面介绍证书内容时，提到了每个证书都包含「签发者（Issuer）」信息，并且还包含「签发者」使用「证书内容」与「签发者私钥」生成的数字签名。 那么在证书交换时，如何验证证书的真实性、完整性及来源身份呢？ 根据「数字签名」算法的原理，显然需要使用「签发者公钥」来验证「被签发证书」中的签名。 仍然辛苦 Alice 与 Bob 来演示下这个流程: 假设现在 Alice 生成了自己的公私钥对，她想将公钥发送给远在千里之外的 Bob，以便与 Bob 进行加密通讯 但是如果 Alice 直接发送公钥给 Bob，Bob 并无法验证其来源是 Alice，也无法验证证书是否被篡改 PKI 引入了一个可信赖的第三者（Trusted third party，TTP）来解决这个问题。 在 Alice 与 Bob 的案例中，就是说还有个第三者 Eve，他使用自己的私钥为自己的公钥证书签了名，生成了一个「自签名证书」，并且已经提前将这个「自签名证书」分发（比如当面交付、物理分发 emmm）给了 Alice 跟 Bob. 现在 Alice 首先使用自己的公钥以及个人信息制作了自己的公钥证书，但是这个证书还缺乏一个 Issuer 属性以及数字签名，我们把它叫做「证书签名请求（Certificate Signing Request, CSR）」 为了实现将证书安全传递给远在千里之外的 Bob，Alice 找到 Eve，将这个 CSR 文件提交给 Eve Eve 验证了 Alice 的身份后，再使用这个 CSR 签发出完整的证书文件（Issuer 就填 Eve，然后 Eve 使用自己的私钥计算出证书的数字签名）交付给 Alice Eve 可是曾经跨越千里之遥，将自己的公钥证书分发给了 Bob，所以在给 Alice 签发证书时，他显然可能会要求 付「签名费」。目前许多证书机构就是靠这个赚钱的，当然也有非盈利的证书机构如 Let’s Encrypt. 现在 Alice 再将经 Eve 签名的证书发送给 Bob Bob 收到证书后，看到 Issuer 是 Eve，于是找出以前 Eve 给他的「自签名证书」，然后使用其中的公钥验证收到的证书 如果验证成功，就说明证书的内容是经过 Eve 认证的。如果 Eve 没老糊涂了，那这个证书应该确实就是 Alice 的。 如果验证失败，那说明这是某个攻击者伪造的证书。 在现实世界中，Eve 这个角色被称作「证书认证机构（Certification Authority, CA）」，全世界只有几十家这样的权威机构，它们都通过了各大软件厂商的严格审核，从而将根证书（CA 证书）直接内置于主流操作系统与浏览器中，也就是说早就提前分发给了因特网世界的几乎所有用户。由于许多操作系统或软件的更新迭代缓慢（2022 年了还有人用 XP 你敢信？），根证书的有效期通常都在十年以上。 但是，如果 CA 机构直接使用自己的私钥处理各种证书签名请求，这将是非常危险的。 因为全世界有海量的 HTTPS 网站，也就是说有海量的证书需求，可一共才几十家 CA 机构。 频繁的动用私钥会产生私钥泄漏的风险，如果这个私钥泄漏了，那将直接影响海量网站的安全性。 PKI 架构使用「数字证书链（也叫做信任链）」的机制来解决这个问题: CA 机构首先生成自己的根证书与私钥，并使用私钥给根证书签名 因为私钥跟证书本身就是一对，因此根证书也被称作「自签名证书」 CA 根证书被直接交付给各大软硬件厂商，内置在主流的操作系统与浏览器中 然后 CA 机构再使用私钥签发一些所谓的「中间证书」，之后就把私钥雪藏了，非必要不会再拿出来使用。 通常离线存储在安全地点 中间层证书的有效期通常会比根证书短一些 部分中间证书会被作为备份使用，平常不会启用。 CA 机构使用这些中间证书的私钥，为用户提交的所有 CSR 请求签名 画个图来表示大概是这么个样子： CA 机构也可能会在经过严格审核后，为其他机构签发中间证书，这样就能赋予其他机构签发证书的权利，而且根证书的安全性不受影响。 如果你访问某个 HTTPS 站点发现浏览器显示小绿锁，那就说明这个证书是由某个权威认证机构签发的，其信息是经过这些机构认证的。 上述这个全球互联网上，由证书认证机构、操作系统与浏览器内置的根证书、TLS 加密认证协议、OCSP 证书吊销协议等等组成的架构，我们可以称它为 Web PKI. Web PKI 通常是可信的，但是并不意味着它们可靠。历史上出现过许多由于安全漏洞（2011 DigiNotar 攻击）或者政府要求，证书认证机构将假证书颁发给黑客或者政府机构的情况。获得假证书的人将可以随意伪造站点，而所有操作系统或浏览器都认为这些假站点是安全的，显示小绿锁。 因为证书认证机构的可靠性问题以及一些其他的原因，部分个人、企业或其他机构（比如金融机构）会生成自己的根证书与中间证书，然后自行签发证书，构建出自己的 PKI 认证架构，我们可以将它称作内部 PKI。 但是这种自己生成的根证书是未内置在操作系统与浏览器中的，为了确保安全性，用户就需要先手动在设备上安装好这个数字证书。 自行签发证书的案例有： 微信、支付宝及各种银行客户端中的数字证书与安全性更高的 USB 硬件证书（U 盾），这种涉及海量资金安全甚至国家安全的场景，显然是不能直接前面提到的几十个权威 CA 机构的。 局域网通信，通常是网络管理员生成一个本地 CA 证书安装到所有局域网设备上，再用它的私钥签发其他证书用于局域网安全通信 典型的例子是各企业的内部通讯网络，比如 Kubernetes 容器集群 现在再拿出前面 https://www.google.com 的证书截图看看，最上方有三个标签页，从左至右依次是「服务器证书」、「中间证书」、「根证书」，可以点进去分别查看这三个证书的各项参数，各位看官可以自行尝试： Google 证书内容 交叉签名按前面的描述，每个权威认证机构都拥有一个正在使用的根证书，使用它签发出几个中间证书后，就会把它离线存储在安全地点，平常仅使用中间证书签发终端实体证书。 这样实际上每个权威认证机构的证书都形成一颗证书树，树的顶端就是根证书。 实际上在 PKI 体系中，一些证书链上的中间证书会被使用多个根证书进行签名——我们称这为交叉签名。 交叉签名的主要目的是提升证书的兼容性——客户端只要安装有其中任何一个根证书，就能正常验证这个中间证书。 从而使中间证书在较老的设备也能顺利通过证书验证。 ","date":"2022-03-14","objectID":"/posts/about-tls-cert/:3:2","series":null,"tags":["Cryptography","密码学","HTTPS","TLS","SSL","OpenSSL","PKI","数字证书","证书","SSH","QUIC","HTTP/3","安全"],"title":"写给开发人员的实用密码学（八）—— 数字证书与 TLS 协议","uri":"/posts/about-tls-cert/#2-证书链"},{"categories":["tech"],"content":" 2. 证书链前面介绍证书内容时，提到了每个证书都包含「签发者（Issuer）」信息，并且还包含「签发者」使用「证书内容」与「签发者私钥」生成的数字签名。 那么在证书交换时，如何验证证书的真实性、完整性及来源身份呢？ 根据「数字签名」算法的原理，显然需要使用「签发者公钥」来验证「被签发证书」中的签名。 仍然辛苦 Alice 与 Bob 来演示下这个流程: 假设现在 Alice 生成了自己的公私钥对，她想将公钥发送给远在千里之外的 Bob，以便与 Bob 进行加密通讯 但是如果 Alice 直接发送公钥给 Bob，Bob 并无法验证其来源是 Alice，也无法验证证书是否被篡改 PKI 引入了一个可信赖的第三者（Trusted third party，TTP）来解决这个问题。 在 Alice 与 Bob 的案例中，就是说还有个第三者 Eve，他使用自己的私钥为自己的公钥证书签了名，生成了一个「自签名证书」，并且已经提前将这个「自签名证书」分发（比如当面交付、物理分发 emmm）给了 Alice 跟 Bob. 现在 Alice 首先使用自己的公钥以及个人信息制作了自己的公钥证书，但是这个证书还缺乏一个 Issuer 属性以及数字签名，我们把它叫做「证书签名请求（Certificate Signing Request, CSR）」 为了实现将证书安全传递给远在千里之外的 Bob，Alice 找到 Eve，将这个 CSR 文件提交给 Eve Eve 验证了 Alice 的身份后，再使用这个 CSR 签发出完整的证书文件（Issuer 就填 Eve，然后 Eve 使用自己的私钥计算出证书的数字签名）交付给 Alice Eve 可是曾经跨越千里之遥，将自己的公钥证书分发给了 Bob，所以在给 Alice 签发证书时，他显然可能会要求 付「签名费」。目前许多证书机构就是靠这个赚钱的，当然也有非盈利的证书机构如 Let’s Encrypt. 现在 Alice 再将经 Eve 签名的证书发送给 Bob Bob 收到证书后，看到 Issuer 是 Eve，于是找出以前 Eve 给他的「自签名证书」，然后使用其中的公钥验证收到的证书 如果验证成功，就说明证书的内容是经过 Eve 认证的。如果 Eve 没老糊涂了，那这个证书应该确实就是 Alice 的。 如果验证失败，那说明这是某个攻击者伪造的证书。 在现实世界中，Eve 这个角色被称作「证书认证机构（Certification Authority, CA）」，全世界只有几十家这样的权威机构，它们都通过了各大软件厂商的严格审核，从而将根证书（CA 证书）直接内置于主流操作系统与浏览器中，也就是说早就提前分发给了因特网世界的几乎所有用户。由于许多操作系统或软件的更新迭代缓慢（2022 年了还有人用 XP 你敢信？），根证书的有效期通常都在十年以上。 但是，如果 CA 机构直接使用自己的私钥处理各种证书签名请求，这将是非常危险的。 因为全世界有海量的 HTTPS 网站，也就是说有海量的证书需求，可一共才几十家 CA 机构。 频繁的动用私钥会产生私钥泄漏的风险，如果这个私钥泄漏了，那将直接影响海量网站的安全性。 PKI 架构使用「数字证书链（也叫做信任链）」的机制来解决这个问题: CA 机构首先生成自己的根证书与私钥，并使用私钥给根证书签名 因为私钥跟证书本身就是一对，因此根证书也被称作「自签名证书」 CA 根证书被直接交付给各大软硬件厂商，内置在主流的操作系统与浏览器中 然后 CA 机构再使用私钥签发一些所谓的「中间证书」，之后就把私钥雪藏了，非必要不会再拿出来使用。 通常离线存储在安全地点 中间层证书的有效期通常会比根证书短一些 部分中间证书会被作为备份使用，平常不会启用。 CA 机构使用这些中间证书的私钥，为用户提交的所有 CSR 请求签名 画个图来表示大概是这么个样子： CA 机构也可能会在经过严格审核后，为其他机构签发中间证书，这样就能赋予其他机构签发证书的权利，而且根证书的安全性不受影响。 如果你访问某个 HTTPS 站点发现浏览器显示小绿锁，那就说明这个证书是由某个权威认证机构签发的，其信息是经过这些机构认证的。 上述这个全球互联网上，由证书认证机构、操作系统与浏览器内置的根证书、TLS 加密认证协议、OCSP 证书吊销协议等等组成的架构，我们可以称它为 Web PKI. Web PKI 通常是可信的，但是并不意味着它们可靠。历史上出现过许多由于安全漏洞（2011 DigiNotar 攻击）或者政府要求，证书认证机构将假证书颁发给黑客或者政府机构的情况。获得假证书的人将可以随意伪造站点，而所有操作系统或浏览器都认为这些假站点是安全的，显示小绿锁。 因为证书认证机构的可靠性问题以及一些其他的原因，部分个人、企业或其他机构（比如金融机构）会生成自己的根证书与中间证书，然后自行签发证书，构建出自己的 PKI 认证架构，我们可以将它称作内部 PKI。 但是这种自己生成的根证书是未内置在操作系统与浏览器中的，为了确保安全性，用户就需要先手动在设备上安装好这个数字证书。 自行签发证书的案例有： 微信、支付宝及各种银行客户端中的数字证书与安全性更高的 USB 硬件证书（U 盾），这种涉及海量资金安全甚至国家安全的场景，显然是不能直接前面提到的几十个权威 CA 机构的。 局域网通信，通常是网络管理员生成一个本地 CA 证书安装到所有局域网设备上，再用它的私钥签发其他证书用于局域网安全通信 典型的例子是各企业的内部通讯网络，比如 Kubernetes 容器集群 现在再拿出前面 https://www.google.com 的证书截图看看，最上方有三个标签页，从左至右依次是「服务器证书」、「中间证书」、「根证书」，可以点进去分别查看这三个证书的各项参数，各位看官可以自行尝试： Google 证书内容 交叉签名按前面的描述，每个权威认证机构都拥有一个正在使用的根证书，使用它签发出几个中间证书后，就会把它离线存储在安全地点，平常仅使用中间证书签发终端实体证书。 这样实际上每个权威认证机构的证书都形成一颗证书树，树的顶端就是根证书。 实际上在 PKI 体系中，一些证书链上的中间证书会被使用多个根证书进行签名——我们称这为交叉签名。 交叉签名的主要目的是提升证书的兼容性——客户端只要安装有其中任何一个根证书，就能正常验证这个中间证书。 从而使中间证书在较老的设备也能顺利通过证书验证。 ","date":"2022-03-14","objectID":"/posts/about-tls-cert/:3:2","series":null,"tags":["Cryptography","密码学","HTTPS","TLS","SSL","OpenSSL","PKI","数字证书","证书","SSH","QUIC","HTTP/3","安全"],"title":"写给开发人员的实用密码学（八）—— 数字证书与 TLS 协议","uri":"/posts/about-tls-cert/#交叉签名"},{"categories":["tech"],"content":" 3. 证书的存储格式与编码标准 证书的格式这一块，是真的五花八门…沉重的历史包袱… X509 只规定了证书应该包含哪些信息，但是未定义证书该如何存储。为了解决证书的描述与编码存储问题，又出现了如下标准： ASN.1 结构：是一种描述证书格式的方法。 它类似 protobuf 数据描述语言、SQL DDL ASN.1 只规定了该如何描述证书，未定义该如何编码。 将 ASN.1 结构编码存储的格式有 DER：一种二进制编码格式 PEM：DER 是二进制格式，不便于复制粘贴，因此出现了 PEM，它是一个文本编码格式（其实就是把 DER 编码后的数据再 Base64 编码下…） 某些场景下，X.509 信息不够丰富，因此又设计了一些信息更丰富（例如可以包含证书 链、秘钥）的证书封装格式，包括 PKCS #7 和 PKCS #12 仍然用 ASN.1 格式描述 基本都是用 DER 编码 下面详细介绍下这些相关的标准与格式。 编码存储格式 DER 与 PEMDER 是由国际电信联盟（ITU）在 ITU-T X.690标准中定义的一种数据编码规则，用于将 ASN.1 结构的信息编码为二进制数据。 直接以 DER 格式存储的证书，大都使用 .cer .crt .der 拓展名，在 Windows 系统比较常见。 而 PEM 格式，即 Privacy-Enhanced Mail，是 openssl 默认使用的证书格式。可用于编码公钥、私钥、公钥证书等多种密码学信息。 PEM 其实就是在 DER 的基础上多做一步——使用 Base64 将 DER 编码出的二进制数据再处理一次，编码成字符串再存储。好处是存储、传输要方便很多，可以直接复制粘贴。 一个 2048 位 RSA 公钥的 PEM 文件内容如下： -----BEGIN PUBLIC KEY----- MIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEAyl6q6BkEcEUi9V1/Q7il bngnh1YzG1tM4Hd6XCZQ35OzDN4my9eXWtjoL8YvLYqlYTJqhTHpuptgjF/lmlhg WIMKNNcuDAbvmWExRyZateVrjO9OtgkyJCuGhaum0TIUC+dbZ9L9xsdK/fU1L5BB nPRSYMloH8uE1CbK/DhFUiKp36aHZFfqLPicY3c6/N+k2kIJCEWBY0SROqpqy2Iz yCIP54JSoOoGz6pdtWhd5cEeicr9e7f/WixEES6fgavqIHzhSJBVctpMgFPjFZ/x JJhQVf23WKb3YQQ/0Uc8O7OTDXoUfuJP9UgqvKNh4hPfJA+a4nxkDYhTPfrLHfKY YwIDAQAB -----END PUBLIC KEY----- PEM 格式的数据通常以 .pem .key .crt .cer 等拓展名存储，直接 cat 一下是不是字符串，就能确认该文件是否是 PEM 格式了。 因为纯文本格式处理起来很方便，大部分场景下证书、公钥、私钥等信息都会被编码成 PEM 格式再进行存储、传输。 openssl 默认使用的输入输出均 PEM 格式。 PKCS#1PKCS#1 是专用于编码 RSA 公私钥的标准，通常被编码为 PEM 格式存储。openssl 生成的 RSA 密钥对默认使用此格式。 这是一个比较陈旧的格式，openssl 之所以默认使用它，主要是为了兼容性。通常建议使用更安全的 PKCS#8 而不是这个。 一个使用 PKCS#1 标准的 2048 位 RSA 公钥文件，内容如下： -----BEGIN PUBLIC KEY----- MIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEAyl6q6BkEcEUi9V1/Q7il bngnh1YzG1tM4Hd6XCZQ35OzDN4my9eXWtjoL8YvLYqlYTJqhTHpuptgjF/lmlhg WIMKNNcuDAbvmWExRyZateVrjO9OtgkyJCuGhaum0TIUC+dbZ9L9xsdK/fU1L5BB nPRSYMloH8uE1CbK/DhFUiKp36aHZFfqLPicY3c6/N+k2kIJCEWBY0SROqpqy2Iz yCIP54JSoOoGz6pdtWhd5cEeicr9e7f/WixEES6fgavqIHzhSJBVctpMgFPjFZ/x JJhQVf23WKb3YQQ/0Uc8O7OTDXoUfuJP9UgqvKNh4hPfJA+a4nxkDYhTPfrLHfKY YwIDAQAB -----END PUBLIC KEY----- PKCS#7 / CMS 头疼… PKCS#7 导致是个啥玩意儿？为什么这么多五花八门的格式… PKCS#7/CMS，是一个多用途的证书描述格式。 它包含一个数据填充规则，这个填充规则常被用在需要数据填充的分组加密、数字签名等算法中。 另外据说 PKCS#7 也可以被用来描述证书，并以 DER/PEM 格式保存，后缀通常使用 .p7b 或者 .p7c, 这个暂时存疑吧，有需要再研究了。 PKCS#8PKCS#8 是一个专门用于编码私钥的标准，可用于编码 DSA/RSA/ECC 私钥。它通常被编码成 PEM 格式存储。 前面介绍了专门用于编码 RSA 的 PKCS#1 标准比较陈旧，而且曾经出过漏洞。因此通常建议使用更安全的 PKCS#8 来取代 PKCS#1. C# Java 等编程语言通常要求使用此格式的私钥，而 Python 的 pyca/cryptography 则支持多种编码格式。 一个非加密 ECC 私钥的 PKCS#8 格式如下： -----BEGIN PRIVATE KEY----- MIGHAgEAMBMGByqGSM49AgEGCCqGSM49AwEHBG0wawIBAQQglQanBRiYVPX7F2Rd 4CqyjEN0K4qfHw4tM/yMIh21wamhRANCAARsxaI4jT1b8zbDlFziuLngPcExbYzz ePAHUmgWL/ZCeqlODF/l/XvimkjaWC2huu1OSWB9EKuG+mKFY2Y5k+vF -----END PRIVATE KEY----- 一个加密 PKCS#8 私钥的 PEM 格式私钥如下： -----BEGIN ENCRYPTED PRIVATE KEY----- Base64 编码内容 -----END ENCRYPTED PRIVATE KEY----- 可使用如下 openssl 命令将 RSA/ECC 私钥转换为 PKCS#8 格式： # RSA openssl pkcs8 -topk8 -inform PEM -in rsa-private-key.pem -outform PEM -nocrypt -out rsa-private-key-pkcs8.pem # ECC 的转换命令与 RSA 完全一致 openssl pkcs8 -topk8 -inform PEM -in ecc-private-key.pem -outform PEM -nocrypt -out ecc-private-key-pkcs8.pem PKCS#12PKCS#12 是一个归档文件格式，用于实现存储多个私钥及相关的 X.509 证书。 因为保存了私钥，为了安全性它通常是加密的，需要使用 passphrase 解密后才能使用。 PKCS#12 的常用拓展名为 .p12 .pfx. PKCS#12 的主要使用场景是安全地保存、传输私钥及相关的 X.509 证书，比如： 微信/支付宝等支付相关的数字证书，通常使用 PKCS#12 格式存储，使用商户号做加密密码，然后编码为 base64 再提供给用户 安卓的 APK 签名证书通常使用 PKCS#12 格式存储，拓展名为 .keystore 或者 .jks. PEM 格式转 PKCS#12（公钥和私钥都放里面）: # openssl pkcs12 -export -in client.crt -inkey client.key -out client.p12 # 按提示输入保护密码 从 PKCS#12 中分别提取出 PEM 格式的公钥与私钥: openssl pkcs12 -in xxx.p12 -out xxx.crt -clcerts -nokeys openssl pkcs12 -in xxx.p12 -out xxx.key -nocerts -nodes ","date":"2022-03-14","objectID":"/posts/about-tls-cert/:3:3","series":null,"tags":["Cryptography","密码学","HTTPS","TLS","SSL","OpenSSL","PKI","数字证书","证书","SSH","QUIC","HTTP/3","安全"],"title":"写给开发人员的实用密码学（八）—— 数字证书与 TLS 协议","uri":"/posts/about-tls-cert/#3-证书的存储格式与编码标准"},{"categories":["tech"],"content":" 3. 证书的存储格式与编码标准 证书的格式这一块，是真的五花八门…沉重的历史包袱… X509 只规定了证书应该包含哪些信息，但是未定义证书该如何存储。为了解决证书的描述与编码存储问题，又出现了如下标准： ASN.1 结构：是一种描述证书格式的方法。 它类似 protobuf 数据描述语言、SQL DDL ASN.1 只规定了该如何描述证书，未定义该如何编码。 将 ASN.1 结构编码存储的格式有 DER：一种二进制编码格式 PEM：DER 是二进制格式，不便于复制粘贴，因此出现了 PEM，它是一个文本编码格式（其实就是把 DER 编码后的数据再 Base64 编码下…） 某些场景下，X.509 信息不够丰富，因此又设计了一些信息更丰富（例如可以包含证书 链、秘钥）的证书封装格式，包括 PKCS #7 和 PKCS #12 仍然用 ASN.1 格式描述 基本都是用 DER 编码 下面详细介绍下这些相关的标准与格式。 编码存储格式 DER 与 PEMDER 是由国际电信联盟（ITU）在 ITU-T X.690标准中定义的一种数据编码规则，用于将 ASN.1 结构的信息编码为二进制数据。 直接以 DER 格式存储的证书，大都使用 .cer .crt .der 拓展名，在 Windows 系统比较常见。 而 PEM 格式，即 Privacy-Enhanced Mail，是 openssl 默认使用的证书格式。可用于编码公钥、私钥、公钥证书等多种密码学信息。 PEM 其实就是在 DER 的基础上多做一步——使用 Base64 将 DER 编码出的二进制数据再处理一次，编码成字符串再存储。好处是存储、传输要方便很多，可以直接复制粘贴。 一个 2048 位 RSA 公钥的 PEM 文件内容如下： -----BEGIN PUBLIC KEY----- MIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEAyl6q6BkEcEUi9V1/Q7il bngnh1YzG1tM4Hd6XCZQ35OzDN4my9eXWtjoL8YvLYqlYTJqhTHpuptgjF/lmlhg WIMKNNcuDAbvmWExRyZateVrjO9OtgkyJCuGhaum0TIUC+dbZ9L9xsdK/fU1L5BB nPRSYMloH8uE1CbK/DhFUiKp36aHZFfqLPicY3c6/N+k2kIJCEWBY0SROqpqy2Iz yCIP54JSoOoGz6pdtWhd5cEeicr9e7f/WixEES6fgavqIHzhSJBVctpMgFPjFZ/x JJhQVf23WKb3YQQ/0Uc8O7OTDXoUfuJP9UgqvKNh4hPfJA+a4nxkDYhTPfrLHfKY YwIDAQAB -----END PUBLIC KEY----- PEM 格式的数据通常以 .pem .key .crt .cer 等拓展名存储，直接 cat 一下是不是字符串，就能确认该文件是否是 PEM 格式了。 因为纯文本格式处理起来很方便，大部分场景下证书、公钥、私钥等信息都会被编码成 PEM 格式再进行存储、传输。 openssl 默认使用的输入输出均 PEM 格式。 PKCS#1PKCS#1 是专用于编码 RSA 公私钥的标准，通常被编码为 PEM 格式存储。openssl 生成的 RSA 密钥对默认使用此格式。 这是一个比较陈旧的格式，openssl 之所以默认使用它，主要是为了兼容性。通常建议使用更安全的 PKCS#8 而不是这个。 一个使用 PKCS#1 标准的 2048 位 RSA 公钥文件，内容如下： -----BEGIN PUBLIC KEY----- MIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEAyl6q6BkEcEUi9V1/Q7il bngnh1YzG1tM4Hd6XCZQ35OzDN4my9eXWtjoL8YvLYqlYTJqhTHpuptgjF/lmlhg WIMKNNcuDAbvmWExRyZateVrjO9OtgkyJCuGhaum0TIUC+dbZ9L9xsdK/fU1L5BB nPRSYMloH8uE1CbK/DhFUiKp36aHZFfqLPicY3c6/N+k2kIJCEWBY0SROqpqy2Iz yCIP54JSoOoGz6pdtWhd5cEeicr9e7f/WixEES6fgavqIHzhSJBVctpMgFPjFZ/x JJhQVf23WKb3YQQ/0Uc8O7OTDXoUfuJP9UgqvKNh4hPfJA+a4nxkDYhTPfrLHfKY YwIDAQAB -----END PUBLIC KEY----- PKCS#7 / CMS 头疼… PKCS#7 导致是个啥玩意儿？为什么这么多五花八门的格式… PKCS#7/CMS，是一个多用途的证书描述格式。 它包含一个数据填充规则，这个填充规则常被用在需要数据填充的分组加密、数字签名等算法中。 另外据说 PKCS#7 也可以被用来描述证书，并以 DER/PEM 格式保存，后缀通常使用 .p7b 或者 .p7c, 这个暂时存疑吧，有需要再研究了。 PKCS#8PKCS#8 是一个专门用于编码私钥的标准，可用于编码 DSA/RSA/ECC 私钥。它通常被编码成 PEM 格式存储。 前面介绍了专门用于编码 RSA 的 PKCS#1 标准比较陈旧，而且曾经出过漏洞。因此通常建议使用更安全的 PKCS#8 来取代 PKCS#1. C# Java 等编程语言通常要求使用此格式的私钥，而 Python 的 pyca/cryptography 则支持多种编码格式。 一个非加密 ECC 私钥的 PKCS#8 格式如下： -----BEGIN PRIVATE KEY----- MIGHAgEAMBMGByqGSM49AgEGCCqGSM49AwEHBG0wawIBAQQglQanBRiYVPX7F2Rd 4CqyjEN0K4qfHw4tM/yMIh21wamhRANCAARsxaI4jT1b8zbDlFziuLngPcExbYzz ePAHUmgWL/ZCeqlODF/l/XvimkjaWC2huu1OSWB9EKuG+mKFY2Y5k+vF -----END PRIVATE KEY----- 一个加密 PKCS#8 私钥的 PEM 格式私钥如下： -----BEGIN ENCRYPTED PRIVATE KEY----- Base64 编码内容 -----END ENCRYPTED PRIVATE KEY----- 可使用如下 openssl 命令将 RSA/ECC 私钥转换为 PKCS#8 格式： # RSA openssl pkcs8 -topk8 -inform PEM -in rsa-private-key.pem -outform PEM -nocrypt -out rsa-private-key-pkcs8.pem # ECC 的转换命令与 RSA 完全一致 openssl pkcs8 -topk8 -inform PEM -in ecc-private-key.pem -outform PEM -nocrypt -out ecc-private-key-pkcs8.pem PKCS#12PKCS#12 是一个归档文件格式，用于实现存储多个私钥及相关的 X.509 证书。 因为保存了私钥，为了安全性它通常是加密的，需要使用 passphrase 解密后才能使用。 PKCS#12 的常用拓展名为 .p12 .pfx. PKCS#12 的主要使用场景是安全地保存、传输私钥及相关的 X.509 证书，比如： 微信/支付宝等支付相关的数字证书，通常使用 PKCS#12 格式存储，使用商户号做加密密码，然后编码为 base64 再提供给用户 安卓的 APK 签名证书通常使用 PKCS#12 格式存储，拓展名为 .keystore 或者 .jks. PEM 格式转 PKCS#12（公钥和私钥都放里面）: # openssl pkcs12 -export -in client.crt -inkey client.key -out client.p12 # 按提示输入保护密码 从 PKCS#12 中分别提取出 PEM 格式的公钥与私钥: openssl pkcs12 -in xxx.p12 -out xxx.crt -clcerts -nokeys openssl pkcs12 -in xxx.p12 -out xxx.key -nocerts -nodes ","date":"2022-03-14","objectID":"/posts/about-tls-cert/:3:3","series":null,"tags":["Cryptography","密码学","HTTPS","TLS","SSL","OpenSSL","PKI","数字证书","证书","SSH","QUIC","HTTP/3","安全"],"title":"写给开发人员的实用密码学（八）—— 数字证书与 TLS 协议","uri":"/posts/about-tls-cert/#编码存储格式-der-与-pem"},{"categories":["tech"],"content":" 3. 证书的存储格式与编码标准 证书的格式这一块，是真的五花八门…沉重的历史包袱… X509 只规定了证书应该包含哪些信息，但是未定义证书该如何存储。为了解决证书的描述与编码存储问题，又出现了如下标准： ASN.1 结构：是一种描述证书格式的方法。 它类似 protobuf 数据描述语言、SQL DDL ASN.1 只规定了该如何描述证书，未定义该如何编码。 将 ASN.1 结构编码存储的格式有 DER：一种二进制编码格式 PEM：DER 是二进制格式，不便于复制粘贴，因此出现了 PEM，它是一个文本编码格式（其实就是把 DER 编码后的数据再 Base64 编码下…） 某些场景下，X.509 信息不够丰富，因此又设计了一些信息更丰富（例如可以包含证书 链、秘钥）的证书封装格式，包括 PKCS #7 和 PKCS #12 仍然用 ASN.1 格式描述 基本都是用 DER 编码 下面详细介绍下这些相关的标准与格式。 编码存储格式 DER 与 PEMDER 是由国际电信联盟（ITU）在 ITU-T X.690标准中定义的一种数据编码规则，用于将 ASN.1 结构的信息编码为二进制数据。 直接以 DER 格式存储的证书，大都使用 .cer .crt .der 拓展名，在 Windows 系统比较常见。 而 PEM 格式，即 Privacy-Enhanced Mail，是 openssl 默认使用的证书格式。可用于编码公钥、私钥、公钥证书等多种密码学信息。 PEM 其实就是在 DER 的基础上多做一步——使用 Base64 将 DER 编码出的二进制数据再处理一次，编码成字符串再存储。好处是存储、传输要方便很多，可以直接复制粘贴。 一个 2048 位 RSA 公钥的 PEM 文件内容如下： -----BEGIN PUBLIC KEY----- MIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEAyl6q6BkEcEUi9V1/Q7il bngnh1YzG1tM4Hd6XCZQ35OzDN4my9eXWtjoL8YvLYqlYTJqhTHpuptgjF/lmlhg WIMKNNcuDAbvmWExRyZateVrjO9OtgkyJCuGhaum0TIUC+dbZ9L9xsdK/fU1L5BB nPRSYMloH8uE1CbK/DhFUiKp36aHZFfqLPicY3c6/N+k2kIJCEWBY0SROqpqy2Iz yCIP54JSoOoGz6pdtWhd5cEeicr9e7f/WixEES6fgavqIHzhSJBVctpMgFPjFZ/x JJhQVf23WKb3YQQ/0Uc8O7OTDXoUfuJP9UgqvKNh4hPfJA+a4nxkDYhTPfrLHfKY YwIDAQAB -----END PUBLIC KEY----- PEM 格式的数据通常以 .pem .key .crt .cer 等拓展名存储，直接 cat 一下是不是字符串，就能确认该文件是否是 PEM 格式了。 因为纯文本格式处理起来很方便，大部分场景下证书、公钥、私钥等信息都会被编码成 PEM 格式再进行存储、传输。 openssl 默认使用的输入输出均 PEM 格式。 PKCS#1PKCS#1 是专用于编码 RSA 公私钥的标准，通常被编码为 PEM 格式存储。openssl 生成的 RSA 密钥对默认使用此格式。 这是一个比较陈旧的格式，openssl 之所以默认使用它，主要是为了兼容性。通常建议使用更安全的 PKCS#8 而不是这个。 一个使用 PKCS#1 标准的 2048 位 RSA 公钥文件，内容如下： -----BEGIN PUBLIC KEY----- MIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEAyl6q6BkEcEUi9V1/Q7il bngnh1YzG1tM4Hd6XCZQ35OzDN4my9eXWtjoL8YvLYqlYTJqhTHpuptgjF/lmlhg WIMKNNcuDAbvmWExRyZateVrjO9OtgkyJCuGhaum0TIUC+dbZ9L9xsdK/fU1L5BB nPRSYMloH8uE1CbK/DhFUiKp36aHZFfqLPicY3c6/N+k2kIJCEWBY0SROqpqy2Iz yCIP54JSoOoGz6pdtWhd5cEeicr9e7f/WixEES6fgavqIHzhSJBVctpMgFPjFZ/x JJhQVf23WKb3YQQ/0Uc8O7OTDXoUfuJP9UgqvKNh4hPfJA+a4nxkDYhTPfrLHfKY YwIDAQAB -----END PUBLIC KEY----- PKCS#7 / CMS 头疼… PKCS#7 导致是个啥玩意儿？为什么这么多五花八门的格式… PKCS#7/CMS，是一个多用途的证书描述格式。 它包含一个数据填充规则，这个填充规则常被用在需要数据填充的分组加密、数字签名等算法中。 另外据说 PKCS#7 也可以被用来描述证书，并以 DER/PEM 格式保存，后缀通常使用 .p7b 或者 .p7c, 这个暂时存疑吧，有需要再研究了。 PKCS#8PKCS#8 是一个专门用于编码私钥的标准，可用于编码 DSA/RSA/ECC 私钥。它通常被编码成 PEM 格式存储。 前面介绍了专门用于编码 RSA 的 PKCS#1 标准比较陈旧，而且曾经出过漏洞。因此通常建议使用更安全的 PKCS#8 来取代 PKCS#1. C# Java 等编程语言通常要求使用此格式的私钥，而 Python 的 pyca/cryptography 则支持多种编码格式。 一个非加密 ECC 私钥的 PKCS#8 格式如下： -----BEGIN PRIVATE KEY----- MIGHAgEAMBMGByqGSM49AgEGCCqGSM49AwEHBG0wawIBAQQglQanBRiYVPX7F2Rd 4CqyjEN0K4qfHw4tM/yMIh21wamhRANCAARsxaI4jT1b8zbDlFziuLngPcExbYzz ePAHUmgWL/ZCeqlODF/l/XvimkjaWC2huu1OSWB9EKuG+mKFY2Y5k+vF -----END PRIVATE KEY----- 一个加密 PKCS#8 私钥的 PEM 格式私钥如下： -----BEGIN ENCRYPTED PRIVATE KEY----- Base64 编码内容 -----END ENCRYPTED PRIVATE KEY----- 可使用如下 openssl 命令将 RSA/ECC 私钥转换为 PKCS#8 格式： # RSA openssl pkcs8 -topk8 -inform PEM -in rsa-private-key.pem -outform PEM -nocrypt -out rsa-private-key-pkcs8.pem # ECC 的转换命令与 RSA 完全一致 openssl pkcs8 -topk8 -inform PEM -in ecc-private-key.pem -outform PEM -nocrypt -out ecc-private-key-pkcs8.pem PKCS#12PKCS#12 是一个归档文件格式，用于实现存储多个私钥及相关的 X.509 证书。 因为保存了私钥，为了安全性它通常是加密的，需要使用 passphrase 解密后才能使用。 PKCS#12 的常用拓展名为 .p12 .pfx. PKCS#12 的主要使用场景是安全地保存、传输私钥及相关的 X.509 证书，比如： 微信/支付宝等支付相关的数字证书，通常使用 PKCS#12 格式存储，使用商户号做加密密码，然后编码为 base64 再提供给用户 安卓的 APK 签名证书通常使用 PKCS#12 格式存储，拓展名为 .keystore 或者 .jks. PEM 格式转 PKCS#12（公钥和私钥都放里面）: # openssl pkcs12 -export -in client.crt -inkey client.key -out client.p12 # 按提示输入保护密码 从 PKCS#12 中分别提取出 PEM 格式的公钥与私钥: openssl pkcs12 -in xxx.p12 -out xxx.crt -clcerts -nokeys openssl pkcs12 -in xxx.p12 -out xxx.key -nocerts -nodes ","date":"2022-03-14","objectID":"/posts/about-tls-cert/:3:3","series":null,"tags":["Cryptography","密码学","HTTPS","TLS","SSL","OpenSSL","PKI","数字证书","证书","SSH","QUIC","HTTP/3","安全"],"title":"写给开发人员的实用密码学（八）—— 数字证书与 TLS 协议","uri":"/posts/about-tls-cert/#pkcs1"},{"categories":["tech"],"content":" 3. 证书的存储格式与编码标准 证书的格式这一块，是真的五花八门…沉重的历史包袱… X509 只规定了证书应该包含哪些信息，但是未定义证书该如何存储。为了解决证书的描述与编码存储问题，又出现了如下标准： ASN.1 结构：是一种描述证书格式的方法。 它类似 protobuf 数据描述语言、SQL DDL ASN.1 只规定了该如何描述证书，未定义该如何编码。 将 ASN.1 结构编码存储的格式有 DER：一种二进制编码格式 PEM：DER 是二进制格式，不便于复制粘贴，因此出现了 PEM，它是一个文本编码格式（其实就是把 DER 编码后的数据再 Base64 编码下…） 某些场景下，X.509 信息不够丰富，因此又设计了一些信息更丰富（例如可以包含证书 链、秘钥）的证书封装格式，包括 PKCS #7 和 PKCS #12 仍然用 ASN.1 格式描述 基本都是用 DER 编码 下面详细介绍下这些相关的标准与格式。 编码存储格式 DER 与 PEMDER 是由国际电信联盟（ITU）在 ITU-T X.690标准中定义的一种数据编码规则，用于将 ASN.1 结构的信息编码为二进制数据。 直接以 DER 格式存储的证书，大都使用 .cer .crt .der 拓展名，在 Windows 系统比较常见。 而 PEM 格式，即 Privacy-Enhanced Mail，是 openssl 默认使用的证书格式。可用于编码公钥、私钥、公钥证书等多种密码学信息。 PEM 其实就是在 DER 的基础上多做一步——使用 Base64 将 DER 编码出的二进制数据再处理一次，编码成字符串再存储。好处是存储、传输要方便很多，可以直接复制粘贴。 一个 2048 位 RSA 公钥的 PEM 文件内容如下： -----BEGIN PUBLIC KEY----- MIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEAyl6q6BkEcEUi9V1/Q7il bngnh1YzG1tM4Hd6XCZQ35OzDN4my9eXWtjoL8YvLYqlYTJqhTHpuptgjF/lmlhg WIMKNNcuDAbvmWExRyZateVrjO9OtgkyJCuGhaum0TIUC+dbZ9L9xsdK/fU1L5BB nPRSYMloH8uE1CbK/DhFUiKp36aHZFfqLPicY3c6/N+k2kIJCEWBY0SROqpqy2Iz yCIP54JSoOoGz6pdtWhd5cEeicr9e7f/WixEES6fgavqIHzhSJBVctpMgFPjFZ/x JJhQVf23WKb3YQQ/0Uc8O7OTDXoUfuJP9UgqvKNh4hPfJA+a4nxkDYhTPfrLHfKY YwIDAQAB -----END PUBLIC KEY----- PEM 格式的数据通常以 .pem .key .crt .cer 等拓展名存储，直接 cat 一下是不是字符串，就能确认该文件是否是 PEM 格式了。 因为纯文本格式处理起来很方便，大部分场景下证书、公钥、私钥等信息都会被编码成 PEM 格式再进行存储、传输。 openssl 默认使用的输入输出均 PEM 格式。 PKCS#1PKCS#1 是专用于编码 RSA 公私钥的标准，通常被编码为 PEM 格式存储。openssl 生成的 RSA 密钥对默认使用此格式。 这是一个比较陈旧的格式，openssl 之所以默认使用它，主要是为了兼容性。通常建议使用更安全的 PKCS#8 而不是这个。 一个使用 PKCS#1 标准的 2048 位 RSA 公钥文件，内容如下： -----BEGIN PUBLIC KEY----- MIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEAyl6q6BkEcEUi9V1/Q7il bngnh1YzG1tM4Hd6XCZQ35OzDN4my9eXWtjoL8YvLYqlYTJqhTHpuptgjF/lmlhg WIMKNNcuDAbvmWExRyZateVrjO9OtgkyJCuGhaum0TIUC+dbZ9L9xsdK/fU1L5BB nPRSYMloH8uE1CbK/DhFUiKp36aHZFfqLPicY3c6/N+k2kIJCEWBY0SROqpqy2Iz yCIP54JSoOoGz6pdtWhd5cEeicr9e7f/WixEES6fgavqIHzhSJBVctpMgFPjFZ/x JJhQVf23WKb3YQQ/0Uc8O7OTDXoUfuJP9UgqvKNh4hPfJA+a4nxkDYhTPfrLHfKY YwIDAQAB -----END PUBLIC KEY----- PKCS#7 / CMS 头疼… PKCS#7 导致是个啥玩意儿？为什么这么多五花八门的格式… PKCS#7/CMS，是一个多用途的证书描述格式。 它包含一个数据填充规则，这个填充规则常被用在需要数据填充的分组加密、数字签名等算法中。 另外据说 PKCS#7 也可以被用来描述证书，并以 DER/PEM 格式保存，后缀通常使用 .p7b 或者 .p7c, 这个暂时存疑吧，有需要再研究了。 PKCS#8PKCS#8 是一个专门用于编码私钥的标准，可用于编码 DSA/RSA/ECC 私钥。它通常被编码成 PEM 格式存储。 前面介绍了专门用于编码 RSA 的 PKCS#1 标准比较陈旧，而且曾经出过漏洞。因此通常建议使用更安全的 PKCS#8 来取代 PKCS#1. C# Java 等编程语言通常要求使用此格式的私钥，而 Python 的 pyca/cryptography 则支持多种编码格式。 一个非加密 ECC 私钥的 PKCS#8 格式如下： -----BEGIN PRIVATE KEY----- MIGHAgEAMBMGByqGSM49AgEGCCqGSM49AwEHBG0wawIBAQQglQanBRiYVPX7F2Rd 4CqyjEN0K4qfHw4tM/yMIh21wamhRANCAARsxaI4jT1b8zbDlFziuLngPcExbYzz ePAHUmgWL/ZCeqlODF/l/XvimkjaWC2huu1OSWB9EKuG+mKFY2Y5k+vF -----END PRIVATE KEY----- 一个加密 PKCS#8 私钥的 PEM 格式私钥如下： -----BEGIN ENCRYPTED PRIVATE KEY----- Base64 编码内容 -----END ENCRYPTED PRIVATE KEY----- 可使用如下 openssl 命令将 RSA/ECC 私钥转换为 PKCS#8 格式： # RSA openssl pkcs8 -topk8 -inform PEM -in rsa-private-key.pem -outform PEM -nocrypt -out rsa-private-key-pkcs8.pem # ECC 的转换命令与 RSA 完全一致 openssl pkcs8 -topk8 -inform PEM -in ecc-private-key.pem -outform PEM -nocrypt -out ecc-private-key-pkcs8.pem PKCS#12PKCS#12 是一个归档文件格式，用于实现存储多个私钥及相关的 X.509 证书。 因为保存了私钥，为了安全性它通常是加密的，需要使用 passphrase 解密后才能使用。 PKCS#12 的常用拓展名为 .p12 .pfx. PKCS#12 的主要使用场景是安全地保存、传输私钥及相关的 X.509 证书，比如： 微信/支付宝等支付相关的数字证书，通常使用 PKCS#12 格式存储，使用商户号做加密密码，然后编码为 base64 再提供给用户 安卓的 APK 签名证书通常使用 PKCS#12 格式存储，拓展名为 .keystore 或者 .jks. PEM 格式转 PKCS#12（公钥和私钥都放里面）: # openssl pkcs12 -export -in client.crt -inkey client.key -out client.p12 # 按提示输入保护密码 从 PKCS#12 中分别提取出 PEM 格式的公钥与私钥: openssl pkcs12 -in xxx.p12 -out xxx.crt -clcerts -nokeys openssl pkcs12 -in xxx.p12 -out xxx.key -nocerts -nodes ","date":"2022-03-14","objectID":"/posts/about-tls-cert/:3:3","series":null,"tags":["Cryptography","密码学","HTTPS","TLS","SSL","OpenSSL","PKI","数字证书","证书","SSH","QUIC","HTTP/3","安全"],"title":"写给开发人员的实用密码学（八）—— 数字证书与 TLS 协议","uri":"/posts/about-tls-cert/#pkcs7--cms"},{"categories":["tech"],"content":" 3. 证书的存储格式与编码标准 证书的格式这一块，是真的五花八门…沉重的历史包袱… X509 只规定了证书应该包含哪些信息，但是未定义证书该如何存储。为了解决证书的描述与编码存储问题，又出现了如下标准： ASN.1 结构：是一种描述证书格式的方法。 它类似 protobuf 数据描述语言、SQL DDL ASN.1 只规定了该如何描述证书，未定义该如何编码。 将 ASN.1 结构编码存储的格式有 DER：一种二进制编码格式 PEM：DER 是二进制格式，不便于复制粘贴，因此出现了 PEM，它是一个文本编码格式（其实就是把 DER 编码后的数据再 Base64 编码下…） 某些场景下，X.509 信息不够丰富，因此又设计了一些信息更丰富（例如可以包含证书 链、秘钥）的证书封装格式，包括 PKCS #7 和 PKCS #12 仍然用 ASN.1 格式描述 基本都是用 DER 编码 下面详细介绍下这些相关的标准与格式。 编码存储格式 DER 与 PEMDER 是由国际电信联盟（ITU）在 ITU-T X.690标准中定义的一种数据编码规则，用于将 ASN.1 结构的信息编码为二进制数据。 直接以 DER 格式存储的证书，大都使用 .cer .crt .der 拓展名，在 Windows 系统比较常见。 而 PEM 格式，即 Privacy-Enhanced Mail，是 openssl 默认使用的证书格式。可用于编码公钥、私钥、公钥证书等多种密码学信息。 PEM 其实就是在 DER 的基础上多做一步——使用 Base64 将 DER 编码出的二进制数据再处理一次，编码成字符串再存储。好处是存储、传输要方便很多，可以直接复制粘贴。 一个 2048 位 RSA 公钥的 PEM 文件内容如下： -----BEGIN PUBLIC KEY----- MIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEAyl6q6BkEcEUi9V1/Q7il bngnh1YzG1tM4Hd6XCZQ35OzDN4my9eXWtjoL8YvLYqlYTJqhTHpuptgjF/lmlhg WIMKNNcuDAbvmWExRyZateVrjO9OtgkyJCuGhaum0TIUC+dbZ9L9xsdK/fU1L5BB nPRSYMloH8uE1CbK/DhFUiKp36aHZFfqLPicY3c6/N+k2kIJCEWBY0SROqpqy2Iz yCIP54JSoOoGz6pdtWhd5cEeicr9e7f/WixEES6fgavqIHzhSJBVctpMgFPjFZ/x JJhQVf23WKb3YQQ/0Uc8O7OTDXoUfuJP9UgqvKNh4hPfJA+a4nxkDYhTPfrLHfKY YwIDAQAB -----END PUBLIC KEY----- PEM 格式的数据通常以 .pem .key .crt .cer 等拓展名存储，直接 cat 一下是不是字符串，就能确认该文件是否是 PEM 格式了。 因为纯文本格式处理起来很方便，大部分场景下证书、公钥、私钥等信息都会被编码成 PEM 格式再进行存储、传输。 openssl 默认使用的输入输出均 PEM 格式。 PKCS#1PKCS#1 是专用于编码 RSA 公私钥的标准，通常被编码为 PEM 格式存储。openssl 生成的 RSA 密钥对默认使用此格式。 这是一个比较陈旧的格式，openssl 之所以默认使用它，主要是为了兼容性。通常建议使用更安全的 PKCS#8 而不是这个。 一个使用 PKCS#1 标准的 2048 位 RSA 公钥文件，内容如下： -----BEGIN PUBLIC KEY----- MIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEAyl6q6BkEcEUi9V1/Q7il bngnh1YzG1tM4Hd6XCZQ35OzDN4my9eXWtjoL8YvLYqlYTJqhTHpuptgjF/lmlhg WIMKNNcuDAbvmWExRyZateVrjO9OtgkyJCuGhaum0TIUC+dbZ9L9xsdK/fU1L5BB nPRSYMloH8uE1CbK/DhFUiKp36aHZFfqLPicY3c6/N+k2kIJCEWBY0SROqpqy2Iz yCIP54JSoOoGz6pdtWhd5cEeicr9e7f/WixEES6fgavqIHzhSJBVctpMgFPjFZ/x JJhQVf23WKb3YQQ/0Uc8O7OTDXoUfuJP9UgqvKNh4hPfJA+a4nxkDYhTPfrLHfKY YwIDAQAB -----END PUBLIC KEY----- PKCS#7 / CMS 头疼… PKCS#7 导致是个啥玩意儿？为什么这么多五花八门的格式… PKCS#7/CMS，是一个多用途的证书描述格式。 它包含一个数据填充规则，这个填充规则常被用在需要数据填充的分组加密、数字签名等算法中。 另外据说 PKCS#7 也可以被用来描述证书，并以 DER/PEM 格式保存，后缀通常使用 .p7b 或者 .p7c, 这个暂时存疑吧，有需要再研究了。 PKCS#8PKCS#8 是一个专门用于编码私钥的标准，可用于编码 DSA/RSA/ECC 私钥。它通常被编码成 PEM 格式存储。 前面介绍了专门用于编码 RSA 的 PKCS#1 标准比较陈旧，而且曾经出过漏洞。因此通常建议使用更安全的 PKCS#8 来取代 PKCS#1. C# Java 等编程语言通常要求使用此格式的私钥，而 Python 的 pyca/cryptography 则支持多种编码格式。 一个非加密 ECC 私钥的 PKCS#8 格式如下： -----BEGIN PRIVATE KEY----- MIGHAgEAMBMGByqGSM49AgEGCCqGSM49AwEHBG0wawIBAQQglQanBRiYVPX7F2Rd 4CqyjEN0K4qfHw4tM/yMIh21wamhRANCAARsxaI4jT1b8zbDlFziuLngPcExbYzz ePAHUmgWL/ZCeqlODF/l/XvimkjaWC2huu1OSWB9EKuG+mKFY2Y5k+vF -----END PRIVATE KEY----- 一个加密 PKCS#8 私钥的 PEM 格式私钥如下： -----BEGIN ENCRYPTED PRIVATE KEY----- Base64 编码内容 -----END ENCRYPTED PRIVATE KEY----- 可使用如下 openssl 命令将 RSA/ECC 私钥转换为 PKCS#8 格式： # RSA openssl pkcs8 -topk8 -inform PEM -in rsa-private-key.pem -outform PEM -nocrypt -out rsa-private-key-pkcs8.pem # ECC 的转换命令与 RSA 完全一致 openssl pkcs8 -topk8 -inform PEM -in ecc-private-key.pem -outform PEM -nocrypt -out ecc-private-key-pkcs8.pem PKCS#12PKCS#12 是一个归档文件格式，用于实现存储多个私钥及相关的 X.509 证书。 因为保存了私钥，为了安全性它通常是加密的，需要使用 passphrase 解密后才能使用。 PKCS#12 的常用拓展名为 .p12 .pfx. PKCS#12 的主要使用场景是安全地保存、传输私钥及相关的 X.509 证书，比如： 微信/支付宝等支付相关的数字证书，通常使用 PKCS#12 格式存储，使用商户号做加密密码，然后编码为 base64 再提供给用户 安卓的 APK 签名证书通常使用 PKCS#12 格式存储，拓展名为 .keystore 或者 .jks. PEM 格式转 PKCS#12（公钥和私钥都放里面）: # openssl pkcs12 -export -in client.crt -inkey client.key -out client.p12 # 按提示输入保护密码 从 PKCS#12 中分别提取出 PEM 格式的公钥与私钥: openssl pkcs12 -in xxx.p12 -out xxx.crt -clcerts -nokeys openssl pkcs12 -in xxx.p12 -out xxx.key -nocerts -nodes ","date":"2022-03-14","objectID":"/posts/about-tls-cert/:3:3","series":null,"tags":["Cryptography","密码学","HTTPS","TLS","SSL","OpenSSL","PKI","数字证书","证书","SSH","QUIC","HTTP/3","安全"],"title":"写给开发人员的实用密码学（八）—— 数字证书与 TLS 协议","uri":"/posts/about-tls-cert/#pkcs8"},{"categories":["tech"],"content":" 3. 证书的存储格式与编码标准 证书的格式这一块，是真的五花八门…沉重的历史包袱… X509 只规定了证书应该包含哪些信息，但是未定义证书该如何存储。为了解决证书的描述与编码存储问题，又出现了如下标准： ASN.1 结构：是一种描述证书格式的方法。 它类似 protobuf 数据描述语言、SQL DDL ASN.1 只规定了该如何描述证书，未定义该如何编码。 将 ASN.1 结构编码存储的格式有 DER：一种二进制编码格式 PEM：DER 是二进制格式，不便于复制粘贴，因此出现了 PEM，它是一个文本编码格式（其实就是把 DER 编码后的数据再 Base64 编码下…） 某些场景下，X.509 信息不够丰富，因此又设计了一些信息更丰富（例如可以包含证书 链、秘钥）的证书封装格式，包括 PKCS #7 和 PKCS #12 仍然用 ASN.1 格式描述 基本都是用 DER 编码 下面详细介绍下这些相关的标准与格式。 编码存储格式 DER 与 PEMDER 是由国际电信联盟（ITU）在 ITU-T X.690标准中定义的一种数据编码规则，用于将 ASN.1 结构的信息编码为二进制数据。 直接以 DER 格式存储的证书，大都使用 .cer .crt .der 拓展名，在 Windows 系统比较常见。 而 PEM 格式，即 Privacy-Enhanced Mail，是 openssl 默认使用的证书格式。可用于编码公钥、私钥、公钥证书等多种密码学信息。 PEM 其实就是在 DER 的基础上多做一步——使用 Base64 将 DER 编码出的二进制数据再处理一次，编码成字符串再存储。好处是存储、传输要方便很多，可以直接复制粘贴。 一个 2048 位 RSA 公钥的 PEM 文件内容如下： -----BEGIN PUBLIC KEY----- MIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEAyl6q6BkEcEUi9V1/Q7il bngnh1YzG1tM4Hd6XCZQ35OzDN4my9eXWtjoL8YvLYqlYTJqhTHpuptgjF/lmlhg WIMKNNcuDAbvmWExRyZateVrjO9OtgkyJCuGhaum0TIUC+dbZ9L9xsdK/fU1L5BB nPRSYMloH8uE1CbK/DhFUiKp36aHZFfqLPicY3c6/N+k2kIJCEWBY0SROqpqy2Iz yCIP54JSoOoGz6pdtWhd5cEeicr9e7f/WixEES6fgavqIHzhSJBVctpMgFPjFZ/x JJhQVf23WKb3YQQ/0Uc8O7OTDXoUfuJP9UgqvKNh4hPfJA+a4nxkDYhTPfrLHfKY YwIDAQAB -----END PUBLIC KEY----- PEM 格式的数据通常以 .pem .key .crt .cer 等拓展名存储，直接 cat 一下是不是字符串，就能确认该文件是否是 PEM 格式了。 因为纯文本格式处理起来很方便，大部分场景下证书、公钥、私钥等信息都会被编码成 PEM 格式再进行存储、传输。 openssl 默认使用的输入输出均 PEM 格式。 PKCS#1PKCS#1 是专用于编码 RSA 公私钥的标准，通常被编码为 PEM 格式存储。openssl 生成的 RSA 密钥对默认使用此格式。 这是一个比较陈旧的格式，openssl 之所以默认使用它，主要是为了兼容性。通常建议使用更安全的 PKCS#8 而不是这个。 一个使用 PKCS#1 标准的 2048 位 RSA 公钥文件，内容如下： -----BEGIN PUBLIC KEY----- MIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEAyl6q6BkEcEUi9V1/Q7il bngnh1YzG1tM4Hd6XCZQ35OzDN4my9eXWtjoL8YvLYqlYTJqhTHpuptgjF/lmlhg WIMKNNcuDAbvmWExRyZateVrjO9OtgkyJCuGhaum0TIUC+dbZ9L9xsdK/fU1L5BB nPRSYMloH8uE1CbK/DhFUiKp36aHZFfqLPicY3c6/N+k2kIJCEWBY0SROqpqy2Iz yCIP54JSoOoGz6pdtWhd5cEeicr9e7f/WixEES6fgavqIHzhSJBVctpMgFPjFZ/x JJhQVf23WKb3YQQ/0Uc8O7OTDXoUfuJP9UgqvKNh4hPfJA+a4nxkDYhTPfrLHfKY YwIDAQAB -----END PUBLIC KEY----- PKCS#7 / CMS 头疼… PKCS#7 导致是个啥玩意儿？为什么这么多五花八门的格式… PKCS#7/CMS，是一个多用途的证书描述格式。 它包含一个数据填充规则，这个填充规则常被用在需要数据填充的分组加密、数字签名等算法中。 另外据说 PKCS#7 也可以被用来描述证书，并以 DER/PEM 格式保存，后缀通常使用 .p7b 或者 .p7c, 这个暂时存疑吧，有需要再研究了。 PKCS#8PKCS#8 是一个专门用于编码私钥的标准，可用于编码 DSA/RSA/ECC 私钥。它通常被编码成 PEM 格式存储。 前面介绍了专门用于编码 RSA 的 PKCS#1 标准比较陈旧，而且曾经出过漏洞。因此通常建议使用更安全的 PKCS#8 来取代 PKCS#1. C# Java 等编程语言通常要求使用此格式的私钥，而 Python 的 pyca/cryptography 则支持多种编码格式。 一个非加密 ECC 私钥的 PKCS#8 格式如下： -----BEGIN PRIVATE KEY----- MIGHAgEAMBMGByqGSM49AgEGCCqGSM49AwEHBG0wawIBAQQglQanBRiYVPX7F2Rd 4CqyjEN0K4qfHw4tM/yMIh21wamhRANCAARsxaI4jT1b8zbDlFziuLngPcExbYzz ePAHUmgWL/ZCeqlODF/l/XvimkjaWC2huu1OSWB9EKuG+mKFY2Y5k+vF -----END PRIVATE KEY----- 一个加密 PKCS#8 私钥的 PEM 格式私钥如下： -----BEGIN ENCRYPTED PRIVATE KEY----- Base64 编码内容 -----END ENCRYPTED PRIVATE KEY----- 可使用如下 openssl 命令将 RSA/ECC 私钥转换为 PKCS#8 格式： # RSA openssl pkcs8 -topk8 -inform PEM -in rsa-private-key.pem -outform PEM -nocrypt -out rsa-private-key-pkcs8.pem # ECC 的转换命令与 RSA 完全一致 openssl pkcs8 -topk8 -inform PEM -in ecc-private-key.pem -outform PEM -nocrypt -out ecc-private-key-pkcs8.pem PKCS#12PKCS#12 是一个归档文件格式，用于实现存储多个私钥及相关的 X.509 证书。 因为保存了私钥，为了安全性它通常是加密的，需要使用 passphrase 解密后才能使用。 PKCS#12 的常用拓展名为 .p12 .pfx. PKCS#12 的主要使用场景是安全地保存、传输私钥及相关的 X.509 证书，比如： 微信/支付宝等支付相关的数字证书，通常使用 PKCS#12 格式存储，使用商户号做加密密码，然后编码为 base64 再提供给用户 安卓的 APK 签名证书通常使用 PKCS#12 格式存储，拓展名为 .keystore 或者 .jks. PEM 格式转 PKCS#12（公钥和私钥都放里面）: # openssl pkcs12 -export -in client.crt -inkey client.key -out client.p12 # 按提示输入保护密码 从 PKCS#12 中分别提取出 PEM 格式的公钥与私钥: openssl pkcs12 -in xxx.p12 -out xxx.crt -clcerts -nokeys openssl pkcs12 -in xxx.p12 -out xxx.key -nocerts -nodes ","date":"2022-03-14","objectID":"/posts/about-tls-cert/:3:3","series":null,"tags":["Cryptography","密码学","HTTPS","TLS","SSL","OpenSSL","PKI","数字证书","证书","SSH","QUIC","HTTP/3","安全"],"title":"写给开发人员的实用密码学（八）—— 数字证书与 TLS 协议","uri":"/posts/about-tls-cert/#pkcs12"},{"categories":["tech"],"content":" 4. 证书支持保护的域名类型TLS 证书支持配置多个域名，并且支持所谓的通配符（泛）域名。 但是通配符域名证书的匹配规则，和 DNS 解析中的匹配规则并不一致！ 根据证书选型和购买 - **阿里云文档 的解释，通配符证书只支持同级匹配，详细说明如下: 一级通配符域名: 可保护该通配符域名（主域名）自身和该域名所有的一级子域名。 例如: 一级通配符域名 *.aliyun.com 可以用于保护 aliyun.com、www.aliyun.com 以及其他所有一级子域名。 但是不能用于保护任何二级子域名，如 xx.aa.aliyun.com 二级或二级以上通配符域名: 只能保护该域名同级的所有通配域名，不支持保护该通配符域名本身。 例如: *.a.aliyun.com 只支持保护它的所有同级域名，不能用于保护三级子域名。 要想保护多个二三级子域，只能在生成 TLS 证书时，添加多个通配符域名。 因此设计域名规则时，要考虑到这点，尽量不要使用层级太深的域名！有些信息可以通过 - 来拼接以减少域名层级，比如阿里云的 oss 域名: 公网: oss-cn-shenzhen.aliyuncs.com 内网: oss-cn-shenzhen-internal.aliyuncs.com 此外也可直接为 IP 地址签发证书，IP 地址可以记录在证书的 SAN 属性中。 在自己生成的证书链中可以为局域网 IP 或局域网域名生成本地签名证书。 此外在因特网中也有一些权威认证机构提供为公网 IP 签发证书的服务，一个例子是 Cloudflare 的 https://1.1.1.1, 使用 Firefox 查看其证书，可以看到是一个由 DigiCert 签发的 ECC 证书，使用了 P-256 曲线： Cloudflare 的 IP 证书 ","date":"2022-03-14","objectID":"/posts/about-tls-cert/:3:4","series":null,"tags":["Cryptography","密码学","HTTPS","TLS","SSL","OpenSSL","PKI","数字证书","证书","SSH","QUIC","HTTP/3","安全"],"title":"写给开发人员的实用密码学（八）—— 数字证书与 TLS 协议","uri":"/posts/about-tls-cert/#4-证书支持保护的域名类型"},{"categories":["tech"],"content":" 5. 生成自己的证书链 OpenSSL 是目前使用最广泛的网络加密算法库，这里以它为例介绍证书的生成。 另外也可以考虑使用 CloudFalre 开源的 PKI 工具 cfssl. 前面介绍了，在局域网通信中通常使用本地证书链来保障通信安全，这通常有如下几个原因。 在内网环境下，管理员将本地 CA 证书安装到所有局域网设备上，因此并无必要向权威 CA 机构申请证书 内网环境使用的可能是非公网域名（xxx.local/xxx.lan/xxx.srv 等），甚至可能直接使用局域网 IP 通信，权威 CA 机构不签发这种类型的证书 本地证书链完全受自己控制，可以自己设置安全强度、证书年限等等，而且不受权威 CA 机构影响。 权威 CA 机构不签发客户端证书，因为客户端不一定有固定的 IP 地址或者域名。客户端证书需要自己签发。 下面介绍下如何使用 OpenSSL 生成一个本地 CA 证书链，并签发用于安全通信的服务端证书，可用于 HTTPS/QUIC 等协议。 1. 生成 RSA 证书链到目前为止 RSA 仍然是应用最广泛的非对称加密方案，几乎所有的根证书都是使用的 2048 位或者 4096 位的 RSA 密钥对。 对于 RSA 算法而言，越长的密钥能提供越高的安全性，当前使用最多的 RSA 密钥长度仍然是 2048 位，但是 2048 位已被一些人认为不够安全了，密码学家更建议使用 3072 位或者 4096 位的密钥。 生成一个 2048 位的 RSA 证书链的流程如下: OpenSSL 的 CSR 配置文件官方文档: https://www.openssl.org/docs/manmaster/man1/openssl-req.html 编写证书签名请求的配置文件 csr.conf: [ req ] prompt = no default_md = sha256 # 在签名算法中使用 SHA-256 计算哈希值 req_extensions = req_ext distinguished_name = dn [ dn ] C = CN # Contountry ST = Guangdong L = Shenzhen O = Xxx OU = Xxx-SRE CN = *.svc.local # 泛域名，这个字段已经被 chrome/apple 弃用了。 [ alt_names ] # 备用名称，chrome/apple 目前只信任这里面的域名。 DNS.1 = *.svc.local # 一级泛域名 DNS.2 = *.aaa.svc.local # 二级泛域名 DNS.3 = *.bbb.svc.local # 二级泛域名 [ req_ext ] subjectAltName = @alt_names [ v3_ext ] subjectAltName=@alt_names # Chrome 要求必须要有 subjectAltName(SAN) authorityKeyIdentifier=keyid,issuer:always basicConstraints=CA:FALSE keyUsage=keyEncipherment,dataEncipherment,digitalSignature extendedKeyUsage=serverAuth,clientAuth 此文件的详细文档: OpenSSL file formats and conventions 生成证书链与服务端证书: # 1. 生成本地 CA 根证书的私钥 openssl genrsa -out ca.key 2048 # 2. 使用私钥签发出 CA 根证书 ## CA 根证书的有效期尽量设长一点，因为不方便更新换代，这里设了 100 年 openssl req -x509 -new -nodes -key ca.key -subj \"/CN=MyLocalRootCA\" -days 36500 -out ca.crt # 3. 生成服务端证书的 RSA 私钥（2048 位） openssl genrsa -out server.key 2048 # 4. 通过第一步编写的配置文件，生成证书签名请求（公钥+申请者信息） openssl req -new -key server.key -out server.csr -config csr.conf # 5. 使用 CA 根证书直接签发服务端证书，这里指定服务端证书的有效期为 3650 天 openssl x509 -req -in server.csr -CA ca.crt -CAkey ca.key \\ -CAcreateserial -out server.crt -days 3650 \\ -extensions v3_ext -extfile csr.conf 简单起见这里没有生成中间证书，直接使用根证书签发了用于安全通信的服务端证书。 2. 生成 ECC 证书链在上一篇文章中我们已经介绍过了，ECC 加密方案是新一代非对称加密算法，是 RSA 的继任者，在安全性相同的情况下，ECC 拥有比 RSA 更快的计算速度、更少的内存以及更短的密钥长度。 对于 ECC 加密方案而言，不同的椭圆曲线生成的密钥对提供了不同程度的安全性。 各个组织（ANSI X9.62、NIST、SECG）命名了多种曲线，可通过如下命名查看 openssl 支持的所有椭圆曲线名称: openssl ecparam -list_curves 目前在 TLS 协议以及 JWT 签名算法中，目前应该最广泛的椭圆曲线仍然是 NIST 系列： P-256: 到目前为止 P-256 应该仍然是应用最为广泛的椭圆曲线 在 openssl 中对应的名称为 prime256v1 P-384 在 openssl 中对应的名称为 secp384r1 P-521 在 openssl 中对应的名称为 secp521r1 生成一个使用 P-384 曲线的 ECC 证书的示例如下: 编写证书签名请求的配置文件 ecc-csr.conf: [ req ] prompt = no default_md = sha256 # 在签名算法中使用 SHA-256 计算哈希值 req_extensions = req_ext distinguished_name = dn [ dn ] C = CN # Contountry ST = Guangdong L = Shenzhen O = Xxx OU = Xxx-SRE CN = *.svc.local # 泛域名，这个字段已经被 chrome/apple 弃用了。 [ alt_names ] # 备用名称，chrome/apple 目前只信任这里面的域名。 DNS.1 = *.svc.local # 一级泛域名 DNS.2 = *.aaa.svc.local # 二级泛域名 DNS.3 = *.bbb.svc.local # 二级泛域名 [ req_ext ] subjectAltName = @alt_names [ v3_ext ] subjectAltName=@alt_names # Chrome 要求必须要有 subjectAltName(SAN) authorityKeyIdentifier=keyid,issuer:always basicConstraints=CA:FALSE keyUsage=keyEncipherment,dataEncipherment,digitalSignature extendedKeyUsage=serverAuth,clientAuth 此文件的详细文档: OpenSSL file formats and conventions 生成证书链与服务端证书: # 1. 生成本地 CA 根证书的私钥，使用 P-384 曲线，密钥长度 384 位 openssl ecparam -genkey -name secp384r1 -out ecc-ca.key # 2. 使用私钥签发出 CA 根证书 ## CA 根证书的有效期尽量设长一点，因为不方便更新换代，这里设了 100 年 openssl req -x509 -new -nodes -key ecc-ca.key -subj \"/CN=MyLocalRootCA\" -days 36500 -out ecc-ca.crt # 3. 生成服务端证书的 EC 私钥，使用 P-384 曲线，密钥长度 384 位 openssl ecparam -genkey -name secp384r1 -out ecc-server.key # 4. 通过第一步编写的配置文件，生成证书签名请求（公钥+申请者信息） openssl req -new -key ecc-server.key -out ecc-server.csr -config ecc-csr.conf # 5. 使用 CA 根证书直接签发 ECC 服务端证书，这里指定服务端证书的有效期为 3650 天 openssl x509 -req -in ecc-server.csr -CA ecc-ca.crt -CAkey ecc-ca.key \\ -CAcreateserial -out ecc-server.crt -days 3650 \\ -extensions v3_ext -extfile ecc-csr.conf 简单起见这里没有生成中间证书，直接使用根证书签发了用于安全通信的服务端证","date":"2022-03-14","objectID":"/posts/about-tls-cert/:3:5","series":null,"tags":["Cryptography","密码学","HTTPS","TLS","SSL","OpenSSL","PKI","数字证书","证书","SSH","QUIC","HTTP/3","安全"],"title":"写给开发人员的实用密码学（八）—— 数字证书与 TLS 协议","uri":"/posts/about-tls-cert/#5-生成自己的证书链"},{"categories":["tech"],"content":" 5. 生成自己的证书链 OpenSSL 是目前使用最广泛的网络加密算法库，这里以它为例介绍证书的生成。 另外也可以考虑使用 CloudFalre 开源的 PKI 工具 cfssl. 前面介绍了，在局域网通信中通常使用本地证书链来保障通信安全，这通常有如下几个原因。 在内网环境下，管理员将本地 CA 证书安装到所有局域网设备上，因此并无必要向权威 CA 机构申请证书 内网环境使用的可能是非公网域名（xxx.local/xxx.lan/xxx.srv 等），甚至可能直接使用局域网 IP 通信，权威 CA 机构不签发这种类型的证书 本地证书链完全受自己控制，可以自己设置安全强度、证书年限等等，而且不受权威 CA 机构影响。 权威 CA 机构不签发客户端证书，因为客户端不一定有固定的 IP 地址或者域名。客户端证书需要自己签发。 下面介绍下如何使用 OpenSSL 生成一个本地 CA 证书链，并签发用于安全通信的服务端证书，可用于 HTTPS/QUIC 等协议。 1. 生成 RSA 证书链到目前为止 RSA 仍然是应用最广泛的非对称加密方案，几乎所有的根证书都是使用的 2048 位或者 4096 位的 RSA 密钥对。 对于 RSA 算法而言，越长的密钥能提供越高的安全性，当前使用最多的 RSA 密钥长度仍然是 2048 位，但是 2048 位已被一些人认为不够安全了，密码学家更建议使用 3072 位或者 4096 位的密钥。 生成一个 2048 位的 RSA 证书链的流程如下: OpenSSL 的 CSR 配置文件官方文档: https://www.openssl.org/docs/manmaster/man1/openssl-req.html 编写证书签名请求的配置文件 csr.conf: [ req ] prompt = no default_md = sha256 # 在签名算法中使用 SHA-256 计算哈希值 req_extensions = req_ext distinguished_name = dn [ dn ] C = CN # Contountry ST = Guangdong L = Shenzhen O = Xxx OU = Xxx-SRE CN = *.svc.local # 泛域名，这个字段已经被 chrome/apple 弃用了。 [ alt_names ] # 备用名称，chrome/apple 目前只信任这里面的域名。 DNS.1 = *.svc.local # 一级泛域名 DNS.2 = *.aaa.svc.local # 二级泛域名 DNS.3 = *.bbb.svc.local # 二级泛域名 [ req_ext ] subjectAltName = @alt_names [ v3_ext ] subjectAltName=@alt_names # Chrome 要求必须要有 subjectAltName(SAN) authorityKeyIdentifier=keyid,issuer:always basicConstraints=CA:FALSE keyUsage=keyEncipherment,dataEncipherment,digitalSignature extendedKeyUsage=serverAuth,clientAuth 此文件的详细文档: OpenSSL file formats and conventions 生成证书链与服务端证书: # 1. 生成本地 CA 根证书的私钥 openssl genrsa -out ca.key 2048 # 2. 使用私钥签发出 CA 根证书 ## CA 根证书的有效期尽量设长一点，因为不方便更新换代，这里设了 100 年 openssl req -x509 -new -nodes -key ca.key -subj \"/CN=MyLocalRootCA\" -days 36500 -out ca.crt # 3. 生成服务端证书的 RSA 私钥（2048 位） openssl genrsa -out server.key 2048 # 4. 通过第一步编写的配置文件，生成证书签名请求（公钥+申请者信息） openssl req -new -key server.key -out server.csr -config csr.conf # 5. 使用 CA 根证书直接签发服务端证书，这里指定服务端证书的有效期为 3650 天 openssl x509 -req -in server.csr -CA ca.crt -CAkey ca.key \\ -CAcreateserial -out server.crt -days 3650 \\ -extensions v3_ext -extfile csr.conf 简单起见这里没有生成中间证书，直接使用根证书签发了用于安全通信的服务端证书。 2. 生成 ECC 证书链在上一篇文章中我们已经介绍过了，ECC 加密方案是新一代非对称加密算法，是 RSA 的继任者，在安全性相同的情况下，ECC 拥有比 RSA 更快的计算速度、更少的内存以及更短的密钥长度。 对于 ECC 加密方案而言，不同的椭圆曲线生成的密钥对提供了不同程度的安全性。 各个组织（ANSI X9.62、NIST、SECG）命名了多种曲线，可通过如下命名查看 openssl 支持的所有椭圆曲线名称: openssl ecparam -list_curves 目前在 TLS 协议以及 JWT 签名算法中，目前应该最广泛的椭圆曲线仍然是 NIST 系列： P-256: 到目前为止 P-256 应该仍然是应用最为广泛的椭圆曲线 在 openssl 中对应的名称为 prime256v1 P-384 在 openssl 中对应的名称为 secp384r1 P-521 在 openssl 中对应的名称为 secp521r1 生成一个使用 P-384 曲线的 ECC 证书的示例如下: 编写证书签名请求的配置文件 ecc-csr.conf: [ req ] prompt = no default_md = sha256 # 在签名算法中使用 SHA-256 计算哈希值 req_extensions = req_ext distinguished_name = dn [ dn ] C = CN # Contountry ST = Guangdong L = Shenzhen O = Xxx OU = Xxx-SRE CN = *.svc.local # 泛域名，这个字段已经被 chrome/apple 弃用了。 [ alt_names ] # 备用名称，chrome/apple 目前只信任这里面的域名。 DNS.1 = *.svc.local # 一级泛域名 DNS.2 = *.aaa.svc.local # 二级泛域名 DNS.3 = *.bbb.svc.local # 二级泛域名 [ req_ext ] subjectAltName = @alt_names [ v3_ext ] subjectAltName=@alt_names # Chrome 要求必须要有 subjectAltName(SAN) authorityKeyIdentifier=keyid,issuer:always basicConstraints=CA:FALSE keyUsage=keyEncipherment,dataEncipherment,digitalSignature extendedKeyUsage=serverAuth,clientAuth 此文件的详细文档: OpenSSL file formats and conventions 生成证书链与服务端证书: # 1. 生成本地 CA 根证书的私钥，使用 P-384 曲线，密钥长度 384 位 openssl ecparam -genkey -name secp384r1 -out ecc-ca.key # 2. 使用私钥签发出 CA 根证书 ## CA 根证书的有效期尽量设长一点，因为不方便更新换代，这里设了 100 年 openssl req -x509 -new -nodes -key ecc-ca.key -subj \"/CN=MyLocalRootCA\" -days 36500 -out ecc-ca.crt # 3. 生成服务端证书的 EC 私钥，使用 P-384 曲线，密钥长度 384 位 openssl ecparam -genkey -name secp384r1 -out ecc-server.key # 4. 通过第一步编写的配置文件，生成证书签名请求（公钥+申请者信息） openssl req -new -key ecc-server.key -out ecc-server.csr -config ecc-csr.conf # 5. 使用 CA 根证书直接签发 ECC 服务端证书，这里指定服务端证书的有效期为 3650 天 openssl x509 -req -in ecc-server.csr -CA ecc-ca.crt -CAkey ecc-ca.key \\ -CAcreateserial -out ecc-server.crt -days 3650 \\ -extensions v3_ext -extfile ecc-csr.conf 简单起见这里没有生成中间证书，直接使用根证书签发了用于安全通信的服务端证","date":"2022-03-14","objectID":"/posts/about-tls-cert/:3:5","series":null,"tags":["Cryptography","密码学","HTTPS","TLS","SSL","OpenSSL","PKI","数字证书","证书","SSH","QUIC","HTTP/3","安全"],"title":"写给开发人员的实用密码学（八）—— 数字证书与 TLS 协议","uri":"/posts/about-tls-cert/#1-生成-rsa-证书链"},{"categories":["tech"],"content":" 5. 生成自己的证书链 OpenSSL 是目前使用最广泛的网络加密算法库，这里以它为例介绍证书的生成。 另外也可以考虑使用 CloudFalre 开源的 PKI 工具 cfssl. 前面介绍了，在局域网通信中通常使用本地证书链来保障通信安全，这通常有如下几个原因。 在内网环境下，管理员将本地 CA 证书安装到所有局域网设备上，因此并无必要向权威 CA 机构申请证书 内网环境使用的可能是非公网域名（xxx.local/xxx.lan/xxx.srv 等），甚至可能直接使用局域网 IP 通信，权威 CA 机构不签发这种类型的证书 本地证书链完全受自己控制，可以自己设置安全强度、证书年限等等，而且不受权威 CA 机构影响。 权威 CA 机构不签发客户端证书，因为客户端不一定有固定的 IP 地址或者域名。客户端证书需要自己签发。 下面介绍下如何使用 OpenSSL 生成一个本地 CA 证书链，并签发用于安全通信的服务端证书，可用于 HTTPS/QUIC 等协议。 1. 生成 RSA 证书链到目前为止 RSA 仍然是应用最广泛的非对称加密方案，几乎所有的根证书都是使用的 2048 位或者 4096 位的 RSA 密钥对。 对于 RSA 算法而言，越长的密钥能提供越高的安全性，当前使用最多的 RSA 密钥长度仍然是 2048 位，但是 2048 位已被一些人认为不够安全了，密码学家更建议使用 3072 位或者 4096 位的密钥。 生成一个 2048 位的 RSA 证书链的流程如下: OpenSSL 的 CSR 配置文件官方文档: https://www.openssl.org/docs/manmaster/man1/openssl-req.html 编写证书签名请求的配置文件 csr.conf: [ req ] prompt = no default_md = sha256 # 在签名算法中使用 SHA-256 计算哈希值 req_extensions = req_ext distinguished_name = dn [ dn ] C = CN # Contountry ST = Guangdong L = Shenzhen O = Xxx OU = Xxx-SRE CN = *.svc.local # 泛域名，这个字段已经被 chrome/apple 弃用了。 [ alt_names ] # 备用名称，chrome/apple 目前只信任这里面的域名。 DNS.1 = *.svc.local # 一级泛域名 DNS.2 = *.aaa.svc.local # 二级泛域名 DNS.3 = *.bbb.svc.local # 二级泛域名 [ req_ext ] subjectAltName = @alt_names [ v3_ext ] subjectAltName=@alt_names # Chrome 要求必须要有 subjectAltName(SAN) authorityKeyIdentifier=keyid,issuer:always basicConstraints=CA:FALSE keyUsage=keyEncipherment,dataEncipherment,digitalSignature extendedKeyUsage=serverAuth,clientAuth 此文件的详细文档: OpenSSL file formats and conventions 生成证书链与服务端证书: # 1. 生成本地 CA 根证书的私钥 openssl genrsa -out ca.key 2048 # 2. 使用私钥签发出 CA 根证书 ## CA 根证书的有效期尽量设长一点，因为不方便更新换代，这里设了 100 年 openssl req -x509 -new -nodes -key ca.key -subj \"/CN=MyLocalRootCA\" -days 36500 -out ca.crt # 3. 生成服务端证书的 RSA 私钥（2048 位） openssl genrsa -out server.key 2048 # 4. 通过第一步编写的配置文件，生成证书签名请求（公钥+申请者信息） openssl req -new -key server.key -out server.csr -config csr.conf # 5. 使用 CA 根证书直接签发服务端证书，这里指定服务端证书的有效期为 3650 天 openssl x509 -req -in server.csr -CA ca.crt -CAkey ca.key \\ -CAcreateserial -out server.crt -days 3650 \\ -extensions v3_ext -extfile csr.conf 简单起见这里没有生成中间证书，直接使用根证书签发了用于安全通信的服务端证书。 2. 生成 ECC 证书链在上一篇文章中我们已经介绍过了，ECC 加密方案是新一代非对称加密算法，是 RSA 的继任者，在安全性相同的情况下，ECC 拥有比 RSA 更快的计算速度、更少的内存以及更短的密钥长度。 对于 ECC 加密方案而言，不同的椭圆曲线生成的密钥对提供了不同程度的安全性。 各个组织（ANSI X9.62、NIST、SECG）命名了多种曲线，可通过如下命名查看 openssl 支持的所有椭圆曲线名称: openssl ecparam -list_curves 目前在 TLS 协议以及 JWT 签名算法中，目前应该最广泛的椭圆曲线仍然是 NIST 系列： P-256: 到目前为止 P-256 应该仍然是应用最为广泛的椭圆曲线 在 openssl 中对应的名称为 prime256v1 P-384 在 openssl 中对应的名称为 secp384r1 P-521 在 openssl 中对应的名称为 secp521r1 生成一个使用 P-384 曲线的 ECC 证书的示例如下: 编写证书签名请求的配置文件 ecc-csr.conf: [ req ] prompt = no default_md = sha256 # 在签名算法中使用 SHA-256 计算哈希值 req_extensions = req_ext distinguished_name = dn [ dn ] C = CN # Contountry ST = Guangdong L = Shenzhen O = Xxx OU = Xxx-SRE CN = *.svc.local # 泛域名，这个字段已经被 chrome/apple 弃用了。 [ alt_names ] # 备用名称，chrome/apple 目前只信任这里面的域名。 DNS.1 = *.svc.local # 一级泛域名 DNS.2 = *.aaa.svc.local # 二级泛域名 DNS.3 = *.bbb.svc.local # 二级泛域名 [ req_ext ] subjectAltName = @alt_names [ v3_ext ] subjectAltName=@alt_names # Chrome 要求必须要有 subjectAltName(SAN) authorityKeyIdentifier=keyid,issuer:always basicConstraints=CA:FALSE keyUsage=keyEncipherment,dataEncipherment,digitalSignature extendedKeyUsage=serverAuth,clientAuth 此文件的详细文档: OpenSSL file formats and conventions 生成证书链与服务端证书: # 1. 生成本地 CA 根证书的私钥，使用 P-384 曲线，密钥长度 384 位 openssl ecparam -genkey -name secp384r1 -out ecc-ca.key # 2. 使用私钥签发出 CA 根证书 ## CA 根证书的有效期尽量设长一点，因为不方便更新换代，这里设了 100 年 openssl req -x509 -new -nodes -key ecc-ca.key -subj \"/CN=MyLocalRootCA\" -days 36500 -out ecc-ca.crt # 3. 生成服务端证书的 EC 私钥，使用 P-384 曲线，密钥长度 384 位 openssl ecparam -genkey -name secp384r1 -out ecc-server.key # 4. 通过第一步编写的配置文件，生成证书签名请求（公钥+申请者信息） openssl req -new -key ecc-server.key -out ecc-server.csr -config ecc-csr.conf # 5. 使用 CA 根证书直接签发 ECC 服务端证书，这里指定服务端证书的有效期为 3650 天 openssl x509 -req -in ecc-server.csr -CA ecc-ca.crt -CAkey ecc-ca.key \\ -CAcreateserial -out ecc-server.crt -days 3650 \\ -extensions v3_ext -extfile ecc-csr.conf 简单起见这里没有生成中间证书，直接使用根证书签发了用于安全通信的服务端证","date":"2022-03-14","objectID":"/posts/about-tls-cert/:3:5","series":null,"tags":["Cryptography","密码学","HTTPS","TLS","SSL","OpenSSL","PKI","数字证书","证书","SSH","QUIC","HTTP/3","安全"],"title":"写给开发人员的实用密码学（八）—— 数字证书与 TLS 协议","uri":"/posts/about-tls-cert/#2-生成-ecc-证书链"},{"categories":["tech"],"content":" 6. 证书的类型按照数字证书的生成方式进行分类，证书有三种类型: 由权威 CA 机构签名的「公网受信任证书」: 这类证书会被浏览器、小程序等第三方应用/服务商信任 申请证书时需要验证你对域名/IP 的所有权，也就使证书无法伪造 如果你的 API 需要提供给第三方应用/服务商/用户访问，那就需要向权威 CA 机构申请此类证书 本地签名证书 - tls_locally_signed_cert: 即由本地 CA 证书签名的 TLS 证书 本地 CA 证书，就是自己使用 openssl 等工具生成的 CA 证书 这类证书的缺点是无法与第三方应用/服务商建立安全的连接 如果客户端是完全可控的（比如是自家的 APP，或者是接入了域控的企业局域网设备），完全可以在所有客户端都安装上自己生成的 CA 证书。这种场景下使用此类证书是安全可靠的，可以不向权威 CA 机构申请证书 自签名证书 - tls_self_signed_cert: 前面介绍了根证书是一个自签名证书，它使用根证书的私钥为根证书签名 这里的「自签名证书」是指直接使用根证书进行网络通讯，缺点是证书的更新迭代会很麻烦，而且安全性低。 总的来说，权威CA机构颁发的「公网受信任证书」，可以被第三方应用信任，但是自己生成的不行。 而越贵的权威证书，安全性与可信度就越高，或者可以保护更多的域名。 在客户端可控的情况下，可以考虑自己生成证书链并签发「本地签名证书」，将本地 CA 证书预先安装在客户端中用于验证。 而「自签名证书」主要是方便，能不用还是尽量不要使用。 ","date":"2022-03-14","objectID":"/posts/about-tls-cert/:3:6","series":null,"tags":["Cryptography","密码学","HTTPS","TLS","SSL","OpenSSL","PKI","数字证书","证书","SSH","QUIC","HTTP/3","安全"],"title":"写给开发人员的实用密码学（八）—— 数字证书与 TLS 协议","uri":"/posts/about-tls-cert/#6-证书的类型"},{"categories":["tech"],"content":" 7. 向权威 CA 机构申请「公网受信任证书」向权威机构申请的公网受信任证书，可以直接应用在边界网关上，用于给公网用户提供 TLS 加密访问服务，比如各种 HTTPS 站点、API。这是需求最广的一类数字证书服务。 而证书的申请与管理方式又分为两种： 通过 ACMEv2（Automated Certificate Management Environment (ACME) 协议进行证书的自动化申请与管理。支持使用此开放协议申请证书的权威机构有： 免费服务 Let’s Encrypt: 众所周知，它提供三个月有效期的免费证书。 ZeroSSL: 貌似也是一个比较有名的 SSL 证书服务 通过 ACME 协议支持不限数量的 90 天证书，也支持多域名证书与泛域名证书。 它相比 Let’s Encrypt 的优势是，它提供一个证书控制台，可以查看与管理用户当前的所有证书，了解其状态。 付费服务 DigiCert: 这个非常有名（但也是相当贵），官方文档 Digicert - Third-party ACME client automation Google Trust Services: Google 推出的公网证书服务，也是三个月有效期，其根证书交叉验证了 GlobalSign。官方文档 Automate Public Certificates Lifecycle Management via RFC 8555 (ACME) Entrust: 官方文档 Entrust’s ACME implementation GlobalSign: 官方文档 GlobalSign ACME Service 相关的自动化工具 很多代理工具都有提供基于 ACMEv2 协议的证书申请与自动更新，比如: Traefik Caddy docker-letsencrypt-nginx-proxy-companion 网上也有一些 certbot 插件，可以通过 DNS 提供商的 API 进行 ACMEv2 证书的申请与自动更新，比如: certbot-dns-aliyun terraform 也有相关 provider: terraform-provider-acme cert-manager: kubernetes 中的证书管理工具，支持 ACMEv2，也支持创建与管理私有证书。 通过一些权威 CA 机构或代理商提供的 Web 网站，手动填写信息来申请与更新证书。 这个流程相对会比较繁琐。 这些权威机构提供的证书服务，提供的证书又有不同的分级，这里详细介绍下三种不同的证书级别，以及该如何选用： Domain Validated（DV）证书 仅验证域名所有权，验证步骤最少，价格最低，仅需要数分钟即可签发。 优点就是易于签发，很适合做自动化。 各云厂商（AWS/GCP/Cloudflare，以及 Vercel/Github 的站点服务）给自家服务提供的免费证书都是 DV 证书，Let’s Encrypt 的证书也是这个类型。 很明显这些证书的签发都非常方便，而且仅验证域名所有权。 但是 AWS/GCP/Cloudflare/Vercel/Github 提供的 DV 证书都仅能在它们的云服务上使用，不提供私钥功能！ Organization Validated (OV) 证书 是企业 SSL 证书的首选，通过企业认证确保企业 SSL 证书的真实性。 除域名所有权外，CA 机构还会审核组织及企业的真实性，包括注册状况、联系方式、恶意软件等内容。 如果要做合规化，可能至少也得用 OV 这个级别的证书。 Extended Validation（EV）证书 最严格的认证方式，CA 机构会深度审核组织及企业各方面的信息。 被认为适合用于大型企业、金融机构等组织或企业。 而且仅支持签发单域名、多域名证书，不支持签发泛域名证书，安全性杠杠的。 完整的证书申请流程如下: 证书申请流程 为了方便用户，图中的申请人（Applicant）自行处理的部分，目前很多证书申请网站也可以自动处理，用户只需要提供相关信息即可。 ","date":"2022-03-14","objectID":"/posts/about-tls-cert/:3:7","series":null,"tags":["Cryptography","密码学","HTTPS","TLS","SSL","OpenSSL","PKI","数字证书","证书","SSH","QUIC","HTTP/3","安全"],"title":"写给开发人员的实用密码学（八）—— 数字证书与 TLS 协议","uri":"/posts/about-tls-cert/#7-向权威-ca-机构申请公网受信任证书"},{"categories":["tech"],"content":" 8. 证书的寿命对于公开服务，服务端证书的有效期不要超过 825 天（27 个月）！ 另外从 2020 年 11 月起，新申请的服务端证书有效期已经缩短到了 398 天（13 个月）。 目前 Apple/Mozilla/Chrome 都发表了相应声明，证书有效期超过上述限制的，将被浏览器/Apple设备禁止使用。 而对于其他用途的证书，如果更换起来很麻烦，可以考虑放宽条件。 比如 kubernetes 集群的加密证书，可以考虑有效期设长一些，比如 10 年。 据云原生安全破局｜如何管理周期越来越短的数字证书？所述，大量知名企业如特斯拉/微软/领英/爱立信都曾因未及时更换 TLS 证书导致服务暂时不可用。 因此 TLS 证书最好是设置自动轮转！人工维护不可靠！ 目前很多 Web 服务器/代理，都支持自动轮转 Let’s Encrypt 证书。 另外 Vault 等安全工具，也支持自动轮转私有证书。 ","date":"2022-03-14","objectID":"/posts/about-tls-cert/:3:8","series":null,"tags":["Cryptography","密码学","HTTPS","TLS","SSL","OpenSSL","PKI","数字证书","证书","SSH","QUIC","HTTP/3","安全"],"title":"写给开发人员的实用密码学（八）—— 数字证书与 TLS 协议","uri":"/posts/about-tls-cert/#8-证书的寿命"},{"categories":["tech"],"content":" 9. 使用 OpenSSL 验证证书、查看证书信息 # 查看证书(crt)信息 openssl x509 -noout -text -in server.crt # 查看证书请求(csr)信息 openssl req -noout -text -in server.csr # 查看 RSA 私钥(key)信息 openssl rsa -noout -text -in server.key # 验证证书是否可信 ## 1. 使用系统的证书链进行验证 openssl verify server.crt ## 2. 使用指定的 CA 证书进行验证 openssl verify -CAfile ca.crt server.crt ","date":"2022-03-14","objectID":"/posts/about-tls-cert/:3:9","series":null,"tags":["Cryptography","密码学","HTTPS","TLS","SSL","OpenSSL","PKI","数字证书","证书","SSH","QUIC","HTTP/3","安全"],"title":"写给开发人员的实用密码学（八）—— 数字证书与 TLS 协议","uri":"/posts/about-tls-cert/#9-使用-openssl-验证证书查看证书信息"},{"categories":["tech"],"content":" 二、TLS 协议TLS 协议，中文名为「传输层安全协议」，是一个安全通信协议，被用于在网络上进行安全通信。 TLS 协议通常与 HTTP / FTP / SMTP 等协议一起使用以实现加密通讯，这种组合协议通常被缩写为 HTTPS / SFTP / SMTPS. 在讲 TLS 协议前，还是先复习下「对称密码算法」与「非对称密码算法」两个密码体系的特点。 对称密码算法（如 AES/ChaCha20）: 计算速度快、安全强度高，但是缺乏安全交换密钥的手段、密钥的保存和管理也很困难 非对称密码算法（如 RSA/ECC）: 解决了上述对称密码算法的两个缺陷——通过数字证书 + PKI 公钥基础架构实现了身份认证，再通过 DHE/ECDHE 实现了安全的对称密钥交换。 但是非对称密码算法要比对称密码算法更复杂，计算速度也慢得多。 因此实际使用上通常结合使用这两种密码算法，各取其长，以实现高速且安全的网络通讯。 我们通常称结合使用对称密码算法以及非对称密码算法的加密方案为「混合加密方案」。 TLS 协议就是一个「混合加密方案」，它借助数字证书与 PKI 公钥基础架构、DHE/ECDHE 密钥交换协议以及对称加密方案这三者，实现了安全的加密通讯。 基于经典 DHKE 协议的 TLS 握手流程如下： 基于经典 DHKE 协议的 TLS 握手 而在支持「完美前向保密（Perfect Forward Secrecy）」的 TLS1.2 或 TLS1.3 协议中，经典 DH 协议被 ECDHE 协议取代。 变化之一是进行最初的握手协议从经典 DHKE 换成了基于 ECC 的 ECDH 协议， 变化之二是在每次通讯过程中也在不断地进行密钥交换，生成新的对称密钥供下次通讯使用，其细节参见 写给开发人员的实用密码学（五）—— 密钥交换 DHKE 与完美前向保密 PFS。 TLS 协议通过应用 ECDHE 密钥交换协议，提供了「完美前向保密（Perfect Forward Secrecy）」特性，也就是说它能够保护过去进行的通讯不受密钥在未来暴露的威胁。 即使攻击者破解出了一个「对称密钥」，也只能获取到一次事务中的数据，其他事务的数据安全性完全不受影响。 另外注意一点是，CA 证书和服务端证书都只在 TLS 协议握手的前三个步骤中有用到，之后的通信就与它们无关了。 ","date":"2022-03-14","objectID":"/posts/about-tls-cert/:4:0","series":null,"tags":["Cryptography","密码学","HTTPS","TLS","SSL","OpenSSL","PKI","数字证书","证书","SSH","QUIC","HTTP/3","安全"],"title":"写给开发人员的实用密码学（八）—— 数字证书与 TLS 协议","uri":"/posts/about-tls-cert/#二tls-协议"},{"categories":["tech"],"content":" 1. 密码套件与 TLS 历史版本密码套件（Cipher_suite）是 TLS 协议中一组用于实现安全通讯的密码学算法，类似于我们前面学习过的加密方案。 不同密码学算法的组合形成不同的密码套件，算法组合的差异使这些密码套件具有不同的性能与安全性，另外 TLS 协议的更新迭代也导致各密码套件拥有不同的兼容性。 通常越新推出的密码套件的安全性越高，但是兼容性就越差（旧设备不支持）。 密码套件的名称由它使用的各种密码学算法名称组成，而且有固定的格式，以 TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256 为例介绍下： TLS: 定义了此套件适用的协议，通常固定为 TLS ECDHE: 密钥交换算法 RSA: 数字证书认证算法 AES_128_GCM: 使用的对称加密方案，这是一个基于 AES 与 GCM 模式的对称认证加密方案，使用 128 位密钥 SHA256: 哈希函数，用于 HMAC 算法实现消息认证 TLS 固定使用 HMAC 算法进行消息认证 TLS 协议的前身是 SSL 协议，TLS/SSL 的发展历程展示如下： SSL/TLS 的历史版本 SSL 协议早在 2015 年就被各大主流浏览器废除了，TLS1.0 感觉也基本没站点在用了，这俩就直接跳过了。 下面分别介绍下 TLS1.1 TLS1.2 与 TLS1.3. TLS 1.1TLS 1.1 在 RFC4346 中定义，于 2006 年 4 月发布。 TLS 1.1 是 TLS 1.0 的一个补丁，主要更新包括： 添加对CBC攻击的保护 隐式初始向量 IV 被替换成一个显式的 IV 修复分组密码模式中填充算法的 bug 支持 IANA 登记的参数 TLS 1.1及其之前的算法曾经被广泛应用，它目前已知的缺陷如下： 不支持 PFS 完全前向保密 不支持 AEAD 认证加密算法 为了兼容性，保留了很多不安全的算法 TLS 1.1 已经不够安全了，不过一些陈年老站点或许还在使用它。 TLS 1.2TLS 1.2 在 RFC5246 中定义，于 2008 年 8 月发发布。 可选支持 PFS 完全前向保密 移除对 MD5 与 SHA-1 签名算法的支持 添加对 HMAC-SHA-256 及 HMAC-SHA-384 消息认证算法的支持 添加对 AEAD 加密认证方案的支持 去除 forback 回到 SSL 协议的能力，提升安全性 为了兼容性，保留了很多不安全的算法 如果你使用 TLS 1.2，需要小心地选择密码套件，避开不安全的套件，就能实现足够高的安全性。 TLS 1.3TLS 1.3 做了一次大刀阔斧的更新，是一个里程碑式的版本，其更新总结如下： 移除对如下算法的支持 哈希函数 SHA1/MD5 所有非 AEAD 加密认证的密码方案（CBC 模式） 移除对 RC4 与 3DES 加密算法的支持 移除了静态 RSA 与 DH 密钥交换算法 支持高性能的 Ed25519/Ed448 签名认证算法、X25519 密钥协商算法 支持高性能的 ChaCha20-Poly1305 对称认证加密方案 将密钥交换算法与公钥认证算法从密码套件中分离出来 比如原来的 TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256 密码套件将被拆分为 ECDHE 算法、RSA 身份认证算法、以及 TLS_AES_128_GCM_SHA256 密码套件 这样密码套件就只包含一个 AEAD 认证加密方案，以及一个哈希函数了 仅支持前向安全的密钥交换算法 DHE 或 ECDHE 支持最短 0-RTT 的 TLS 握手（会话恢复） TLS 1.3 从协议中删除了所有不安全的算法或协议，可以说只要你的通讯用了 TLS 1.3，那你的数据就安全了（当然前提是你的私钥没泄漏）。 如何设置 TLS 协议的版本、密码套件参数我们前面已经学习了对称加密、非对称加密、密钥交换三部分知识，对照 TLS 套件的名称，应该能很容易判断出哪些是安全的、哪些不够安全，哪些支持前向保密、哪些不支持。 一个非常好用的「站点 HTTPS 安全检测」网站是 https://myssl.com/，使用它测试知乎网的检测结果如下： SSL/TLS 的历史版本 能看到知乎为了兼容性，目前仍然支持 TLS1.0 与 TLS1.1，另外目前还不支持 TLS1.3. 此外，知乎仍然支持很多已经不安全的加密套件，myssl.com 专门使用黄色标识出了这些不安全的加密套件，我们总结下主要特征： 部分密码套件使用了不安全的对称加密算法 3DES 其他被标识为黄色的套件虽然使用了安全的对称加密算法，但是不支持 PFS 前向保密 此外 myssl.com 还列出了许多站点更详细的信息，包括 TLS1.3 的会话恢复，以及后面将会介绍的公钥固定、HTTP严格传输安全等信息： SSL/TLS 的历史版本 Nginx 的 TLS 协议配置以前为 Nginx 等程序配置 HTTPS 协议时，我最头疼的就是其中密码套件参数 ssl_ciphers，为了安全性，需要配置超长的一大堆选用的密码套件名称，我可以说一个都看不懂，但是为了把网站搞好还是得硬着头皮搜索复制粘贴，实际上也不清楚安全性导致咋样。 为了解决这个问题，Mozilla/DigitalOcean 都搞过流行 Web 服务器的 TLS 配置生成工具，比如 ssl-config - **mozilla，这个网站提供三个安全等级的配置**: 「Intermediate」: 查看生成出的 ssl-cipher 属性，发现它只支持 ECDHE/DHE 开头的算法。因此它保证前向保密。 对于需要通过浏览器访问的 API，推荐选择这个等级。 「Mordern」: 只支持 TLSv1.3，该协议废弃掉了过往所有不安全的算法，保证前向保密，安全性极高，性能也更好。 对于不需要通过浏览器等旧终端访问的 API，请直接选择这个等级。 「Old」: 除非你的用户使用非常老的终端进行访问，否则请不要考虑这个选项！ 可以点进去查看详细的 TLS 套件配置。 OCSP 证书验证协议 https://www.ssl.com/blogs/how-do-browsers-handle-revoked-ssl-tls-certificates/ https://imququ.com/post/why-can-not-turn-on-ocsp-stapling.html https://www.digicert.com/help/ 前面提到除了数字证书自带的有效期外，为了在私钥泄漏的情况下，能够吊销对应的证书，PKI 公钥基础设施还提供了 OCSP（Online Certificate Status Protocol）证书状态查询协议。 可以使用如下命令测试，确认站点是否启用了 ocsp stapling: $ openssl s_client -connect www.digicert.com:443 -servername www.digicert.com -status -tlsextdebug \u003c /dev/null 2\u003e\u00261 | grep -i \"OCSP response\" 如果输出包含 OCSP Response Status: successful 就说明站点支持 ocsp stapling， 如果输出内容为 OCSP response: no response sent 则说明站点不支持ocsp stapling。 实际上 Google/AWS 等大多数站点都不会启用也不需要启用 ocsp stapling，一是因为它们自己就是证书颁发机构，OCSP 服务器也归它们自己管，不存在隐私的问题。二是它们的 OCSP 服务器遍布全球，也不存在性能问题。 这种情况下开个 OCSP Stapling 反而是浪费流量，因为每次 TLS 握手都得发送一个 OCSP 状态信息。 我测试发现只有 www.digicert.com/www.douban.com 等少数站点启用了 ocsp stapling，www.baidu.com/www.google.com/www.zhihu.com 都未启用 ocsp stapling. 这导致了一些问题： Chrome/Firefox 等浏览器都会定期通过 OCSP 协议去请求 CA 机构的 OCSP 服务器验证证书状态，这可能会拖慢 HTTPS 协议的响应速度。 所谓的定期是指超过上一个 OCSP 响应的 nextUpdate 时间（一般为 7 天），或者如果该值为空的话，Firefox 默认 24h 后会重新查询 OCSP 状态。 因为客户端直接去请求 CA 机构的 OCSP 地址获取证书状态，这就导致 CA 机构可以获取到一些对应站点的用户信息（IP 地址、网络状态等）。 如果因为某些原因导致客户端无法访问 OCSP 服务器，会导致站点的初次访问时间用时变得很长。因为浏览器会每隔一阵时间就重新尝试去访问 OCSP 服务器！ 一个典型的例子就是 提高https载入速度，记一次nginx升级优化，因为 Let’s Encrypt 的 OCSP 服务器被 GFW 屏蔽，导致国内使用该证书的站点首次访问速度非常慢。 为了解决这两个问题，rfc6066 定义了 OCSP stapling 功能，它使服务器可以提前访问 OCSP 获取证书状态信息并缓存到本地。 在客户端使用 TLS 协议访问 HTTPS 服务时，服务端会直接在握手阶段将缓存的 OCSP 信息发送给客户端。 因为 OCSP","date":"2022-03-14","objectID":"/posts/about-tls-cert/:4:1","series":null,"tags":["Cryptography","密码学","HTTPS","TLS","SSL","OpenSSL","PKI","数字证书","证书","SSH","QUIC","HTTP/3","安全"],"title":"写给开发人员的实用密码学（八）—— 数字证书与 TLS 协议","uri":"/posts/about-tls-cert/#1-密码套件与-tls-历史版本"},{"categories":["tech"],"content":" 1. 密码套件与 TLS 历史版本密码套件（Cipher_suite）是 TLS 协议中一组用于实现安全通讯的密码学算法，类似于我们前面学习过的加密方案。 不同密码学算法的组合形成不同的密码套件，算法组合的差异使这些密码套件具有不同的性能与安全性，另外 TLS 协议的更新迭代也导致各密码套件拥有不同的兼容性。 通常越新推出的密码套件的安全性越高，但是兼容性就越差（旧设备不支持）。 密码套件的名称由它使用的各种密码学算法名称组成，而且有固定的格式，以 TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256 为例介绍下： TLS: 定义了此套件适用的协议，通常固定为 TLS ECDHE: 密钥交换算法 RSA: 数字证书认证算法 AES_128_GCM: 使用的对称加密方案，这是一个基于 AES 与 GCM 模式的对称认证加密方案，使用 128 位密钥 SHA256: 哈希函数，用于 HMAC 算法实现消息认证 TLS 固定使用 HMAC 算法进行消息认证 TLS 协议的前身是 SSL 协议，TLS/SSL 的发展历程展示如下： SSL/TLS 的历史版本 SSL 协议早在 2015 年就被各大主流浏览器废除了，TLS1.0 感觉也基本没站点在用了，这俩就直接跳过了。 下面分别介绍下 TLS1.1 TLS1.2 与 TLS1.3. TLS 1.1TLS 1.1 在 RFC4346 中定义，于 2006 年 4 月发布。 TLS 1.1 是 TLS 1.0 的一个补丁，主要更新包括： 添加对CBC攻击的保护 隐式初始向量 IV 被替换成一个显式的 IV 修复分组密码模式中填充算法的 bug 支持 IANA 登记的参数 TLS 1.1及其之前的算法曾经被广泛应用，它目前已知的缺陷如下： 不支持 PFS 完全前向保密 不支持 AEAD 认证加密算法 为了兼容性，保留了很多不安全的算法 TLS 1.1 已经不够安全了，不过一些陈年老站点或许还在使用它。 TLS 1.2TLS 1.2 在 RFC5246 中定义，于 2008 年 8 月发发布。 可选支持 PFS 完全前向保密 移除对 MD5 与 SHA-1 签名算法的支持 添加对 HMAC-SHA-256 及 HMAC-SHA-384 消息认证算法的支持 添加对 AEAD 加密认证方案的支持 去除 forback 回到 SSL 协议的能力，提升安全性 为了兼容性，保留了很多不安全的算法 如果你使用 TLS 1.2，需要小心地选择密码套件，避开不安全的套件，就能实现足够高的安全性。 TLS 1.3TLS 1.3 做了一次大刀阔斧的更新，是一个里程碑式的版本，其更新总结如下： 移除对如下算法的支持 哈希函数 SHA1/MD5 所有非 AEAD 加密认证的密码方案（CBC 模式） 移除对 RC4 与 3DES 加密算法的支持 移除了静态 RSA 与 DH 密钥交换算法 支持高性能的 Ed25519/Ed448 签名认证算法、X25519 密钥协商算法 支持高性能的 ChaCha20-Poly1305 对称认证加密方案 将密钥交换算法与公钥认证算法从密码套件中分离出来 比如原来的 TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256 密码套件将被拆分为 ECDHE 算法、RSA 身份认证算法、以及 TLS_AES_128_GCM_SHA256 密码套件 这样密码套件就只包含一个 AEAD 认证加密方案，以及一个哈希函数了 仅支持前向安全的密钥交换算法 DHE 或 ECDHE 支持最短 0-RTT 的 TLS 握手（会话恢复） TLS 1.3 从协议中删除了所有不安全的算法或协议，可以说只要你的通讯用了 TLS 1.3，那你的数据就安全了（当然前提是你的私钥没泄漏）。 如何设置 TLS 协议的版本、密码套件参数我们前面已经学习了对称加密、非对称加密、密钥交换三部分知识，对照 TLS 套件的名称，应该能很容易判断出哪些是安全的、哪些不够安全，哪些支持前向保密、哪些不支持。 一个非常好用的「站点 HTTPS 安全检测」网站是 https://myssl.com/，使用它测试知乎网的检测结果如下： SSL/TLS 的历史版本 能看到知乎为了兼容性，目前仍然支持 TLS1.0 与 TLS1.1，另外目前还不支持 TLS1.3. 此外，知乎仍然支持很多已经不安全的加密套件，myssl.com 专门使用黄色标识出了这些不安全的加密套件，我们总结下主要特征： 部分密码套件使用了不安全的对称加密算法 3DES 其他被标识为黄色的套件虽然使用了安全的对称加密算法，但是不支持 PFS 前向保密 此外 myssl.com 还列出了许多站点更详细的信息，包括 TLS1.3 的会话恢复，以及后面将会介绍的公钥固定、HTTP严格传输安全等信息： SSL/TLS 的历史版本 Nginx 的 TLS 协议配置以前为 Nginx 等程序配置 HTTPS 协议时，我最头疼的就是其中密码套件参数 ssl_ciphers，为了安全性，需要配置超长的一大堆选用的密码套件名称，我可以说一个都看不懂，但是为了把网站搞好还是得硬着头皮搜索复制粘贴，实际上也不清楚安全性导致咋样。 为了解决这个问题，Mozilla/DigitalOcean 都搞过流行 Web 服务器的 TLS 配置生成工具，比如 ssl-config - **mozilla，这个网站提供三个安全等级的配置**: 「Intermediate」: 查看生成出的 ssl-cipher 属性，发现它只支持 ECDHE/DHE 开头的算法。因此它保证前向保密。 对于需要通过浏览器访问的 API，推荐选择这个等级。 「Mordern」: 只支持 TLSv1.3，该协议废弃掉了过往所有不安全的算法，保证前向保密，安全性极高，性能也更好。 对于不需要通过浏览器等旧终端访问的 API，请直接选择这个等级。 「Old」: 除非你的用户使用非常老的终端进行访问，否则请不要考虑这个选项！ 可以点进去查看详细的 TLS 套件配置。 OCSP 证书验证协议 https://www.ssl.com/blogs/how-do-browsers-handle-revoked-ssl-tls-certificates/ https://imququ.com/post/why-can-not-turn-on-ocsp-stapling.html https://www.digicert.com/help/ 前面提到除了数字证书自带的有效期外，为了在私钥泄漏的情况下，能够吊销对应的证书，PKI 公钥基础设施还提供了 OCSP（Online Certificate Status Protocol）证书状态查询协议。 可以使用如下命令测试，确认站点是否启用了 ocsp stapling: $ openssl s_client -connect www.digicert.com:443 -servername www.digicert.com -status -tlsextdebug \u003c /dev/null 2\u003e\u00261 | grep -i \"OCSP response\" 如果输出包含 OCSP Response Status: successful 就说明站点支持 ocsp stapling， 如果输出内容为 OCSP response: no response sent 则说明站点不支持ocsp stapling。 实际上 Google/AWS 等大多数站点都不会启用也不需要启用 ocsp stapling，一是因为它们自己就是证书颁发机构，OCSP 服务器也归它们自己管，不存在隐私的问题。二是它们的 OCSP 服务器遍布全球，也不存在性能问题。 这种情况下开个 OCSP Stapling 反而是浪费流量，因为每次 TLS 握手都得发送一个 OCSP 状态信息。 我测试发现只有 www.digicert.com/www.douban.com 等少数站点启用了 ocsp stapling，www.baidu.com/www.google.com/www.zhihu.com 都未启用 ocsp stapling. 这导致了一些问题： Chrome/Firefox 等浏览器都会定期通过 OCSP 协议去请求 CA 机构的 OCSP 服务器验证证书状态，这可能会拖慢 HTTPS 协议的响应速度。 所谓的定期是指超过上一个 OCSP 响应的 nextUpdate 时间（一般为 7 天），或者如果该值为空的话，Firefox 默认 24h 后会重新查询 OCSP 状态。 因为客户端直接去请求 CA 机构的 OCSP 地址获取证书状态，这就导致 CA 机构可以获取到一些对应站点的用户信息（IP 地址、网络状态等）。 如果因为某些原因导致客户端无法访问 OCSP 服务器，会导致站点的初次访问时间用时变得很长。因为浏览器会每隔一阵时间就重新尝试去访问 OCSP 服务器！ 一个典型的例子就是 提高https载入速度，记一次nginx升级优化，因为 Let’s Encrypt 的 OCSP 服务器被 GFW 屏蔽，导致国内使用该证书的站点首次访问速度非常慢。 为了解决这两个问题，rfc6066 定义了 OCSP stapling 功能，它使服务器可以提前访问 OCSP 获取证书状态信息并缓存到本地。 在客户端使用 TLS 协议访问 HTTPS 服务时，服务端会直接在握手阶段将缓存的 OCSP 信息发送给客户端。 因为 OCSP","date":"2022-03-14","objectID":"/posts/about-tls-cert/:4:1","series":null,"tags":["Cryptography","密码学","HTTPS","TLS","SSL","OpenSSL","PKI","数字证书","证书","SSH","QUIC","HTTP/3","安全"],"title":"写给开发人员的实用密码学（八）—— 数字证书与 TLS 协议","uri":"/posts/about-tls-cert/#tls-11"},{"categories":["tech"],"content":" 1. 密码套件与 TLS 历史版本密码套件（Cipher_suite）是 TLS 协议中一组用于实现安全通讯的密码学算法，类似于我们前面学习过的加密方案。 不同密码学算法的组合形成不同的密码套件，算法组合的差异使这些密码套件具有不同的性能与安全性，另外 TLS 协议的更新迭代也导致各密码套件拥有不同的兼容性。 通常越新推出的密码套件的安全性越高，但是兼容性就越差（旧设备不支持）。 密码套件的名称由它使用的各种密码学算法名称组成，而且有固定的格式，以 TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256 为例介绍下： TLS: 定义了此套件适用的协议，通常固定为 TLS ECDHE: 密钥交换算法 RSA: 数字证书认证算法 AES_128_GCM: 使用的对称加密方案，这是一个基于 AES 与 GCM 模式的对称认证加密方案，使用 128 位密钥 SHA256: 哈希函数，用于 HMAC 算法实现消息认证 TLS 固定使用 HMAC 算法进行消息认证 TLS 协议的前身是 SSL 协议，TLS/SSL 的发展历程展示如下： SSL/TLS 的历史版本 SSL 协议早在 2015 年就被各大主流浏览器废除了，TLS1.0 感觉也基本没站点在用了，这俩就直接跳过了。 下面分别介绍下 TLS1.1 TLS1.2 与 TLS1.3. TLS 1.1TLS 1.1 在 RFC4346 中定义，于 2006 年 4 月发布。 TLS 1.1 是 TLS 1.0 的一个补丁，主要更新包括： 添加对CBC攻击的保护 隐式初始向量 IV 被替换成一个显式的 IV 修复分组密码模式中填充算法的 bug 支持 IANA 登记的参数 TLS 1.1及其之前的算法曾经被广泛应用，它目前已知的缺陷如下： 不支持 PFS 完全前向保密 不支持 AEAD 认证加密算法 为了兼容性，保留了很多不安全的算法 TLS 1.1 已经不够安全了，不过一些陈年老站点或许还在使用它。 TLS 1.2TLS 1.2 在 RFC5246 中定义，于 2008 年 8 月发发布。 可选支持 PFS 完全前向保密 移除对 MD5 与 SHA-1 签名算法的支持 添加对 HMAC-SHA-256 及 HMAC-SHA-384 消息认证算法的支持 添加对 AEAD 加密认证方案的支持 去除 forback 回到 SSL 协议的能力，提升安全性 为了兼容性，保留了很多不安全的算法 如果你使用 TLS 1.2，需要小心地选择密码套件，避开不安全的套件，就能实现足够高的安全性。 TLS 1.3TLS 1.3 做了一次大刀阔斧的更新，是一个里程碑式的版本，其更新总结如下： 移除对如下算法的支持 哈希函数 SHA1/MD5 所有非 AEAD 加密认证的密码方案（CBC 模式） 移除对 RC4 与 3DES 加密算法的支持 移除了静态 RSA 与 DH 密钥交换算法 支持高性能的 Ed25519/Ed448 签名认证算法、X25519 密钥协商算法 支持高性能的 ChaCha20-Poly1305 对称认证加密方案 将密钥交换算法与公钥认证算法从密码套件中分离出来 比如原来的 TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256 密码套件将被拆分为 ECDHE 算法、RSA 身份认证算法、以及 TLS_AES_128_GCM_SHA256 密码套件 这样密码套件就只包含一个 AEAD 认证加密方案，以及一个哈希函数了 仅支持前向安全的密钥交换算法 DHE 或 ECDHE 支持最短 0-RTT 的 TLS 握手（会话恢复） TLS 1.3 从协议中删除了所有不安全的算法或协议，可以说只要你的通讯用了 TLS 1.3，那你的数据就安全了（当然前提是你的私钥没泄漏）。 如何设置 TLS 协议的版本、密码套件参数我们前面已经学习了对称加密、非对称加密、密钥交换三部分知识，对照 TLS 套件的名称，应该能很容易判断出哪些是安全的、哪些不够安全，哪些支持前向保密、哪些不支持。 一个非常好用的「站点 HTTPS 安全检测」网站是 https://myssl.com/，使用它测试知乎网的检测结果如下： SSL/TLS 的历史版本 能看到知乎为了兼容性，目前仍然支持 TLS1.0 与 TLS1.1，另外目前还不支持 TLS1.3. 此外，知乎仍然支持很多已经不安全的加密套件，myssl.com 专门使用黄色标识出了这些不安全的加密套件，我们总结下主要特征： 部分密码套件使用了不安全的对称加密算法 3DES 其他被标识为黄色的套件虽然使用了安全的对称加密算法，但是不支持 PFS 前向保密 此外 myssl.com 还列出了许多站点更详细的信息，包括 TLS1.3 的会话恢复，以及后面将会介绍的公钥固定、HTTP严格传输安全等信息： SSL/TLS 的历史版本 Nginx 的 TLS 协议配置以前为 Nginx 等程序配置 HTTPS 协议时，我最头疼的就是其中密码套件参数 ssl_ciphers，为了安全性，需要配置超长的一大堆选用的密码套件名称，我可以说一个都看不懂，但是为了把网站搞好还是得硬着头皮搜索复制粘贴，实际上也不清楚安全性导致咋样。 为了解决这个问题，Mozilla/DigitalOcean 都搞过流行 Web 服务器的 TLS 配置生成工具，比如 ssl-config - **mozilla，这个网站提供三个安全等级的配置**: 「Intermediate」: 查看生成出的 ssl-cipher 属性，发现它只支持 ECDHE/DHE 开头的算法。因此它保证前向保密。 对于需要通过浏览器访问的 API，推荐选择这个等级。 「Mordern」: 只支持 TLSv1.3，该协议废弃掉了过往所有不安全的算法，保证前向保密，安全性极高，性能也更好。 对于不需要通过浏览器等旧终端访问的 API，请直接选择这个等级。 「Old」: 除非你的用户使用非常老的终端进行访问，否则请不要考虑这个选项！ 可以点进去查看详细的 TLS 套件配置。 OCSP 证书验证协议 https://www.ssl.com/blogs/how-do-browsers-handle-revoked-ssl-tls-certificates/ https://imququ.com/post/why-can-not-turn-on-ocsp-stapling.html https://www.digicert.com/help/ 前面提到除了数字证书自带的有效期外，为了在私钥泄漏的情况下，能够吊销对应的证书，PKI 公钥基础设施还提供了 OCSP（Online Certificate Status Protocol）证书状态查询协议。 可以使用如下命令测试，确认站点是否启用了 ocsp stapling: $ openssl s_client -connect www.digicert.com:443 -servername www.digicert.com -status -tlsextdebug \u003c /dev/null 2\u003e\u00261 | grep -i \"OCSP response\" 如果输出包含 OCSP Response Status: successful 就说明站点支持 ocsp stapling， 如果输出内容为 OCSP response: no response sent 则说明站点不支持ocsp stapling。 实际上 Google/AWS 等大多数站点都不会启用也不需要启用 ocsp stapling，一是因为它们自己就是证书颁发机构，OCSP 服务器也归它们自己管，不存在隐私的问题。二是它们的 OCSP 服务器遍布全球，也不存在性能问题。 这种情况下开个 OCSP Stapling 反而是浪费流量，因为每次 TLS 握手都得发送一个 OCSP 状态信息。 我测试发现只有 www.digicert.com/www.douban.com 等少数站点启用了 ocsp stapling，www.baidu.com/www.google.com/www.zhihu.com 都未启用 ocsp stapling. 这导致了一些问题： Chrome/Firefox 等浏览器都会定期通过 OCSP 协议去请求 CA 机构的 OCSP 服务器验证证书状态，这可能会拖慢 HTTPS 协议的响应速度。 所谓的定期是指超过上一个 OCSP 响应的 nextUpdate 时间（一般为 7 天），或者如果该值为空的话，Firefox 默认 24h 后会重新查询 OCSP 状态。 因为客户端直接去请求 CA 机构的 OCSP 地址获取证书状态，这就导致 CA 机构可以获取到一些对应站点的用户信息（IP 地址、网络状态等）。 如果因为某些原因导致客户端无法访问 OCSP 服务器，会导致站点的初次访问时间用时变得很长。因为浏览器会每隔一阵时间就重新尝试去访问 OCSP 服务器！ 一个典型的例子就是 提高https载入速度，记一次nginx升级优化，因为 Let’s Encrypt 的 OCSP 服务器被 GFW 屏蔽，导致国内使用该证书的站点首次访问速度非常慢。 为了解决这两个问题，rfc6066 定义了 OCSP stapling 功能，它使服务器可以提前访问 OCSP 获取证书状态信息并缓存到本地。 在客户端使用 TLS 协议访问 HTTPS 服务时，服务端会直接在握手阶段将缓存的 OCSP 信息发送给客户端。 因为 OCSP","date":"2022-03-14","objectID":"/posts/about-tls-cert/:4:1","series":null,"tags":["Cryptography","密码学","HTTPS","TLS","SSL","OpenSSL","PKI","数字证书","证书","SSH","QUIC","HTTP/3","安全"],"title":"写给开发人员的实用密码学（八）—— 数字证书与 TLS 协议","uri":"/posts/about-tls-cert/#tls-12"},{"categories":["tech"],"content":" 1. 密码套件与 TLS 历史版本密码套件（Cipher_suite）是 TLS 协议中一组用于实现安全通讯的密码学算法，类似于我们前面学习过的加密方案。 不同密码学算法的组合形成不同的密码套件，算法组合的差异使这些密码套件具有不同的性能与安全性，另外 TLS 协议的更新迭代也导致各密码套件拥有不同的兼容性。 通常越新推出的密码套件的安全性越高，但是兼容性就越差（旧设备不支持）。 密码套件的名称由它使用的各种密码学算法名称组成，而且有固定的格式，以 TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256 为例介绍下： TLS: 定义了此套件适用的协议，通常固定为 TLS ECDHE: 密钥交换算法 RSA: 数字证书认证算法 AES_128_GCM: 使用的对称加密方案，这是一个基于 AES 与 GCM 模式的对称认证加密方案，使用 128 位密钥 SHA256: 哈希函数，用于 HMAC 算法实现消息认证 TLS 固定使用 HMAC 算法进行消息认证 TLS 协议的前身是 SSL 协议，TLS/SSL 的发展历程展示如下： SSL/TLS 的历史版本 SSL 协议早在 2015 年就被各大主流浏览器废除了，TLS1.0 感觉也基本没站点在用了，这俩就直接跳过了。 下面分别介绍下 TLS1.1 TLS1.2 与 TLS1.3. TLS 1.1TLS 1.1 在 RFC4346 中定义，于 2006 年 4 月发布。 TLS 1.1 是 TLS 1.0 的一个补丁，主要更新包括： 添加对CBC攻击的保护 隐式初始向量 IV 被替换成一个显式的 IV 修复分组密码模式中填充算法的 bug 支持 IANA 登记的参数 TLS 1.1及其之前的算法曾经被广泛应用，它目前已知的缺陷如下： 不支持 PFS 完全前向保密 不支持 AEAD 认证加密算法 为了兼容性，保留了很多不安全的算法 TLS 1.1 已经不够安全了，不过一些陈年老站点或许还在使用它。 TLS 1.2TLS 1.2 在 RFC5246 中定义，于 2008 年 8 月发发布。 可选支持 PFS 完全前向保密 移除对 MD5 与 SHA-1 签名算法的支持 添加对 HMAC-SHA-256 及 HMAC-SHA-384 消息认证算法的支持 添加对 AEAD 加密认证方案的支持 去除 forback 回到 SSL 协议的能力，提升安全性 为了兼容性，保留了很多不安全的算法 如果你使用 TLS 1.2，需要小心地选择密码套件，避开不安全的套件，就能实现足够高的安全性。 TLS 1.3TLS 1.3 做了一次大刀阔斧的更新，是一个里程碑式的版本，其更新总结如下： 移除对如下算法的支持 哈希函数 SHA1/MD5 所有非 AEAD 加密认证的密码方案（CBC 模式） 移除对 RC4 与 3DES 加密算法的支持 移除了静态 RSA 与 DH 密钥交换算法 支持高性能的 Ed25519/Ed448 签名认证算法、X25519 密钥协商算法 支持高性能的 ChaCha20-Poly1305 对称认证加密方案 将密钥交换算法与公钥认证算法从密码套件中分离出来 比如原来的 TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256 密码套件将被拆分为 ECDHE 算法、RSA 身份认证算法、以及 TLS_AES_128_GCM_SHA256 密码套件 这样密码套件就只包含一个 AEAD 认证加密方案，以及一个哈希函数了 仅支持前向安全的密钥交换算法 DHE 或 ECDHE 支持最短 0-RTT 的 TLS 握手（会话恢复） TLS 1.3 从协议中删除了所有不安全的算法或协议，可以说只要你的通讯用了 TLS 1.3，那你的数据就安全了（当然前提是你的私钥没泄漏）。 如何设置 TLS 协议的版本、密码套件参数我们前面已经学习了对称加密、非对称加密、密钥交换三部分知识，对照 TLS 套件的名称，应该能很容易判断出哪些是安全的、哪些不够安全，哪些支持前向保密、哪些不支持。 一个非常好用的「站点 HTTPS 安全检测」网站是 https://myssl.com/，使用它测试知乎网的检测结果如下： SSL/TLS 的历史版本 能看到知乎为了兼容性，目前仍然支持 TLS1.0 与 TLS1.1，另外目前还不支持 TLS1.3. 此外，知乎仍然支持很多已经不安全的加密套件，myssl.com 专门使用黄色标识出了这些不安全的加密套件，我们总结下主要特征： 部分密码套件使用了不安全的对称加密算法 3DES 其他被标识为黄色的套件虽然使用了安全的对称加密算法，但是不支持 PFS 前向保密 此外 myssl.com 还列出了许多站点更详细的信息，包括 TLS1.3 的会话恢复，以及后面将会介绍的公钥固定、HTTP严格传输安全等信息： SSL/TLS 的历史版本 Nginx 的 TLS 协议配置以前为 Nginx 等程序配置 HTTPS 协议时，我最头疼的就是其中密码套件参数 ssl_ciphers，为了安全性，需要配置超长的一大堆选用的密码套件名称，我可以说一个都看不懂，但是为了把网站搞好还是得硬着头皮搜索复制粘贴，实际上也不清楚安全性导致咋样。 为了解决这个问题，Mozilla/DigitalOcean 都搞过流行 Web 服务器的 TLS 配置生成工具，比如 ssl-config - **mozilla，这个网站提供三个安全等级的配置**: 「Intermediate」: 查看生成出的 ssl-cipher 属性，发现它只支持 ECDHE/DHE 开头的算法。因此它保证前向保密。 对于需要通过浏览器访问的 API，推荐选择这个等级。 「Mordern」: 只支持 TLSv1.3，该协议废弃掉了过往所有不安全的算法，保证前向保密，安全性极高，性能也更好。 对于不需要通过浏览器等旧终端访问的 API，请直接选择这个等级。 「Old」: 除非你的用户使用非常老的终端进行访问，否则请不要考虑这个选项！ 可以点进去查看详细的 TLS 套件配置。 OCSP 证书验证协议 https://www.ssl.com/blogs/how-do-browsers-handle-revoked-ssl-tls-certificates/ https://imququ.com/post/why-can-not-turn-on-ocsp-stapling.html https://www.digicert.com/help/ 前面提到除了数字证书自带的有效期外，为了在私钥泄漏的情况下，能够吊销对应的证书，PKI 公钥基础设施还提供了 OCSP（Online Certificate Status Protocol）证书状态查询协议。 可以使用如下命令测试，确认站点是否启用了 ocsp stapling: $ openssl s_client -connect www.digicert.com:443 -servername www.digicert.com -status -tlsextdebug \u003c /dev/null 2\u003e\u00261 | grep -i \"OCSP response\" 如果输出包含 OCSP Response Status: successful 就说明站点支持 ocsp stapling， 如果输出内容为 OCSP response: no response sent 则说明站点不支持ocsp stapling。 实际上 Google/AWS 等大多数站点都不会启用也不需要启用 ocsp stapling，一是因为它们自己就是证书颁发机构，OCSP 服务器也归它们自己管，不存在隐私的问题。二是它们的 OCSP 服务器遍布全球，也不存在性能问题。 这种情况下开个 OCSP Stapling 反而是浪费流量，因为每次 TLS 握手都得发送一个 OCSP 状态信息。 我测试发现只有 www.digicert.com/www.douban.com 等少数站点启用了 ocsp stapling，www.baidu.com/www.google.com/www.zhihu.com 都未启用 ocsp stapling. 这导致了一些问题： Chrome/Firefox 等浏览器都会定期通过 OCSP 协议去请求 CA 机构的 OCSP 服务器验证证书状态，这可能会拖慢 HTTPS 协议的响应速度。 所谓的定期是指超过上一个 OCSP 响应的 nextUpdate 时间（一般为 7 天），或者如果该值为空的话，Firefox 默认 24h 后会重新查询 OCSP 状态。 因为客户端直接去请求 CA 机构的 OCSP 地址获取证书状态，这就导致 CA 机构可以获取到一些对应站点的用户信息（IP 地址、网络状态等）。 如果因为某些原因导致客户端无法访问 OCSP 服务器，会导致站点的初次访问时间用时变得很长。因为浏览器会每隔一阵时间就重新尝试去访问 OCSP 服务器！ 一个典型的例子就是 提高https载入速度，记一次nginx升级优化，因为 Let’s Encrypt 的 OCSP 服务器被 GFW 屏蔽，导致国内使用该证书的站点首次访问速度非常慢。 为了解决这两个问题，rfc6066 定义了 OCSP stapling 功能，它使服务器可以提前访问 OCSP 获取证书状态信息并缓存到本地。 在客户端使用 TLS 协议访问 HTTPS 服务时，服务端会直接在握手阶段将缓存的 OCSP 信息发送给客户端。 因为 OCSP","date":"2022-03-14","objectID":"/posts/about-tls-cert/:4:1","series":null,"tags":["Cryptography","密码学","HTTPS","TLS","SSL","OpenSSL","PKI","数字证书","证书","SSH","QUIC","HTTP/3","安全"],"title":"写给开发人员的实用密码学（八）—— 数字证书与 TLS 协议","uri":"/posts/about-tls-cert/#tls-13"},{"categories":["tech"],"content":" 1. 密码套件与 TLS 历史版本密码套件（Cipher_suite）是 TLS 协议中一组用于实现安全通讯的密码学算法，类似于我们前面学习过的加密方案。 不同密码学算法的组合形成不同的密码套件，算法组合的差异使这些密码套件具有不同的性能与安全性，另外 TLS 协议的更新迭代也导致各密码套件拥有不同的兼容性。 通常越新推出的密码套件的安全性越高，但是兼容性就越差（旧设备不支持）。 密码套件的名称由它使用的各种密码学算法名称组成，而且有固定的格式，以 TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256 为例介绍下： TLS: 定义了此套件适用的协议，通常固定为 TLS ECDHE: 密钥交换算法 RSA: 数字证书认证算法 AES_128_GCM: 使用的对称加密方案，这是一个基于 AES 与 GCM 模式的对称认证加密方案，使用 128 位密钥 SHA256: 哈希函数，用于 HMAC 算法实现消息认证 TLS 固定使用 HMAC 算法进行消息认证 TLS 协议的前身是 SSL 协议，TLS/SSL 的发展历程展示如下： SSL/TLS 的历史版本 SSL 协议早在 2015 年就被各大主流浏览器废除了，TLS1.0 感觉也基本没站点在用了，这俩就直接跳过了。 下面分别介绍下 TLS1.1 TLS1.2 与 TLS1.3. TLS 1.1TLS 1.1 在 RFC4346 中定义，于 2006 年 4 月发布。 TLS 1.1 是 TLS 1.0 的一个补丁，主要更新包括： 添加对CBC攻击的保护 隐式初始向量 IV 被替换成一个显式的 IV 修复分组密码模式中填充算法的 bug 支持 IANA 登记的参数 TLS 1.1及其之前的算法曾经被广泛应用，它目前已知的缺陷如下： 不支持 PFS 完全前向保密 不支持 AEAD 认证加密算法 为了兼容性，保留了很多不安全的算法 TLS 1.1 已经不够安全了，不过一些陈年老站点或许还在使用它。 TLS 1.2TLS 1.2 在 RFC5246 中定义，于 2008 年 8 月发发布。 可选支持 PFS 完全前向保密 移除对 MD5 与 SHA-1 签名算法的支持 添加对 HMAC-SHA-256 及 HMAC-SHA-384 消息认证算法的支持 添加对 AEAD 加密认证方案的支持 去除 forback 回到 SSL 协议的能力，提升安全性 为了兼容性，保留了很多不安全的算法 如果你使用 TLS 1.2，需要小心地选择密码套件，避开不安全的套件，就能实现足够高的安全性。 TLS 1.3TLS 1.3 做了一次大刀阔斧的更新，是一个里程碑式的版本，其更新总结如下： 移除对如下算法的支持 哈希函数 SHA1/MD5 所有非 AEAD 加密认证的密码方案（CBC 模式） 移除对 RC4 与 3DES 加密算法的支持 移除了静态 RSA 与 DH 密钥交换算法 支持高性能的 Ed25519/Ed448 签名认证算法、X25519 密钥协商算法 支持高性能的 ChaCha20-Poly1305 对称认证加密方案 将密钥交换算法与公钥认证算法从密码套件中分离出来 比如原来的 TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256 密码套件将被拆分为 ECDHE 算法、RSA 身份认证算法、以及 TLS_AES_128_GCM_SHA256 密码套件 这样密码套件就只包含一个 AEAD 认证加密方案，以及一个哈希函数了 仅支持前向安全的密钥交换算法 DHE 或 ECDHE 支持最短 0-RTT 的 TLS 握手（会话恢复） TLS 1.3 从协议中删除了所有不安全的算法或协议，可以说只要你的通讯用了 TLS 1.3，那你的数据就安全了（当然前提是你的私钥没泄漏）。 如何设置 TLS 协议的版本、密码套件参数我们前面已经学习了对称加密、非对称加密、密钥交换三部分知识，对照 TLS 套件的名称，应该能很容易判断出哪些是安全的、哪些不够安全，哪些支持前向保密、哪些不支持。 一个非常好用的「站点 HTTPS 安全检测」网站是 https://myssl.com/，使用它测试知乎网的检测结果如下： SSL/TLS 的历史版本 能看到知乎为了兼容性，目前仍然支持 TLS1.0 与 TLS1.1，另外目前还不支持 TLS1.3. 此外，知乎仍然支持很多已经不安全的加密套件，myssl.com 专门使用黄色标识出了这些不安全的加密套件，我们总结下主要特征： 部分密码套件使用了不安全的对称加密算法 3DES 其他被标识为黄色的套件虽然使用了安全的对称加密算法，但是不支持 PFS 前向保密 此外 myssl.com 还列出了许多站点更详细的信息，包括 TLS1.3 的会话恢复，以及后面将会介绍的公钥固定、HTTP严格传输安全等信息： SSL/TLS 的历史版本 Nginx 的 TLS 协议配置以前为 Nginx 等程序配置 HTTPS 协议时，我最头疼的就是其中密码套件参数 ssl_ciphers，为了安全性，需要配置超长的一大堆选用的密码套件名称，我可以说一个都看不懂，但是为了把网站搞好还是得硬着头皮搜索复制粘贴，实际上也不清楚安全性导致咋样。 为了解决这个问题，Mozilla/DigitalOcean 都搞过流行 Web 服务器的 TLS 配置生成工具，比如 ssl-config - **mozilla，这个网站提供三个安全等级的配置**: 「Intermediate」: 查看生成出的 ssl-cipher 属性，发现它只支持 ECDHE/DHE 开头的算法。因此它保证前向保密。 对于需要通过浏览器访问的 API，推荐选择这个等级。 「Mordern」: 只支持 TLSv1.3，该协议废弃掉了过往所有不安全的算法，保证前向保密，安全性极高，性能也更好。 对于不需要通过浏览器等旧终端访问的 API，请直接选择这个等级。 「Old」: 除非你的用户使用非常老的终端进行访问，否则请不要考虑这个选项！ 可以点进去查看详细的 TLS 套件配置。 OCSP 证书验证协议 https://www.ssl.com/blogs/how-do-browsers-handle-revoked-ssl-tls-certificates/ https://imququ.com/post/why-can-not-turn-on-ocsp-stapling.html https://www.digicert.com/help/ 前面提到除了数字证书自带的有效期外，为了在私钥泄漏的情况下，能够吊销对应的证书，PKI 公钥基础设施还提供了 OCSP（Online Certificate Status Protocol）证书状态查询协议。 可以使用如下命令测试，确认站点是否启用了 ocsp stapling: $ openssl s_client -connect www.digicert.com:443 -servername www.digicert.com -status -tlsextdebug \u003c /dev/null 2\u003e\u00261 | grep -i \"OCSP response\" 如果输出包含 OCSP Response Status: successful 就说明站点支持 ocsp stapling， 如果输出内容为 OCSP response: no response sent 则说明站点不支持ocsp stapling。 实际上 Google/AWS 等大多数站点都不会启用也不需要启用 ocsp stapling，一是因为它们自己就是证书颁发机构，OCSP 服务器也归它们自己管，不存在隐私的问题。二是它们的 OCSP 服务器遍布全球，也不存在性能问题。 这种情况下开个 OCSP Stapling 反而是浪费流量，因为每次 TLS 握手都得发送一个 OCSP 状态信息。 我测试发现只有 www.digicert.com/www.douban.com 等少数站点启用了 ocsp stapling，www.baidu.com/www.google.com/www.zhihu.com 都未启用 ocsp stapling. 这导致了一些问题： Chrome/Firefox 等浏览器都会定期通过 OCSP 协议去请求 CA 机构的 OCSP 服务器验证证书状态，这可能会拖慢 HTTPS 协议的响应速度。 所谓的定期是指超过上一个 OCSP 响应的 nextUpdate 时间（一般为 7 天），或者如果该值为空的话，Firefox 默认 24h 后会重新查询 OCSP 状态。 因为客户端直接去请求 CA 机构的 OCSP 地址获取证书状态，这就导致 CA 机构可以获取到一些对应站点的用户信息（IP 地址、网络状态等）。 如果因为某些原因导致客户端无法访问 OCSP 服务器，会导致站点的初次访问时间用时变得很长。因为浏览器会每隔一阵时间就重新尝试去访问 OCSP 服务器！ 一个典型的例子就是 提高https载入速度，记一次nginx升级优化，因为 Let’s Encrypt 的 OCSP 服务器被 GFW 屏蔽，导致国内使用该证书的站点首次访问速度非常慢。 为了解决这两个问题，rfc6066 定义了 OCSP stapling 功能，它使服务器可以提前访问 OCSP 获取证书状态信息并缓存到本地。 在客户端使用 TLS 协议访问 HTTPS 服务时，服务端会直接在握手阶段将缓存的 OCSP 信息发送给客户端。 因为 OCSP","date":"2022-03-14","objectID":"/posts/about-tls-cert/:4:1","series":null,"tags":["Cryptography","密码学","HTTPS","TLS","SSL","OpenSSL","PKI","数字证书","证书","SSH","QUIC","HTTP/3","安全"],"title":"写给开发人员的实用密码学（八）—— 数字证书与 TLS 协议","uri":"/posts/about-tls-cert/#如何设置-tls-协议的版本密码套件参数"},{"categories":["tech"],"content":" 1. 密码套件与 TLS 历史版本密码套件（Cipher_suite）是 TLS 协议中一组用于实现安全通讯的密码学算法，类似于我们前面学习过的加密方案。 不同密码学算法的组合形成不同的密码套件，算法组合的差异使这些密码套件具有不同的性能与安全性，另外 TLS 协议的更新迭代也导致各密码套件拥有不同的兼容性。 通常越新推出的密码套件的安全性越高，但是兼容性就越差（旧设备不支持）。 密码套件的名称由它使用的各种密码学算法名称组成，而且有固定的格式，以 TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256 为例介绍下： TLS: 定义了此套件适用的协议，通常固定为 TLS ECDHE: 密钥交换算法 RSA: 数字证书认证算法 AES_128_GCM: 使用的对称加密方案，这是一个基于 AES 与 GCM 模式的对称认证加密方案，使用 128 位密钥 SHA256: 哈希函数，用于 HMAC 算法实现消息认证 TLS 固定使用 HMAC 算法进行消息认证 TLS 协议的前身是 SSL 协议，TLS/SSL 的发展历程展示如下： SSL/TLS 的历史版本 SSL 协议早在 2015 年就被各大主流浏览器废除了，TLS1.0 感觉也基本没站点在用了，这俩就直接跳过了。 下面分别介绍下 TLS1.1 TLS1.2 与 TLS1.3. TLS 1.1TLS 1.1 在 RFC4346 中定义，于 2006 年 4 月发布。 TLS 1.1 是 TLS 1.0 的一个补丁，主要更新包括： 添加对CBC攻击的保护 隐式初始向量 IV 被替换成一个显式的 IV 修复分组密码模式中填充算法的 bug 支持 IANA 登记的参数 TLS 1.1及其之前的算法曾经被广泛应用，它目前已知的缺陷如下： 不支持 PFS 完全前向保密 不支持 AEAD 认证加密算法 为了兼容性，保留了很多不安全的算法 TLS 1.1 已经不够安全了，不过一些陈年老站点或许还在使用它。 TLS 1.2TLS 1.2 在 RFC5246 中定义，于 2008 年 8 月发发布。 可选支持 PFS 完全前向保密 移除对 MD5 与 SHA-1 签名算法的支持 添加对 HMAC-SHA-256 及 HMAC-SHA-384 消息认证算法的支持 添加对 AEAD 加密认证方案的支持 去除 forback 回到 SSL 协议的能力，提升安全性 为了兼容性，保留了很多不安全的算法 如果你使用 TLS 1.2，需要小心地选择密码套件，避开不安全的套件，就能实现足够高的安全性。 TLS 1.3TLS 1.3 做了一次大刀阔斧的更新，是一个里程碑式的版本，其更新总结如下： 移除对如下算法的支持 哈希函数 SHA1/MD5 所有非 AEAD 加密认证的密码方案（CBC 模式） 移除对 RC4 与 3DES 加密算法的支持 移除了静态 RSA 与 DH 密钥交换算法 支持高性能的 Ed25519/Ed448 签名认证算法、X25519 密钥协商算法 支持高性能的 ChaCha20-Poly1305 对称认证加密方案 将密钥交换算法与公钥认证算法从密码套件中分离出来 比如原来的 TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256 密码套件将被拆分为 ECDHE 算法、RSA 身份认证算法、以及 TLS_AES_128_GCM_SHA256 密码套件 这样密码套件就只包含一个 AEAD 认证加密方案，以及一个哈希函数了 仅支持前向安全的密钥交换算法 DHE 或 ECDHE 支持最短 0-RTT 的 TLS 握手（会话恢复） TLS 1.3 从协议中删除了所有不安全的算法或协议，可以说只要你的通讯用了 TLS 1.3，那你的数据就安全了（当然前提是你的私钥没泄漏）。 如何设置 TLS 协议的版本、密码套件参数我们前面已经学习了对称加密、非对称加密、密钥交换三部分知识，对照 TLS 套件的名称，应该能很容易判断出哪些是安全的、哪些不够安全，哪些支持前向保密、哪些不支持。 一个非常好用的「站点 HTTPS 安全检测」网站是 https://myssl.com/，使用它测试知乎网的检测结果如下： SSL/TLS 的历史版本 能看到知乎为了兼容性，目前仍然支持 TLS1.0 与 TLS1.1，另外目前还不支持 TLS1.3. 此外，知乎仍然支持很多已经不安全的加密套件，myssl.com 专门使用黄色标识出了这些不安全的加密套件，我们总结下主要特征： 部分密码套件使用了不安全的对称加密算法 3DES 其他被标识为黄色的套件虽然使用了安全的对称加密算法，但是不支持 PFS 前向保密 此外 myssl.com 还列出了许多站点更详细的信息，包括 TLS1.3 的会话恢复，以及后面将会介绍的公钥固定、HTTP严格传输安全等信息： SSL/TLS 的历史版本 Nginx 的 TLS 协议配置以前为 Nginx 等程序配置 HTTPS 协议时，我最头疼的就是其中密码套件参数 ssl_ciphers，为了安全性，需要配置超长的一大堆选用的密码套件名称，我可以说一个都看不懂，但是为了把网站搞好还是得硬着头皮搜索复制粘贴，实际上也不清楚安全性导致咋样。 为了解决这个问题，Mozilla/DigitalOcean 都搞过流行 Web 服务器的 TLS 配置生成工具，比如 ssl-config - **mozilla，这个网站提供三个安全等级的配置**: 「Intermediate」: 查看生成出的 ssl-cipher 属性，发现它只支持 ECDHE/DHE 开头的算法。因此它保证前向保密。 对于需要通过浏览器访问的 API，推荐选择这个等级。 「Mordern」: 只支持 TLSv1.3，该协议废弃掉了过往所有不安全的算法，保证前向保密，安全性极高，性能也更好。 对于不需要通过浏览器等旧终端访问的 API，请直接选择这个等级。 「Old」: 除非你的用户使用非常老的终端进行访问，否则请不要考虑这个选项！ 可以点进去查看详细的 TLS 套件配置。 OCSP 证书验证协议 https://www.ssl.com/blogs/how-do-browsers-handle-revoked-ssl-tls-certificates/ https://imququ.com/post/why-can-not-turn-on-ocsp-stapling.html https://www.digicert.com/help/ 前面提到除了数字证书自带的有效期外，为了在私钥泄漏的情况下，能够吊销对应的证书，PKI 公钥基础设施还提供了 OCSP（Online Certificate Status Protocol）证书状态查询协议。 可以使用如下命令测试，确认站点是否启用了 ocsp stapling: $ openssl s_client -connect www.digicert.com:443 -servername www.digicert.com -status -tlsextdebug \u003c /dev/null 2\u003e\u00261 | grep -i \"OCSP response\" 如果输出包含 OCSP Response Status: successful 就说明站点支持 ocsp stapling， 如果输出内容为 OCSP response: no response sent 则说明站点不支持ocsp stapling。 实际上 Google/AWS 等大多数站点都不会启用也不需要启用 ocsp stapling，一是因为它们自己就是证书颁发机构，OCSP 服务器也归它们自己管，不存在隐私的问题。二是它们的 OCSP 服务器遍布全球，也不存在性能问题。 这种情况下开个 OCSP Stapling 反而是浪费流量，因为每次 TLS 握手都得发送一个 OCSP 状态信息。 我测试发现只有 www.digicert.com/www.douban.com 等少数站点启用了 ocsp stapling，www.baidu.com/www.google.com/www.zhihu.com 都未启用 ocsp stapling. 这导致了一些问题： Chrome/Firefox 等浏览器都会定期通过 OCSP 协议去请求 CA 机构的 OCSP 服务器验证证书状态，这可能会拖慢 HTTPS 协议的响应速度。 所谓的定期是指超过上一个 OCSP 响应的 nextUpdate 时间（一般为 7 天），或者如果该值为空的话，Firefox 默认 24h 后会重新查询 OCSP 状态。 因为客户端直接去请求 CA 机构的 OCSP 地址获取证书状态，这就导致 CA 机构可以获取到一些对应站点的用户信息（IP 地址、网络状态等）。 如果因为某些原因导致客户端无法访问 OCSP 服务器，会导致站点的初次访问时间用时变得很长。因为浏览器会每隔一阵时间就重新尝试去访问 OCSP 服务器！ 一个典型的例子就是 提高https载入速度，记一次nginx升级优化，因为 Let’s Encrypt 的 OCSP 服务器被 GFW 屏蔽，导致国内使用该证书的站点首次访问速度非常慢。 为了解决这两个问题，rfc6066 定义了 OCSP stapling 功能，它使服务器可以提前访问 OCSP 获取证书状态信息并缓存到本地。 在客户端使用 TLS 协议访问 HTTPS 服务时，服务端会直接在握手阶段将缓存的 OCSP 信息发送给客户端。 因为 OCSP","date":"2022-03-14","objectID":"/posts/about-tls-cert/:4:1","series":null,"tags":["Cryptography","密码学","HTTPS","TLS","SSL","OpenSSL","PKI","数字证书","证书","SSH","QUIC","HTTP/3","安全"],"title":"写给开发人员的实用密码学（八）—— 数字证书与 TLS 协议","uri":"/posts/about-tls-cert/#nginx-的-tls-协议配置"},{"categories":["tech"],"content":" 1. 密码套件与 TLS 历史版本密码套件（Cipher_suite）是 TLS 协议中一组用于实现安全通讯的密码学算法，类似于我们前面学习过的加密方案。 不同密码学算法的组合形成不同的密码套件，算法组合的差异使这些密码套件具有不同的性能与安全性，另外 TLS 协议的更新迭代也导致各密码套件拥有不同的兼容性。 通常越新推出的密码套件的安全性越高，但是兼容性就越差（旧设备不支持）。 密码套件的名称由它使用的各种密码学算法名称组成，而且有固定的格式，以 TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256 为例介绍下： TLS: 定义了此套件适用的协议，通常固定为 TLS ECDHE: 密钥交换算法 RSA: 数字证书认证算法 AES_128_GCM: 使用的对称加密方案，这是一个基于 AES 与 GCM 模式的对称认证加密方案，使用 128 位密钥 SHA256: 哈希函数，用于 HMAC 算法实现消息认证 TLS 固定使用 HMAC 算法进行消息认证 TLS 协议的前身是 SSL 协议，TLS/SSL 的发展历程展示如下： SSL/TLS 的历史版本 SSL 协议早在 2015 年就被各大主流浏览器废除了，TLS1.0 感觉也基本没站点在用了，这俩就直接跳过了。 下面分别介绍下 TLS1.1 TLS1.2 与 TLS1.3. TLS 1.1TLS 1.1 在 RFC4346 中定义，于 2006 年 4 月发布。 TLS 1.1 是 TLS 1.0 的一个补丁，主要更新包括： 添加对CBC攻击的保护 隐式初始向量 IV 被替换成一个显式的 IV 修复分组密码模式中填充算法的 bug 支持 IANA 登记的参数 TLS 1.1及其之前的算法曾经被广泛应用，它目前已知的缺陷如下： 不支持 PFS 完全前向保密 不支持 AEAD 认证加密算法 为了兼容性，保留了很多不安全的算法 TLS 1.1 已经不够安全了，不过一些陈年老站点或许还在使用它。 TLS 1.2TLS 1.2 在 RFC5246 中定义，于 2008 年 8 月发发布。 可选支持 PFS 完全前向保密 移除对 MD5 与 SHA-1 签名算法的支持 添加对 HMAC-SHA-256 及 HMAC-SHA-384 消息认证算法的支持 添加对 AEAD 加密认证方案的支持 去除 forback 回到 SSL 协议的能力，提升安全性 为了兼容性，保留了很多不安全的算法 如果你使用 TLS 1.2，需要小心地选择密码套件，避开不安全的套件，就能实现足够高的安全性。 TLS 1.3TLS 1.3 做了一次大刀阔斧的更新，是一个里程碑式的版本，其更新总结如下： 移除对如下算法的支持 哈希函数 SHA1/MD5 所有非 AEAD 加密认证的密码方案（CBC 模式） 移除对 RC4 与 3DES 加密算法的支持 移除了静态 RSA 与 DH 密钥交换算法 支持高性能的 Ed25519/Ed448 签名认证算法、X25519 密钥协商算法 支持高性能的 ChaCha20-Poly1305 对称认证加密方案 将密钥交换算法与公钥认证算法从密码套件中分离出来 比如原来的 TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256 密码套件将被拆分为 ECDHE 算法、RSA 身份认证算法、以及 TLS_AES_128_GCM_SHA256 密码套件 这样密码套件就只包含一个 AEAD 认证加密方案，以及一个哈希函数了 仅支持前向安全的密钥交换算法 DHE 或 ECDHE 支持最短 0-RTT 的 TLS 握手（会话恢复） TLS 1.3 从协议中删除了所有不安全的算法或协议，可以说只要你的通讯用了 TLS 1.3，那你的数据就安全了（当然前提是你的私钥没泄漏）。 如何设置 TLS 协议的版本、密码套件参数我们前面已经学习了对称加密、非对称加密、密钥交换三部分知识，对照 TLS 套件的名称，应该能很容易判断出哪些是安全的、哪些不够安全，哪些支持前向保密、哪些不支持。 一个非常好用的「站点 HTTPS 安全检测」网站是 https://myssl.com/，使用它测试知乎网的检测结果如下： SSL/TLS 的历史版本 能看到知乎为了兼容性，目前仍然支持 TLS1.0 与 TLS1.1，另外目前还不支持 TLS1.3. 此外，知乎仍然支持很多已经不安全的加密套件，myssl.com 专门使用黄色标识出了这些不安全的加密套件，我们总结下主要特征： 部分密码套件使用了不安全的对称加密算法 3DES 其他被标识为黄色的套件虽然使用了安全的对称加密算法，但是不支持 PFS 前向保密 此外 myssl.com 还列出了许多站点更详细的信息，包括 TLS1.3 的会话恢复，以及后面将会介绍的公钥固定、HTTP严格传输安全等信息： SSL/TLS 的历史版本 Nginx 的 TLS 协议配置以前为 Nginx 等程序配置 HTTPS 协议时，我最头疼的就是其中密码套件参数 ssl_ciphers，为了安全性，需要配置超长的一大堆选用的密码套件名称，我可以说一个都看不懂，但是为了把网站搞好还是得硬着头皮搜索复制粘贴，实际上也不清楚安全性导致咋样。 为了解决这个问题，Mozilla/DigitalOcean 都搞过流行 Web 服务器的 TLS 配置生成工具，比如 ssl-config - **mozilla，这个网站提供三个安全等级的配置**: 「Intermediate」: 查看生成出的 ssl-cipher 属性，发现它只支持 ECDHE/DHE 开头的算法。因此它保证前向保密。 对于需要通过浏览器访问的 API，推荐选择这个等级。 「Mordern」: 只支持 TLSv1.3，该协议废弃掉了过往所有不安全的算法，保证前向保密，安全性极高，性能也更好。 对于不需要通过浏览器等旧终端访问的 API，请直接选择这个等级。 「Old」: 除非你的用户使用非常老的终端进行访问，否则请不要考虑这个选项！ 可以点进去查看详细的 TLS 套件配置。 OCSP 证书验证协议 https://www.ssl.com/blogs/how-do-browsers-handle-revoked-ssl-tls-certificates/ https://imququ.com/post/why-can-not-turn-on-ocsp-stapling.html https://www.digicert.com/help/ 前面提到除了数字证书自带的有效期外，为了在私钥泄漏的情况下，能够吊销对应的证书，PKI 公钥基础设施还提供了 OCSP（Online Certificate Status Protocol）证书状态查询协议。 可以使用如下命令测试，确认站点是否启用了 ocsp stapling: $ openssl s_client -connect www.digicert.com:443 -servername www.digicert.com -status -tlsextdebug \u003c /dev/null 2\u003e\u00261 | grep -i \"OCSP response\" 如果输出包含 OCSP Response Status: successful 就说明站点支持 ocsp stapling， 如果输出内容为 OCSP response: no response sent 则说明站点不支持ocsp stapling。 实际上 Google/AWS 等大多数站点都不会启用也不需要启用 ocsp stapling，一是因为它们自己就是证书颁发机构，OCSP 服务器也归它们自己管，不存在隐私的问题。二是它们的 OCSP 服务器遍布全球，也不存在性能问题。 这种情况下开个 OCSP Stapling 反而是浪费流量，因为每次 TLS 握手都得发送一个 OCSP 状态信息。 我测试发现只有 www.digicert.com/www.douban.com 等少数站点启用了 ocsp stapling，www.baidu.com/www.google.com/www.zhihu.com 都未启用 ocsp stapling. 这导致了一些问题： Chrome/Firefox 等浏览器都会定期通过 OCSP 协议去请求 CA 机构的 OCSP 服务器验证证书状态，这可能会拖慢 HTTPS 协议的响应速度。 所谓的定期是指超过上一个 OCSP 响应的 nextUpdate 时间（一般为 7 天），或者如果该值为空的话，Firefox 默认 24h 后会重新查询 OCSP 状态。 因为客户端直接去请求 CA 机构的 OCSP 地址获取证书状态，这就导致 CA 机构可以获取到一些对应站点的用户信息（IP 地址、网络状态等）。 如果因为某些原因导致客户端无法访问 OCSP 服务器，会导致站点的初次访问时间用时变得很长。因为浏览器会每隔一阵时间就重新尝试去访问 OCSP 服务器！ 一个典型的例子就是 提高https载入速度，记一次nginx升级优化，因为 Let’s Encrypt 的 OCSP 服务器被 GFW 屏蔽，导致国内使用该证书的站点首次访问速度非常慢。 为了解决这两个问题，rfc6066 定义了 OCSP stapling 功能，它使服务器可以提前访问 OCSP 获取证书状态信息并缓存到本地。 在客户端使用 TLS 协议访问 HTTPS 服务时，服务端会直接在握手阶段将缓存的 OCSP 信息发送给客户端。 因为 OCSP","date":"2022-03-14","objectID":"/posts/about-tls-cert/:4:1","series":null,"tags":["Cryptography","密码学","HTTPS","TLS","SSL","OpenSSL","PKI","数字证书","证书","SSH","QUIC","HTTP/3","安全"],"title":"写给开发人员的实用密码学（八）—— 数字证书与 TLS 协议","uri":"/posts/about-tls-cert/#ocsp-证书验证协议"},{"categories":["tech"],"content":" 1. 密码套件与 TLS 历史版本密码套件（Cipher_suite）是 TLS 协议中一组用于实现安全通讯的密码学算法，类似于我们前面学习过的加密方案。 不同密码学算法的组合形成不同的密码套件，算法组合的差异使这些密码套件具有不同的性能与安全性，另外 TLS 协议的更新迭代也导致各密码套件拥有不同的兼容性。 通常越新推出的密码套件的安全性越高，但是兼容性就越差（旧设备不支持）。 密码套件的名称由它使用的各种密码学算法名称组成，而且有固定的格式，以 TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256 为例介绍下： TLS: 定义了此套件适用的协议，通常固定为 TLS ECDHE: 密钥交换算法 RSA: 数字证书认证算法 AES_128_GCM: 使用的对称加密方案，这是一个基于 AES 与 GCM 模式的对称认证加密方案，使用 128 位密钥 SHA256: 哈希函数，用于 HMAC 算法实现消息认证 TLS 固定使用 HMAC 算法进行消息认证 TLS 协议的前身是 SSL 协议，TLS/SSL 的发展历程展示如下： SSL/TLS 的历史版本 SSL 协议早在 2015 年就被各大主流浏览器废除了，TLS1.0 感觉也基本没站点在用了，这俩就直接跳过了。 下面分别介绍下 TLS1.1 TLS1.2 与 TLS1.3. TLS 1.1TLS 1.1 在 RFC4346 中定义，于 2006 年 4 月发布。 TLS 1.1 是 TLS 1.0 的一个补丁，主要更新包括： 添加对CBC攻击的保护 隐式初始向量 IV 被替换成一个显式的 IV 修复分组密码模式中填充算法的 bug 支持 IANA 登记的参数 TLS 1.1及其之前的算法曾经被广泛应用，它目前已知的缺陷如下： 不支持 PFS 完全前向保密 不支持 AEAD 认证加密算法 为了兼容性，保留了很多不安全的算法 TLS 1.1 已经不够安全了，不过一些陈年老站点或许还在使用它。 TLS 1.2TLS 1.2 在 RFC5246 中定义，于 2008 年 8 月发发布。 可选支持 PFS 完全前向保密 移除对 MD5 与 SHA-1 签名算法的支持 添加对 HMAC-SHA-256 及 HMAC-SHA-384 消息认证算法的支持 添加对 AEAD 加密认证方案的支持 去除 forback 回到 SSL 协议的能力，提升安全性 为了兼容性，保留了很多不安全的算法 如果你使用 TLS 1.2，需要小心地选择密码套件，避开不安全的套件，就能实现足够高的安全性。 TLS 1.3TLS 1.3 做了一次大刀阔斧的更新，是一个里程碑式的版本，其更新总结如下： 移除对如下算法的支持 哈希函数 SHA1/MD5 所有非 AEAD 加密认证的密码方案（CBC 模式） 移除对 RC4 与 3DES 加密算法的支持 移除了静态 RSA 与 DH 密钥交换算法 支持高性能的 Ed25519/Ed448 签名认证算法、X25519 密钥协商算法 支持高性能的 ChaCha20-Poly1305 对称认证加密方案 将密钥交换算法与公钥认证算法从密码套件中分离出来 比如原来的 TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256 密码套件将被拆分为 ECDHE 算法、RSA 身份认证算法、以及 TLS_AES_128_GCM_SHA256 密码套件 这样密码套件就只包含一个 AEAD 认证加密方案，以及一个哈希函数了 仅支持前向安全的密钥交换算法 DHE 或 ECDHE 支持最短 0-RTT 的 TLS 握手（会话恢复） TLS 1.3 从协议中删除了所有不安全的算法或协议，可以说只要你的通讯用了 TLS 1.3，那你的数据就安全了（当然前提是你的私钥没泄漏）。 如何设置 TLS 协议的版本、密码套件参数我们前面已经学习了对称加密、非对称加密、密钥交换三部分知识，对照 TLS 套件的名称，应该能很容易判断出哪些是安全的、哪些不够安全，哪些支持前向保密、哪些不支持。 一个非常好用的「站点 HTTPS 安全检测」网站是 https://myssl.com/，使用它测试知乎网的检测结果如下： SSL/TLS 的历史版本 能看到知乎为了兼容性，目前仍然支持 TLS1.0 与 TLS1.1，另外目前还不支持 TLS1.3. 此外，知乎仍然支持很多已经不安全的加密套件，myssl.com 专门使用黄色标识出了这些不安全的加密套件，我们总结下主要特征： 部分密码套件使用了不安全的对称加密算法 3DES 其他被标识为黄色的套件虽然使用了安全的对称加密算法，但是不支持 PFS 前向保密 此外 myssl.com 还列出了许多站点更详细的信息，包括 TLS1.3 的会话恢复，以及后面将会介绍的公钥固定、HTTP严格传输安全等信息： SSL/TLS 的历史版本 Nginx 的 TLS 协议配置以前为 Nginx 等程序配置 HTTPS 协议时，我最头疼的就是其中密码套件参数 ssl_ciphers，为了安全性，需要配置超长的一大堆选用的密码套件名称，我可以说一个都看不懂，但是为了把网站搞好还是得硬着头皮搜索复制粘贴，实际上也不清楚安全性导致咋样。 为了解决这个问题，Mozilla/DigitalOcean 都搞过流行 Web 服务器的 TLS 配置生成工具，比如 ssl-config - **mozilla，这个网站提供三个安全等级的配置**: 「Intermediate」: 查看生成出的 ssl-cipher 属性，发现它只支持 ECDHE/DHE 开头的算法。因此它保证前向保密。 对于需要通过浏览器访问的 API，推荐选择这个等级。 「Mordern」: 只支持 TLSv1.3，该协议废弃掉了过往所有不安全的算法，保证前向保密，安全性极高，性能也更好。 对于不需要通过浏览器等旧终端访问的 API，请直接选择这个等级。 「Old」: 除非你的用户使用非常老的终端进行访问，否则请不要考虑这个选项！ 可以点进去查看详细的 TLS 套件配置。 OCSP 证书验证协议 https://www.ssl.com/blogs/how-do-browsers-handle-revoked-ssl-tls-certificates/ https://imququ.com/post/why-can-not-turn-on-ocsp-stapling.html https://www.digicert.com/help/ 前面提到除了数字证书自带的有效期外，为了在私钥泄漏的情况下，能够吊销对应的证书，PKI 公钥基础设施还提供了 OCSP（Online Certificate Status Protocol）证书状态查询协议。 可以使用如下命令测试，确认站点是否启用了 ocsp stapling: $ openssl s_client -connect www.digicert.com:443 -servername www.digicert.com -status -tlsextdebug \u003c /dev/null 2\u003e\u00261 | grep -i \"OCSP response\" 如果输出包含 OCSP Response Status: successful 就说明站点支持 ocsp stapling， 如果输出内容为 OCSP response: no response sent 则说明站点不支持ocsp stapling。 实际上 Google/AWS 等大多数站点都不会启用也不需要启用 ocsp stapling，一是因为它们自己就是证书颁发机构，OCSP 服务器也归它们自己管，不存在隐私的问题。二是它们的 OCSP 服务器遍布全球，也不存在性能问题。 这种情况下开个 OCSP Stapling 反而是浪费流量，因为每次 TLS 握手都得发送一个 OCSP 状态信息。 我测试发现只有 www.digicert.com/www.douban.com 等少数站点启用了 ocsp stapling，www.baidu.com/www.google.com/www.zhihu.com 都未启用 ocsp stapling. 这导致了一些问题： Chrome/Firefox 等浏览器都会定期通过 OCSP 协议去请求 CA 机构的 OCSP 服务器验证证书状态，这可能会拖慢 HTTPS 协议的响应速度。 所谓的定期是指超过上一个 OCSP 响应的 nextUpdate 时间（一般为 7 天），或者如果该值为空的话，Firefox 默认 24h 后会重新查询 OCSP 状态。 因为客户端直接去请求 CA 机构的 OCSP 地址获取证书状态，这就导致 CA 机构可以获取到一些对应站点的用户信息（IP 地址、网络状态等）。 如果因为某些原因导致客户端无法访问 OCSP 服务器，会导致站点的初次访问时间用时变得很长。因为浏览器会每隔一阵时间就重新尝试去访问 OCSP 服务器！ 一个典型的例子就是 提高https载入速度，记一次nginx升级优化，因为 Let’s Encrypt 的 OCSP 服务器被 GFW 屏蔽，导致国内使用该证书的站点首次访问速度非常慢。 为了解决这两个问题，rfc6066 定义了 OCSP stapling 功能，它使服务器可以提前访问 OCSP 获取证书状态信息并缓存到本地。 在客户端使用 TLS 协议访问 HTTPS 服务时，服务端会直接在握手阶段将缓存的 OCSP 信息发送给客户端。 因为 OCSP","date":"2022-03-14","objectID":"/posts/about-tls-cert/:4:1","series":null,"tags":["Cryptography","密码学","HTTPS","TLS","SSL","OpenSSL","PKI","数字证书","证书","SSH","QUIC","HTTP/3","安全"],"title":"写给开发人员的实用密码学（八）—— 数字证书与 TLS 协议","uri":"/posts/about-tls-cert/#alpn-应用层协议协商"},{"categories":["tech"],"content":" 2. mTLS 双向认证TLS 协议（tls1.0+，RFC: TLS1.2 - RFC5246）也定义了可选的服务端请求验证客户端证书的方法。这 个方法是可选的。如果使用上这个方法，那客户端和服务端就会在 TLS 协议的握手阶段进行互相认证。这种验证方式被称为双向 TLS 认证(mTLS, mutual TLS)。 传统的「TLS 单向认证」技术，只在客户端去验证服务端是否可信。 而「TLS 双向认证（mTLS）」，则添加了服务端验证客户端是否可信的步骤（第三步）: 客户端发起请求 「验证服务端是否可信」: 服务端将自己的 TLS 证书发送给客户端，客户端通过自己的 CA 证书链验证这个服务端证书。 「验证客户端是否可信」: 客户端将自己的 TLS 证书发送给服务端，服务端使用它的 CA 证书链验证该客户端证书。 协商对称加密算法及密钥 使用对称加密进行后续通信。 因为相比传统的 TLS，mTLS 只是添加了「验证客户端」这样一个步骤，所以这项技术也被称为「Client Authetication」. mTLS 需要用到两套 TLS 证书: 服务端证书: 这个证书签名已经介绍过了。 客户端证书: 客户端证书貌似对证书信息（如 CN/SAN 域名）没有任何要求，只要证书能通过 CA 签名验证就行。 使用 openssl 生成 TLS 客户端证书（ca 和 csr.conf 可以直接使用前面生成服务端证书用到的，也可以另外生成）: # 1. 生成 2048 位 的 RSA 密钥 openssl genrsa -out client.key 2048 # 2. 通过第一步编写的配置文件，生成证书签名请求 openssl req -new -key client.key -out client.csr -config csr.conf # 3. 生成最终的证书，这里指定证书有效期 3650 天 ### 使用前面生成的 ca 证书对客户端证书进行签名（客户端和服务端共用 ca 证书） openssl x509 -req -in client.csr -CA ca.crt -CAkey ca.key \\ -CAcreateserial -out client.crt -days 3650 \\ -extensions v3_ext -extfile csr.conf mTLS 的应用场景主要在「零信任网络架构」，或者叫「无边界网络」中。 比如微服务之间的互相访问，就可以使用 mTLS。 这样就能保证每个 RPC 调用的客户端，都是其他微服务（或者别的可信方），防止黑客入侵后为所欲为。 目前查到如下几个Web服务器/代理支持 mTLS: Traefik: Docs - Client Authentication (mTLS) Nginx: Using NGINX Reverse Proxy for client certificate authentication 主要参数是两个: ssl_client_certificate /etc/nginx/client-ca.pem 和 ssl_verify_client on mTLS 的安全性如果将 mTLS 用在 App 安全上，存在的风险是: 客户端中隐藏的证书是否可以被提取出来，或者黑客能否 Hook 进 App 中，直接使用证书发送信息。 如果客户端私钥设置了「密码（passphrase）」，那这个密码是否能很容易被逆向出来？ mTLS 和「公钥锁定/证书锁定」对比: 公钥锁定/证书锁定: 只在客户端进行验证。 但是在服务端没有进行验证。这样就无法鉴别并拒绝第三方应用（爬虫）的请求。 加强安全的方法，是通过某种算法生成动态的签名。爬虫生成不出来这个签名，请求就被拒绝。 mTLS: 服务端和客户端都要验证对方。 保证双边可信，在客户端证书不被破解的情况下，就能 Ban 掉所有的爬虫或代理技术。 ","date":"2022-03-14","objectID":"/posts/about-tls-cert/:4:2","series":null,"tags":["Cryptography","密码学","HTTPS","TLS","SSL","OpenSSL","PKI","数字证书","证书","SSH","QUIC","HTTP/3","安全"],"title":"写给开发人员的实用密码学（八）—— 数字证书与 TLS 协议","uri":"/posts/about-tls-cert/#2-mtls-双向认证"},{"categories":["tech"],"content":" 2. mTLS 双向认证TLS 协议（tls1.0+，RFC: TLS1.2 - RFC5246）也定义了可选的服务端请求验证客户端证书的方法。这 个方法是可选的。如果使用上这个方法，那客户端和服务端就会在 TLS 协议的握手阶段进行互相认证。这种验证方式被称为双向 TLS 认证(mTLS, mutual TLS)。 传统的「TLS 单向认证」技术，只在客户端去验证服务端是否可信。 而「TLS 双向认证（mTLS）」，则添加了服务端验证客户端是否可信的步骤（第三步）: 客户端发起请求 「验证服务端是否可信」: 服务端将自己的 TLS 证书发送给客户端，客户端通过自己的 CA 证书链验证这个服务端证书。 「验证客户端是否可信」: 客户端将自己的 TLS 证书发送给服务端，服务端使用它的 CA 证书链验证该客户端证书。 协商对称加密算法及密钥 使用对称加密进行后续通信。 因为相比传统的 TLS，mTLS 只是添加了「验证客户端」这样一个步骤，所以这项技术也被称为「Client Authetication」. mTLS 需要用到两套 TLS 证书: 服务端证书: 这个证书签名已经介绍过了。 客户端证书: 客户端证书貌似对证书信息（如 CN/SAN 域名）没有任何要求，只要证书能通过 CA 签名验证就行。 使用 openssl 生成 TLS 客户端证书（ca 和 csr.conf 可以直接使用前面生成服务端证书用到的，也可以另外生成）: # 1. 生成 2048 位 的 RSA 密钥 openssl genrsa -out client.key 2048 # 2. 通过第一步编写的配置文件，生成证书签名请求 openssl req -new -key client.key -out client.csr -config csr.conf # 3. 生成最终的证书，这里指定证书有效期 3650 天 ### 使用前面生成的 ca 证书对客户端证书进行签名（客户端和服务端共用 ca 证书） openssl x509 -req -in client.csr -CA ca.crt -CAkey ca.key \\ -CAcreateserial -out client.crt -days 3650 \\ -extensions v3_ext -extfile csr.conf mTLS 的应用场景主要在「零信任网络架构」，或者叫「无边界网络」中。 比如微服务之间的互相访问，就可以使用 mTLS。 这样就能保证每个 RPC 调用的客户端，都是其他微服务（或者别的可信方），防止黑客入侵后为所欲为。 目前查到如下几个Web服务器/代理支持 mTLS: Traefik: Docs - Client Authentication (mTLS) Nginx: Using NGINX Reverse Proxy for client certificate authentication 主要参数是两个: ssl_client_certificate /etc/nginx/client-ca.pem 和 ssl_verify_client on mTLS 的安全性如果将 mTLS 用在 App 安全上，存在的风险是: 客户端中隐藏的证书是否可以被提取出来，或者黑客能否 Hook 进 App 中，直接使用证书发送信息。 如果客户端私钥设置了「密码（passphrase）」，那这个密码是否能很容易被逆向出来？ mTLS 和「公钥锁定/证书锁定」对比: 公钥锁定/证书锁定: 只在客户端进行验证。 但是在服务端没有进行验证。这样就无法鉴别并拒绝第三方应用（爬虫）的请求。 加强安全的方法，是通过某种算法生成动态的签名。爬虫生成不出来这个签名，请求就被拒绝。 mTLS: 服务端和客户端都要验证对方。 保证双边可信，在客户端证书不被破解的情况下，就能 Ban 掉所有的爬虫或代理技术。 ","date":"2022-03-14","objectID":"/posts/about-tls-cert/:4:2","series":null,"tags":["Cryptography","密码学","HTTPS","TLS","SSL","OpenSSL","PKI","数字证书","证书","SSH","QUIC","HTTP/3","安全"],"title":"写给开发人员的实用密码学（八）—— 数字证书与 TLS 协议","uri":"/posts/about-tls-cert/#mtls-的安全性"},{"categories":["tech"],"content":" 3. 其他加密通讯协议 SSH 协议首先最容易想到的应该就是是 SSH 协议（Secure SHell protocol）。SSH 与 TLS 一样都能提供加密通讯，是 PKI 公钥基础设施的早期先驱者之一。 OpenSSH 应用最广泛的 SSH 实现，它使用 SSH Key 而非数字证书进行身份认证，这主要是因为 OpenSSH 仅用于用户与主机之间的安全通信，不需要记录 X.509 这么繁多的信息。 我们来手动生成个 OpenSSH ed25519 密钥对试试（RSA 的生成命令完全类似）： ❯ ssh-keygen -t ed25519 Generating public/private ed25519 key pair. Enter file in which to save the key (/Users/admin/.ssh/id_ed25519): ed25519-key Enter passphrase (empty for no passphrase): Enter same passphrase again: Your identification has been saved in ed25519-key. Your public key has been saved in ed25519-key.pub. The key fingerprint is: SHA256:jgeuWVflhNXXrDDzUtW6ZV1lpBWNAj0Rstizh9Lbyg0 admin@ryan-MacBook-Pro.local The key's randomart image is: +--[ED25519 256]--+ | oo++ *%| | o =B ++B| | . = oO.+o| | . B. + +| | . S = o. + | | . + o + . | | + + E . | | + o . + | | o o . | +----[SHA256]-----+ ❯ cat ed25519-key -----BEGIN OPENSSH PRIVATE KEY----- b3BlbnNzaC1rZXktdjEAAAAABG5vbmUAAAAEbm9uZQAAAAAAAAABAAAAMwAAAAtzc2gtZW QyNTUxOQAAACDux4KnrKXVs4iR9mPZnSpur5207ceyMiZP+CDnXdooMQAAAKDnHOSY5xzk mAAAAAtzc2gtZWQyNTUxOQAAACDux4KnrKXVs4iR9mPZnSpur5207ceyMiZP+CDnXdooMQ AAAEADkVL1gZHAvBx4M5+UjVVL7ltVOC4r9tdR23CoI9iV1O7HgqespdWziJH2Y9mdKm6v nbTtx7IyJk/4IOdd2igxAAAAHGFkbWluQHJ5YW4tTWFjQm9vay1Qcm8ubG9jYWwB -----END OPENSSH PRIVATE KEY----- ❯ cat ed25519-key.pub ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAIO7HgqespdWziJH2Y9mdKm6vnbTtx7IyJk/4IOdd2igx admin@ryan-MacBook-Pro.local 可以看到 SSH Key 的结构非常简单，仅包含如下三个部分： 密钥对类型: 最常见的是 ssh-rsa，另外由于安全性目前更推荐使用 ssh-ed25519 公钥的 Base64 字符串 一个 Comment，通常包含这个 Key 的用途，或者 Key 所有者的邮箱地址 通过我们前面学的非对称密码学知识可以知道，公钥能直接从私钥生成，假设你的 ssh 公钥丢失，可以通过如下命令重新生成出公钥： ssh-keygen -y -f xxx_rsa \u003e xxx_rsa.pub HTTP/3 与 QUIC 协议QUIC 协议，是 Google 研发并推动标准化的 TCP 协议的替代品， QUIC 是基于 UDP 协议实现的。基于 QUIC 提出的 HTTP over QUIC 协议已被标准化为 RFC 9114 - HTTP/3，它做了很多大刀阔斧的改革： 传输层协议从 TCP 改成了 UDP，QUIC 自己实现的数据的可靠传输、按序到达、拥塞控制 也就是说 QUIC 绕过了陈旧的内核 TCP 协议实现，直接在用户空间实现了这些功能 通过另起炉灶，它解决了一些 TCP 协议的痛点：队头阻塞、握手延迟高、特性迭代慢、拥塞控制算法不佳等问题 在 TLS1.3 出现之前，QUIC 实现了自己的加密方案 QUIC Crypto 以取代陈旧的 TLS 协议，同时兼容现有的数字证书体系 QUIC Crypto 的特点是它直接在应用层进行加密通讯的握手，并且恢复通信时可以通过缓存实现 0RTT 握手 也就说 QUIC 通过另起炉灶，解决了 TLS 的安全问题，以及握手延迟高的问题 总结一下就是，旧的实验性 HTTP-over-QUIC 协议，重新实现了 HTTP+TLS+TCP 三种协议并将它们整合到一起，这带来了极佳的性能，但也使它变得非常复杂。 QUIC 的 0RTT 握手是一个非常妙的想法，可以显著降低握手时延，TLS1.3 的设计者们将它纳入了 TLS1.3 标准中。 由于 TLS1.3 的良好特性，在 TLS1.3 协议发布后，新的 QUIC 标准 RFC 9001 已经使用 TLS1.3 取代了实验阶段使用的 QUIC Crypto 加密方案，目前只有 Chromium/Chrome 仍然支持 QUIC Crypto，其他 QUIC 实现基本都只支持 TLS1.3, 详见 QUIC Implementations. ","date":"2022-03-14","objectID":"/posts/about-tls-cert/:4:3","series":null,"tags":["Cryptography","密码学","HTTPS","TLS","SSL","OpenSSL","PKI","数字证书","证书","SSH","QUIC","HTTP/3","安全"],"title":"写给开发人员的实用密码学（八）—— 数字证书与 TLS 协议","uri":"/posts/about-tls-cert/#3-其他加密通讯协议"},{"categories":["tech"],"content":" 3. 其他加密通讯协议 SSH 协议首先最容易想到的应该就是是 SSH 协议（Secure SHell protocol）。SSH 与 TLS 一样都能提供加密通讯，是 PKI 公钥基础设施的早期先驱者之一。 OpenSSH 应用最广泛的 SSH 实现，它使用 SSH Key 而非数字证书进行身份认证，这主要是因为 OpenSSH 仅用于用户与主机之间的安全通信，不需要记录 X.509 这么繁多的信息。 我们来手动生成个 OpenSSH ed25519 密钥对试试（RSA 的生成命令完全类似）： ❯ ssh-keygen -t ed25519 Generating public/private ed25519 key pair. Enter file in which to save the key (/Users/admin/.ssh/id_ed25519): ed25519-key Enter passphrase (empty for no passphrase): Enter same passphrase again: Your identification has been saved in ed25519-key. Your public key has been saved in ed25519-key.pub. The key fingerprint is: SHA256:jgeuWVflhNXXrDDzUtW6ZV1lpBWNAj0Rstizh9Lbyg0 admin@ryan-MacBook-Pro.local The key's randomart image is: +--[ED25519 256]--+ | oo++ *%| | o =B ++B| | . = oO.+o| | . B. + +| | . S = o. + | | . + o + . | | + + E . | | + o . + | | o o . | +----[SHA256]-----+ ❯ cat ed25519-key -----BEGIN OPENSSH PRIVATE KEY----- b3BlbnNzaC1rZXktdjEAAAAABG5vbmUAAAAEbm9uZQAAAAAAAAABAAAAMwAAAAtzc2gtZW QyNTUxOQAAACDux4KnrKXVs4iR9mPZnSpur5207ceyMiZP+CDnXdooMQAAAKDnHOSY5xzk mAAAAAtzc2gtZWQyNTUxOQAAACDux4KnrKXVs4iR9mPZnSpur5207ceyMiZP+CDnXdooMQ AAAEADkVL1gZHAvBx4M5+UjVVL7ltVOC4r9tdR23CoI9iV1O7HgqespdWziJH2Y9mdKm6v nbTtx7IyJk/4IOdd2igxAAAAHGFkbWluQHJ5YW4tTWFjQm9vay1Qcm8ubG9jYWwB -----END OPENSSH PRIVATE KEY----- ❯ cat ed25519-key.pub ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAIO7HgqespdWziJH2Y9mdKm6vnbTtx7IyJk/4IOdd2igx admin@ryan-MacBook-Pro.local 可以看到 SSH Key 的结构非常简单，仅包含如下三个部分： 密钥对类型: 最常见的是 ssh-rsa，另外由于安全性目前更推荐使用 ssh-ed25519 公钥的 Base64 字符串 一个 Comment，通常包含这个 Key 的用途，或者 Key 所有者的邮箱地址 通过我们前面学的非对称密码学知识可以知道，公钥能直接从私钥生成，假设你的 ssh 公钥丢失，可以通过如下命令重新生成出公钥： ssh-keygen -y -f xxx_rsa \u003e xxx_rsa.pub HTTP/3 与 QUIC 协议QUIC 协议，是 Google 研发并推动标准化的 TCP 协议的替代品， QUIC 是基于 UDP 协议实现的。基于 QUIC 提出的 HTTP over QUIC 协议已被标准化为 RFC 9114 - HTTP/3，它做了很多大刀阔斧的改革： 传输层协议从 TCP 改成了 UDP，QUIC 自己实现的数据的可靠传输、按序到达、拥塞控制 也就是说 QUIC 绕过了陈旧的内核 TCP 协议实现，直接在用户空间实现了这些功能 通过另起炉灶，它解决了一些 TCP 协议的痛点：队头阻塞、握手延迟高、特性迭代慢、拥塞控制算法不佳等问题 在 TLS1.3 出现之前，QUIC 实现了自己的加密方案 QUIC Crypto 以取代陈旧的 TLS 协议，同时兼容现有的数字证书体系 QUIC Crypto 的特点是它直接在应用层进行加密通讯的握手，并且恢复通信时可以通过缓存实现 0RTT 握手 也就说 QUIC 通过另起炉灶，解决了 TLS 的安全问题，以及握手延迟高的问题 总结一下就是，旧的实验性 HTTP-over-QUIC 协议，重新实现了 HTTP+TLS+TCP 三种协议并将它们整合到一起，这带来了极佳的性能，但也使它变得非常复杂。 QUIC 的 0RTT 握手是一个非常妙的想法，可以显著降低握手时延，TLS1.3 的设计者们将它纳入了 TLS1.3 标准中。 由于 TLS1.3 的良好特性，在 TLS1.3 协议发布后，新的 QUIC 标准 RFC 9001 已经使用 TLS1.3 取代了实验阶段使用的 QUIC Crypto 加密方案，目前只有 Chromium/Chrome 仍然支持 QUIC Crypto，其他 QUIC 实现基本都只支持 TLS1.3, 详见 QUIC Implementations. ","date":"2022-03-14","objectID":"/posts/about-tls-cert/:4:3","series":null,"tags":["Cryptography","密码学","HTTPS","TLS","SSL","OpenSSL","PKI","数字证书","证书","SSH","QUIC","HTTP/3","安全"],"title":"写给开发人员的实用密码学（八）—— 数字证书与 TLS 协议","uri":"/posts/about-tls-cert/#ssh-协议"},{"categories":["tech"],"content":" 3. 其他加密通讯协议 SSH 协议首先最容易想到的应该就是是 SSH 协议（Secure SHell protocol）。SSH 与 TLS 一样都能提供加密通讯，是 PKI 公钥基础设施的早期先驱者之一。 OpenSSH 应用最广泛的 SSH 实现，它使用 SSH Key 而非数字证书进行身份认证，这主要是因为 OpenSSH 仅用于用户与主机之间的安全通信，不需要记录 X.509 这么繁多的信息。 我们来手动生成个 OpenSSH ed25519 密钥对试试（RSA 的生成命令完全类似）： ❯ ssh-keygen -t ed25519 Generating public/private ed25519 key pair. Enter file in which to save the key (/Users/admin/.ssh/id_ed25519): ed25519-key Enter passphrase (empty for no passphrase): Enter same passphrase again: Your identification has been saved in ed25519-key. Your public key has been saved in ed25519-key.pub. The key fingerprint is: SHA256:jgeuWVflhNXXrDDzUtW6ZV1lpBWNAj0Rstizh9Lbyg0 admin@ryan-MacBook-Pro.local The key's randomart image is: +--[ED25519 256]--+ | oo++ *%| | o =B ++B| | . = oO.+o| | . B. + +| | . S = o. + | | . + o + . | | + + E . | | + o . + | | o o . | +----[SHA256]-----+ ❯ cat ed25519-key -----BEGIN OPENSSH PRIVATE KEY----- b3BlbnNzaC1rZXktdjEAAAAABG5vbmUAAAAEbm9uZQAAAAAAAAABAAAAMwAAAAtzc2gtZW QyNTUxOQAAACDux4KnrKXVs4iR9mPZnSpur5207ceyMiZP+CDnXdooMQAAAKDnHOSY5xzk mAAAAAtzc2gtZWQyNTUxOQAAACDux4KnrKXVs4iR9mPZnSpur5207ceyMiZP+CDnXdooMQ AAAEADkVL1gZHAvBx4M5+UjVVL7ltVOC4r9tdR23CoI9iV1O7HgqespdWziJH2Y9mdKm6v nbTtx7IyJk/4IOdd2igxAAAAHGFkbWluQHJ5YW4tTWFjQm9vay1Qcm8ubG9jYWwB -----END OPENSSH PRIVATE KEY----- ❯ cat ed25519-key.pub ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAIO7HgqespdWziJH2Y9mdKm6vnbTtx7IyJk/4IOdd2igx admin@ryan-MacBook-Pro.local 可以看到 SSH Key 的结构非常简单，仅包含如下三个部分： 密钥对类型: 最常见的是 ssh-rsa，另外由于安全性目前更推荐使用 ssh-ed25519 公钥的 Base64 字符串 一个 Comment，通常包含这个 Key 的用途，或者 Key 所有者的邮箱地址 通过我们前面学的非对称密码学知识可以知道，公钥能直接从私钥生成，假设你的 ssh 公钥丢失，可以通过如下命令重新生成出公钥： ssh-keygen -y -f xxx_rsa \u003e xxx_rsa.pub HTTP/3 与 QUIC 协议QUIC 协议，是 Google 研发并推动标准化的 TCP 协议的替代品， QUIC 是基于 UDP 协议实现的。基于 QUIC 提出的 HTTP over QUIC 协议已被标准化为 RFC 9114 - HTTP/3，它做了很多大刀阔斧的改革： 传输层协议从 TCP 改成了 UDP，QUIC 自己实现的数据的可靠传输、按序到达、拥塞控制 也就是说 QUIC 绕过了陈旧的内核 TCP 协议实现，直接在用户空间实现了这些功能 通过另起炉灶，它解决了一些 TCP 协议的痛点：队头阻塞、握手延迟高、特性迭代慢、拥塞控制算法不佳等问题 在 TLS1.3 出现之前，QUIC 实现了自己的加密方案 QUIC Crypto 以取代陈旧的 TLS 协议，同时兼容现有的数字证书体系 QUIC Crypto 的特点是它直接在应用层进行加密通讯的握手，并且恢复通信时可以通过缓存实现 0RTT 握手 也就说 QUIC 通过另起炉灶，解决了 TLS 的安全问题，以及握手延迟高的问题 总结一下就是，旧的实验性 HTTP-over-QUIC 协议，重新实现了 HTTP+TLS+TCP 三种协议并将它们整合到一起，这带来了极佳的性能，但也使它变得非常复杂。 QUIC 的 0RTT 握手是一个非常妙的想法，可以显著降低握手时延，TLS1.3 的设计者们将它纳入了 TLS1.3 标准中。 由于 TLS1.3 的良好特性，在 TLS1.3 协议发布后，新的 QUIC 标准 RFC 9001 已经使用 TLS1.3 取代了实验阶段使用的 QUIC Crypto 加密方案，目前只有 Chromium/Chrome 仍然支持 QUIC Crypto，其他 QUIC 实现基本都只支持 TLS1.3, 详见 QUIC Implementations. ","date":"2022-03-14","objectID":"/posts/about-tls-cert/:4:3","series":null,"tags":["Cryptography","密码学","HTTPS","TLS","SSL","OpenSSL","PKI","数字证书","证书","SSH","QUIC","HTTP/3","安全"],"title":"写给开发人员的实用密码学（八）—— 数字证书与 TLS 协议","uri":"/posts/about-tls-cert/#http3-与-quic-协议"},{"categories":["tech"],"content":" 4. TLS 协议攻防战 1. 证书锁定（Certifacte Pining）技术即使使用了 TLS 协议对流量进行加密，并且保证了前向保密，也无法保证流量不被代理！ 这是因为客户端大多是直接依靠了操作系统内置的 CA 证书库进行证书验证，而 Fiddler 等代理工具可以将自己的 CA 证书添加到该证书库中。 为了防止流量被 Fiddler 等工具使用上述方式监听流量，出现了「证书锁定（Certifacte Pining, 或者 SSL Pinning）」技术。 方法是在客户端中硬编码证书的指纹（Hash值，或者直接保存整个证书的内容也行），在建立 TLS 连接前，先计算使用的证书的指纹是否匹配，否则就中断连接。 这种锁定方式需要以下几个前提才能确保流量不被监听: 客户端中硬编码的证书指纹不会被篡改。 指纹验证不能被绕过。 目前有公开技术（XPosed+JustTrustMe）能破解 Android 上常见的 HTTPS 请求库，直接绕过证书检查。 针对上述问题，可以考虑加大绕过的难度。或者 App 检测自己是否运行在 Xposed 等虚拟环境下。 用于 TLS 协议的证书不会频繁更换。（如果更换了，指纹就对不上了。） 而对于第三方的 API，因为我们不知道它们会不会更换 TLS 证书，就不能直接将证书指纹硬编码在客户端中。 这时可以考虑从服务端获取这些 API 的证书指纹（附带私钥签名用于防伪造）。 为了实现证书的轮转(rotation)，可以在新版本的客户端中包含多个证书指纹，这样能保证同时有多个可信证书，达成证书的轮转。（类比 JWT 的公钥轮转机制） 证书锁定技术几乎等同于 SSH 协议的 StrictHostKeyChecking 选项，客户端会验证服务端的公钥指纹（key fingerprint），验证不通过则断开连接。 2. 公钥锁定（Public Key Pining）技术前面介绍过证书的结构，它其实包含了公钥、有效期与一系列的其他信息。 使用了证书锁定技术，会导致证书的有效期也被锁定，APK 内的证书指纹就必须随着证书一起更新。 更好的做法是指锁定证书中的公钥，即「公钥锁定」技术。 「公钥锁定」比「证书锁定」更灵活，这样证书本身其实就可以直接轮转了（证书有过期时间），而不需要一个旧证书和新证书共存的中间时期。 「公钥锁定」是更推荐的锁定技术。 3. HTTPS 严格传输安全 - HSTSHSTS，即 HTTP Strict Transport Security，是一项安全技术，它允许服务端在返回 HTTPS 响应时，通过 Headers 明确要求客户端，在之后的一段时间内必须使用安全的 HTTPS 协议访问服务端。 比如 https://example.com/ 的响应头中有 Strict-Transport-Security: max-age=31536000; includeSubDomains，表示服务端要求客户端（比如浏览器）： 在接下来的 31536000 秒（即一年）中，客户端向 example.com 或其子域名发送 HTTP 请求时，必须采用HTTPS来发起连接。 比如用户在浏览器地址栏输入 http://example.com/ 时，浏览器应自动将 http 改写为 https 再发起请求 在接下来的 31536000 秒（即一年）中，如果 example.com 服务器提供的证书无效，用户不能忽略浏览器的证书警告继续访问网站。 也就是说一旦证书失效，站点将完全无法访问，直至服务端修复证书问题。 一旦证书失效，HTTPS 其实就不是严格安全的了，可能会遭遇中间人攻击。 3. TLS 协议的逆向手段要获取一个应用的 HTTPS 数据，有两个方向: 服务端入侵: 现代应用的服务端突破难度通常都比较客户端高，注入等漏洞底层框架就有处理。 不过如果你获得了服务器 root 权限，可以在 openssl 上做文章，比如篡改 openssl？ 客户端逆向+爬虫: 客户端是离用户最近的地方，也是最容易被突破的地方。 mTLS 常见的破解手段，是找到老版本的安装包，发现很容易就能提取出客户端证书。。 wiki 列出了一些 TLS 协议的安全问题：https://en.wikipedia.org/wiki/Transport_Layer_Security#Security TO BE DONE… ","date":"2022-03-14","objectID":"/posts/about-tls-cert/:4:4","series":null,"tags":["Cryptography","密码学","HTTPS","TLS","SSL","OpenSSL","PKI","数字证书","证书","SSH","QUIC","HTTP/3","安全"],"title":"写给开发人员的实用密码学（八）—— 数字证书与 TLS 协议","uri":"/posts/about-tls-cert/#4-tls-协议攻防战"},{"categories":["tech"],"content":" 4. TLS 协议攻防战 1. 证书锁定（Certifacte Pining）技术即使使用了 TLS 协议对流量进行加密，并且保证了前向保密，也无法保证流量不被代理！ 这是因为客户端大多是直接依靠了操作系统内置的 CA 证书库进行证书验证，而 Fiddler 等代理工具可以将自己的 CA 证书添加到该证书库中。 为了防止流量被 Fiddler 等工具使用上述方式监听流量，出现了「证书锁定（Certifacte Pining, 或者 SSL Pinning）」技术。 方法是在客户端中硬编码证书的指纹（Hash值，或者直接保存整个证书的内容也行），在建立 TLS 连接前，先计算使用的证书的指纹是否匹配，否则就中断连接。 这种锁定方式需要以下几个前提才能确保流量不被监听: 客户端中硬编码的证书指纹不会被篡改。 指纹验证不能被绕过。 目前有公开技术（XPosed+JustTrustMe）能破解 Android 上常见的 HTTPS 请求库，直接绕过证书检查。 针对上述问题，可以考虑加大绕过的难度。或者 App 检测自己是否运行在 Xposed 等虚拟环境下。 用于 TLS 协议的证书不会频繁更换。（如果更换了，指纹就对不上了。） 而对于第三方的 API，因为我们不知道它们会不会更换 TLS 证书，就不能直接将证书指纹硬编码在客户端中。 这时可以考虑从服务端获取这些 API 的证书指纹（附带私钥签名用于防伪造）。 为了实现证书的轮转(rotation)，可以在新版本的客户端中包含多个证书指纹，这样能保证同时有多个可信证书，达成证书的轮转。（类比 JWT 的公钥轮转机制） 证书锁定技术几乎等同于 SSH 协议的 StrictHostKeyChecking 选项，客户端会验证服务端的公钥指纹（key fingerprint），验证不通过则断开连接。 2. 公钥锁定（Public Key Pining）技术前面介绍过证书的结构，它其实包含了公钥、有效期与一系列的其他信息。 使用了证书锁定技术，会导致证书的有效期也被锁定，APK 内的证书指纹就必须随着证书一起更新。 更好的做法是指锁定证书中的公钥，即「公钥锁定」技术。 「公钥锁定」比「证书锁定」更灵活，这样证书本身其实就可以直接轮转了（证书有过期时间），而不需要一个旧证书和新证书共存的中间时期。 「公钥锁定」是更推荐的锁定技术。 3. HTTPS 严格传输安全 - HSTSHSTS，即 HTTP Strict Transport Security，是一项安全技术，它允许服务端在返回 HTTPS 响应时，通过 Headers 明确要求客户端，在之后的一段时间内必须使用安全的 HTTPS 协议访问服务端。 比如 https://example.com/ 的响应头中有 Strict-Transport-Security: max-age=31536000; includeSubDomains，表示服务端要求客户端（比如浏览器）： 在接下来的 31536000 秒（即一年）中，客户端向 example.com 或其子域名发送 HTTP 请求时，必须采用HTTPS来发起连接。 比如用户在浏览器地址栏输入 http://example.com/ 时，浏览器应自动将 http 改写为 https 再发起请求 在接下来的 31536000 秒（即一年）中，如果 example.com 服务器提供的证书无效，用户不能忽略浏览器的证书警告继续访问网站。 也就是说一旦证书失效，站点将完全无法访问，直至服务端修复证书问题。 一旦证书失效，HTTPS 其实就不是严格安全的了，可能会遭遇中间人攻击。 3. TLS 协议的逆向手段要获取一个应用的 HTTPS 数据，有两个方向: 服务端入侵: 现代应用的服务端突破难度通常都比较客户端高，注入等漏洞底层框架就有处理。 不过如果你获得了服务器 root 权限，可以在 openssl 上做文章，比如篡改 openssl？ 客户端逆向+爬虫: 客户端是离用户最近的地方，也是最容易被突破的地方。 mTLS 常见的破解手段，是找到老版本的安装包，发现很容易就能提取出客户端证书。。 wiki 列出了一些 TLS 协议的安全问题：https://en.wikipedia.org/wiki/Transport_Layer_Security#Security TO BE DONE… ","date":"2022-03-14","objectID":"/posts/about-tls-cert/:4:4","series":null,"tags":["Cryptography","密码学","HTTPS","TLS","SSL","OpenSSL","PKI","数字证书","证书","SSH","QUIC","HTTP/3","安全"],"title":"写给开发人员的实用密码学（八）—— 数字证书与 TLS 协议","uri":"/posts/about-tls-cert/#1-证书锁定certifacte-pining技术"},{"categories":["tech"],"content":" 4. TLS 协议攻防战 1. 证书锁定（Certifacte Pining）技术即使使用了 TLS 协议对流量进行加密，并且保证了前向保密，也无法保证流量不被代理！ 这是因为客户端大多是直接依靠了操作系统内置的 CA 证书库进行证书验证，而 Fiddler 等代理工具可以将自己的 CA 证书添加到该证书库中。 为了防止流量被 Fiddler 等工具使用上述方式监听流量，出现了「证书锁定（Certifacte Pining, 或者 SSL Pinning）」技术。 方法是在客户端中硬编码证书的指纹（Hash值，或者直接保存整个证书的内容也行），在建立 TLS 连接前，先计算使用的证书的指纹是否匹配，否则就中断连接。 这种锁定方式需要以下几个前提才能确保流量不被监听: 客户端中硬编码的证书指纹不会被篡改。 指纹验证不能被绕过。 目前有公开技术（XPosed+JustTrustMe）能破解 Android 上常见的 HTTPS 请求库，直接绕过证书检查。 针对上述问题，可以考虑加大绕过的难度。或者 App 检测自己是否运行在 Xposed 等虚拟环境下。 用于 TLS 协议的证书不会频繁更换。（如果更换了，指纹就对不上了。） 而对于第三方的 API，因为我们不知道它们会不会更换 TLS 证书，就不能直接将证书指纹硬编码在客户端中。 这时可以考虑从服务端获取这些 API 的证书指纹（附带私钥签名用于防伪造）。 为了实现证书的轮转(rotation)，可以在新版本的客户端中包含多个证书指纹，这样能保证同时有多个可信证书，达成证书的轮转。（类比 JWT 的公钥轮转机制） 证书锁定技术几乎等同于 SSH 协议的 StrictHostKeyChecking 选项，客户端会验证服务端的公钥指纹（key fingerprint），验证不通过则断开连接。 2. 公钥锁定（Public Key Pining）技术前面介绍过证书的结构，它其实包含了公钥、有效期与一系列的其他信息。 使用了证书锁定技术，会导致证书的有效期也被锁定，APK 内的证书指纹就必须随着证书一起更新。 更好的做法是指锁定证书中的公钥，即「公钥锁定」技术。 「公钥锁定」比「证书锁定」更灵活，这样证书本身其实就可以直接轮转了（证书有过期时间），而不需要一个旧证书和新证书共存的中间时期。 「公钥锁定」是更推荐的锁定技术。 3. HTTPS 严格传输安全 - HSTSHSTS，即 HTTP Strict Transport Security，是一项安全技术，它允许服务端在返回 HTTPS 响应时，通过 Headers 明确要求客户端，在之后的一段时间内必须使用安全的 HTTPS 协议访问服务端。 比如 https://example.com/ 的响应头中有 Strict-Transport-Security: max-age=31536000; includeSubDomains，表示服务端要求客户端（比如浏览器）： 在接下来的 31536000 秒（即一年）中，客户端向 example.com 或其子域名发送 HTTP 请求时，必须采用HTTPS来发起连接。 比如用户在浏览器地址栏输入 http://example.com/ 时，浏览器应自动将 http 改写为 https 再发起请求 在接下来的 31536000 秒（即一年）中，如果 example.com 服务器提供的证书无效，用户不能忽略浏览器的证书警告继续访问网站。 也就是说一旦证书失效，站点将完全无法访问，直至服务端修复证书问题。 一旦证书失效，HTTPS 其实就不是严格安全的了，可能会遭遇中间人攻击。 3. TLS 协议的逆向手段要获取一个应用的 HTTPS 数据，有两个方向: 服务端入侵: 现代应用的服务端突破难度通常都比较客户端高，注入等漏洞底层框架就有处理。 不过如果你获得了服务器 root 权限，可以在 openssl 上做文章，比如篡改 openssl？ 客户端逆向+爬虫: 客户端是离用户最近的地方，也是最容易被突破的地方。 mTLS 常见的破解手段，是找到老版本的安装包，发现很容易就能提取出客户端证书。。 wiki 列出了一些 TLS 协议的安全问题：https://en.wikipedia.org/wiki/Transport_Layer_Security#Security TO BE DONE… ","date":"2022-03-14","objectID":"/posts/about-tls-cert/:4:4","series":null,"tags":["Cryptography","密码学","HTTPS","TLS","SSL","OpenSSL","PKI","数字证书","证书","SSH","QUIC","HTTP/3","安全"],"title":"写给开发人员的实用密码学（八）—— 数字证书与 TLS 协议","uri":"/posts/about-tls-cert/#2-公钥锁定public-key-pining技术"},{"categories":["tech"],"content":" 4. TLS 协议攻防战 1. 证书锁定（Certifacte Pining）技术即使使用了 TLS 协议对流量进行加密，并且保证了前向保密，也无法保证流量不被代理！ 这是因为客户端大多是直接依靠了操作系统内置的 CA 证书库进行证书验证，而 Fiddler 等代理工具可以将自己的 CA 证书添加到该证书库中。 为了防止流量被 Fiddler 等工具使用上述方式监听流量，出现了「证书锁定（Certifacte Pining, 或者 SSL Pinning）」技术。 方法是在客户端中硬编码证书的指纹（Hash值，或者直接保存整个证书的内容也行），在建立 TLS 连接前，先计算使用的证书的指纹是否匹配，否则就中断连接。 这种锁定方式需要以下几个前提才能确保流量不被监听: 客户端中硬编码的证书指纹不会被篡改。 指纹验证不能被绕过。 目前有公开技术（XPosed+JustTrustMe）能破解 Android 上常见的 HTTPS 请求库，直接绕过证书检查。 针对上述问题，可以考虑加大绕过的难度。或者 App 检测自己是否运行在 Xposed 等虚拟环境下。 用于 TLS 协议的证书不会频繁更换。（如果更换了，指纹就对不上了。） 而对于第三方的 API，因为我们不知道它们会不会更换 TLS 证书，就不能直接将证书指纹硬编码在客户端中。 这时可以考虑从服务端获取这些 API 的证书指纹（附带私钥签名用于防伪造）。 为了实现证书的轮转(rotation)，可以在新版本的客户端中包含多个证书指纹，这样能保证同时有多个可信证书，达成证书的轮转。（类比 JWT 的公钥轮转机制） 证书锁定技术几乎等同于 SSH 协议的 StrictHostKeyChecking 选项，客户端会验证服务端的公钥指纹（key fingerprint），验证不通过则断开连接。 2. 公钥锁定（Public Key Pining）技术前面介绍过证书的结构，它其实包含了公钥、有效期与一系列的其他信息。 使用了证书锁定技术，会导致证书的有效期也被锁定，APK 内的证书指纹就必须随着证书一起更新。 更好的做法是指锁定证书中的公钥，即「公钥锁定」技术。 「公钥锁定」比「证书锁定」更灵活，这样证书本身其实就可以直接轮转了（证书有过期时间），而不需要一个旧证书和新证书共存的中间时期。 「公钥锁定」是更推荐的锁定技术。 3. HTTPS 严格传输安全 - HSTSHSTS，即 HTTP Strict Transport Security，是一项安全技术，它允许服务端在返回 HTTPS 响应时，通过 Headers 明确要求客户端，在之后的一段时间内必须使用安全的 HTTPS 协议访问服务端。 比如 https://example.com/ 的响应头中有 Strict-Transport-Security: max-age=31536000; includeSubDomains，表示服务端要求客户端（比如浏览器）： 在接下来的 31536000 秒（即一年）中，客户端向 example.com 或其子域名发送 HTTP 请求时，必须采用HTTPS来发起连接。 比如用户在浏览器地址栏输入 http://example.com/ 时，浏览器应自动将 http 改写为 https 再发起请求 在接下来的 31536000 秒（即一年）中，如果 example.com 服务器提供的证书无效，用户不能忽略浏览器的证书警告继续访问网站。 也就是说一旦证书失效，站点将完全无法访问，直至服务端修复证书问题。 一旦证书失效，HTTPS 其实就不是严格安全的了，可能会遭遇中间人攻击。 3. TLS 协议的逆向手段要获取一个应用的 HTTPS 数据，有两个方向: 服务端入侵: 现代应用的服务端突破难度通常都比较客户端高，注入等漏洞底层框架就有处理。 不过如果你获得了服务器 root 权限，可以在 openssl 上做文章，比如篡改 openssl？ 客户端逆向+爬虫: 客户端是离用户最近的地方，也是最容易被突破的地方。 mTLS 常见的破解手段，是找到老版本的安装包，发现很容易就能提取出客户端证书。。 wiki 列出了一些 TLS 协议的安全问题：https://en.wikipedia.org/wiki/Transport_Layer_Security#Security TO BE DONE… ","date":"2022-03-14","objectID":"/posts/about-tls-cert/:4:4","series":null,"tags":["Cryptography","密码学","HTTPS","TLS","SSL","OpenSSL","PKI","数字证书","证书","SSH","QUIC","HTTP/3","安全"],"title":"写给开发人员的实用密码学（八）—— 数字证书与 TLS 协议","uri":"/posts/about-tls-cert/#3-https-严格传输安全---hsts"},{"categories":["tech"],"content":" 4. TLS 协议攻防战 1. 证书锁定（Certifacte Pining）技术即使使用了 TLS 协议对流量进行加密，并且保证了前向保密，也无法保证流量不被代理！ 这是因为客户端大多是直接依靠了操作系统内置的 CA 证书库进行证书验证，而 Fiddler 等代理工具可以将自己的 CA 证书添加到该证书库中。 为了防止流量被 Fiddler 等工具使用上述方式监听流量，出现了「证书锁定（Certifacte Pining, 或者 SSL Pinning）」技术。 方法是在客户端中硬编码证书的指纹（Hash值，或者直接保存整个证书的内容也行），在建立 TLS 连接前，先计算使用的证书的指纹是否匹配，否则就中断连接。 这种锁定方式需要以下几个前提才能确保流量不被监听: 客户端中硬编码的证书指纹不会被篡改。 指纹验证不能被绕过。 目前有公开技术（XPosed+JustTrustMe）能破解 Android 上常见的 HTTPS 请求库，直接绕过证书检查。 针对上述问题，可以考虑加大绕过的难度。或者 App 检测自己是否运行在 Xposed 等虚拟环境下。 用于 TLS 协议的证书不会频繁更换。（如果更换了，指纹就对不上了。） 而对于第三方的 API，因为我们不知道它们会不会更换 TLS 证书，就不能直接将证书指纹硬编码在客户端中。 这时可以考虑从服务端获取这些 API 的证书指纹（附带私钥签名用于防伪造）。 为了实现证书的轮转(rotation)，可以在新版本的客户端中包含多个证书指纹，这样能保证同时有多个可信证书，达成证书的轮转。（类比 JWT 的公钥轮转机制） 证书锁定技术几乎等同于 SSH 协议的 StrictHostKeyChecking 选项，客户端会验证服务端的公钥指纹（key fingerprint），验证不通过则断开连接。 2. 公钥锁定（Public Key Pining）技术前面介绍过证书的结构，它其实包含了公钥、有效期与一系列的其他信息。 使用了证书锁定技术，会导致证书的有效期也被锁定，APK 内的证书指纹就必须随着证书一起更新。 更好的做法是指锁定证书中的公钥，即「公钥锁定」技术。 「公钥锁定」比「证书锁定」更灵活，这样证书本身其实就可以直接轮转了（证书有过期时间），而不需要一个旧证书和新证书共存的中间时期。 「公钥锁定」是更推荐的锁定技术。 3. HTTPS 严格传输安全 - HSTSHSTS，即 HTTP Strict Transport Security，是一项安全技术，它允许服务端在返回 HTTPS 响应时，通过 Headers 明确要求客户端，在之后的一段时间内必须使用安全的 HTTPS 协议访问服务端。 比如 https://example.com/ 的响应头中有 Strict-Transport-Security: max-age=31536000; includeSubDomains，表示服务端要求客户端（比如浏览器）： 在接下来的 31536000 秒（即一年）中，客户端向 example.com 或其子域名发送 HTTP 请求时，必须采用HTTPS来发起连接。 比如用户在浏览器地址栏输入 http://example.com/ 时，浏览器应自动将 http 改写为 https 再发起请求 在接下来的 31536000 秒（即一年）中，如果 example.com 服务器提供的证书无效，用户不能忽略浏览器的证书警告继续访问网站。 也就是说一旦证书失效，站点将完全无法访问，直至服务端修复证书问题。 一旦证书失效，HTTPS 其实就不是严格安全的了，可能会遭遇中间人攻击。 3. TLS 协议的逆向手段要获取一个应用的 HTTPS 数据，有两个方向: 服务端入侵: 现代应用的服务端突破难度通常都比较客户端高，注入等漏洞底层框架就有处理。 不过如果你获得了服务器 root 权限，可以在 openssl 上做文章，比如篡改 openssl？ 客户端逆向+爬虫: 客户端是离用户最近的地方，也是最容易被突破的地方。 mTLS 常见的破解手段，是找到老版本的安装包，发现很容易就能提取出客户端证书。。 wiki 列出了一些 TLS 协议的安全问题：https://en.wikipedia.org/wiki/Transport_Layer_Security#Security TO BE DONE… ","date":"2022-03-14","objectID":"/posts/about-tls-cert/:4:4","series":null,"tags":["Cryptography","密码学","HTTPS","TLS","SSL","OpenSSL","PKI","数字证书","证书","SSH","QUIC","HTTP/3","安全"],"title":"写给开发人员的实用密码学（八）—— 数字证书与 TLS 协议","uri":"/posts/about-tls-cert/#3-tls-协议的逆向手段"},{"categories":["tech"],"content":" 三、参考 HTTPS 温故知新（三） —— 直观感受 TLS 握手流程(上) HTTPS 温故知新（五） —— TLS 中的密钥计算 A complete overview of SSL/TLS and its cryptographic system Certificates - Kubernetes Docs 证书选型和购买 - 阿里云文档 云原生安全破局｜如何管理周期越来越短的数字证书？ 另外两个关于 CN(Common Name) 和 SAN(Subject Altnative Name) 的问答: Can not get rid of net::ERR_CERT_COMMON_NAME_INVALID error in chrome with self-signed certificates SSL - How do Common Names (CN) and Subject Alternative Names (SAN) work together? 关于证书锁定/公钥锁定技术: Certificate and Public Key Pinning - OWASP Difference between certificate pinning and public key pinning 其他推荐读物: 图解密码技术 - [日]结城浩 给工程师：关于证书（certificate）和公钥基础设施（PKI）的一切 ","date":"2022-03-14","objectID":"/posts/about-tls-cert/:5:0","series":null,"tags":["Cryptography","密码学","HTTPS","TLS","SSL","OpenSSL","PKI","数字证书","证书","SSH","QUIC","HTTP/3","安全"],"title":"写给开发人员的实用密码学（八）—— 数字证书与 TLS 协议","uri":"/posts/about-tls-cert/#三参考"},{"categories":["tech"],"content":" 本文部分内容翻译自 Practical-Cryptography-for-Developers-Book，笔者补充了密码学历史以及 openssl 命令示例，并重写了 RSA/ECC 算法原理、代码示例等内容。 这篇文章中会涉及到一些数论知识，有点难度，觉得难的可以考虑跳过。 本文不会详细介绍这些数学知识，可以在有疑惑的时候自行查找相关知识。 《写给开发人员的实用密码学》系列文章目录： 写给开发人员的实用密码学（一）—— 概览 写给开发人员的实用密码学（二）—— 哈希函数 写给开发人员的实用密码学（三）—— MAC 与密钥派生函数 KDF 写给开发人员的实用密码学（四）—— 安全随机数生成器 CSPRNG 写给开发人员的实用密码学（五）—— 密钥交换 DHKE 与完美前向保密 PFS 写给开发人员的实用密码学（六）—— 对称密钥加密算法 写给开发人员的实用密码学（七）—— 非对称密钥加密算法 RSA/ECC 写给开发人员的实用密码学（八）—— 数字证书与 TLS 协议 待续 ","date":"2022-03-09","objectID":"/posts/practical-cryptography-basics-7-asymmetric-key-ciphers/:0:0","series":null,"tags":["Cryptography","密码学","非对称加密","安全","RSA","ECC"],"title":"写给开发人员的实用密码学（七）—— 非对称密钥加密算法 RSA/ECC","uri":"/posts/practical-cryptography-basics-7-asymmetric-key-ciphers/#"},{"categories":["tech"],"content":" 一、公钥密码学 / 非对称密码学在介绍非对称密钥加密方案和算法之前，我们首先要了解公钥密码学的概念。 ","date":"2022-03-09","objectID":"/posts/practical-cryptography-basics-7-asymmetric-key-ciphers/:1:0","series":null,"tags":["Cryptography","密码学","非对称加密","安全","RSA","ECC"],"title":"写给开发人员的实用密码学（七）—— 非对称密钥加密算法 RSA/ECC","uri":"/posts/practical-cryptography-basics-7-asymmetric-key-ciphers/#一公钥密码学--非对称密码学"},{"categories":["tech"],"content":" 密码学的历史从第一次世界大战、第二次世界大战到 1976 年这段时期密码的发展阶段，被称为「近代密码阶段」。 在近代密码阶段，所有的密码系统都使用对称密码算法——使用相同的密钥进行加解密。 当时使用的密码算法在拥有海量计算资源的现代人看来都是非常简单的，我们经常看到各种讲述一二战的谍战片，基本都包含破译电报的片段。 第一二次世界大战期间，无线电被广泛应用于军事通讯，围绕无线电通讯的加密破解攻防战极大地影响了战局。 公元20世纪初，第一次世界大战进行到关键时刻，英国破译密码的专门机构「40号房间」利用缴获的德国密码本破译了著名的「齐默尔曼电报」，其内容显示德国打算联合墨西哥对抗可能会参战的美国，这促使美国放弃中立对德宣战，从而彻底改变了一战的走势。 1943 年，美国从破译的日本电报中得知山本五十六将于 4 月 18 日乘中型轰炸机，由 6 架战斗机护航，到中途岛视察。美国总统罗斯福亲自做出决定截击山本，山本乘坐的飞机在去往中途岛的路上被美军击毁，战争天才山本五十六机毁人亡，日本海军从此一蹶不振。 此外，在二次世界大战中，美军将印第安纳瓦霍土著语言作为密码使用，并特别征募使用印第安纳瓦霍通信兵。在二次世界大战日美的太平洋战场上，美国海军军部让北墨西哥和亚历桑那印第安纳瓦霍族人使用纳瓦霍语进行情报传递。纳瓦霍语的语法、音调及词汇都极为独特，不为世人所知道，当时纳瓦霍族以外的美国人中，能听懂这种语言的也就一二十人。这是密码学和语言学的成功结合，纳瓦霍语密码成为历史上从未被破译的密码。 在 1976 年 Malcolm J. Williamson 公开发表了现在被称为「Diffie–Hellman 密钥交换，DHKE」的算法，并提出了「公钥密码学」的概念，这是密码学领域一项划时代的发明，它宣告了「近代密码阶段」的终结，是「现代密码学」的起点。 言归正传，对称密码算法的问题有两点： 「需要安全的通道进行密钥交换」，早期最常见的是面对面交换密钥 每个点对点通信都需要使用不同的密钥，密钥的管理会变得很困难 如果你需要跟 100 个朋友安全通信，你就要维护 100 个不同的对称密钥，而且还得确保它们不泄漏。 这会导致巨大的「密钥交换」跟「密钥保存与管理」的成本。「公钥密码学」最大的优势就是，它解决了这两个问题： 「公钥密码学」可以在不安全的信道上安全地进行密钥交换，第三方即使监听到通信过程，但是（几乎）无法破解出密钥。 每个人只需要公开自己的公钥，就可以跟其他任何人安全地通信。 如果你需要跟 100 个朋友安全通信，你们只需要公开自己的公钥。发送消息时使用对方的公钥加密，接收消息时使用自己的私钥解密即可。 只有你自己的私钥需要保密，所有的公钥都可以公开，这就显著降低了密钥的维护成本。 因此公钥密码学成为了现代密码学的基石，而「公钥密码学」的诞生时间 1976 年被认为是现代密码学的开端。 ","date":"2022-03-09","objectID":"/posts/practical-cryptography-basics-7-asymmetric-key-ciphers/:1:1","series":null,"tags":["Cryptography","密码学","非对称加密","安全","RSA","ECC"],"title":"写给开发人员的实用密码学（七）—— 非对称密钥加密算法 RSA/ECC","uri":"/posts/practical-cryptography-basics-7-asymmetric-key-ciphers/#密码学的历史"},{"categories":["tech"],"content":" 公钥密码学的概念公钥密码系统的密钥始终以公钥 + 私钥对的形式出现，公钥密码系统提供数学框架和算法来生成公钥+私钥对。 公钥通常与所有人共享，而私钥则保密。 公钥密码系统在设计时就确保了在预期的算力下，几乎不可能从其公开的公钥逆向演算出对应的私钥。 公钥密码系统主要有三大用途：加密与解密、签名与验证、密钥交换。 每种算法都需要使用到公钥和私钥，比如由公钥加密的消息只能由私钥解密，由私钥签名的消息需要用公钥验证。 由于加密解密、签名验证均需要两个不同的密钥，故「公钥密码学」也被称为「非对称密码学」。 比较著名的公钥密码系统有：RSA、ECC（椭圆曲线密码学）、ElGamal、Diffie-Hellman、ECDH、ECDSA 和 EdDSA。许多密码算法都是以这些密码系统为基础实现的，例如 RSA 签名、RSA 加密/解密、ECDH 密钥交换以及 ECDSA 和 EdDSA 签名。 ","date":"2022-03-09","objectID":"/posts/practical-cryptography-basics-7-asymmetric-key-ciphers/:1:2","series":null,"tags":["Cryptography","密码学","非对称加密","安全","RSA","ECC"],"title":"写给开发人员的实用密码学（七）—— 非对称密钥加密算法 RSA/ECC","uri":"/posts/practical-cryptography-basics-7-asymmetric-key-ciphers/#公钥密码学的概念"},{"categories":["tech"],"content":" 量子安全性 参考文档：https://en.wikipedia.org/wiki/Post-quantum_cryptography 目前流行的公钥密码系统基本都依赖于 IFP（整数分解问题）、DLP（离散对数问题）或者 ECDLP（椭圆曲线离散对数问题），这导致这些算法都是量子不安全（quantum-unsafe）的。 如果人类进入量子时代，IFP / DLP / ECDLP 的难度将大大降低，目前流行的 RSA、ECC、ElGamal、Diffie-Hellman、ECDH、ECDSA 和 EdDSA 等公钥密码算法都将被淘汰。 目前已经有一些量子安全的公钥密码系统问世，但是因为它们需要更长的密钥、更长的签名等原因，目前还未被广泛使用。 一些量子安全的公钥密码算法举例：NewHope、NTRU、GLYPH、BLISS、XMSS、Picnic 等，有兴趣的可以自行搜索相关文档。 ","date":"2022-03-09","objectID":"/posts/practical-cryptography-basics-7-asymmetric-key-ciphers/:1:3","series":null,"tags":["Cryptography","密码学","非对称加密","安全","RSA","ECC"],"title":"写给开发人员的实用密码学（七）—— 非对称密钥加密算法 RSA/ECC","uri":"/posts/practical-cryptography-basics-7-asymmetric-key-ciphers/#量子安全性"},{"categories":["tech"],"content":" 二、非对称加密方案简介非对称加密要比对称加密复杂，有如下几个原因： 使用密钥对进行加解密，导致其算法更为复杂 只能加密/解密很短的消息 在 RSA 系统中，输入消息应该被转换为大整数（例如使用 OAEP 填充），然后才能进行加密。 一些非对称密码系统（如 ECC）不直接提供加密能力，需要结合使用更复杂的方案才能实现加解密 此外，非对称密码比对称密码慢非常多。比如 RSA 加密比 AES 慢 1000 倍，跟 ChaCha20 就更没法比了。 为了解决上面提到的这些困难并支持加密任意长度的消息，现代密码学使用「非对称加密方案」来实现消息加解密。 又因为「对称加密方案」具有速度快、支持加密任意长度消息等特性，「非对称加密方案」通常直接直接组合使用对称加密算法与非对称加密算法。比如「密钥封装机制 KEM（key encapsulation mechanisms)）」与「集成加密方案 IES（Integrated Encryption Scheme）」 ","date":"2022-03-09","objectID":"/posts/practical-cryptography-basics-7-asymmetric-key-ciphers/:2:0","series":null,"tags":["Cryptography","密码学","非对称加密","安全","RSA","ECC"],"title":"写给开发人员的实用密码学（七）—— 非对称密钥加密算法 RSA/ECC","uri":"/posts/practical-cryptography-basics-7-asymmetric-key-ciphers/#二非对称加密方案简介"},{"categories":["tech"],"content":" 1. 密钥封装机制 KEM顾名思义，KEM 就是仅使用非对称加密算法加密另一个密钥，实际数据的加解密由该密钥完成。 密钥封装机制 KEM 的加密流程（使用公钥加密传输对称密钥）： 密钥封装机制 KEM 的解密流程（使用私钥解密出对称密钥，然后再使用这个对称密钥解密数据）： RSA-OAEP, RSA-KEM, ECIES-KEM 和 PSEC-KEM. 都是 KEM 加密方案。 密钥封装（Key encapsulation）与密钥包裹（Key wrapping）主要区别在于使用的是对称加密算法、还是非对称加密算法： 密钥封装（Key encapsulation）指使用非对称密码算法的公钥加密另一个密钥。 密钥包裹（Key wrapping）指使用对称密码算法加密另一个密钥。 ","date":"2022-03-09","objectID":"/posts/practical-cryptography-basics-7-asymmetric-key-ciphers/:2:1","series":null,"tags":["Cryptography","密码学","非对称加密","安全","RSA","ECC"],"title":"写给开发人员的实用密码学（七）—— 非对称密钥加密算法 RSA/ECC","uri":"/posts/practical-cryptography-basics-7-asymmetric-key-ciphers/#1-密钥封装机制-kem"},{"categories":["tech"],"content":" 1. 密钥封装机制 KEM顾名思义，KEM 就是仅使用非对称加密算法加密另一个密钥，实际数据的加解密由该密钥完成。 密钥封装机制 KEM 的加密流程（使用公钥加密传输对称密钥）： 密钥封装机制 KEM 的解密流程（使用私钥解密出对称密钥，然后再使用这个对称密钥解密数据）： RSA-OAEP, RSA-KEM, ECIES-KEM 和 PSEC-KEM. 都是 KEM 加密方案。 密钥封装（Key encapsulation）与密钥包裹（Key wrapping）主要区别在于使用的是对称加密算法、还是非对称加密算法： 密钥封装（Key encapsulation）指使用非对称密码算法的公钥加密另一个密钥。 密钥包裹（Key wrapping）指使用对称密码算法加密另一个密钥。 ","date":"2022-03-09","objectID":"/posts/practical-cryptography-basics-7-asymmetric-key-ciphers/:2:1","series":null,"tags":["Cryptography","密码学","非对称加密","安全","RSA","ECC"],"title":"写给开发人员的实用密码学（七）—— 非对称密钥加密算法 RSA/ECC","uri":"/posts/practical-cryptography-basics-7-asymmetric-key-ciphers/#密钥封装key-encapsulation与密钥包裹key-wrapping"},{"categories":["tech"],"content":" 2. 集成加密方案 IES集成加密方案 (IES) 在密钥封装机制（KEM）的基础上，添加了密钥派生算法 KDF、消息认证算法 MAC 等其他密码学算法以达成更高的安全性。 在 IES 方案中，非对称算法（如 RSA 或 ECC）跟 KEM 一样，都是用于加密或封装对称密钥，然后通过对称密钥（如 AES 或 Chacha20）来加密输入消息。 DLIES（离散对数集成加密方案）和 ECIES（椭圆曲线集成加密方案）都是 IES 方案。 ","date":"2022-03-09","objectID":"/posts/practical-cryptography-basics-7-asymmetric-key-ciphers/:2:2","series":null,"tags":["Cryptography","密码学","非对称加密","安全","RSA","ECC"],"title":"写给开发人员的实用密码学（七）—— 非对称密钥加密算法 RSA/ECC","uri":"/posts/practical-cryptography-basics-7-asymmetric-key-ciphers/#2-集成加密方案-ies"},{"categories":["tech"],"content":" 三、RSA 密码系统RSA 密码系统是最早的公钥密码系统之一，它基于 RSA 问题和整数分解问题 （IFP）的计算难度。 RSA 算法以其作者（Rivest–Shamir–Adleman）的首字母命名。 RSA 算法在计算机密码学的早期被广泛使用，至今仍然是数字世界应用最广泛的密码算法。 但是随着 ECC 密码学的发展，ECC 正在非对称密码系统中慢慢占据主导地位，因为它比 RSA 具有更高的安全性和更短的密钥长度。 RSA 算法提供如下几种功能： 密钥对生成：生成随机私钥（通常大小为 1024-4096 位）和相应的公钥。 加密解密：使用公钥加密消息（消息要先转换为 [0…key_length] 范围内的整数），然后使用密钥解密。 数字签名：签署消息（使用私钥）和验证消息签名（使用公钥）。 数字签名实际上是通过 Hash 算法 + 加密解密功能实现的。后面会介绍到，它与一般加解密流程的区别，在于数字签名使用私钥加密，再使用公钥解密。 密钥交换：安全地传输密钥，用于以后的加密通信。 RSA 可以使用不同长度的密钥：1024、2048、3072、4096、8129、16384 甚至更多位。目前 3072 位及以上的密钥长度被认为是安全的，曾经大量使用的 2048 位 RSA 现在被破解的风险在不断提升，已经不推荐使用了。 更长的密钥提供更高的安全性，但会消耗更多的计算时间，同时签名也会变得更长，因此需要在安全性和速度之间进行权衡。 非常长的 RSA 密钥（例如 50000 位或 65536 位）对于实际使用可能太慢，例如密钥生成可能需要几分钟到几个小时。 ","date":"2022-03-09","objectID":"/posts/practical-cryptography-basics-7-asymmetric-key-ciphers/:3:0","series":null,"tags":["Cryptography","密码学","非对称加密","安全","RSA","ECC"],"title":"写给开发人员的实用密码学（七）—— 非对称密钥加密算法 RSA/ECC","uri":"/posts/practical-cryptography-basics-7-asymmetric-key-ciphers/#三rsa-密码系统"},{"categories":["tech"],"content":" RSA 密钥对生成RSA 密钥对的生成跟我们在本系列文章的第 5 篇介绍的「DHKE 密钥交换算法」会有些类似，但是要更复杂一点。 首先看下我们怎么使用 openssl 生成一个 1024 位的 RSA 密钥对（仅用做演示，实际应用中建议 3072 位）： OpenSSL 是目前使用最广泛的网络加密算法库，支持非常多流行的现代密码学算法，几乎所有操作系统都会内置 openssl. # 生成 1024 位的 RSA 私钥 ❯ openssl genrsa -out rsa-private-key.pem 1024 Generating RSA private key, 1024 bit long modulus .................+++ .....+++ e is 65537 (0x10001) # 使用私钥生成对应的公钥文件 ❯ openssl rsa -in rsa-private-key.pem -pubout -out rsa-public-key.pem writing RSA key # 查看私钥内容 ❯ cat rsa-private-key.pem -----BEGIN RSA PRIVATE KEY----- MIICXAIBAAKBgQDNE8QZLJZXREOeWZ2ilAzGC4Kjq/PfsFzrXGj8g3IaS4/J3JrB o3qEq/k9XoRzOmNPyvWCj2FAY7A099d7qX4ztthBpUM2ePDIYDvhL0EpfQqbhe+Q aagcFpuKTshGR2wBjH0Cl1/WxJkfIUMmWYU+m4iKLw9KfLX6BjmSgWB6HQIDAQAB AoGADb5NXgKG8MI6ZdpLniGd2Yfb8WwMo+kF0SAYSRPmCa0WrciC9ocmJs3/ngU/ ixlWnnpTibRiKBaGMIaLglYRhvbvibUo8PH4woIidTho2e6swF2aqILk6YFJDpxX FCFdbXM4Cm2MqbD4VtmhCYqbvuiyEUci83YrRP0jJGNt0GECQQDyZgdi8JlFQFH8 1QRHjLN57v5bHQamv7Qb77hlbdbg1wTYO+H8tsOB181TEHA7uN8hxkzyYZy+goRx n0hvJcQXAkEA2JWhCb7oG1eal1aUdgofxhlWnkoFeWHay2zgDWSqmGKyDt0Cb1jq XTdN9dchnqfptWN2/QPLDgM+/9g39/zv6wJATC1sXNeoE29nVMHNGn9JWCSXoyK4 GGdevvjTRm0Cfp6UUzBekQEO6Btd16Du5JXw6bhcLkAm9mgmH18jcGq5+QJBALnr aDv3d0PRZdE372WMt03UfniOzjgueiVaJtMYcSEyx+reabKvvy+ZxACfVirdtU+S PJhhYzN6MeBp+VGV/VUCQBXz0LyM08roWi6DiaRwJIbYx+WCKEOGXQ9QsZND+sGr pOpugr3mcUge5dcZGKtsOUx2xRVmg88nSWMQVkTlsjQ= -----END RSA PRIVATE KEY----- # 查看私钥的详细参数 ❯ openssl rsa -noout -text -in rsa-private-key.pem Private-Key: (1024 bit) modulus: 00💿13:c4:19:2c:96:57:44:43:9e:59:9d:a2:94: 0c:c6:0b:82:a3🆎f3:df:b0:5c:eb:5c:68:fc:83: 72:1a:4b:8f:c9:dc:9a:c1:a3:7a:84🆎f9:3d:5e: 84:73:3a:63:4f:ca:f5:82:8f:61:40:63:b0:34:f7: d7:7b:a9:7e:33:b6:d8:41:a5:43:36:78:f0:c8:60: 3b:e1:2f:41:29:7d:0a:9b:85:ef:90:69:a8:1c:16: 9b:8a:4e:c8:46:47:6c:01:8c:7d:02:97:5f:d6:c4: 99:1f:21:43:26:59:85:3e:9b:88:8a:2f:0f:4a:7c: b5:fa:06:39:92:81:60:7a:1d publicExponent: 65537 (0x10001) privateExponent: 0d:be:4d:5e:02:86:f0:c2:3a:65:da:4b:9e:21:9d: d9:87:db:f1:6c:0c:a3:e9:05:d1:20:18:49:13:e6: 09:ad:16:ad:c8:82:f6:87:26:26:cd:ff:9e:05:3f: 8b:19:56:9e:7a:53:89:b4:62:28:16:86:30:86:8b: 82:56:11:86:f6:ef:89:b5:28:f0:f1:f8:c2:82:22: 75:38:68:d9:ee:ac:c0:5d:9a:a8:82:e4:e9:81:49: 0e:9c:57:14:21:5d:6d:73:38:0a:6d:8c:a9:b0:f8: 56:d9:a1:09:8a:9b:be:e8:b2:11:47:22:f3:76:2b: 44:fd:23:24:63:6d:d0:61 prime1: 00:f2:66:07:62:f0:99:45:40:51:fc:d5:04:47:8c: b3:79:ee:fe:5b:1d:06:a6:bf:b4:1b:ef:b8:65:6d: d6:e0:d7:04:d8:3b:e1:fc:b6:c3:81:d7:cd:53:10: 70:3b:b8:df:21:c6:4c:f2:61:9c:be:82:84:71:9f: 48:6f:25:c4:17 prime2: 00:d8:95:a1:09:be:e8:1b:57:9a:97:56:94:76:0a: 1f:c6:19:56:9e:4a:05:79:61:da:cb:6c:e0:0d:64: aa:98:62:b2:0e:dd:02:6f:58:ea:5d:37:4d:f5:d7: 21:9e:a7:e9:b5:63:76:fd:03:cb:0e:03:3e:ff:d8: 37:f7:fc:ef:eb exponent1: 4c:2d:6c:5c:d7:a8:13:6f:67:54:c1💿1a:7f:49: 58:24:97:a3:22:b8:18:67:5e:be:f8:d3:46:6d:02: 7e:9e:94:53:30:5e:91:01:0e:e8:1b:5d:d7:a0:ee: e4:95:f0:e9:b8:5c:2e:40:26:f6:68:26:1f:5f:23: 70:6a:b9:f9 exponent2: 00:b9:eb:68:3b:f7:77:43:d1:65:d1:37:ef:65:8c: b7:4d:d4:7e:78:8e:ce:38:2e:7a:25:5a:26:d3:18: 71:21:32:c7:ea🇩🇪69:b2:af:bf:2f:99:c4:00:9f: 56:2a:dd:b5:4f:92:3c:98:61:63:33:7a:31:e0:69: f9:51:95:fd:55 coefficient: 15:f3:d0:bc:8c:d3:ca:e8:5a:2e:83:89:a4:70:24: 86:d8:c7:e5:82:28:43:86:5d:0f:50:b1:93:43:fa: c1🆎a4:ea:6e:82:bd:e6:71:48:1e:e5:d7:19:18: ab:6c:39:4c:76:c5:15:66:83:cf:27:49:63:10:56: 44:e5:b2:34 # 查看私钥内容 ❯ cat rsa-public-key.pem -----BEGIN PUBLIC KEY----- MIGfMA0GCSqGSIb3DQEBAQUAA4GNADCBiQKBgQDNE8QZLJZXREOeWZ2ilAzGC4Kj q/PfsFzrXGj8g3IaS4/J3JrBo3qEq/k9XoRzOmNPyvWCj2FAY7A099d7qX4ztthB pUM2ePDIYDvhL0EpfQqbhe+QaagcFpuKTshGR2wBjH0Cl1/WxJkfIUMmWYU+m4iK Lw9KfLX6BjmSgWB6HQIDAQAB -----END PUBLIC KEY----- # 查看公钥的参数 ❯ openssl rsa -noout -text -pubin -in rsa-public-key.pem Public-Key: (1024 bit) Modulus: 00💿13:c4:19:2c:96:57:44:43:9e:59:9d:a2:94: 0c:c6:0b:82:a3🆎f3:df:b0:5c:eb:5c:68:fc:83: 72:1a:4b:8f:c9:dc:9a:c1:a3:7a:84🆎f9:3d:5e: 84:73:3a:63:4f:ca:f5:82:8f:61:40:63:b0:34:f7: d7:7b:a9:7e:33:b6:d8:41:a5:43:36:78:f0:c8:60: 3b:e1:2f:41:29:7d:0a:9b:85:ef:90:6","date":"2022-03-09","objectID":"/posts/practical-cryptography-basics-7-asymmetric-key-ciphers/:3:1","series":null,"tags":["Cryptography","密码学","非对称加密","安全","RSA","ECC"],"title":"写给开发人员的实用密码学（七）—— 非对称密钥加密算法 RSA/ECC","uri":"/posts/practical-cryptography-basics-7-asymmetric-key-ciphers/#rsa-密钥对生成"},{"categories":["tech"],"content":" RSA 加密与解密RSA 加密算法，一次只能加密一个小于 $n$ 的非负整数，假设明文为整数 $msg$，加密算法如下： $$ \\text{encryptedMsg} = msg^e \\mod n $$ 通常的手段是，先使用 EAOP 将被加密消息编码成一个个符合条件的整数，再使用上述公式一个个加密。 解密的方法，就是对被每一段加密的数据 $encryptedMsg$，进行如下运算： $$ \\text{decryptedMsg} = \\text{encryptedMsg}^d \\mod n $$ RSA 解密运算的证明 这里的证明需要用到一些数论知识，觉得不容易理解的话，建议自行查找相关资料。 证明流程如下： $$ \\begin{alignedat}{2} \\text{decryptedMsg} \u0026= \u0026\\text{encryptedMsg}^d \u0026\\mod n \\\\ \u0026= \u0026{(msg^e \\mod n)}^d \u0026\\mod n \\\\ \u0026= \u0026{msg^{ed}} \u0026\\mod n \\\\ \u0026= \u0026{msg^{ed}} \u0026\\mod {pq} \\end{alignedat} $$ 接下来将下面两个等式代入上述计算中： 我们在前面的「密钥对生成」一节中有给出等式：$ed = 1 + (p-1)(q-1) \\cdot k$ 因为 $0 \\le msg \\lt n$ 以及 $n = pq$，有 $msg \\mod pq = msg$ 这样就得到： $$ \\begin{alignedat}{2} \\text{decryptedMsg} \u0026= \u0026{msg^{ed}} \u0026\\mod {pq} \\\\ \u0026= \u0026{(msg \\mod pq) \\cdot (msg^{ed-1} \\mod pq)} \u0026\\mod {pq} \\\\ \u0026= \u0026{msg \\cdot (msg^{(p-1)(q-1) \\cdot k} \\mod pq)} \u0026\\mod {pq} \\end{alignedat} $$ 又有费马小定理指出，在 $a$ 为整数，$p$ 为质数的情况下，有同余等式 $$a^{p-1} \\equiv 1 {\\pmod p}$$ 因为我们的模数 $n=pq$ 并不是质数，不能直接利用费马小定理给出的同余公式。 但是 $p$, $q$ 两数都为质数，我们可以分别计算方程 对 $p$ 以及 $q$ 取模的结果，然后再根据中国剩余定理得出通解，也就得到我们需要的结果。 对于模 $p$ 的情况，计算方法如下： 当 $msg = 0 \\mod p$ 时，${msg^{ed}} \\mod p = 0 \\equiv msg \\pmod p$ 当 $msg \\ne 0 \\mod p$ 时，利用费马小定理，有 $$ \\begin{alignedat}{2} msg^{ed} \u0026= \u0026{msg \\cdot (msg^{(p-1)(q-1) \\cdot k} \\mod p)} \u0026\\pmod {p} \\\\ \u0026= \u0026msg \\cdot (msg^{(p-1)} \\mod p)^{(q-1) \\cdot k} \u0026\\pmod p \\\\ \u0026= \u0026msg \\cdot 1^{(q-1) \\cdot k} \u0026\\pmod p \\\\ \u0026\\equiv \u0026msg \\pmod p \\end{alignedat} $$ 同理，对模 $q$ 的情况，也能得到等式 $$msg^{ed} \\equiv msg \\pmod q$$ 有了上面两个结果，根据中国剩余定理，就能得到 $$msg^{ed} \\equiv msg \\pmod {pq}$$ 现在再接续前面的计算： $$ \\begin{alignedat}{2} \\text{decryptedMsg} \u0026= \u0026{msg^{ed}} \u0026\\pmod {pq} \\\\ \u0026= \u0026msg \u0026\\pmod {pq} \\\\ \u0026= \u0026msg \\end{alignedat} $$ 这样就证明了，解密操作得到的就是原始信息。 因为非对称加解密非常慢，对于较大的文件，通常会分成两步加密来提升性能：首先用使用对称加密算法来加密数据，再使用 RSA 等非对称加密算法加密上一步用到的「对称密钥」。 下面我们用 Python 来验证下 RSA 算法的加解密流程： # pip install cryptography==36.0.1 from pathlib import Path from cryptography.hazmat.primitives import serialization # 私钥 key_path = Path(\"./rsa-private-key.pem\") private_key = serialization.load_pem_private_key( key_path.read_bytes(), password=None, ) private = private_key.private_numbers() public = private_key.public_key().public_numbers() d = private.d # 公钥 n = public.n e = public.e def int_to_bytes(x: int) -\u003e bytes: return x.to_bytes((x.bit_length() + 7) // 8, 'big') def int_from_bytes(xbytes: bytes) -\u003e int: return int.from_bytes(xbytes, 'big') def fast_power_modular(b: int, p: int, m: int): \"\"\" 快速模幂运算：b^p % m 复杂度： O(log p) 因为 RSA 的底数跟指数都非常大，如果先进行幂运算，最后再取模，计算结果会越来越大，导致速度非常非常慢 根据模幂运算的性质 b^(ab) % m = (b^a % m)^b % m, 可以通过边进行幂运算边取模，极大地提升计算速度 \"\"\" res = 1 while p: if p \u0026 0x1: res *= b b = b ** 2 % m p \u003e\u003e= 1 return res % m # 明文 original_msg = b\"an example\" print(f\"{original_msg=}\") # 加密 msg_int = int_from_bytes(original_msg) encrypt_int = msg_int ** e % n encrypt_msg = int_to_bytes(encrypt_int) print(f\"{encrypt_msg=}\") # 解密 # decrypt_int = encrypt_int ** d % n # 因为 d 非常大，直接使用公式计算会非常非常慢，所以不能这么算 decrypt_int = fast_power_modular(encrypt_int, d, n) decrypt_msg = int_to_bytes(decrypt_int) print(f\"{decrypt_msg=}\") # 应该与原信息完全一致 ","date":"2022-03-09","objectID":"/posts/practical-cryptography-basics-7-asymmetric-key-ciphers/:3:2","series":null,"tags":["Cryptography","密码学","非对称加密","安全","RSA","ECC"],"title":"写给开发人员的实用密码学（七）—— 非对称密钥加密算法 RSA/ECC","uri":"/posts/practical-cryptography-basics-7-asymmetric-key-ciphers/#rsa-加密与解密"},{"categories":["tech"],"content":" RSA 加密与解密RSA 加密算法，一次只能加密一个小于 $n$ 的非负整数，假设明文为整数 $msg$，加密算法如下： $$ \\text{encryptedMsg} = msg^e \\mod n $$ 通常的手段是，先使用 EAOP 将被加密消息编码成一个个符合条件的整数，再使用上述公式一个个加密。 解密的方法，就是对被每一段加密的数据 $encryptedMsg$，进行如下运算： $$ \\text{decryptedMsg} = \\text{encryptedMsg}^d \\mod n $$ RSA 解密运算的证明 这里的证明需要用到一些数论知识，觉得不容易理解的话，建议自行查找相关资料。 证明流程如下： $$ \\begin{alignedat}{2} \\text{decryptedMsg} \u0026= \u0026\\text{encryptedMsg}^d \u0026\\mod n \\\\ \u0026= \u0026{(msg^e \\mod n)}^d \u0026\\mod n \\\\ \u0026= \u0026{msg^{ed}} \u0026\\mod n \\\\ \u0026= \u0026{msg^{ed}} \u0026\\mod {pq} \\end{alignedat} $$ 接下来将下面两个等式代入上述计算中： 我们在前面的「密钥对生成」一节中有给出等式：$ed = 1 + (p-1)(q-1) \\cdot k$ 因为 $0 \\le msg \\lt n$ 以及 $n = pq$，有 $msg \\mod pq = msg$ 这样就得到： $$ \\begin{alignedat}{2} \\text{decryptedMsg} \u0026= \u0026{msg^{ed}} \u0026\\mod {pq} \\\\ \u0026= \u0026{(msg \\mod pq) \\cdot (msg^{ed-1} \\mod pq)} \u0026\\mod {pq} \\\\ \u0026= \u0026{msg \\cdot (msg^{(p-1)(q-1) \\cdot k} \\mod pq)} \u0026\\mod {pq} \\end{alignedat} $$ 又有费马小定理指出，在 $a$ 为整数，$p$ 为质数的情况下，有同余等式 $$a^{p-1} \\equiv 1 {\\pmod p}$$ 因为我们的模数 $n=pq$ 并不是质数，不能直接利用费马小定理给出的同余公式。 但是 $p$, $q$ 两数都为质数，我们可以分别计算方程 对 $p$ 以及 $q$ 取模的结果，然后再根据中国剩余定理得出通解，也就得到我们需要的结果。 对于模 $p$ 的情况，计算方法如下： 当 $msg = 0 \\mod p$ 时，${msg^{ed}} \\mod p = 0 \\equiv msg \\pmod p$ 当 $msg \\ne 0 \\mod p$ 时，利用费马小定理，有 $$ \\begin{alignedat}{2} msg^{ed} \u0026= \u0026{msg \\cdot (msg^{(p-1)(q-1) \\cdot k} \\mod p)} \u0026\\pmod {p} \\\\ \u0026= \u0026msg \\cdot (msg^{(p-1)} \\mod p)^{(q-1) \\cdot k} \u0026\\pmod p \\\\ \u0026= \u0026msg \\cdot 1^{(q-1) \\cdot k} \u0026\\pmod p \\\\ \u0026\\equiv \u0026msg \\pmod p \\end{alignedat} $$ 同理，对模 $q$ 的情况，也能得到等式 $$msg^{ed} \\equiv msg \\pmod q$$ 有了上面两个结果，根据中国剩余定理，就能得到 $$msg^{ed} \\equiv msg \\pmod {pq}$$ 现在再接续前面的计算： $$ \\begin{alignedat}{2} \\text{decryptedMsg} \u0026= \u0026{msg^{ed}} \u0026\\pmod {pq} \\\\ \u0026= \u0026msg \u0026\\pmod {pq} \\\\ \u0026= \u0026msg \\end{alignedat} $$ 这样就证明了，解密操作得到的就是原始信息。 因为非对称加解密非常慢，对于较大的文件，通常会分成两步加密来提升性能：首先用使用对称加密算法来加密数据，再使用 RSA 等非对称加密算法加密上一步用到的「对称密钥」。 下面我们用 Python 来验证下 RSA 算法的加解密流程： # pip install cryptography==36.0.1 from pathlib import Path from cryptography.hazmat.primitives import serialization # 私钥 key_path = Path(\"./rsa-private-key.pem\") private_key = serialization.load_pem_private_key( key_path.read_bytes(), password=None, ) private = private_key.private_numbers() public = private_key.public_key().public_numbers() d = private.d # 公钥 n = public.n e = public.e def int_to_bytes(x: int) -\u003e bytes: return x.to_bytes((x.bit_length() + 7) // 8, 'big') def int_from_bytes(xbytes: bytes) -\u003e int: return int.from_bytes(xbytes, 'big') def fast_power_modular(b: int, p: int, m: int): \"\"\" 快速模幂运算：b^p % m 复杂度： O(log p) 因为 RSA 的底数跟指数都非常大，如果先进行幂运算，最后再取模，计算结果会越来越大，导致速度非常非常慢 根据模幂运算的性质 b^(ab) % m = (b^a % m)^b % m, 可以通过边进行幂运算边取模，极大地提升计算速度 \"\"\" res = 1 while p: if p \u0026 0x1: res *= b b = b ** 2 % m p \u003e\u003e= 1 return res % m # 明文 original_msg = b\"an example\" print(f\"{original_msg=}\") # 加密 msg_int = int_from_bytes(original_msg) encrypt_int = msg_int ** e % n encrypt_msg = int_to_bytes(encrypt_int) print(f\"{encrypt_msg=}\") # 解密 # decrypt_int = encrypt_int ** d % n # 因为 d 非常大，直接使用公式计算会非常非常慢，所以不能这么算 decrypt_int = fast_power_modular(encrypt_int, d, n) decrypt_msg = int_to_bytes(decrypt_int) print(f\"{decrypt_msg=}\") # 应该与原信息完全一致 ","date":"2022-03-09","objectID":"/posts/practical-cryptography-basics-7-asymmetric-key-ciphers/:3:2","series":null,"tags":["Cryptography","密码学","非对称加密","安全","RSA","ECC"],"title":"写给开发人员的实用密码学（七）—— 非对称密钥加密算法 RSA/ECC","uri":"/posts/practical-cryptography-basics-7-asymmetric-key-ciphers/#rsa-解密运算的证明"},{"categories":["tech"],"content":" RSA 数字签名前面证明了可以使用公钥加密，再使用私钥解密。 实际上从上面的证明也可以看出来，顺序是完全可逆的，先使用私钥加密，再使用公钥解密也完全是可行的。这种运算被我们用在数字签名算法中。 数字签名的方法为： 首先计算原始数据的 Hash 值，比如 SHA256 使用私钥对计算出的 Hash 值进行加密，得到数字签名 其他人使用公开的公钥进行解密出 Hash 值，再对原始数据计算 Hash 值对比，如果一致，就说明数据未被篡改 Python 演示： # pip install cryptography==36.0.1 from hashlib import sha512 from pathlib import Path from cryptography.hazmat.primitives import serialization key_path = Path(\"./rsa-private-key.pem\") private_key = serialization.load_pem_private_key( key_path.read_bytes(), password=None, ) private = private_key.private_numbers() public = private_key.public_key().public_numbers() d = private.d n = public.n e = public.e # RSA sign the message msg = b'A message for signing' hash = int.from_bytes(sha512(msg).digest(), byteorder='big') signature = pow(hash, d, n) print(\"Signature:\", hex(signature)) # RSA verify signature msg = b'A message for signing' hash = int.from_bytes(sha512(msg).digest(), byteorder='big') hashFromSignature = pow(signature, e, n) print(\"Signature valid:\", hash == hashFromSignature) ","date":"2022-03-09","objectID":"/posts/practical-cryptography-basics-7-asymmetric-key-ciphers/:3:3","series":null,"tags":["Cryptography","密码学","非对称加密","安全","RSA","ECC"],"title":"写给开发人员的实用密码学（七）—— 非对称密钥加密算法 RSA/ECC","uri":"/posts/practical-cryptography-basics-7-asymmetric-key-ciphers/#rsa-数字签名"},{"categories":["tech"],"content":" 四、ECC 密码系统 ECC 椭圆曲线密码学，于 1985 年被首次提出，并于 2004 年开始被广泛应用。 ECC 被认为是 RSA 的继任者，新一代的非对称加密算法。 其最大的特点在于相同密码强度下，ECC 的密钥和签名的大小都要显著低于 RSA. 256bits 的 ECC 密钥，安全性与 3072bits 的 RSA 密钥安全性相当。 其次 ECC 的密钥对生成、密钥交换与签名算法的速度都要比 RSA 快。 ","date":"2022-03-09","objectID":"/posts/practical-cryptography-basics-7-asymmetric-key-ciphers/:4:0","series":null,"tags":["Cryptography","密码学","非对称加密","安全","RSA","ECC"],"title":"写给开发人员的实用密码学（七）—— 非对称密钥加密算法 RSA/ECC","uri":"/posts/practical-cryptography-basics-7-asymmetric-key-ciphers/#四ecc-密码系统"},{"categories":["tech"],"content":" 椭圆曲线的数学原理简介在数学中，椭圆曲线（Elliptic Curves）是一种平面曲线，由如下方程定义的点的集合组成（$A-J$ 均为常数）： $$ Ax^3 + Bx^2y + Cxy^2 + Dy^3 + Ex^2 + Fxy + Gy^2 + Hx + Iy + J = 0 $$ 而 ECC 只使用了其中很简单的一个子集（$a, b$ 均为常数）： $$ y^2 = x^3 + ax + b $$ 比如著名的 NIST 曲线 secp256k1 就是基于如下椭圆曲线方程： $$ y^2 = x^3 + 7 $$ 椭圆曲线大概长这么个形状： 椭圆曲线跟椭圆的关系，就犹如雷锋跟雷峰塔、Java 跟 JavaScript… 你可以通过如下网站手动调整 $a$ 与 $b$ 的值，拖动曲线的交点： https://www.desmos.com/calculator/ialhd71we3?lang=zh-CN 椭圆曲线上的运算数学家在椭圆曲线上定义了一些运算规则，ECC 就依赖于这些规则，下面简单介绍下我们用得到的部分。 1. 加法与负元对于曲线上的任意两点 $A$ 与 $B$，我们定义过 $A, B$ 的直线与曲线的交点为 $-(A+B)$，而 $-(A+B)$ 相对于 x 轴的对称点即为 $A+B$: 上述描述一是定义了椭圆曲线的加法规则，二是定义了椭圆曲线上的负元运算。 2. 二倍运算在加法规则中，如果 $A=B$，我们定义曲线在 $A$ 点的切线与曲线的交点为 $-2A$，于是得到二倍运算的规则： 3. 无穷远点对于 $(-A) + A$ 这种一个值与其负元本身相加的情况，我们会发现过这两点的直线没有交点，前面定义的加法规则在这种情况下失效。 为了解决这个问题，我们假设这直线与椭圆曲线相交于无穷远点 $O_{\\infty}$. 4. k 倍运算我们在前面已经定义了椭圆曲线上的加法运算、二倍运算以及无穷远点，有了这三个概念，我们就能定义k 倍运算 了。 K 倍运算最简单的计算方法，就是不断地进行加法运算，但是也有许多更高效的算法。 其中最简单的算法是「Double-and-add」，它要求首先 $k$ 拆分成如下形式 $$ k = k_{0}+2k_{1}+2^{2}k_{2}+\\cdots +2^{m}k_{m} \\\\ \\text{其中} k_{0}~..~k_{m}\\in {0,1},m=\\lfloor \\log _{2}{k}\\rfloor $$ 然后再迭代计算其中各项的值，它的运算复杂度为 $log_{2}(k)$. 因 Double 和 add 的执行时间不同，根据执行时间就可以知道是执行 Double 还是 add，间接可以推算出 k. 因此这个算法会有计时攻击的风险。 基于「Double-and-add」修改的蒙哥马利阶梯（Montgomery Ladder）是可以避免计时分析的作法，这里就不详细介绍了。 5. 有限域上的椭圆曲线椭圆曲线是连续且无限的，而计算机却更擅长处理离散的、存在上限的整数，因此 ECC 使用「有限域上的椭圆曲线」进行计算。 「有限域（也被称作 Galois Filed, 缩写为 GF）」顾名思义，就是指只有有限个数值的域。 有限域上的椭圆曲线方程，通过取模的方式将曲线上的所有值都映射到同一个有限域内。 有限域 $\\mathbb {F} _{p}$ 上的 EC 椭圆曲线方程为： $$ y^2 = x^3 + ax + b (\\mod p), 0 \\le x \\le p $$ 目前主要有两种有限域在 ECC 中被广泛应用： 以素数为模的整数域: $\\mathbb {F} _{p}$ 在通用处理器上计算很快 以 2 的幂为模的整数域: $\\mathbb {F} _{2^{m}}$ 当使用专用硬件时，计算速度很快 通过限制 x 为整数，并使用取模进行了映射后，椭圆曲线的形状已经面目全非了，它的加减法也不再具有几何意义。 但是它的一些特性仍然跟椭圆曲线很类似，各种公式基本加个 $\\mod p$ 就变成了它的有限域版本： 无穷远点 $O_{\\infty}$ 是零元，$O_{\\infty} + O_{\\infty} = O_{\\infty}$，$O_{\\infty} + P = P$ $P_{x, y}$ 的负元为 $P_{x, -y}$,，并且有 $P + (-P) = O_{\\infty}$ $P * 0 = O_{\\infty}$ 如果 $P_{x1, y1} + Q_{x2, y2} = R_{x3, y3}$，则其坐标有如下关系 $x3 = (k^2 - x1 - x2) \\mod p$ $y3 = (k(x1 - x3) - y1) \\mod p$ 斜率 $k$ 的计算 如果 $P=Q$，则 $k=\\dfrac {3x^{2}+a} {2y_{1}}$ 否则 $k=\\dfrac {y_{2}-y_{1}} {x_{2}-x_{1}} $ ECDLP 椭圆曲线离散对数问题前面已经介绍了椭圆曲线上的 k 倍运算 及相关的高效算法，但是我们还没有涉及到除法。 椭圆曲线上的除法是一个尚未被解决的难题——「ECDLP 椭圆曲线离散对数问题」： 已知 $kG$ 与基点 $G$，求整数 $k$ 的值。 目前并没有有效的手段可以快速计算出 $k$ 的值。 比较直观的方法应该是从基点 $G$ 开始不断进行加法运算，直到结果与 $kG$ 相等。 目前已知的 ECDLP 最快的解法所需步骤为 $\\sqrt{k}$，而 k 倍运算高效算法前面已经介绍过了，所需步骤为 $log_2(k)$。 在 $k$ 非常大的情况下，它们的计算用时将会有指数级的差距。 椭圆曲线上的 k 倍运算与素数上的幂运算很类似，因此 ECC 底层的数学难题 ECDLP 与 RSA 的离散对数问题 DLP 也有很大相似性。 ","date":"2022-03-09","objectID":"/posts/practical-cryptography-basics-7-asymmetric-key-ciphers/:4:1","series":null,"tags":["Cryptography","密码学","非对称加密","安全","RSA","ECC"],"title":"写给开发人员的实用密码学（七）—— 非对称密钥加密算法 RSA/ECC","uri":"/posts/practical-cryptography-basics-7-asymmetric-key-ciphers/#椭圆曲线的数学原理简介"},{"categories":["tech"],"content":" 椭圆曲线的数学原理简介在数学中，椭圆曲线（Elliptic Curves）是一种平面曲线，由如下方程定义的点的集合组成（$A-J$ 均为常数）： $$ Ax^3 + Bx^2y + Cxy^2 + Dy^3 + Ex^2 + Fxy + Gy^2 + Hx + Iy + J = 0 $$ 而 ECC 只使用了其中很简单的一个子集（$a, b$ 均为常数）： $$ y^2 = x^3 + ax + b $$ 比如著名的 NIST 曲线 secp256k1 就是基于如下椭圆曲线方程： $$ y^2 = x^3 + 7 $$ 椭圆曲线大概长这么个形状： 椭圆曲线跟椭圆的关系，就犹如雷锋跟雷峰塔、Java 跟 JavaScript… 你可以通过如下网站手动调整 $a$ 与 $b$ 的值，拖动曲线的交点： https://www.desmos.com/calculator/ialhd71we3?lang=zh-CN 椭圆曲线上的运算数学家在椭圆曲线上定义了一些运算规则，ECC 就依赖于这些规则，下面简单介绍下我们用得到的部分。 1. 加法与负元对于曲线上的任意两点 $A$ 与 $B$，我们定义过 $A, B$ 的直线与曲线的交点为 $-(A+B)$，而 $-(A+B)$ 相对于 x 轴的对称点即为 $A+B$: 上述描述一是定义了椭圆曲线的加法规则，二是定义了椭圆曲线上的负元运算。 2. 二倍运算在加法规则中，如果 $A=B$，我们定义曲线在 $A$ 点的切线与曲线的交点为 $-2A$，于是得到二倍运算的规则： 3. 无穷远点对于 $(-A) + A$ 这种一个值与其负元本身相加的情况，我们会发现过这两点的直线没有交点，前面定义的加法规则在这种情况下失效。 为了解决这个问题，我们假设这直线与椭圆曲线相交于无穷远点 $O_{\\infty}$. 4. k 倍运算我们在前面已经定义了椭圆曲线上的加法运算、二倍运算以及无穷远点，有了这三个概念，我们就能定义k 倍运算 了。 K 倍运算最简单的计算方法，就是不断地进行加法运算，但是也有许多更高效的算法。 其中最简单的算法是「Double-and-add」，它要求首先 $k$ 拆分成如下形式 $$ k = k_{0}+2k_{1}+2^{2}k_{2}+\\cdots +2^{m}k_{m} \\\\ \\text{其中} k_{0}~..~k_{m}\\in {0,1},m=\\lfloor \\log _{2}{k}\\rfloor $$ 然后再迭代计算其中各项的值，它的运算复杂度为 $log_{2}(k)$. 因 Double 和 add 的执行时间不同，根据执行时间就可以知道是执行 Double 还是 add，间接可以推算出 k. 因此这个算法会有计时攻击的风险。 基于「Double-and-add」修改的蒙哥马利阶梯（Montgomery Ladder）是可以避免计时分析的作法，这里就不详细介绍了。 5. 有限域上的椭圆曲线椭圆曲线是连续且无限的，而计算机却更擅长处理离散的、存在上限的整数，因此 ECC 使用「有限域上的椭圆曲线」进行计算。 「有限域（也被称作 Galois Filed, 缩写为 GF）」顾名思义，就是指只有有限个数值的域。 有限域上的椭圆曲线方程，通过取模的方式将曲线上的所有值都映射到同一个有限域内。 有限域 $\\mathbb {F} _{p}$ 上的 EC 椭圆曲线方程为： $$ y^2 = x^3 + ax + b (\\mod p), 0 \\le x \\le p $$ 目前主要有两种有限域在 ECC 中被广泛应用： 以素数为模的整数域: $\\mathbb {F} _{p}$ 在通用处理器上计算很快 以 2 的幂为模的整数域: $\\mathbb {F} _{2^{m}}$ 当使用专用硬件时，计算速度很快 通过限制 x 为整数，并使用取模进行了映射后，椭圆曲线的形状已经面目全非了，它的加减法也不再具有几何意义。 但是它的一些特性仍然跟椭圆曲线很类似，各种公式基本加个 $\\mod p$ 就变成了它的有限域版本： 无穷远点 $O_{\\infty}$ 是零元，$O_{\\infty} + O_{\\infty} = O_{\\infty}$，$O_{\\infty} + P = P$ $P_{x, y}$ 的负元为 $P_{x, -y}$,，并且有 $P + (-P) = O_{\\infty}$ $P * 0 = O_{\\infty}$ 如果 $P_{x1, y1} + Q_{x2, y2} = R_{x3, y3}$，则其坐标有如下关系 $x3 = (k^2 - x1 - x2) \\mod p$ $y3 = (k(x1 - x3) - y1) \\mod p$ 斜率 $k$ 的计算 如果 $P=Q$，则 $k=\\dfrac {3x^{2}+a} {2y_{1}}$ 否则 $k=\\dfrac {y_{2}-y_{1}} {x_{2}-x_{1}} $ ECDLP 椭圆曲线离散对数问题前面已经介绍了椭圆曲线上的 k 倍运算 及相关的高效算法，但是我们还没有涉及到除法。 椭圆曲线上的除法是一个尚未被解决的难题——「ECDLP 椭圆曲线离散对数问题」： 已知 $kG$ 与基点 $G$，求整数 $k$ 的值。 目前并没有有效的手段可以快速计算出 $k$ 的值。 比较直观的方法应该是从基点 $G$ 开始不断进行加法运算，直到结果与 $kG$ 相等。 目前已知的 ECDLP 最快的解法所需步骤为 $\\sqrt{k}$，而 k 倍运算高效算法前面已经介绍过了，所需步骤为 $log_2(k)$。 在 $k$ 非常大的情况下，它们的计算用时将会有指数级的差距。 椭圆曲线上的 k 倍运算与素数上的幂运算很类似，因此 ECC 底层的数学难题 ECDLP 与 RSA 的离散对数问题 DLP 也有很大相似性。 ","date":"2022-03-09","objectID":"/posts/practical-cryptography-basics-7-asymmetric-key-ciphers/:4:1","series":null,"tags":["Cryptography","密码学","非对称加密","安全","RSA","ECC"],"title":"写给开发人员的实用密码学（七）—— 非对称密钥加密算法 RSA/ECC","uri":"/posts/practical-cryptography-basics-7-asymmetric-key-ciphers/#椭圆曲线上的运算"},{"categories":["tech"],"content":" 椭圆曲线的数学原理简介在数学中，椭圆曲线（Elliptic Curves）是一种平面曲线，由如下方程定义的点的集合组成（$A-J$ 均为常数）： $$ Ax^3 + Bx^2y + Cxy^2 + Dy^3 + Ex^2 + Fxy + Gy^2 + Hx + Iy + J = 0 $$ 而 ECC 只使用了其中很简单的一个子集（$a, b$ 均为常数）： $$ y^2 = x^3 + ax + b $$ 比如著名的 NIST 曲线 secp256k1 就是基于如下椭圆曲线方程： $$ y^2 = x^3 + 7 $$ 椭圆曲线大概长这么个形状： 椭圆曲线跟椭圆的关系，就犹如雷锋跟雷峰塔、Java 跟 JavaScript… 你可以通过如下网站手动调整 $a$ 与 $b$ 的值，拖动曲线的交点： https://www.desmos.com/calculator/ialhd71we3?lang=zh-CN 椭圆曲线上的运算数学家在椭圆曲线上定义了一些运算规则，ECC 就依赖于这些规则，下面简单介绍下我们用得到的部分。 1. 加法与负元对于曲线上的任意两点 $A$ 与 $B$，我们定义过 $A, B$ 的直线与曲线的交点为 $-(A+B)$，而 $-(A+B)$ 相对于 x 轴的对称点即为 $A+B$: 上述描述一是定义了椭圆曲线的加法规则，二是定义了椭圆曲线上的负元运算。 2. 二倍运算在加法规则中，如果 $A=B$，我们定义曲线在 $A$ 点的切线与曲线的交点为 $-2A$，于是得到二倍运算的规则： 3. 无穷远点对于 $(-A) + A$ 这种一个值与其负元本身相加的情况，我们会发现过这两点的直线没有交点，前面定义的加法规则在这种情况下失效。 为了解决这个问题，我们假设这直线与椭圆曲线相交于无穷远点 $O_{\\infty}$. 4. k 倍运算我们在前面已经定义了椭圆曲线上的加法运算、二倍运算以及无穷远点，有了这三个概念，我们就能定义k 倍运算 了。 K 倍运算最简单的计算方法，就是不断地进行加法运算，但是也有许多更高效的算法。 其中最简单的算法是「Double-and-add」，它要求首先 $k$ 拆分成如下形式 $$ k = k_{0}+2k_{1}+2^{2}k_{2}+\\cdots +2^{m}k_{m} \\\\ \\text{其中} k_{0}~..~k_{m}\\in {0,1},m=\\lfloor \\log _{2}{k}\\rfloor $$ 然后再迭代计算其中各项的值，它的运算复杂度为 $log_{2}(k)$. 因 Double 和 add 的执行时间不同，根据执行时间就可以知道是执行 Double 还是 add，间接可以推算出 k. 因此这个算法会有计时攻击的风险。 基于「Double-and-add」修改的蒙哥马利阶梯（Montgomery Ladder）是可以避免计时分析的作法，这里就不详细介绍了。 5. 有限域上的椭圆曲线椭圆曲线是连续且无限的，而计算机却更擅长处理离散的、存在上限的整数，因此 ECC 使用「有限域上的椭圆曲线」进行计算。 「有限域（也被称作 Galois Filed, 缩写为 GF）」顾名思义，就是指只有有限个数值的域。 有限域上的椭圆曲线方程，通过取模的方式将曲线上的所有值都映射到同一个有限域内。 有限域 $\\mathbb {F} _{p}$ 上的 EC 椭圆曲线方程为： $$ y^2 = x^3 + ax + b (\\mod p), 0 \\le x \\le p $$ 目前主要有两种有限域在 ECC 中被广泛应用： 以素数为模的整数域: $\\mathbb {F} _{p}$ 在通用处理器上计算很快 以 2 的幂为模的整数域: $\\mathbb {F} _{2^{m}}$ 当使用专用硬件时，计算速度很快 通过限制 x 为整数，并使用取模进行了映射后，椭圆曲线的形状已经面目全非了，它的加减法也不再具有几何意义。 但是它的一些特性仍然跟椭圆曲线很类似，各种公式基本加个 $\\mod p$ 就变成了它的有限域版本： 无穷远点 $O_{\\infty}$ 是零元，$O_{\\infty} + O_{\\infty} = O_{\\infty}$，$O_{\\infty} + P = P$ $P_{x, y}$ 的负元为 $P_{x, -y}$,，并且有 $P + (-P) = O_{\\infty}$ $P * 0 = O_{\\infty}$ 如果 $P_{x1, y1} + Q_{x2, y2} = R_{x3, y3}$，则其坐标有如下关系 $x3 = (k^2 - x1 - x2) \\mod p$ $y3 = (k(x1 - x3) - y1) \\mod p$ 斜率 $k$ 的计算 如果 $P=Q$，则 $k=\\dfrac {3x^{2}+a} {2y_{1}}$ 否则 $k=\\dfrac {y_{2}-y_{1}} {x_{2}-x_{1}} $ ECDLP 椭圆曲线离散对数问题前面已经介绍了椭圆曲线上的 k 倍运算 及相关的高效算法，但是我们还没有涉及到除法。 椭圆曲线上的除法是一个尚未被解决的难题——「ECDLP 椭圆曲线离散对数问题」： 已知 $kG$ 与基点 $G$，求整数 $k$ 的值。 目前并没有有效的手段可以快速计算出 $k$ 的值。 比较直观的方法应该是从基点 $G$ 开始不断进行加法运算，直到结果与 $kG$ 相等。 目前已知的 ECDLP 最快的解法所需步骤为 $\\sqrt{k}$，而 k 倍运算高效算法前面已经介绍过了，所需步骤为 $log_2(k)$。 在 $k$ 非常大的情况下，它们的计算用时将会有指数级的差距。 椭圆曲线上的 k 倍运算与素数上的幂运算很类似，因此 ECC 底层的数学难题 ECDLP 与 RSA 的离散对数问题 DLP 也有很大相似性。 ","date":"2022-03-09","objectID":"/posts/practical-cryptography-basics-7-asymmetric-key-ciphers/:4:1","series":null,"tags":["Cryptography","密码学","非对称加密","安全","RSA","ECC"],"title":"写给开发人员的实用密码学（七）—— 非对称密钥加密算法 RSA/ECC","uri":"/posts/practical-cryptography-basics-7-asymmetric-key-ciphers/#1-加法与负元"},{"categories":["tech"],"content":" 椭圆曲线的数学原理简介在数学中，椭圆曲线（Elliptic Curves）是一种平面曲线，由如下方程定义的点的集合组成（$A-J$ 均为常数）： $$ Ax^3 + Bx^2y + Cxy^2 + Dy^3 + Ex^2 + Fxy + Gy^2 + Hx + Iy + J = 0 $$ 而 ECC 只使用了其中很简单的一个子集（$a, b$ 均为常数）： $$ y^2 = x^3 + ax + b $$ 比如著名的 NIST 曲线 secp256k1 就是基于如下椭圆曲线方程： $$ y^2 = x^3 + 7 $$ 椭圆曲线大概长这么个形状： 椭圆曲线跟椭圆的关系，就犹如雷锋跟雷峰塔、Java 跟 JavaScript… 你可以通过如下网站手动调整 $a$ 与 $b$ 的值，拖动曲线的交点： https://www.desmos.com/calculator/ialhd71we3?lang=zh-CN 椭圆曲线上的运算数学家在椭圆曲线上定义了一些运算规则，ECC 就依赖于这些规则，下面简单介绍下我们用得到的部分。 1. 加法与负元对于曲线上的任意两点 $A$ 与 $B$，我们定义过 $A, B$ 的直线与曲线的交点为 $-(A+B)$，而 $-(A+B)$ 相对于 x 轴的对称点即为 $A+B$: 上述描述一是定义了椭圆曲线的加法规则，二是定义了椭圆曲线上的负元运算。 2. 二倍运算在加法规则中，如果 $A=B$，我们定义曲线在 $A$ 点的切线与曲线的交点为 $-2A$，于是得到二倍运算的规则： 3. 无穷远点对于 $(-A) + A$ 这种一个值与其负元本身相加的情况，我们会发现过这两点的直线没有交点，前面定义的加法规则在这种情况下失效。 为了解决这个问题，我们假设这直线与椭圆曲线相交于无穷远点 $O_{\\infty}$. 4. k 倍运算我们在前面已经定义了椭圆曲线上的加法运算、二倍运算以及无穷远点，有了这三个概念，我们就能定义k 倍运算 了。 K 倍运算最简单的计算方法，就是不断地进行加法运算，但是也有许多更高效的算法。 其中最简单的算法是「Double-and-add」，它要求首先 $k$ 拆分成如下形式 $$ k = k_{0}+2k_{1}+2^{2}k_{2}+\\cdots +2^{m}k_{m} \\\\ \\text{其中} k_{0}~..~k_{m}\\in {0,1},m=\\lfloor \\log _{2}{k}\\rfloor $$ 然后再迭代计算其中各项的值，它的运算复杂度为 $log_{2}(k)$. 因 Double 和 add 的执行时间不同，根据执行时间就可以知道是执行 Double 还是 add，间接可以推算出 k. 因此这个算法会有计时攻击的风险。 基于「Double-and-add」修改的蒙哥马利阶梯（Montgomery Ladder）是可以避免计时分析的作法，这里就不详细介绍了。 5. 有限域上的椭圆曲线椭圆曲线是连续且无限的，而计算机却更擅长处理离散的、存在上限的整数，因此 ECC 使用「有限域上的椭圆曲线」进行计算。 「有限域（也被称作 Galois Filed, 缩写为 GF）」顾名思义，就是指只有有限个数值的域。 有限域上的椭圆曲线方程，通过取模的方式将曲线上的所有值都映射到同一个有限域内。 有限域 $\\mathbb {F} _{p}$ 上的 EC 椭圆曲线方程为： $$ y^2 = x^3 + ax + b (\\mod p), 0 \\le x \\le p $$ 目前主要有两种有限域在 ECC 中被广泛应用： 以素数为模的整数域: $\\mathbb {F} _{p}$ 在通用处理器上计算很快 以 2 的幂为模的整数域: $\\mathbb {F} _{2^{m}}$ 当使用专用硬件时，计算速度很快 通过限制 x 为整数，并使用取模进行了映射后，椭圆曲线的形状已经面目全非了，它的加减法也不再具有几何意义。 但是它的一些特性仍然跟椭圆曲线很类似，各种公式基本加个 $\\mod p$ 就变成了它的有限域版本： 无穷远点 $O_{\\infty}$ 是零元，$O_{\\infty} + O_{\\infty} = O_{\\infty}$，$O_{\\infty} + P = P$ $P_{x, y}$ 的负元为 $P_{x, -y}$,，并且有 $P + (-P) = O_{\\infty}$ $P * 0 = O_{\\infty}$ 如果 $P_{x1, y1} + Q_{x2, y2} = R_{x3, y3}$，则其坐标有如下关系 $x3 = (k^2 - x1 - x2) \\mod p$ $y3 = (k(x1 - x3) - y1) \\mod p$ 斜率 $k$ 的计算 如果 $P=Q$，则 $k=\\dfrac {3x^{2}+a} {2y_{1}}$ 否则 $k=\\dfrac {y_{2}-y_{1}} {x_{2}-x_{1}} $ ECDLP 椭圆曲线离散对数问题前面已经介绍了椭圆曲线上的 k 倍运算 及相关的高效算法，但是我们还没有涉及到除法。 椭圆曲线上的除法是一个尚未被解决的难题——「ECDLP 椭圆曲线离散对数问题」： 已知 $kG$ 与基点 $G$，求整数 $k$ 的值。 目前并没有有效的手段可以快速计算出 $k$ 的值。 比较直观的方法应该是从基点 $G$ 开始不断进行加法运算，直到结果与 $kG$ 相等。 目前已知的 ECDLP 最快的解法所需步骤为 $\\sqrt{k}$，而 k 倍运算高效算法前面已经介绍过了，所需步骤为 $log_2(k)$。 在 $k$ 非常大的情况下，它们的计算用时将会有指数级的差距。 椭圆曲线上的 k 倍运算与素数上的幂运算很类似，因此 ECC 底层的数学难题 ECDLP 与 RSA 的离散对数问题 DLP 也有很大相似性。 ","date":"2022-03-09","objectID":"/posts/practical-cryptography-basics-7-asymmetric-key-ciphers/:4:1","series":null,"tags":["Cryptography","密码学","非对称加密","安全","RSA","ECC"],"title":"写给开发人员的实用密码学（七）—— 非对称密钥加密算法 RSA/ECC","uri":"/posts/practical-cryptography-basics-7-asymmetric-key-ciphers/#2-二倍运算"},{"categories":["tech"],"content":" 椭圆曲线的数学原理简介在数学中，椭圆曲线（Elliptic Curves）是一种平面曲线，由如下方程定义的点的集合组成（$A-J$ 均为常数）： $$ Ax^3 + Bx^2y + Cxy^2 + Dy^3 + Ex^2 + Fxy + Gy^2 + Hx + Iy + J = 0 $$ 而 ECC 只使用了其中很简单的一个子集（$a, b$ 均为常数）： $$ y^2 = x^3 + ax + b $$ 比如著名的 NIST 曲线 secp256k1 就是基于如下椭圆曲线方程： $$ y^2 = x^3 + 7 $$ 椭圆曲线大概长这么个形状： 椭圆曲线跟椭圆的关系，就犹如雷锋跟雷峰塔、Java 跟 JavaScript… 你可以通过如下网站手动调整 $a$ 与 $b$ 的值，拖动曲线的交点： https://www.desmos.com/calculator/ialhd71we3?lang=zh-CN 椭圆曲线上的运算数学家在椭圆曲线上定义了一些运算规则，ECC 就依赖于这些规则，下面简单介绍下我们用得到的部分。 1. 加法与负元对于曲线上的任意两点 $A$ 与 $B$，我们定义过 $A, B$ 的直线与曲线的交点为 $-(A+B)$，而 $-(A+B)$ 相对于 x 轴的对称点即为 $A+B$: 上述描述一是定义了椭圆曲线的加法规则，二是定义了椭圆曲线上的负元运算。 2. 二倍运算在加法规则中，如果 $A=B$，我们定义曲线在 $A$ 点的切线与曲线的交点为 $-2A$，于是得到二倍运算的规则： 3. 无穷远点对于 $(-A) + A$ 这种一个值与其负元本身相加的情况，我们会发现过这两点的直线没有交点，前面定义的加法规则在这种情况下失效。 为了解决这个问题，我们假设这直线与椭圆曲线相交于无穷远点 $O_{\\infty}$. 4. k 倍运算我们在前面已经定义了椭圆曲线上的加法运算、二倍运算以及无穷远点，有了这三个概念，我们就能定义k 倍运算 了。 K 倍运算最简单的计算方法，就是不断地进行加法运算，但是也有许多更高效的算法。 其中最简单的算法是「Double-and-add」，它要求首先 $k$ 拆分成如下形式 $$ k = k_{0}+2k_{1}+2^{2}k_{2}+\\cdots +2^{m}k_{m} \\\\ \\text{其中} k_{0}~..~k_{m}\\in {0,1},m=\\lfloor \\log _{2}{k}\\rfloor $$ 然后再迭代计算其中各项的值，它的运算复杂度为 $log_{2}(k)$. 因 Double 和 add 的执行时间不同，根据执行时间就可以知道是执行 Double 还是 add，间接可以推算出 k. 因此这个算法会有计时攻击的风险。 基于「Double-and-add」修改的蒙哥马利阶梯（Montgomery Ladder）是可以避免计时分析的作法，这里就不详细介绍了。 5. 有限域上的椭圆曲线椭圆曲线是连续且无限的，而计算机却更擅长处理离散的、存在上限的整数，因此 ECC 使用「有限域上的椭圆曲线」进行计算。 「有限域（也被称作 Galois Filed, 缩写为 GF）」顾名思义，就是指只有有限个数值的域。 有限域上的椭圆曲线方程，通过取模的方式将曲线上的所有值都映射到同一个有限域内。 有限域 $\\mathbb {F} _{p}$ 上的 EC 椭圆曲线方程为： $$ y^2 = x^3 + ax + b (\\mod p), 0 \\le x \\le p $$ 目前主要有两种有限域在 ECC 中被广泛应用： 以素数为模的整数域: $\\mathbb {F} _{p}$ 在通用处理器上计算很快 以 2 的幂为模的整数域: $\\mathbb {F} _{2^{m}}$ 当使用专用硬件时，计算速度很快 通过限制 x 为整数，并使用取模进行了映射后，椭圆曲线的形状已经面目全非了，它的加减法也不再具有几何意义。 但是它的一些特性仍然跟椭圆曲线很类似，各种公式基本加个 $\\mod p$ 就变成了它的有限域版本： 无穷远点 $O_{\\infty}$ 是零元，$O_{\\infty} + O_{\\infty} = O_{\\infty}$，$O_{\\infty} + P = P$ $P_{x, y}$ 的负元为 $P_{x, -y}$,，并且有 $P + (-P) = O_{\\infty}$ $P * 0 = O_{\\infty}$ 如果 $P_{x1, y1} + Q_{x2, y2} = R_{x3, y3}$，则其坐标有如下关系 $x3 = (k^2 - x1 - x2) \\mod p$ $y3 = (k(x1 - x3) - y1) \\mod p$ 斜率 $k$ 的计算 如果 $P=Q$，则 $k=\\dfrac {3x^{2}+a} {2y_{1}}$ 否则 $k=\\dfrac {y_{2}-y_{1}} {x_{2}-x_{1}} $ ECDLP 椭圆曲线离散对数问题前面已经介绍了椭圆曲线上的 k 倍运算 及相关的高效算法，但是我们还没有涉及到除法。 椭圆曲线上的除法是一个尚未被解决的难题——「ECDLP 椭圆曲线离散对数问题」： 已知 $kG$ 与基点 $G$，求整数 $k$ 的值。 目前并没有有效的手段可以快速计算出 $k$ 的值。 比较直观的方法应该是从基点 $G$ 开始不断进行加法运算，直到结果与 $kG$ 相等。 目前已知的 ECDLP 最快的解法所需步骤为 $\\sqrt{k}$，而 k 倍运算高效算法前面已经介绍过了，所需步骤为 $log_2(k)$。 在 $k$ 非常大的情况下，它们的计算用时将会有指数级的差距。 椭圆曲线上的 k 倍运算与素数上的幂运算很类似，因此 ECC 底层的数学难题 ECDLP 与 RSA 的离散对数问题 DLP 也有很大相似性。 ","date":"2022-03-09","objectID":"/posts/practical-cryptography-basics-7-asymmetric-key-ciphers/:4:1","series":null,"tags":["Cryptography","密码学","非对称加密","安全","RSA","ECC"],"title":"写给开发人员的实用密码学（七）—— 非对称密钥加密算法 RSA/ECC","uri":"/posts/practical-cryptography-basics-7-asymmetric-key-ciphers/#3-无穷远点"},{"categories":["tech"],"content":" 椭圆曲线的数学原理简介在数学中，椭圆曲线（Elliptic Curves）是一种平面曲线，由如下方程定义的点的集合组成（$A-J$ 均为常数）： $$ Ax^3 + Bx^2y + Cxy^2 + Dy^3 + Ex^2 + Fxy + Gy^2 + Hx + Iy + J = 0 $$ 而 ECC 只使用了其中很简单的一个子集（$a, b$ 均为常数）： $$ y^2 = x^3 + ax + b $$ 比如著名的 NIST 曲线 secp256k1 就是基于如下椭圆曲线方程： $$ y^2 = x^3 + 7 $$ 椭圆曲线大概长这么个形状： 椭圆曲线跟椭圆的关系，就犹如雷锋跟雷峰塔、Java 跟 JavaScript… 你可以通过如下网站手动调整 $a$ 与 $b$ 的值，拖动曲线的交点： https://www.desmos.com/calculator/ialhd71we3?lang=zh-CN 椭圆曲线上的运算数学家在椭圆曲线上定义了一些运算规则，ECC 就依赖于这些规则，下面简单介绍下我们用得到的部分。 1. 加法与负元对于曲线上的任意两点 $A$ 与 $B$，我们定义过 $A, B$ 的直线与曲线的交点为 $-(A+B)$，而 $-(A+B)$ 相对于 x 轴的对称点即为 $A+B$: 上述描述一是定义了椭圆曲线的加法规则，二是定义了椭圆曲线上的负元运算。 2. 二倍运算在加法规则中，如果 $A=B$，我们定义曲线在 $A$ 点的切线与曲线的交点为 $-2A$，于是得到二倍运算的规则： 3. 无穷远点对于 $(-A) + A$ 这种一个值与其负元本身相加的情况，我们会发现过这两点的直线没有交点，前面定义的加法规则在这种情况下失效。 为了解决这个问题，我们假设这直线与椭圆曲线相交于无穷远点 $O_{\\infty}$. 4. k 倍运算我们在前面已经定义了椭圆曲线上的加法运算、二倍运算以及无穷远点，有了这三个概念，我们就能定义k 倍运算 了。 K 倍运算最简单的计算方法，就是不断地进行加法运算，但是也有许多更高效的算法。 其中最简单的算法是「Double-and-add」，它要求首先 $k$ 拆分成如下形式 $$ k = k_{0}+2k_{1}+2^{2}k_{2}+\\cdots +2^{m}k_{m} \\\\ \\text{其中} k_{0}~..~k_{m}\\in {0,1},m=\\lfloor \\log _{2}{k}\\rfloor $$ 然后再迭代计算其中各项的值，它的运算复杂度为 $log_{2}(k)$. 因 Double 和 add 的执行时间不同，根据执行时间就可以知道是执行 Double 还是 add，间接可以推算出 k. 因此这个算法会有计时攻击的风险。 基于「Double-and-add」修改的蒙哥马利阶梯（Montgomery Ladder）是可以避免计时分析的作法，这里就不详细介绍了。 5. 有限域上的椭圆曲线椭圆曲线是连续且无限的，而计算机却更擅长处理离散的、存在上限的整数，因此 ECC 使用「有限域上的椭圆曲线」进行计算。 「有限域（也被称作 Galois Filed, 缩写为 GF）」顾名思义，就是指只有有限个数值的域。 有限域上的椭圆曲线方程，通过取模的方式将曲线上的所有值都映射到同一个有限域内。 有限域 $\\mathbb {F} _{p}$ 上的 EC 椭圆曲线方程为： $$ y^2 = x^3 + ax + b (\\mod p), 0 \\le x \\le p $$ 目前主要有两种有限域在 ECC 中被广泛应用： 以素数为模的整数域: $\\mathbb {F} _{p}$ 在通用处理器上计算很快 以 2 的幂为模的整数域: $\\mathbb {F} _{2^{m}}$ 当使用专用硬件时，计算速度很快 通过限制 x 为整数，并使用取模进行了映射后，椭圆曲线的形状已经面目全非了，它的加减法也不再具有几何意义。 但是它的一些特性仍然跟椭圆曲线很类似，各种公式基本加个 $\\mod p$ 就变成了它的有限域版本： 无穷远点 $O_{\\infty}$ 是零元，$O_{\\infty} + O_{\\infty} = O_{\\infty}$，$O_{\\infty} + P = P$ $P_{x, y}$ 的负元为 $P_{x, -y}$,，并且有 $P + (-P) = O_{\\infty}$ $P * 0 = O_{\\infty}$ 如果 $P_{x1, y1} + Q_{x2, y2} = R_{x3, y3}$，则其坐标有如下关系 $x3 = (k^2 - x1 - x2) \\mod p$ $y3 = (k(x1 - x3) - y1) \\mod p$ 斜率 $k$ 的计算 如果 $P=Q$，则 $k=\\dfrac {3x^{2}+a} {2y_{1}}$ 否则 $k=\\dfrac {y_{2}-y_{1}} {x_{2}-x_{1}} $ ECDLP 椭圆曲线离散对数问题前面已经介绍了椭圆曲线上的 k 倍运算 及相关的高效算法，但是我们还没有涉及到除法。 椭圆曲线上的除法是一个尚未被解决的难题——「ECDLP 椭圆曲线离散对数问题」： 已知 $kG$ 与基点 $G$，求整数 $k$ 的值。 目前并没有有效的手段可以快速计算出 $k$ 的值。 比较直观的方法应该是从基点 $G$ 开始不断进行加法运算，直到结果与 $kG$ 相等。 目前已知的 ECDLP 最快的解法所需步骤为 $\\sqrt{k}$，而 k 倍运算高效算法前面已经介绍过了，所需步骤为 $log_2(k)$。 在 $k$ 非常大的情况下，它们的计算用时将会有指数级的差距。 椭圆曲线上的 k 倍运算与素数上的幂运算很类似，因此 ECC 底层的数学难题 ECDLP 与 RSA 的离散对数问题 DLP 也有很大相似性。 ","date":"2022-03-09","objectID":"/posts/practical-cryptography-basics-7-asymmetric-key-ciphers/:4:1","series":null,"tags":["Cryptography","密码学","非对称加密","安全","RSA","ECC"],"title":"写给开发人员的实用密码学（七）—— 非对称密钥加密算法 RSA/ECC","uri":"/posts/practical-cryptography-basics-7-asymmetric-key-ciphers/#4-k-倍运算"},{"categories":["tech"],"content":" 椭圆曲线的数学原理简介在数学中，椭圆曲线（Elliptic Curves）是一种平面曲线，由如下方程定义的点的集合组成（$A-J$ 均为常数）： $$ Ax^3 + Bx^2y + Cxy^2 + Dy^3 + Ex^2 + Fxy + Gy^2 + Hx + Iy + J = 0 $$ 而 ECC 只使用了其中很简单的一个子集（$a, b$ 均为常数）： $$ y^2 = x^3 + ax + b $$ 比如著名的 NIST 曲线 secp256k1 就是基于如下椭圆曲线方程： $$ y^2 = x^3 + 7 $$ 椭圆曲线大概长这么个形状： 椭圆曲线跟椭圆的关系，就犹如雷锋跟雷峰塔、Java 跟 JavaScript… 你可以通过如下网站手动调整 $a$ 与 $b$ 的值，拖动曲线的交点： https://www.desmos.com/calculator/ialhd71we3?lang=zh-CN 椭圆曲线上的运算数学家在椭圆曲线上定义了一些运算规则，ECC 就依赖于这些规则，下面简单介绍下我们用得到的部分。 1. 加法与负元对于曲线上的任意两点 $A$ 与 $B$，我们定义过 $A, B$ 的直线与曲线的交点为 $-(A+B)$，而 $-(A+B)$ 相对于 x 轴的对称点即为 $A+B$: 上述描述一是定义了椭圆曲线的加法规则，二是定义了椭圆曲线上的负元运算。 2. 二倍运算在加法规则中，如果 $A=B$，我们定义曲线在 $A$ 点的切线与曲线的交点为 $-2A$，于是得到二倍运算的规则： 3. 无穷远点对于 $(-A) + A$ 这种一个值与其负元本身相加的情况，我们会发现过这两点的直线没有交点，前面定义的加法规则在这种情况下失效。 为了解决这个问题，我们假设这直线与椭圆曲线相交于无穷远点 $O_{\\infty}$. 4. k 倍运算我们在前面已经定义了椭圆曲线上的加法运算、二倍运算以及无穷远点，有了这三个概念，我们就能定义k 倍运算 了。 K 倍运算最简单的计算方法，就是不断地进行加法运算，但是也有许多更高效的算法。 其中最简单的算法是「Double-and-add」，它要求首先 $k$ 拆分成如下形式 $$ k = k_{0}+2k_{1}+2^{2}k_{2}+\\cdots +2^{m}k_{m} \\\\ \\text{其中} k_{0}~..~k_{m}\\in {0,1},m=\\lfloor \\log _{2}{k}\\rfloor $$ 然后再迭代计算其中各项的值，它的运算复杂度为 $log_{2}(k)$. 因 Double 和 add 的执行时间不同，根据执行时间就可以知道是执行 Double 还是 add，间接可以推算出 k. 因此这个算法会有计时攻击的风险。 基于「Double-and-add」修改的蒙哥马利阶梯（Montgomery Ladder）是可以避免计时分析的作法，这里就不详细介绍了。 5. 有限域上的椭圆曲线椭圆曲线是连续且无限的，而计算机却更擅长处理离散的、存在上限的整数，因此 ECC 使用「有限域上的椭圆曲线」进行计算。 「有限域（也被称作 Galois Filed, 缩写为 GF）」顾名思义，就是指只有有限个数值的域。 有限域上的椭圆曲线方程，通过取模的方式将曲线上的所有值都映射到同一个有限域内。 有限域 $\\mathbb {F} _{p}$ 上的 EC 椭圆曲线方程为： $$ y^2 = x^3 + ax + b (\\mod p), 0 \\le x \\le p $$ 目前主要有两种有限域在 ECC 中被广泛应用： 以素数为模的整数域: $\\mathbb {F} _{p}$ 在通用处理器上计算很快 以 2 的幂为模的整数域: $\\mathbb {F} _{2^{m}}$ 当使用专用硬件时，计算速度很快 通过限制 x 为整数，并使用取模进行了映射后，椭圆曲线的形状已经面目全非了，它的加减法也不再具有几何意义。 但是它的一些特性仍然跟椭圆曲线很类似，各种公式基本加个 $\\mod p$ 就变成了它的有限域版本： 无穷远点 $O_{\\infty}$ 是零元，$O_{\\infty} + O_{\\infty} = O_{\\infty}$，$O_{\\infty} + P = P$ $P_{x, y}$ 的负元为 $P_{x, -y}$,，并且有 $P + (-P) = O_{\\infty}$ $P * 0 = O_{\\infty}$ 如果 $P_{x1, y1} + Q_{x2, y2} = R_{x3, y3}$，则其坐标有如下关系 $x3 = (k^2 - x1 - x2) \\mod p$ $y3 = (k(x1 - x3) - y1) \\mod p$ 斜率 $k$ 的计算 如果 $P=Q$，则 $k=\\dfrac {3x^{2}+a} {2y_{1}}$ 否则 $k=\\dfrac {y_{2}-y_{1}} {x_{2}-x_{1}} $ ECDLP 椭圆曲线离散对数问题前面已经介绍了椭圆曲线上的 k 倍运算 及相关的高效算法，但是我们还没有涉及到除法。 椭圆曲线上的除法是一个尚未被解决的难题——「ECDLP 椭圆曲线离散对数问题」： 已知 $kG$ 与基点 $G$，求整数 $k$ 的值。 目前并没有有效的手段可以快速计算出 $k$ 的值。 比较直观的方法应该是从基点 $G$ 开始不断进行加法运算，直到结果与 $kG$ 相等。 目前已知的 ECDLP 最快的解法所需步骤为 $\\sqrt{k}$，而 k 倍运算高效算法前面已经介绍过了，所需步骤为 $log_2(k)$。 在 $k$ 非常大的情况下，它们的计算用时将会有指数级的差距。 椭圆曲线上的 k 倍运算与素数上的幂运算很类似，因此 ECC 底层的数学难题 ECDLP 与 RSA 的离散对数问题 DLP 也有很大相似性。 ","date":"2022-03-09","objectID":"/posts/practical-cryptography-basics-7-asymmetric-key-ciphers/:4:1","series":null,"tags":["Cryptography","密码学","非对称加密","安全","RSA","ECC"],"title":"写给开发人员的实用密码学（七）—— 非对称密钥加密算法 RSA/ECC","uri":"/posts/practical-cryptography-basics-7-asymmetric-key-ciphers/#5-有限域上的椭圆曲线"},{"categories":["tech"],"content":" 椭圆曲线的数学原理简介在数学中，椭圆曲线（Elliptic Curves）是一种平面曲线，由如下方程定义的点的集合组成（$A-J$ 均为常数）： $$ Ax^3 + Bx^2y + Cxy^2 + Dy^3 + Ex^2 + Fxy + Gy^2 + Hx + Iy + J = 0 $$ 而 ECC 只使用了其中很简单的一个子集（$a, b$ 均为常数）： $$ y^2 = x^3 + ax + b $$ 比如著名的 NIST 曲线 secp256k1 就是基于如下椭圆曲线方程： $$ y^2 = x^3 + 7 $$ 椭圆曲线大概长这么个形状： 椭圆曲线跟椭圆的关系，就犹如雷锋跟雷峰塔、Java 跟 JavaScript… 你可以通过如下网站手动调整 $a$ 与 $b$ 的值，拖动曲线的交点： https://www.desmos.com/calculator/ialhd71we3?lang=zh-CN 椭圆曲线上的运算数学家在椭圆曲线上定义了一些运算规则，ECC 就依赖于这些规则，下面简单介绍下我们用得到的部分。 1. 加法与负元对于曲线上的任意两点 $A$ 与 $B$，我们定义过 $A, B$ 的直线与曲线的交点为 $-(A+B)$，而 $-(A+B)$ 相对于 x 轴的对称点即为 $A+B$: 上述描述一是定义了椭圆曲线的加法规则，二是定义了椭圆曲线上的负元运算。 2. 二倍运算在加法规则中，如果 $A=B$，我们定义曲线在 $A$ 点的切线与曲线的交点为 $-2A$，于是得到二倍运算的规则： 3. 无穷远点对于 $(-A) + A$ 这种一个值与其负元本身相加的情况，我们会发现过这两点的直线没有交点，前面定义的加法规则在这种情况下失效。 为了解决这个问题，我们假设这直线与椭圆曲线相交于无穷远点 $O_{\\infty}$. 4. k 倍运算我们在前面已经定义了椭圆曲线上的加法运算、二倍运算以及无穷远点，有了这三个概念，我们就能定义k 倍运算 了。 K 倍运算最简单的计算方法，就是不断地进行加法运算，但是也有许多更高效的算法。 其中最简单的算法是「Double-and-add」，它要求首先 $k$ 拆分成如下形式 $$ k = k_{0}+2k_{1}+2^{2}k_{2}+\\cdots +2^{m}k_{m} \\\\ \\text{其中} k_{0}~..~k_{m}\\in {0,1},m=\\lfloor \\log _{2}{k}\\rfloor $$ 然后再迭代计算其中各项的值，它的运算复杂度为 $log_{2}(k)$. 因 Double 和 add 的执行时间不同，根据执行时间就可以知道是执行 Double 还是 add，间接可以推算出 k. 因此这个算法会有计时攻击的风险。 基于「Double-and-add」修改的蒙哥马利阶梯（Montgomery Ladder）是可以避免计时分析的作法，这里就不详细介绍了。 5. 有限域上的椭圆曲线椭圆曲线是连续且无限的，而计算机却更擅长处理离散的、存在上限的整数，因此 ECC 使用「有限域上的椭圆曲线」进行计算。 「有限域（也被称作 Galois Filed, 缩写为 GF）」顾名思义，就是指只有有限个数值的域。 有限域上的椭圆曲线方程，通过取模的方式将曲线上的所有值都映射到同一个有限域内。 有限域 $\\mathbb {F} _{p}$ 上的 EC 椭圆曲线方程为： $$ y^2 = x^3 + ax + b (\\mod p), 0 \\le x \\le p $$ 目前主要有两种有限域在 ECC 中被广泛应用： 以素数为模的整数域: $\\mathbb {F} _{p}$ 在通用处理器上计算很快 以 2 的幂为模的整数域: $\\mathbb {F} _{2^{m}}$ 当使用专用硬件时，计算速度很快 通过限制 x 为整数，并使用取模进行了映射后，椭圆曲线的形状已经面目全非了，它的加减法也不再具有几何意义。 但是它的一些特性仍然跟椭圆曲线很类似，各种公式基本加个 $\\mod p$ 就变成了它的有限域版本： 无穷远点 $O_{\\infty}$ 是零元，$O_{\\infty} + O_{\\infty} = O_{\\infty}$，$O_{\\infty} + P = P$ $P_{x, y}$ 的负元为 $P_{x, -y}$,，并且有 $P + (-P) = O_{\\infty}$ $P * 0 = O_{\\infty}$ 如果 $P_{x1, y1} + Q_{x2, y2} = R_{x3, y3}$，则其坐标有如下关系 $x3 = (k^2 - x1 - x2) \\mod p$ $y3 = (k(x1 - x3) - y1) \\mod p$ 斜率 $k$ 的计算 如果 $P=Q$，则 $k=\\dfrac {3x^{2}+a} {2y_{1}}$ 否则 $k=\\dfrac {y_{2}-y_{1}} {x_{2}-x_{1}} $ ECDLP 椭圆曲线离散对数问题前面已经介绍了椭圆曲线上的 k 倍运算 及相关的高效算法，但是我们还没有涉及到除法。 椭圆曲线上的除法是一个尚未被解决的难题——「ECDLP 椭圆曲线离散对数问题」： 已知 $kG$ 与基点 $G$，求整数 $k$ 的值。 目前并没有有效的手段可以快速计算出 $k$ 的值。 比较直观的方法应该是从基点 $G$ 开始不断进行加法运算，直到结果与 $kG$ 相等。 目前已知的 ECDLP 最快的解法所需步骤为 $\\sqrt{k}$，而 k 倍运算高效算法前面已经介绍过了，所需步骤为 $log_2(k)$。 在 $k$ 非常大的情况下，它们的计算用时将会有指数级的差距。 椭圆曲线上的 k 倍运算与素数上的幂运算很类似，因此 ECC 底层的数学难题 ECDLP 与 RSA 的离散对数问题 DLP 也有很大相似性。 ","date":"2022-03-09","objectID":"/posts/practical-cryptography-basics-7-asymmetric-key-ciphers/:4:1","series":null,"tags":["Cryptography","密码学","非对称加密","安全","RSA","ECC"],"title":"写给开发人员的实用密码学（七）—— 非对称密钥加密算法 RSA/ECC","uri":"/posts/practical-cryptography-basics-7-asymmetric-key-ciphers/#ecdlp-椭圆曲线离散对数问题"},{"categories":["tech"],"content":" ECC 密钥对生成首先，跟 RSA 一样，让我们先看下怎么使用 openssl 生成一个使用 prime256v1 曲线的 ECC 密钥对： # 列出 openssl 支持的所有曲线名称 openssl ecparam -list_curves # 生成 ec 算法的私钥，使用 prime256v1 算法，密钥长度 256 位。（强度大于 2048 位的 RSA 密钥） openssl ecparam -genkey -name prime256v1 -out ecc-private-key.pem # 通过密钥生成公钥 openssl ec -in ecc-private-key.pem -pubout -out ecc-public-key.pem # 查看私钥内容 ❯ cat ecc-private-key.pem -----BEGIN EC PARAMETERS----- BggqhkjOPQMBBw== -----END EC PARAMETERS----- -----BEGIN EC PRIVATE KEY----- MHcCAQEEIGm3wT/m4gDaoJGKfAHDXV2BVtdyb/aPTITJR5B6KVEtoAoGCCqGSM49 AwEHoUQDQgAE5IEIorw0WU5+om/UgfyYSKosiGO6Hpe8hxkqL5GUVPyu4LJkfw/e 99zhNJatliZ1Az/yCKww5KrXC8bQ9wGQvw== -----END EC PRIVATE KEY----- # 查看私钥的详细参数 ❯ openssl ec -noout -text -in ecc-private-key.pem read EC key Private-Key: (256 bit) priv: 69:b7:c1:3f:e6:e2:00:da:a0:91:8a:7c:01:c3:5d: 5d:81:56:d7:72:6f:f6:8f:4c:84:c9:47:90:7a:29: 51:2d pub: 04:e4:81:08:a2:bc:34:59:4e:7e:a2:6f:d4:81:fc: 98:48:aa:2c:88:63:ba:1e:97:bc:87:19:2a:2f:91: 94:54:fc:ae:e0:b2:64:7f:0f🇩🇪f7:dc:e1:34:96: ad:96:26:75:03:3f:f2:08:ac:30:e4:aa:d7:0b:c6: d0:f7:01:90:bf ASN1 OID: prime256v1 NIST CURVE: P-256 # 查看公钥内容 ❯ cat ecc-public-key.pem -----BEGIN PUBLIC KEY----- MFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAE5IEIorw0WU5+om/UgfyYSKosiGO6 Hpe8hxkqL5GUVPyu4LJkfw/e99zhNJatliZ1Az/yCKww5KrXC8bQ9wGQvw== -----END PUBLIC KEY----- # 查看公钥的参数 ❯ openssl ec -noout -text -pubin -in ecc-public-key.pem read EC key Private-Key: (256 bit) pub: 04:e4:81:08:a2:bc:34:59:4e:7e:a2:6f:d4:81:fc: 98:48:aa:2c:88:63:ba:1e:97:bc:87:19:2a:2f:91: 94:54:fc:ae:e0:b2:64:7f:0f🇩🇪f7:dc:e1:34:96: ad:96:26:75:03:3f:f2:08:ac:30:e4:aa:d7:0b:c6: d0:f7:01:90:bf ASN1 OID: prime256v1 NIST CURVE: P-256 可以看到 ECC 算法的公钥私钥都比 RSA 小了非常多，数据量小，却能带来同等的安全强度，这是 ECC 相比 RSA 最大的优势。 私钥的参数： priv: 私钥，一个 256bits 的大整数，对应我们前面介绍的 $k 倍运算$中的 $k$ pub: 公钥，是一个椭圆曲线（EC）上的坐标 ${x, y}$，也就是我们 well-known 的基点 $G$ ASN1 OID: prime256v1, 椭圆曲线的名称 NIST CURVE: P-256 使用安全随机数生成器即可直接生成出 ECC 的私钥 priv，因此 ECC 的密钥对生成速度非常快。 ","date":"2022-03-09","objectID":"/posts/practical-cryptography-basics-7-asymmetric-key-ciphers/:4:2","series":null,"tags":["Cryptography","密码学","非对称加密","安全","RSA","ECC"],"title":"写给开发人员的实用密码学（七）—— 非对称密钥加密算法 RSA/ECC","uri":"/posts/practical-cryptography-basics-7-asymmetric-key-ciphers/#ecc-密钥对生成"},{"categories":["tech"],"content":" ECDH 密钥交换这个在前面写给开发人员的实用密码学（五）—— 密钥交换 DHKE 与完美前向保密 PFS已经介绍过了，不过这里再复述一遍： Alice 跟 Bob 协商好椭圆曲线的各项参数，以及基点 G，这些参数都是公开的。 Alice 生成一个随机的 ECC 密钥对（公钥：$alicePrivate * G$, 私钥: $alicePrivate$） Bob 生成一个随机的 ECC 密钥对（公钥：$bobPrivate * G$, 私钥: $bobPrivate$） 两人通过不安全的信道交换公钥 Alice 将 Bob 的公钥乘上自己的私钥，得到共享密钥 $sharedKey = (bobPrivate * G) * alicePrivate$ Bob 将 Alice 的公钥乘上自己的私钥，得到共享密钥 $sharedKey = (alicePrivate * G) * bobPrivate$ 因为 $(a * G) * b = (b * G) * a$，Alice 与 Bob 计算出的共享密钥应该是相等的 这样两方就通过 ECDH 完成了密钥交换。 而 ECDH 的安全性，则由 ECDLP 问题提供保证。 ","date":"2022-03-09","objectID":"/posts/practical-cryptography-basics-7-asymmetric-key-ciphers/:4:3","series":null,"tags":["Cryptography","密码学","非对称加密","安全","RSA","ECC"],"title":"写给开发人员的实用密码学（七）—— 非对称密钥加密算法 RSA/ECC","uri":"/posts/practical-cryptography-basics-7-asymmetric-key-ciphers/#ecdh-密钥交换"},{"categories":["tech"],"content":" ECC 加密与解密ECC 本身并没有提供加密与解密的功能，但是我们可以借助 ECDH 迂回实现加解密。流程如下： Bob 想要将消息 M 安全地发送给 Alice，他手上已经拥有了 Alice 的 ECC 公钥 alicePubKey Bob 首先使用如下算法生成出「共享密钥」+「密文公钥」 随机生成一个临时 ECC 密钥对 私钥：安全随机数 ciphertextPrivKey 公钥：ciphertextPubKey = ciphertextPrivKey * G 使用 ECDH 计算出共享密钥：sharedECCKey = alicePubKey * ciphertextPrivKey Bob 使用「共享密钥」与对称加密算法加密消息，得到密文 C 比如使用 AES-256-GCM 或者 ChaCha20-Poly1305 进行对称加密 Bob 将 C + ciphertextPubKey 打包传输给 Alice Alice 使用 ciphertextPubKey 与自己的私钥计算出共享密钥 sharedECCKey = ciphertextPubKey * alicePrivKey Alice 使用计算出的共享密钥解密 C 得到消息 M 实际上就是消息的发送方先生成一个临时的 ECC 密钥对，然后借助 ECDH 协议计算出共享密钥用于加密。 消息的接收方同样通过 ECDH 协议计算出共享密钥再解密数据。 使用 Python 演示如下： # pip install tinyec # \u003c= ECC 曲线库 from tinyec import registry import secrets # 使用这条曲线进行演示 curve = registry.get_curve('brainpoolP256r1') def compress_point(point): return hex(point.x) + hex(point.y % 2)[2:] def ecc_calc_encryption_keys(pubKey): \"\"\" 安全地生成一个随机 ECC 密钥对，然后按 ECDH 流程计算出共享密钥 sharedECCKey 最后返回（共享密钥, 临时 ECC 公钥 ciphertextPubKey） \"\"\" ciphertextPrivKey = secrets.randbelow(curve.field.n) ciphertextPubKey = ciphertextPrivKey * curve.g sharedECCKey = pubKey * ciphertextPrivKey return (sharedECCKey, ciphertextPubKey) def ecc_calc_decryption_key(privKey, ciphertextPubKey): sharedECCKey = ciphertextPubKey * privKey return sharedECCKey # 1. 首先生成出 Alice 的 ECC 密钥对 privKey = secrets.randbelow(curve.field.n) pubKey = privKey * curve.g print(\"private key:\", hex(privKey)) print(\"public key:\", compress_point(pubKey)) # 2. Alice 将公钥发送给 Bob # 3. Bob 使用 Alice 的公钥生成出（共享密钥, 临时 ECC 公钥 ciphertextPubKey） (encryptKey, ciphertextPubKey) = ecc_calc_encryption_keys(pubKey) print(\"ciphertext pubKey:\", compress_point(ciphertextPubKey)) print(\"encryption key:\", compress_point(encryptKey)) # 4. Bob 使用共享密钥 encryptKey 加密数据，然后将密文与 ciphertextPubKey 一起发送给 Alice # 5. Alice 使用自己的私钥 + ciphertextPubKey 计算出共享密钥 decryptKey decryptKey = ecc_calc_decryption_key(privKey, ciphertextPubKey) print(\"decryption key:\", compress_point(decryptKey)) # 6. Alice 使用 decryptKey 解密密文得到原始消息 ","date":"2022-03-09","objectID":"/posts/practical-cryptography-basics-7-asymmetric-key-ciphers/:4:4","series":null,"tags":["Cryptography","密码学","非对称加密","安全","RSA","ECC"],"title":"写给开发人员的实用密码学（七）—— 非对称密钥加密算法 RSA/ECC","uri":"/posts/practical-cryptography-basics-7-asymmetric-key-ciphers/#ecc-加密与解密"},{"categories":["tech"],"content":" ECC 数字签名前面已经介绍了 RSA 签名，这里介绍下基于 ECC 的签名算法。 基于 ECC 的签名算法主要有两种：ECDSA 与 EdDSA，以及 EdDSA 的变体。 其中 ECDSA 算法稍微有点复杂，而安全强度跟它基本一致的 EdDSA 的算法更简洁更易于理解，在使用特定曲线的情况下 EdDSA 还要比 ECDSA 更快一点，因此现在通常更推荐使用 EdDSA 算法。 EdDSA 与 Ed25519 签名算法EdDSA（Edwards-curve Digital Signature Algorithm）是一种现代的安全数字签名算法，它使用专为性能优化的椭圆曲线，如 255bits 曲线 edwards25519 和 448bits 曲线 edwards448. EdDSA 签名算法及其变体 Ed25519 和 Ed448 在技术上在 RFC8032 中进行了描述。 首先，用户需要基于 edwards25519 或者 edwards448 曲线，生成一个 ECC 密钥对。 生成私钥的时候，算法首先生成一个随机数，然后会对随机数做一些变换以确保安全性，防范计时攻击等攻击手段。 对于 edwards25519 公私钥都是 32 字节，而对于 edwards448 公私钥都是 57 字节。 对于 edwards25519 输出的签名长度为 64 字节，而对于 Ed448 输出为 114 字节。 具体的算法虽然比 ECDSA 简单，但还是有点难度的，这里就直接略过了。 下面给出个 ed25519 的计算示例： # pip install cryptography==36.0.1 from cryptography.hazmat.primitives.asymmetric.ed25519 import Ed25519PrivateKey # 也可用 openssl 生成，都没啥毛病 private_key = Ed25519PrivateKey.generate() # 签名 signature = private_key.sign(b\"my authenticated message\") # 显然 ECC 的公钥 kG 也能直接从私钥 k 生成 public_key = private_key.public_key() # 验证 # Raises InvalidSignature if verification fails public_key.verify(signature, b\"my authenticated message\") ed448 的代码也完全类似： # pip install cryptography==36.0.1 from cryptography.hazmat.primitives.asymmetric.ed448 import Ed448PrivateKey private_key = Ed448PrivateKey.generate() signature = private_key.sign(b\"my authenticated message\") public_key = private_key.public_key() # Raises InvalidSignature if verification fails public_key.verify(signature, b\"my authenticated message\") ","date":"2022-03-09","objectID":"/posts/practical-cryptography-basics-7-asymmetric-key-ciphers/:4:5","series":null,"tags":["Cryptography","密码学","非对称加密","安全","RSA","ECC"],"title":"写给开发人员的实用密码学（七）—— 非对称密钥加密算法 RSA/ECC","uri":"/posts/practical-cryptography-basics-7-asymmetric-key-ciphers/#ecc-数字签名"},{"categories":["tech"],"content":" ECC 数字签名前面已经介绍了 RSA 签名，这里介绍下基于 ECC 的签名算法。 基于 ECC 的签名算法主要有两种：ECDSA 与 EdDSA，以及 EdDSA 的变体。 其中 ECDSA 算法稍微有点复杂，而安全强度跟它基本一致的 EdDSA 的算法更简洁更易于理解，在使用特定曲线的情况下 EdDSA 还要比 ECDSA 更快一点，因此现在通常更推荐使用 EdDSA 算法。 EdDSA 与 Ed25519 签名算法EdDSA（Edwards-curve Digital Signature Algorithm）是一种现代的安全数字签名算法，它使用专为性能优化的椭圆曲线，如 255bits 曲线 edwards25519 和 448bits 曲线 edwards448. EdDSA 签名算法及其变体 Ed25519 和 Ed448 在技术上在 RFC8032 中进行了描述。 首先，用户需要基于 edwards25519 或者 edwards448 曲线，生成一个 ECC 密钥对。 生成私钥的时候，算法首先生成一个随机数，然后会对随机数做一些变换以确保安全性，防范计时攻击等攻击手段。 对于 edwards25519 公私钥都是 32 字节，而对于 edwards448 公私钥都是 57 字节。 对于 edwards25519 输出的签名长度为 64 字节，而对于 Ed448 输出为 114 字节。 具体的算法虽然比 ECDSA 简单，但还是有点难度的，这里就直接略过了。 下面给出个 ed25519 的计算示例： # pip install cryptography==36.0.1 from cryptography.hazmat.primitives.asymmetric.ed25519 import Ed25519PrivateKey # 也可用 openssl 生成，都没啥毛病 private_key = Ed25519PrivateKey.generate() # 签名 signature = private_key.sign(b\"my authenticated message\") # 显然 ECC 的公钥 kG 也能直接从私钥 k 生成 public_key = private_key.public_key() # 验证 # Raises InvalidSignature if verification fails public_key.verify(signature, b\"my authenticated message\") ed448 的代码也完全类似： # pip install cryptography==36.0.1 from cryptography.hazmat.primitives.asymmetric.ed448 import Ed448PrivateKey private_key = Ed448PrivateKey.generate() signature = private_key.sign(b\"my authenticated message\") public_key = private_key.public_key() # Raises InvalidSignature if verification fails public_key.verify(signature, b\"my authenticated message\") ","date":"2022-03-09","objectID":"/posts/practical-cryptography-basics-7-asymmetric-key-ciphers/:4:5","series":null,"tags":["Cryptography","密码学","非对称加密","安全","RSA","ECC"],"title":"写给开发人员的实用密码学（七）—— 非对称密钥加密算法 RSA/ECC","uri":"/posts/practical-cryptography-basics-7-asymmetric-key-ciphers/#eddsa-与-ed25519-签名算法"},{"categories":["tech"],"content":" 密码学常用椭圆曲线介绍在介绍密码学中的常用椭圆曲线前，需要先介绍一下椭圆曲线的阶（order）以及辅助因子（cofactor）这两个概念。 首先还得介绍下数学中「循环群」的概念，它是指能由单个元素所生成的群，在 ECC 中这就是预先定义好的基点 $G$. 一个有限域上的椭圆曲线可以形成一个有限「循环代数群」，它由曲线上的所有点组成。椭圆曲线的阶被定义为该曲线上所有点的个数（包括无穷远点）。 有些曲线加上 G 点可以形成一个单一循环群，这一个群包含了曲线上的所有点。而其他的曲线加上 G 点则形成多个不相交的循环子群，每个子群包含了曲线的一个子集。 对于上述第二种情况，假设曲线上的点被拆分到了 h 个循环子群中，每个子群的阶都是 r，那这时整个群的阶就是 $n = h * r$，其中子群的个数 h 被称为辅助因子。 有限域上的椭圆曲线的阶都是有限的，也就是说对于曲线上任意一点 $G$，我们计算它的数乘 $kG$，随着整数 $k$ 的增大，一定会存在某个 $k$ 使 $kG = O_{\\infty}$ 成立，然后 $k$ 继续增大时，因为 $O_{\\infty} * P = O_{\\infty}$，$kG$ 的值就固定为 $O_{\\infty}$ 了，更大的 $k$ 值已经失去了意义。 因此 ECC 中要求 $kG$ 中的私钥 $k$ 符合条件 $0 \\le k \\le r$，也就是说总的私钥数量是受 $r$ 限制的。 辅助因子通过用如下公式表示： $$ h = n / r $$ 其中 $n$ 是曲线的阶，$r$ 是每个子群的阶，$h$ 是辅助因子。 如果曲线形成了一个单一循环群，那显然 $h = 1$，否则 $h \u003e 1$ 举例如下： secp256k1 的辅助因子为 1 Curve25519 的辅助因子为 8 Curve448 的辅助因子为 4 生成点 G生成点 G 的选择是很有讲究的，虽然每个循环子群都包含有很多个生成点，但是 ECC 只会谨慎的选择其中一个。 首先 G 点必须要能生成出整个循环子群，其次还需要有尽可能高的计算性能。 数学上已知某些椭圆曲线上，不同的生成点生成出的循环子群，阶也是不同的。如果 G 点选得不好，可能会导致生成出的子群的阶较小。 前面我们已经提过子群的阶 $r$ 会限制总的私钥数量，导致算法强度变弱！因此不恰当的 $G$ 点可能会导致我们遭受「小子群攻击」。 为了避免这种风险，建议尽量使用被广泛使用的加密库，而不是自己撸一个。 椭圆曲线的域参数ECC椭圆曲线由一组椭圆曲线域参数描述，如曲线方程参数、场参数和生成点坐标。这些参数在各种密码学标准中指定，你可以网上搜到相应的 RFC 或 NIST 文档。 这些标准定义了一组命名曲线的参数，例如 secp256k1、P-521、brainpoolP512t1 和 SM2. 这些加密标准中描述的有限域上的椭圆曲线得到了密码学家的充分研究和分析，并被认为具有一定的安全强度。 也有一些密码学家（如 Daniel Bernstein）认为，官方密码标准中描述的大多数曲线都是「不安全的」，并定义了他们自己的密码标准，这些标准在更广泛的层面上考虑了 ECC 安全性。 开发人员应该仅使用各项标准文档给出的、经过密码学家充分研究的命名曲线。 secp256k1此曲线被应用在比特币中，它的域参数如下： p (modulus) = 0xFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFEFFFFFC2F n (order; size; the count of all possible EC points) = 0xFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFEBAAEDCE6AF48A03BBFD25E8CD0364141 a (方程 $y^2 ≡ x^3 + a*x + b (\\mod p)$ 中的常数) = 0x0000000000000000000000000000000000000000000000000000000000000000 b (方程 $y^2 ≡ x^3 + a*x + b (\\mod p)$ 中的常数)= 0x0000000000000000000000000000000000000000000000000000000000000007 g (the curve generator point G {x, y}) = (0x79BE667EF9DCBBAC55A06295CE870B07029BFCDB2DCE28D959F2815B16F81798, 0x483ada7726a3c4655da4fbfc0e1108a8fd17b448a68554199c47d08ffb10d4b8) h (cofactor, typically 1) = 01 Edwards 曲线椭圆曲线方程除了我们前面使用的 Weierstrass 形式 $$y^2 = (x^3 + ax + b) \\mod p$$ 外，还可以被写成其他多种形式，这些不同的形式是双有理等价的（别问，我也不懂什么叫「双有理等价」…）。 不同的方程形式在计算机的数值计算上可能会存在区别。 为了性能考虑，ECC 在部分场景下会考虑使用 Edwards 曲线形式进行计算，该方程形式如下： $$ x^{2}+y^{2}=1+dx^{2}y^{2} $$ 画个图长这样： 知名的 Edwards 曲线有： Curve1174 (251-bit) Curve25519 (255-bit) Curve383187 (383-bit) Curve41417 (414-bit) Curve448 (448-bit) E-521 (521-bit) … Curve25519, X25519 和 Ed25519 https://cryptography.io/en/latest/hazmat/primitives/asymmetric/ed25519/ 只要域参数选得好，Edwards 就可以以非常高的性能实现 ECC 密钥交换、数字签名、混合加密方案。 一个例子就是 Curve25519，它是 Edwards 曲线，其 Montgomery 形式的定义如下： $$ y^{2}=x^{3}+486662x^{2}+x $$ 其被定义在有限域 $\\mathbb {F} _{p}$ 上，$p = 2255 - 19$, 其他域参数如下： 阶 n = 2252 + 0x14def9dea2f79cd65812631a5cf5d3ed 辅因子 h = 8 虽然此曲线并未以 Edwards 形式定义，但是它已被证明与如下扭曲 Edwards 曲线（edwards25519）双有理等价： $$ -x^2 + y^2 = 1 + 37095705934669439343138083508754565189542113879843219016388785533085940283555 x^2 y^2 $$ 上面给出的这种 Edwards 形式与前文给出的 Weierstrass 形式完全等价，是专为计算速度优化而设计成这样的。 Curve25519 由 Daniel Bernstein 领导的密码学家团队精心设计，在多个设计和实现层面上达成了非常高的性能，同时不影响安全性。 Curve25519 的构造使其避免了许多潜在的实现缺陷。根据设计，它不受定时攻击的影响，并且它接受任何 32 字节的字符串作为有效的公钥，并且不需要验证。 它能提供 125.8bits 的安全强度（有时称为 ~ 128bits 安全性） Curve25519 的私钥为 251 位，通常编码为 256 位整数（32 个字节，64 个十六进制数字）。 公钥通常也编码为 256 位整数（255 位 y 坐标 + 1 位 x 坐标），这对开发人员来说非常方便。 基于 Curve25519 派生出了名为 X25519 的 ECDH 算法，以及基于 EdDSA 的高速数字签名算法 Ed25519. Curve448, X448 和 Ed448 https://cryptography.io/en/latest/hazmat/primitives/asymmetric/ed448/ Curve448（Curve448-Goldilocks）是一种非扭曲 Edwards 曲线，它的方程定义如下： $$ x^2 + y^2 = 1 - 39081 x^2 y^2 $$ 其被定义在有限域 $\\mathbb {F} _{p}$ 上，$p = 2448 - 2224 - 1$，其他域参数： 阶 n = 2446 - 0x8335dc163bb124b65129c96fde933d8d723a70aadc873d6d54a7bb0d 辅助因子 h = 4 与 Curve25519 一样，Curve448 也等价于前面给出的 Weierstrass 形式，选择 Edwards 形式主要是因为它能显著提升性能。 Curve448 提供 222.8 位的安全强度。 Curve448 的私钥为 446 位，通常编码为 448 位整数（56 个字节，112 个十六进制数字）。 公钥也被编码为 448 位整数。 基于 Curve448 派生出了名为 X448 的 ECDH 算法，以及基于 EdDSA 的高速数字签名算法 Ed448. 该选择哪种椭圆曲线首先，Bernstein 的 SafeCurves 标准列出了符合一组 ECC 安全要求的安","date":"2022-03-09","objectID":"/posts/practical-cryptography-basics-7-asymmetric-key-ciphers/:4:6","series":null,"tags":["Cryptography","密码学","非对称加密","安全","RSA","ECC"],"title":"写给开发人员的实用密码学（七）—— 非对称密钥加密算法 RSA/ECC","uri":"/posts/practical-cryptography-basics-7-asymmetric-key-ciphers/#密码学常用椭圆曲线介绍"},{"categories":["tech"],"content":" 密码学常用椭圆曲线介绍在介绍密码学中的常用椭圆曲线前，需要先介绍一下椭圆曲线的阶（order）以及辅助因子（cofactor）这两个概念。 首先还得介绍下数学中「循环群」的概念，它是指能由单个元素所生成的群，在 ECC 中这就是预先定义好的基点 $G$. 一个有限域上的椭圆曲线可以形成一个有限「循环代数群」，它由曲线上的所有点组成。椭圆曲线的阶被定义为该曲线上所有点的个数（包括无穷远点）。 有些曲线加上 G 点可以形成一个单一循环群，这一个群包含了曲线上的所有点。而其他的曲线加上 G 点则形成多个不相交的循环子群，每个子群包含了曲线的一个子集。 对于上述第二种情况，假设曲线上的点被拆分到了 h 个循环子群中，每个子群的阶都是 r，那这时整个群的阶就是 $n = h * r$，其中子群的个数 h 被称为辅助因子。 有限域上的椭圆曲线的阶都是有限的，也就是说对于曲线上任意一点 $G$，我们计算它的数乘 $kG$，随着整数 $k$ 的增大，一定会存在某个 $k$ 使 $kG = O_{\\infty}$ 成立，然后 $k$ 继续增大时，因为 $O_{\\infty} * P = O_{\\infty}$，$kG$ 的值就固定为 $O_{\\infty}$ 了，更大的 $k$ 值已经失去了意义。 因此 ECC 中要求 $kG$ 中的私钥 $k$ 符合条件 $0 \\le k \\le r$，也就是说总的私钥数量是受 $r$ 限制的。 辅助因子通过用如下公式表示： $$ h = n / r $$ 其中 $n$ 是曲线的阶，$r$ 是每个子群的阶，$h$ 是辅助因子。 如果曲线形成了一个单一循环群，那显然 $h = 1$，否则 $h \u003e 1$ 举例如下： secp256k1 的辅助因子为 1 Curve25519 的辅助因子为 8 Curve448 的辅助因子为 4 生成点 G生成点 G 的选择是很有讲究的，虽然每个循环子群都包含有很多个生成点，但是 ECC 只会谨慎的选择其中一个。 首先 G 点必须要能生成出整个循环子群，其次还需要有尽可能高的计算性能。 数学上已知某些椭圆曲线上，不同的生成点生成出的循环子群，阶也是不同的。如果 G 点选得不好，可能会导致生成出的子群的阶较小。 前面我们已经提过子群的阶 $r$ 会限制总的私钥数量，导致算法强度变弱！因此不恰当的 $G$ 点可能会导致我们遭受「小子群攻击」。 为了避免这种风险，建议尽量使用被广泛使用的加密库，而不是自己撸一个。 椭圆曲线的域参数ECC椭圆曲线由一组椭圆曲线域参数描述，如曲线方程参数、场参数和生成点坐标。这些参数在各种密码学标准中指定，你可以网上搜到相应的 RFC 或 NIST 文档。 这些标准定义了一组命名曲线的参数，例如 secp256k1、P-521、brainpoolP512t1 和 SM2. 这些加密标准中描述的有限域上的椭圆曲线得到了密码学家的充分研究和分析，并被认为具有一定的安全强度。 也有一些密码学家（如 Daniel Bernstein）认为，官方密码标准中描述的大多数曲线都是「不安全的」，并定义了他们自己的密码标准，这些标准在更广泛的层面上考虑了 ECC 安全性。 开发人员应该仅使用各项标准文档给出的、经过密码学家充分研究的命名曲线。 secp256k1此曲线被应用在比特币中，它的域参数如下： p (modulus) = 0xFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFEFFFFFC2F n (order; size; the count of all possible EC points) = 0xFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFEBAAEDCE6AF48A03BBFD25E8CD0364141 a (方程 $y^2 ≡ x^3 + a*x + b (\\mod p)$ 中的常数) = 0x0000000000000000000000000000000000000000000000000000000000000000 b (方程 $y^2 ≡ x^3 + a*x + b (\\mod p)$ 中的常数)= 0x0000000000000000000000000000000000000000000000000000000000000007 g (the curve generator point G {x, y}) = (0x79BE667EF9DCBBAC55A06295CE870B07029BFCDB2DCE28D959F2815B16F81798, 0x483ada7726a3c4655da4fbfc0e1108a8fd17b448a68554199c47d08ffb10d4b8) h (cofactor, typically 1) = 01 Edwards 曲线椭圆曲线方程除了我们前面使用的 Weierstrass 形式 $$y^2 = (x^3 + ax + b) \\mod p$$ 外，还可以被写成其他多种形式，这些不同的形式是双有理等价的（别问，我也不懂什么叫「双有理等价」…）。 不同的方程形式在计算机的数值计算上可能会存在区别。 为了性能考虑，ECC 在部分场景下会考虑使用 Edwards 曲线形式进行计算，该方程形式如下： $$ x^{2}+y^{2}=1+dx^{2}y^{2} $$ 画个图长这样： 知名的 Edwards 曲线有： Curve1174 (251-bit) Curve25519 (255-bit) Curve383187 (383-bit) Curve41417 (414-bit) Curve448 (448-bit) E-521 (521-bit) … Curve25519, X25519 和 Ed25519 https://cryptography.io/en/latest/hazmat/primitives/asymmetric/ed25519/ 只要域参数选得好，Edwards 就可以以非常高的性能实现 ECC 密钥交换、数字签名、混合加密方案。 一个例子就是 Curve25519，它是 Edwards 曲线，其 Montgomery 形式的定义如下： $$ y^{2}=x^{3}+486662x^{2}+x $$ 其被定义在有限域 $\\mathbb {F} _{p}$ 上，$p = 2255 - 19$, 其他域参数如下： 阶 n = 2252 + 0x14def9dea2f79cd65812631a5cf5d3ed 辅因子 h = 8 虽然此曲线并未以 Edwards 形式定义，但是它已被证明与如下扭曲 Edwards 曲线（edwards25519）双有理等价： $$ -x^2 + y^2 = 1 + 37095705934669439343138083508754565189542113879843219016388785533085940283555 x^2 y^2 $$ 上面给出的这种 Edwards 形式与前文给出的 Weierstrass 形式完全等价，是专为计算速度优化而设计成这样的。 Curve25519 由 Daniel Bernstein 领导的密码学家团队精心设计，在多个设计和实现层面上达成了非常高的性能，同时不影响安全性。 Curve25519 的构造使其避免了许多潜在的实现缺陷。根据设计，它不受定时攻击的影响，并且它接受任何 32 字节的字符串作为有效的公钥，并且不需要验证。 它能提供 125.8bits 的安全强度（有时称为 ~ 128bits 安全性） Curve25519 的私钥为 251 位，通常编码为 256 位整数（32 个字节，64 个十六进制数字）。 公钥通常也编码为 256 位整数（255 位 y 坐标 + 1 位 x 坐标），这对开发人员来说非常方便。 基于 Curve25519 派生出了名为 X25519 的 ECDH 算法，以及基于 EdDSA 的高速数字签名算法 Ed25519. Curve448, X448 和 Ed448 https://cryptography.io/en/latest/hazmat/primitives/asymmetric/ed448/ Curve448（Curve448-Goldilocks）是一种非扭曲 Edwards 曲线，它的方程定义如下： $$ x^2 + y^2 = 1 - 39081 x^2 y^2 $$ 其被定义在有限域 $\\mathbb {F} _{p}$ 上，$p = 2448 - 2224 - 1$，其他域参数： 阶 n = 2446 - 0x8335dc163bb124b65129c96fde933d8d723a70aadc873d6d54a7bb0d 辅助因子 h = 4 与 Curve25519 一样，Curve448 也等价于前面给出的 Weierstrass 形式，选择 Edwards 形式主要是因为它能显著提升性能。 Curve448 提供 222.8 位的安全强度。 Curve448 的私钥为 446 位，通常编码为 448 位整数（56 个字节，112 个十六进制数字）。 公钥也被编码为 448 位整数。 基于 Curve448 派生出了名为 X448 的 ECDH 算法，以及基于 EdDSA 的高速数字签名算法 Ed448. 该选择哪种椭圆曲线首先，Bernstein 的 SafeCurves 标准列出了符合一组 ECC 安全要求的安","date":"2022-03-09","objectID":"/posts/practical-cryptography-basics-7-asymmetric-key-ciphers/:4:6","series":null,"tags":["Cryptography","密码学","非对称加密","安全","RSA","ECC"],"title":"写给开发人员的实用密码学（七）—— 非对称密钥加密算法 RSA/ECC","uri":"/posts/practical-cryptography-basics-7-asymmetric-key-ciphers/#生成点-g"},{"categories":["tech"],"content":" 密码学常用椭圆曲线介绍在介绍密码学中的常用椭圆曲线前，需要先介绍一下椭圆曲线的阶（order）以及辅助因子（cofactor）这两个概念。 首先还得介绍下数学中「循环群」的概念，它是指能由单个元素所生成的群，在 ECC 中这就是预先定义好的基点 $G$. 一个有限域上的椭圆曲线可以形成一个有限「循环代数群」，它由曲线上的所有点组成。椭圆曲线的阶被定义为该曲线上所有点的个数（包括无穷远点）。 有些曲线加上 G 点可以形成一个单一循环群，这一个群包含了曲线上的所有点。而其他的曲线加上 G 点则形成多个不相交的循环子群，每个子群包含了曲线的一个子集。 对于上述第二种情况，假设曲线上的点被拆分到了 h 个循环子群中，每个子群的阶都是 r，那这时整个群的阶就是 $n = h * r$，其中子群的个数 h 被称为辅助因子。 有限域上的椭圆曲线的阶都是有限的，也就是说对于曲线上任意一点 $G$，我们计算它的数乘 $kG$，随着整数 $k$ 的增大，一定会存在某个 $k$ 使 $kG = O_{\\infty}$ 成立，然后 $k$ 继续增大时，因为 $O_{\\infty} * P = O_{\\infty}$，$kG$ 的值就固定为 $O_{\\infty}$ 了，更大的 $k$ 值已经失去了意义。 因此 ECC 中要求 $kG$ 中的私钥 $k$ 符合条件 $0 \\le k \\le r$，也就是说总的私钥数量是受 $r$ 限制的。 辅助因子通过用如下公式表示： $$ h = n / r $$ 其中 $n$ 是曲线的阶，$r$ 是每个子群的阶，$h$ 是辅助因子。 如果曲线形成了一个单一循环群，那显然 $h = 1$，否则 $h \u003e 1$ 举例如下： secp256k1 的辅助因子为 1 Curve25519 的辅助因子为 8 Curve448 的辅助因子为 4 生成点 G生成点 G 的选择是很有讲究的，虽然每个循环子群都包含有很多个生成点，但是 ECC 只会谨慎的选择其中一个。 首先 G 点必须要能生成出整个循环子群，其次还需要有尽可能高的计算性能。 数学上已知某些椭圆曲线上，不同的生成点生成出的循环子群，阶也是不同的。如果 G 点选得不好，可能会导致生成出的子群的阶较小。 前面我们已经提过子群的阶 $r$ 会限制总的私钥数量，导致算法强度变弱！因此不恰当的 $G$ 点可能会导致我们遭受「小子群攻击」。 为了避免这种风险，建议尽量使用被广泛使用的加密库，而不是自己撸一个。 椭圆曲线的域参数ECC椭圆曲线由一组椭圆曲线域参数描述，如曲线方程参数、场参数和生成点坐标。这些参数在各种密码学标准中指定，你可以网上搜到相应的 RFC 或 NIST 文档。 这些标准定义了一组命名曲线的参数，例如 secp256k1、P-521、brainpoolP512t1 和 SM2. 这些加密标准中描述的有限域上的椭圆曲线得到了密码学家的充分研究和分析，并被认为具有一定的安全强度。 也有一些密码学家（如 Daniel Bernstein）认为，官方密码标准中描述的大多数曲线都是「不安全的」，并定义了他们自己的密码标准，这些标准在更广泛的层面上考虑了 ECC 安全性。 开发人员应该仅使用各项标准文档给出的、经过密码学家充分研究的命名曲线。 secp256k1此曲线被应用在比特币中，它的域参数如下： p (modulus) = 0xFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFEFFFFFC2F n (order; size; the count of all possible EC points) = 0xFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFEBAAEDCE6AF48A03BBFD25E8CD0364141 a (方程 $y^2 ≡ x^3 + a*x + b (\\mod p)$ 中的常数) = 0x0000000000000000000000000000000000000000000000000000000000000000 b (方程 $y^2 ≡ x^3 + a*x + b (\\mod p)$ 中的常数)= 0x0000000000000000000000000000000000000000000000000000000000000007 g (the curve generator point G {x, y}) = (0x79BE667EF9DCBBAC55A06295CE870B07029BFCDB2DCE28D959F2815B16F81798, 0x483ada7726a3c4655da4fbfc0e1108a8fd17b448a68554199c47d08ffb10d4b8) h (cofactor, typically 1) = 01 Edwards 曲线椭圆曲线方程除了我们前面使用的 Weierstrass 形式 $$y^2 = (x^3 + ax + b) \\mod p$$ 外，还可以被写成其他多种形式，这些不同的形式是双有理等价的（别问，我也不懂什么叫「双有理等价」…）。 不同的方程形式在计算机的数值计算上可能会存在区别。 为了性能考虑，ECC 在部分场景下会考虑使用 Edwards 曲线形式进行计算，该方程形式如下： $$ x^{2}+y^{2}=1+dx^{2}y^{2} $$ 画个图长这样： 知名的 Edwards 曲线有： Curve1174 (251-bit) Curve25519 (255-bit) Curve383187 (383-bit) Curve41417 (414-bit) Curve448 (448-bit) E-521 (521-bit) … Curve25519, X25519 和 Ed25519 https://cryptography.io/en/latest/hazmat/primitives/asymmetric/ed25519/ 只要域参数选得好，Edwards 就可以以非常高的性能实现 ECC 密钥交换、数字签名、混合加密方案。 一个例子就是 Curve25519，它是 Edwards 曲线，其 Montgomery 形式的定义如下： $$ y^{2}=x^{3}+486662x^{2}+x $$ 其被定义在有限域 $\\mathbb {F} _{p}$ 上，$p = 2255 - 19$, 其他域参数如下： 阶 n = 2252 + 0x14def9dea2f79cd65812631a5cf5d3ed 辅因子 h = 8 虽然此曲线并未以 Edwards 形式定义，但是它已被证明与如下扭曲 Edwards 曲线（edwards25519）双有理等价： $$ -x^2 + y^2 = 1 + 37095705934669439343138083508754565189542113879843219016388785533085940283555 x^2 y^2 $$ 上面给出的这种 Edwards 形式与前文给出的 Weierstrass 形式完全等价，是专为计算速度优化而设计成这样的。 Curve25519 由 Daniel Bernstein 领导的密码学家团队精心设计，在多个设计和实现层面上达成了非常高的性能，同时不影响安全性。 Curve25519 的构造使其避免了许多潜在的实现缺陷。根据设计，它不受定时攻击的影响，并且它接受任何 32 字节的字符串作为有效的公钥，并且不需要验证。 它能提供 125.8bits 的安全强度（有时称为 ~ 128bits 安全性） Curve25519 的私钥为 251 位，通常编码为 256 位整数（32 个字节，64 个十六进制数字）。 公钥通常也编码为 256 位整数（255 位 y 坐标 + 1 位 x 坐标），这对开发人员来说非常方便。 基于 Curve25519 派生出了名为 X25519 的 ECDH 算法，以及基于 EdDSA 的高速数字签名算法 Ed25519. Curve448, X448 和 Ed448 https://cryptography.io/en/latest/hazmat/primitives/asymmetric/ed448/ Curve448（Curve448-Goldilocks）是一种非扭曲 Edwards 曲线，它的方程定义如下： $$ x^2 + y^2 = 1 - 39081 x^2 y^2 $$ 其被定义在有限域 $\\mathbb {F} _{p}$ 上，$p = 2448 - 2224 - 1$，其他域参数： 阶 n = 2446 - 0x8335dc163bb124b65129c96fde933d8d723a70aadc873d6d54a7bb0d 辅助因子 h = 4 与 Curve25519 一样，Curve448 也等价于前面给出的 Weierstrass 形式，选择 Edwards 形式主要是因为它能显著提升性能。 Curve448 提供 222.8 位的安全强度。 Curve448 的私钥为 446 位，通常编码为 448 位整数（56 个字节，112 个十六进制数字）。 公钥也被编码为 448 位整数。 基于 Curve448 派生出了名为 X448 的 ECDH 算法，以及基于 EdDSA 的高速数字签名算法 Ed448. 该选择哪种椭圆曲线首先，Bernstein 的 SafeCurves 标准列出了符合一组 ECC 安全要求的安","date":"2022-03-09","objectID":"/posts/practical-cryptography-basics-7-asymmetric-key-ciphers/:4:6","series":null,"tags":["Cryptography","密码学","非对称加密","安全","RSA","ECC"],"title":"写给开发人员的实用密码学（七）—— 非对称密钥加密算法 RSA/ECC","uri":"/posts/practical-cryptography-basics-7-asymmetric-key-ciphers/#椭圆曲线的域参数"},{"categories":["tech"],"content":" 密码学常用椭圆曲线介绍在介绍密码学中的常用椭圆曲线前，需要先介绍一下椭圆曲线的阶（order）以及辅助因子（cofactor）这两个概念。 首先还得介绍下数学中「循环群」的概念，它是指能由单个元素所生成的群，在 ECC 中这就是预先定义好的基点 $G$. 一个有限域上的椭圆曲线可以形成一个有限「循环代数群」，它由曲线上的所有点组成。椭圆曲线的阶被定义为该曲线上所有点的个数（包括无穷远点）。 有些曲线加上 G 点可以形成一个单一循环群，这一个群包含了曲线上的所有点。而其他的曲线加上 G 点则形成多个不相交的循环子群，每个子群包含了曲线的一个子集。 对于上述第二种情况，假设曲线上的点被拆分到了 h 个循环子群中，每个子群的阶都是 r，那这时整个群的阶就是 $n = h * r$，其中子群的个数 h 被称为辅助因子。 有限域上的椭圆曲线的阶都是有限的，也就是说对于曲线上任意一点 $G$，我们计算它的数乘 $kG$，随着整数 $k$ 的增大，一定会存在某个 $k$ 使 $kG = O_{\\infty}$ 成立，然后 $k$ 继续增大时，因为 $O_{\\infty} * P = O_{\\infty}$，$kG$ 的值就固定为 $O_{\\infty}$ 了，更大的 $k$ 值已经失去了意义。 因此 ECC 中要求 $kG$ 中的私钥 $k$ 符合条件 $0 \\le k \\le r$，也就是说总的私钥数量是受 $r$ 限制的。 辅助因子通过用如下公式表示： $$ h = n / r $$ 其中 $n$ 是曲线的阶，$r$ 是每个子群的阶，$h$ 是辅助因子。 如果曲线形成了一个单一循环群，那显然 $h = 1$，否则 $h \u003e 1$ 举例如下： secp256k1 的辅助因子为 1 Curve25519 的辅助因子为 8 Curve448 的辅助因子为 4 生成点 G生成点 G 的选择是很有讲究的，虽然每个循环子群都包含有很多个生成点，但是 ECC 只会谨慎的选择其中一个。 首先 G 点必须要能生成出整个循环子群，其次还需要有尽可能高的计算性能。 数学上已知某些椭圆曲线上，不同的生成点生成出的循环子群，阶也是不同的。如果 G 点选得不好，可能会导致生成出的子群的阶较小。 前面我们已经提过子群的阶 $r$ 会限制总的私钥数量，导致算法强度变弱！因此不恰当的 $G$ 点可能会导致我们遭受「小子群攻击」。 为了避免这种风险，建议尽量使用被广泛使用的加密库，而不是自己撸一个。 椭圆曲线的域参数ECC椭圆曲线由一组椭圆曲线域参数描述，如曲线方程参数、场参数和生成点坐标。这些参数在各种密码学标准中指定，你可以网上搜到相应的 RFC 或 NIST 文档。 这些标准定义了一组命名曲线的参数，例如 secp256k1、P-521、brainpoolP512t1 和 SM2. 这些加密标准中描述的有限域上的椭圆曲线得到了密码学家的充分研究和分析，并被认为具有一定的安全强度。 也有一些密码学家（如 Daniel Bernstein）认为，官方密码标准中描述的大多数曲线都是「不安全的」，并定义了他们自己的密码标准，这些标准在更广泛的层面上考虑了 ECC 安全性。 开发人员应该仅使用各项标准文档给出的、经过密码学家充分研究的命名曲线。 secp256k1此曲线被应用在比特币中，它的域参数如下： p (modulus) = 0xFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFEFFFFFC2F n (order; size; the count of all possible EC points) = 0xFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFEBAAEDCE6AF48A03BBFD25E8CD0364141 a (方程 $y^2 ≡ x^3 + a*x + b (\\mod p)$ 中的常数) = 0x0000000000000000000000000000000000000000000000000000000000000000 b (方程 $y^2 ≡ x^3 + a*x + b (\\mod p)$ 中的常数)= 0x0000000000000000000000000000000000000000000000000000000000000007 g (the curve generator point G {x, y}) = (0x79BE667EF9DCBBAC55A06295CE870B07029BFCDB2DCE28D959F2815B16F81798, 0x483ada7726a3c4655da4fbfc0e1108a8fd17b448a68554199c47d08ffb10d4b8) h (cofactor, typically 1) = 01 Edwards 曲线椭圆曲线方程除了我们前面使用的 Weierstrass 形式 $$y^2 = (x^3 + ax + b) \\mod p$$ 外，还可以被写成其他多种形式，这些不同的形式是双有理等价的（别问，我也不懂什么叫「双有理等价」…）。 不同的方程形式在计算机的数值计算上可能会存在区别。 为了性能考虑，ECC 在部分场景下会考虑使用 Edwards 曲线形式进行计算，该方程形式如下： $$ x^{2}+y^{2}=1+dx^{2}y^{2} $$ 画个图长这样： 知名的 Edwards 曲线有： Curve1174 (251-bit) Curve25519 (255-bit) Curve383187 (383-bit) Curve41417 (414-bit) Curve448 (448-bit) E-521 (521-bit) … Curve25519, X25519 和 Ed25519 https://cryptography.io/en/latest/hazmat/primitives/asymmetric/ed25519/ 只要域参数选得好，Edwards 就可以以非常高的性能实现 ECC 密钥交换、数字签名、混合加密方案。 一个例子就是 Curve25519，它是 Edwards 曲线，其 Montgomery 形式的定义如下： $$ y^{2}=x^{3}+486662x^{2}+x $$ 其被定义在有限域 $\\mathbb {F} _{p}$ 上，$p = 2255 - 19$, 其他域参数如下： 阶 n = 2252 + 0x14def9dea2f79cd65812631a5cf5d3ed 辅因子 h = 8 虽然此曲线并未以 Edwards 形式定义，但是它已被证明与如下扭曲 Edwards 曲线（edwards25519）双有理等价： $$ -x^2 + y^2 = 1 + 37095705934669439343138083508754565189542113879843219016388785533085940283555 x^2 y^2 $$ 上面给出的这种 Edwards 形式与前文给出的 Weierstrass 形式完全等价，是专为计算速度优化而设计成这样的。 Curve25519 由 Daniel Bernstein 领导的密码学家团队精心设计，在多个设计和实现层面上达成了非常高的性能，同时不影响安全性。 Curve25519 的构造使其避免了许多潜在的实现缺陷。根据设计，它不受定时攻击的影响，并且它接受任何 32 字节的字符串作为有效的公钥，并且不需要验证。 它能提供 125.8bits 的安全强度（有时称为 ~ 128bits 安全性） Curve25519 的私钥为 251 位，通常编码为 256 位整数（32 个字节，64 个十六进制数字）。 公钥通常也编码为 256 位整数（255 位 y 坐标 + 1 位 x 坐标），这对开发人员来说非常方便。 基于 Curve25519 派生出了名为 X25519 的 ECDH 算法，以及基于 EdDSA 的高速数字签名算法 Ed25519. Curve448, X448 和 Ed448 https://cryptography.io/en/latest/hazmat/primitives/asymmetric/ed448/ Curve448（Curve448-Goldilocks）是一种非扭曲 Edwards 曲线，它的方程定义如下： $$ x^2 + y^2 = 1 - 39081 x^2 y^2 $$ 其被定义在有限域 $\\mathbb {F} _{p}$ 上，$p = 2448 - 2224 - 1$，其他域参数： 阶 n = 2446 - 0x8335dc163bb124b65129c96fde933d8d723a70aadc873d6d54a7bb0d 辅助因子 h = 4 与 Curve25519 一样，Curve448 也等价于前面给出的 Weierstrass 形式，选择 Edwards 形式主要是因为它能显著提升性能。 Curve448 提供 222.8 位的安全强度。 Curve448 的私钥为 446 位，通常编码为 448 位整数（56 个字节，112 个十六进制数字）。 公钥也被编码为 448 位整数。 基于 Curve448 派生出了名为 X448 的 ECDH 算法，以及基于 EdDSA 的高速数字签名算法 Ed448. 该选择哪种椭圆曲线首先，Bernstein 的 SafeCurves 标准列出了符合一组 ECC 安全要求的安","date":"2022-03-09","objectID":"/posts/practical-cryptography-basics-7-asymmetric-key-ciphers/:4:6","series":null,"tags":["Cryptography","密码学","非对称加密","安全","RSA","ECC"],"title":"写给开发人员的实用密码学（七）—— 非对称密钥加密算法 RSA/ECC","uri":"/posts/practical-cryptography-basics-7-asymmetric-key-ciphers/#secp256k1"},{"categories":["tech"],"content":" 密码学常用椭圆曲线介绍在介绍密码学中的常用椭圆曲线前，需要先介绍一下椭圆曲线的阶（order）以及辅助因子（cofactor）这两个概念。 首先还得介绍下数学中「循环群」的概念，它是指能由单个元素所生成的群，在 ECC 中这就是预先定义好的基点 $G$. 一个有限域上的椭圆曲线可以形成一个有限「循环代数群」，它由曲线上的所有点组成。椭圆曲线的阶被定义为该曲线上所有点的个数（包括无穷远点）。 有些曲线加上 G 点可以形成一个单一循环群，这一个群包含了曲线上的所有点。而其他的曲线加上 G 点则形成多个不相交的循环子群，每个子群包含了曲线的一个子集。 对于上述第二种情况，假设曲线上的点被拆分到了 h 个循环子群中，每个子群的阶都是 r，那这时整个群的阶就是 $n = h * r$，其中子群的个数 h 被称为辅助因子。 有限域上的椭圆曲线的阶都是有限的，也就是说对于曲线上任意一点 $G$，我们计算它的数乘 $kG$，随着整数 $k$ 的增大，一定会存在某个 $k$ 使 $kG = O_{\\infty}$ 成立，然后 $k$ 继续增大时，因为 $O_{\\infty} * P = O_{\\infty}$，$kG$ 的值就固定为 $O_{\\infty}$ 了，更大的 $k$ 值已经失去了意义。 因此 ECC 中要求 $kG$ 中的私钥 $k$ 符合条件 $0 \\le k \\le r$，也就是说总的私钥数量是受 $r$ 限制的。 辅助因子通过用如下公式表示： $$ h = n / r $$ 其中 $n$ 是曲线的阶，$r$ 是每个子群的阶，$h$ 是辅助因子。 如果曲线形成了一个单一循环群，那显然 $h = 1$，否则 $h \u003e 1$ 举例如下： secp256k1 的辅助因子为 1 Curve25519 的辅助因子为 8 Curve448 的辅助因子为 4 生成点 G生成点 G 的选择是很有讲究的，虽然每个循环子群都包含有很多个生成点，但是 ECC 只会谨慎的选择其中一个。 首先 G 点必须要能生成出整个循环子群，其次还需要有尽可能高的计算性能。 数学上已知某些椭圆曲线上，不同的生成点生成出的循环子群，阶也是不同的。如果 G 点选得不好，可能会导致生成出的子群的阶较小。 前面我们已经提过子群的阶 $r$ 会限制总的私钥数量，导致算法强度变弱！因此不恰当的 $G$ 点可能会导致我们遭受「小子群攻击」。 为了避免这种风险，建议尽量使用被广泛使用的加密库，而不是自己撸一个。 椭圆曲线的域参数ECC椭圆曲线由一组椭圆曲线域参数描述，如曲线方程参数、场参数和生成点坐标。这些参数在各种密码学标准中指定，你可以网上搜到相应的 RFC 或 NIST 文档。 这些标准定义了一组命名曲线的参数，例如 secp256k1、P-521、brainpoolP512t1 和 SM2. 这些加密标准中描述的有限域上的椭圆曲线得到了密码学家的充分研究和分析，并被认为具有一定的安全强度。 也有一些密码学家（如 Daniel Bernstein）认为，官方密码标准中描述的大多数曲线都是「不安全的」，并定义了他们自己的密码标准，这些标准在更广泛的层面上考虑了 ECC 安全性。 开发人员应该仅使用各项标准文档给出的、经过密码学家充分研究的命名曲线。 secp256k1此曲线被应用在比特币中，它的域参数如下： p (modulus) = 0xFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFEFFFFFC2F n (order; size; the count of all possible EC points) = 0xFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFEBAAEDCE6AF48A03BBFD25E8CD0364141 a (方程 $y^2 ≡ x^3 + a*x + b (\\mod p)$ 中的常数) = 0x0000000000000000000000000000000000000000000000000000000000000000 b (方程 $y^2 ≡ x^3 + a*x + b (\\mod p)$ 中的常数)= 0x0000000000000000000000000000000000000000000000000000000000000007 g (the curve generator point G {x, y}) = (0x79BE667EF9DCBBAC55A06295CE870B07029BFCDB2DCE28D959F2815B16F81798, 0x483ada7726a3c4655da4fbfc0e1108a8fd17b448a68554199c47d08ffb10d4b8) h (cofactor, typically 1) = 01 Edwards 曲线椭圆曲线方程除了我们前面使用的 Weierstrass 形式 $$y^2 = (x^3 + ax + b) \\mod p$$ 外，还可以被写成其他多种形式，这些不同的形式是双有理等价的（别问，我也不懂什么叫「双有理等价」…）。 不同的方程形式在计算机的数值计算上可能会存在区别。 为了性能考虑，ECC 在部分场景下会考虑使用 Edwards 曲线形式进行计算，该方程形式如下： $$ x^{2}+y^{2}=1+dx^{2}y^{2} $$ 画个图长这样： 知名的 Edwards 曲线有： Curve1174 (251-bit) Curve25519 (255-bit) Curve383187 (383-bit) Curve41417 (414-bit) Curve448 (448-bit) E-521 (521-bit) … Curve25519, X25519 和 Ed25519 https://cryptography.io/en/latest/hazmat/primitives/asymmetric/ed25519/ 只要域参数选得好，Edwards 就可以以非常高的性能实现 ECC 密钥交换、数字签名、混合加密方案。 一个例子就是 Curve25519，它是 Edwards 曲线，其 Montgomery 形式的定义如下： $$ y^{2}=x^{3}+486662x^{2}+x $$ 其被定义在有限域 $\\mathbb {F} _{p}$ 上，$p = 2255 - 19$, 其他域参数如下： 阶 n = 2252 + 0x14def9dea2f79cd65812631a5cf5d3ed 辅因子 h = 8 虽然此曲线并未以 Edwards 形式定义，但是它已被证明与如下扭曲 Edwards 曲线（edwards25519）双有理等价： $$ -x^2 + y^2 = 1 + 37095705934669439343138083508754565189542113879843219016388785533085940283555 x^2 y^2 $$ 上面给出的这种 Edwards 形式与前文给出的 Weierstrass 形式完全等价，是专为计算速度优化而设计成这样的。 Curve25519 由 Daniel Bernstein 领导的密码学家团队精心设计，在多个设计和实现层面上达成了非常高的性能，同时不影响安全性。 Curve25519 的构造使其避免了许多潜在的实现缺陷。根据设计，它不受定时攻击的影响，并且它接受任何 32 字节的字符串作为有效的公钥，并且不需要验证。 它能提供 125.8bits 的安全强度（有时称为 ~ 128bits 安全性） Curve25519 的私钥为 251 位，通常编码为 256 位整数（32 个字节，64 个十六进制数字）。 公钥通常也编码为 256 位整数（255 位 y 坐标 + 1 位 x 坐标），这对开发人员来说非常方便。 基于 Curve25519 派生出了名为 X25519 的 ECDH 算法，以及基于 EdDSA 的高速数字签名算法 Ed25519. Curve448, X448 和 Ed448 https://cryptography.io/en/latest/hazmat/primitives/asymmetric/ed448/ Curve448（Curve448-Goldilocks）是一种非扭曲 Edwards 曲线，它的方程定义如下： $$ x^2 + y^2 = 1 - 39081 x^2 y^2 $$ 其被定义在有限域 $\\mathbb {F} _{p}$ 上，$p = 2448 - 2224 - 1$，其他域参数： 阶 n = 2446 - 0x8335dc163bb124b65129c96fde933d8d723a70aadc873d6d54a7bb0d 辅助因子 h = 4 与 Curve25519 一样，Curve448 也等价于前面给出的 Weierstrass 形式，选择 Edwards 形式主要是因为它能显著提升性能。 Curve448 提供 222.8 位的安全强度。 Curve448 的私钥为 446 位，通常编码为 448 位整数（56 个字节，112 个十六进制数字）。 公钥也被编码为 448 位整数。 基于 Curve448 派生出了名为 X448 的 ECDH 算法，以及基于 EdDSA 的高速数字签名算法 Ed448. 该选择哪种椭圆曲线首先，Bernstein 的 SafeCurves 标准列出了符合一组 ECC 安全要求的安","date":"2022-03-09","objectID":"/posts/practical-cryptography-basics-7-asymmetric-key-ciphers/:4:6","series":null,"tags":["Cryptography","密码学","非对称加密","安全","RSA","ECC"],"title":"写给开发人员的实用密码学（七）—— 非对称密钥加密算法 RSA/ECC","uri":"/posts/practical-cryptography-basics-7-asymmetric-key-ciphers/#edwards-曲线"},{"categories":["tech"],"content":" 密码学常用椭圆曲线介绍在介绍密码学中的常用椭圆曲线前，需要先介绍一下椭圆曲线的阶（order）以及辅助因子（cofactor）这两个概念。 首先还得介绍下数学中「循环群」的概念，它是指能由单个元素所生成的群，在 ECC 中这就是预先定义好的基点 $G$. 一个有限域上的椭圆曲线可以形成一个有限「循环代数群」，它由曲线上的所有点组成。椭圆曲线的阶被定义为该曲线上所有点的个数（包括无穷远点）。 有些曲线加上 G 点可以形成一个单一循环群，这一个群包含了曲线上的所有点。而其他的曲线加上 G 点则形成多个不相交的循环子群，每个子群包含了曲线的一个子集。 对于上述第二种情况，假设曲线上的点被拆分到了 h 个循环子群中，每个子群的阶都是 r，那这时整个群的阶就是 $n = h * r$，其中子群的个数 h 被称为辅助因子。 有限域上的椭圆曲线的阶都是有限的，也就是说对于曲线上任意一点 $G$，我们计算它的数乘 $kG$，随着整数 $k$ 的增大，一定会存在某个 $k$ 使 $kG = O_{\\infty}$ 成立，然后 $k$ 继续增大时，因为 $O_{\\infty} * P = O_{\\infty}$，$kG$ 的值就固定为 $O_{\\infty}$ 了，更大的 $k$ 值已经失去了意义。 因此 ECC 中要求 $kG$ 中的私钥 $k$ 符合条件 $0 \\le k \\le r$，也就是说总的私钥数量是受 $r$ 限制的。 辅助因子通过用如下公式表示： $$ h = n / r $$ 其中 $n$ 是曲线的阶，$r$ 是每个子群的阶，$h$ 是辅助因子。 如果曲线形成了一个单一循环群，那显然 $h = 1$，否则 $h \u003e 1$ 举例如下： secp256k1 的辅助因子为 1 Curve25519 的辅助因子为 8 Curve448 的辅助因子为 4 生成点 G生成点 G 的选择是很有讲究的，虽然每个循环子群都包含有很多个生成点，但是 ECC 只会谨慎的选择其中一个。 首先 G 点必须要能生成出整个循环子群，其次还需要有尽可能高的计算性能。 数学上已知某些椭圆曲线上，不同的生成点生成出的循环子群，阶也是不同的。如果 G 点选得不好，可能会导致生成出的子群的阶较小。 前面我们已经提过子群的阶 $r$ 会限制总的私钥数量，导致算法强度变弱！因此不恰当的 $G$ 点可能会导致我们遭受「小子群攻击」。 为了避免这种风险，建议尽量使用被广泛使用的加密库，而不是自己撸一个。 椭圆曲线的域参数ECC椭圆曲线由一组椭圆曲线域参数描述，如曲线方程参数、场参数和生成点坐标。这些参数在各种密码学标准中指定，你可以网上搜到相应的 RFC 或 NIST 文档。 这些标准定义了一组命名曲线的参数，例如 secp256k1、P-521、brainpoolP512t1 和 SM2. 这些加密标准中描述的有限域上的椭圆曲线得到了密码学家的充分研究和分析，并被认为具有一定的安全强度。 也有一些密码学家（如 Daniel Bernstein）认为，官方密码标准中描述的大多数曲线都是「不安全的」，并定义了他们自己的密码标准，这些标准在更广泛的层面上考虑了 ECC 安全性。 开发人员应该仅使用各项标准文档给出的、经过密码学家充分研究的命名曲线。 secp256k1此曲线被应用在比特币中，它的域参数如下： p (modulus) = 0xFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFEFFFFFC2F n (order; size; the count of all possible EC points) = 0xFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFEBAAEDCE6AF48A03BBFD25E8CD0364141 a (方程 $y^2 ≡ x^3 + a*x + b (\\mod p)$ 中的常数) = 0x0000000000000000000000000000000000000000000000000000000000000000 b (方程 $y^2 ≡ x^3 + a*x + b (\\mod p)$ 中的常数)= 0x0000000000000000000000000000000000000000000000000000000000000007 g (the curve generator point G {x, y}) = (0x79BE667EF9DCBBAC55A06295CE870B07029BFCDB2DCE28D959F2815B16F81798, 0x483ada7726a3c4655da4fbfc0e1108a8fd17b448a68554199c47d08ffb10d4b8) h (cofactor, typically 1) = 01 Edwards 曲线椭圆曲线方程除了我们前面使用的 Weierstrass 形式 $$y^2 = (x^3 + ax + b) \\mod p$$ 外，还可以被写成其他多种形式，这些不同的形式是双有理等价的（别问，我也不懂什么叫「双有理等价」…）。 不同的方程形式在计算机的数值计算上可能会存在区别。 为了性能考虑，ECC 在部分场景下会考虑使用 Edwards 曲线形式进行计算，该方程形式如下： $$ x^{2}+y^{2}=1+dx^{2}y^{2} $$ 画个图长这样： 知名的 Edwards 曲线有： Curve1174 (251-bit) Curve25519 (255-bit) Curve383187 (383-bit) Curve41417 (414-bit) Curve448 (448-bit) E-521 (521-bit) … Curve25519, X25519 和 Ed25519 https://cryptography.io/en/latest/hazmat/primitives/asymmetric/ed25519/ 只要域参数选得好，Edwards 就可以以非常高的性能实现 ECC 密钥交换、数字签名、混合加密方案。 一个例子就是 Curve25519，它是 Edwards 曲线，其 Montgomery 形式的定义如下： $$ y^{2}=x^{3}+486662x^{2}+x $$ 其被定义在有限域 $\\mathbb {F} _{p}$ 上，$p = 2255 - 19$, 其他域参数如下： 阶 n = 2252 + 0x14def9dea2f79cd65812631a5cf5d3ed 辅因子 h = 8 虽然此曲线并未以 Edwards 形式定义，但是它已被证明与如下扭曲 Edwards 曲线（edwards25519）双有理等价： $$ -x^2 + y^2 = 1 + 37095705934669439343138083508754565189542113879843219016388785533085940283555 x^2 y^2 $$ 上面给出的这种 Edwards 形式与前文给出的 Weierstrass 形式完全等价，是专为计算速度优化而设计成这样的。 Curve25519 由 Daniel Bernstein 领导的密码学家团队精心设计，在多个设计和实现层面上达成了非常高的性能，同时不影响安全性。 Curve25519 的构造使其避免了许多潜在的实现缺陷。根据设计，它不受定时攻击的影响，并且它接受任何 32 字节的字符串作为有效的公钥，并且不需要验证。 它能提供 125.8bits 的安全强度（有时称为 ~ 128bits 安全性） Curve25519 的私钥为 251 位，通常编码为 256 位整数（32 个字节，64 个十六进制数字）。 公钥通常也编码为 256 位整数（255 位 y 坐标 + 1 位 x 坐标），这对开发人员来说非常方便。 基于 Curve25519 派生出了名为 X25519 的 ECDH 算法，以及基于 EdDSA 的高速数字签名算法 Ed25519. Curve448, X448 和 Ed448 https://cryptography.io/en/latest/hazmat/primitives/asymmetric/ed448/ Curve448（Curve448-Goldilocks）是一种非扭曲 Edwards 曲线，它的方程定义如下： $$ x^2 + y^2 = 1 - 39081 x^2 y^2 $$ 其被定义在有限域 $\\mathbb {F} _{p}$ 上，$p = 2448 - 2224 - 1$，其他域参数： 阶 n = 2446 - 0x8335dc163bb124b65129c96fde933d8d723a70aadc873d6d54a7bb0d 辅助因子 h = 4 与 Curve25519 一样，Curve448 也等价于前面给出的 Weierstrass 形式，选择 Edwards 形式主要是因为它能显著提升性能。 Curve448 提供 222.8 位的安全强度。 Curve448 的私钥为 446 位，通常编码为 448 位整数（56 个字节，112 个十六进制数字）。 公钥也被编码为 448 位整数。 基于 Curve448 派生出了名为 X448 的 ECDH 算法，以及基于 EdDSA 的高速数字签名算法 Ed448. 该选择哪种椭圆曲线首先，Bernstein 的 SafeCurves 标准列出了符合一组 ECC 安全要求的安","date":"2022-03-09","objectID":"/posts/practical-cryptography-basics-7-asymmetric-key-ciphers/:4:6","series":null,"tags":["Cryptography","密码学","非对称加密","安全","RSA","ECC"],"title":"写给开发人员的实用密码学（七）—— 非对称密钥加密算法 RSA/ECC","uri":"/posts/practical-cryptography-basics-7-asymmetric-key-ciphers/#curve25519-x25519-和-ed25519"},{"categories":["tech"],"content":" 密码学常用椭圆曲线介绍在介绍密码学中的常用椭圆曲线前，需要先介绍一下椭圆曲线的阶（order）以及辅助因子（cofactor）这两个概念。 首先还得介绍下数学中「循环群」的概念，它是指能由单个元素所生成的群，在 ECC 中这就是预先定义好的基点 $G$. 一个有限域上的椭圆曲线可以形成一个有限「循环代数群」，它由曲线上的所有点组成。椭圆曲线的阶被定义为该曲线上所有点的个数（包括无穷远点）。 有些曲线加上 G 点可以形成一个单一循环群，这一个群包含了曲线上的所有点。而其他的曲线加上 G 点则形成多个不相交的循环子群，每个子群包含了曲线的一个子集。 对于上述第二种情况，假设曲线上的点被拆分到了 h 个循环子群中，每个子群的阶都是 r，那这时整个群的阶就是 $n = h * r$，其中子群的个数 h 被称为辅助因子。 有限域上的椭圆曲线的阶都是有限的，也就是说对于曲线上任意一点 $G$，我们计算它的数乘 $kG$，随着整数 $k$ 的增大，一定会存在某个 $k$ 使 $kG = O_{\\infty}$ 成立，然后 $k$ 继续增大时，因为 $O_{\\infty} * P = O_{\\infty}$，$kG$ 的值就固定为 $O_{\\infty}$ 了，更大的 $k$ 值已经失去了意义。 因此 ECC 中要求 $kG$ 中的私钥 $k$ 符合条件 $0 \\le k \\le r$，也就是说总的私钥数量是受 $r$ 限制的。 辅助因子通过用如下公式表示： $$ h = n / r $$ 其中 $n$ 是曲线的阶，$r$ 是每个子群的阶，$h$ 是辅助因子。 如果曲线形成了一个单一循环群，那显然 $h = 1$，否则 $h \u003e 1$ 举例如下： secp256k1 的辅助因子为 1 Curve25519 的辅助因子为 8 Curve448 的辅助因子为 4 生成点 G生成点 G 的选择是很有讲究的，虽然每个循环子群都包含有很多个生成点，但是 ECC 只会谨慎的选择其中一个。 首先 G 点必须要能生成出整个循环子群，其次还需要有尽可能高的计算性能。 数学上已知某些椭圆曲线上，不同的生成点生成出的循环子群，阶也是不同的。如果 G 点选得不好，可能会导致生成出的子群的阶较小。 前面我们已经提过子群的阶 $r$ 会限制总的私钥数量，导致算法强度变弱！因此不恰当的 $G$ 点可能会导致我们遭受「小子群攻击」。 为了避免这种风险，建议尽量使用被广泛使用的加密库，而不是自己撸一个。 椭圆曲线的域参数ECC椭圆曲线由一组椭圆曲线域参数描述，如曲线方程参数、场参数和生成点坐标。这些参数在各种密码学标准中指定，你可以网上搜到相应的 RFC 或 NIST 文档。 这些标准定义了一组命名曲线的参数，例如 secp256k1、P-521、brainpoolP512t1 和 SM2. 这些加密标准中描述的有限域上的椭圆曲线得到了密码学家的充分研究和分析，并被认为具有一定的安全强度。 也有一些密码学家（如 Daniel Bernstein）认为，官方密码标准中描述的大多数曲线都是「不安全的」，并定义了他们自己的密码标准，这些标准在更广泛的层面上考虑了 ECC 安全性。 开发人员应该仅使用各项标准文档给出的、经过密码学家充分研究的命名曲线。 secp256k1此曲线被应用在比特币中，它的域参数如下： p (modulus) = 0xFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFEFFFFFC2F n (order; size; the count of all possible EC points) = 0xFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFEBAAEDCE6AF48A03BBFD25E8CD0364141 a (方程 $y^2 ≡ x^3 + a*x + b (\\mod p)$ 中的常数) = 0x0000000000000000000000000000000000000000000000000000000000000000 b (方程 $y^2 ≡ x^3 + a*x + b (\\mod p)$ 中的常数)= 0x0000000000000000000000000000000000000000000000000000000000000007 g (the curve generator point G {x, y}) = (0x79BE667EF9DCBBAC55A06295CE870B07029BFCDB2DCE28D959F2815B16F81798, 0x483ada7726a3c4655da4fbfc0e1108a8fd17b448a68554199c47d08ffb10d4b8) h (cofactor, typically 1) = 01 Edwards 曲线椭圆曲线方程除了我们前面使用的 Weierstrass 形式 $$y^2 = (x^3 + ax + b) \\mod p$$ 外，还可以被写成其他多种形式，这些不同的形式是双有理等价的（别问，我也不懂什么叫「双有理等价」…）。 不同的方程形式在计算机的数值计算上可能会存在区别。 为了性能考虑，ECC 在部分场景下会考虑使用 Edwards 曲线形式进行计算，该方程形式如下： $$ x^{2}+y^{2}=1+dx^{2}y^{2} $$ 画个图长这样： 知名的 Edwards 曲线有： Curve1174 (251-bit) Curve25519 (255-bit) Curve383187 (383-bit) Curve41417 (414-bit) Curve448 (448-bit) E-521 (521-bit) … Curve25519, X25519 和 Ed25519 https://cryptography.io/en/latest/hazmat/primitives/asymmetric/ed25519/ 只要域参数选得好，Edwards 就可以以非常高的性能实现 ECC 密钥交换、数字签名、混合加密方案。 一个例子就是 Curve25519，它是 Edwards 曲线，其 Montgomery 形式的定义如下： $$ y^{2}=x^{3}+486662x^{2}+x $$ 其被定义在有限域 $\\mathbb {F} _{p}$ 上，$p = 2255 - 19$, 其他域参数如下： 阶 n = 2252 + 0x14def9dea2f79cd65812631a5cf5d3ed 辅因子 h = 8 虽然此曲线并未以 Edwards 形式定义，但是它已被证明与如下扭曲 Edwards 曲线（edwards25519）双有理等价： $$ -x^2 + y^2 = 1 + 37095705934669439343138083508754565189542113879843219016388785533085940283555 x^2 y^2 $$ 上面给出的这种 Edwards 形式与前文给出的 Weierstrass 形式完全等价，是专为计算速度优化而设计成这样的。 Curve25519 由 Daniel Bernstein 领导的密码学家团队精心设计，在多个设计和实现层面上达成了非常高的性能，同时不影响安全性。 Curve25519 的构造使其避免了许多潜在的实现缺陷。根据设计，它不受定时攻击的影响，并且它接受任何 32 字节的字符串作为有效的公钥，并且不需要验证。 它能提供 125.8bits 的安全强度（有时称为 ~ 128bits 安全性） Curve25519 的私钥为 251 位，通常编码为 256 位整数（32 个字节，64 个十六进制数字）。 公钥通常也编码为 256 位整数（255 位 y 坐标 + 1 位 x 坐标），这对开发人员来说非常方便。 基于 Curve25519 派生出了名为 X25519 的 ECDH 算法，以及基于 EdDSA 的高速数字签名算法 Ed25519. Curve448, X448 和 Ed448 https://cryptography.io/en/latest/hazmat/primitives/asymmetric/ed448/ Curve448（Curve448-Goldilocks）是一种非扭曲 Edwards 曲线，它的方程定义如下： $$ x^2 + y^2 = 1 - 39081 x^2 y^2 $$ 其被定义在有限域 $\\mathbb {F} _{p}$ 上，$p = 2448 - 2224 - 1$，其他域参数： 阶 n = 2446 - 0x8335dc163bb124b65129c96fde933d8d723a70aadc873d6d54a7bb0d 辅助因子 h = 4 与 Curve25519 一样，Curve448 也等价于前面给出的 Weierstrass 形式，选择 Edwards 形式主要是因为它能显著提升性能。 Curve448 提供 222.8 位的安全强度。 Curve448 的私钥为 446 位，通常编码为 448 位整数（56 个字节，112 个十六进制数字）。 公钥也被编码为 448 位整数。 基于 Curve448 派生出了名为 X448 的 ECDH 算法，以及基于 EdDSA 的高速数字签名算法 Ed448. 该选择哪种椭圆曲线首先，Bernstein 的 SafeCurves 标准列出了符合一组 ECC 安全要求的安","date":"2022-03-09","objectID":"/posts/practical-cryptography-basics-7-asymmetric-key-ciphers/:4:6","series":null,"tags":["Cryptography","密码学","非对称加密","安全","RSA","ECC"],"title":"写给开发人员的实用密码学（七）—— 非对称密钥加密算法 RSA/ECC","uri":"/posts/practical-cryptography-basics-7-asymmetric-key-ciphers/#curve448-x448-和-ed448"},{"categories":["tech"],"content":" 密码学常用椭圆曲线介绍在介绍密码学中的常用椭圆曲线前，需要先介绍一下椭圆曲线的阶（order）以及辅助因子（cofactor）这两个概念。 首先还得介绍下数学中「循环群」的概念，它是指能由单个元素所生成的群，在 ECC 中这就是预先定义好的基点 $G$. 一个有限域上的椭圆曲线可以形成一个有限「循环代数群」，它由曲线上的所有点组成。椭圆曲线的阶被定义为该曲线上所有点的个数（包括无穷远点）。 有些曲线加上 G 点可以形成一个单一循环群，这一个群包含了曲线上的所有点。而其他的曲线加上 G 点则形成多个不相交的循环子群，每个子群包含了曲线的一个子集。 对于上述第二种情况，假设曲线上的点被拆分到了 h 个循环子群中，每个子群的阶都是 r，那这时整个群的阶就是 $n = h * r$，其中子群的个数 h 被称为辅助因子。 有限域上的椭圆曲线的阶都是有限的，也就是说对于曲线上任意一点 $G$，我们计算它的数乘 $kG$，随着整数 $k$ 的增大，一定会存在某个 $k$ 使 $kG = O_{\\infty}$ 成立，然后 $k$ 继续增大时，因为 $O_{\\infty} * P = O_{\\infty}$，$kG$ 的值就固定为 $O_{\\infty}$ 了，更大的 $k$ 值已经失去了意义。 因此 ECC 中要求 $kG$ 中的私钥 $k$ 符合条件 $0 \\le k \\le r$，也就是说总的私钥数量是受 $r$ 限制的。 辅助因子通过用如下公式表示： $$ h = n / r $$ 其中 $n$ 是曲线的阶，$r$ 是每个子群的阶，$h$ 是辅助因子。 如果曲线形成了一个单一循环群，那显然 $h = 1$，否则 $h \u003e 1$ 举例如下： secp256k1 的辅助因子为 1 Curve25519 的辅助因子为 8 Curve448 的辅助因子为 4 生成点 G生成点 G 的选择是很有讲究的，虽然每个循环子群都包含有很多个生成点，但是 ECC 只会谨慎的选择其中一个。 首先 G 点必须要能生成出整个循环子群，其次还需要有尽可能高的计算性能。 数学上已知某些椭圆曲线上，不同的生成点生成出的循环子群，阶也是不同的。如果 G 点选得不好，可能会导致生成出的子群的阶较小。 前面我们已经提过子群的阶 $r$ 会限制总的私钥数量，导致算法强度变弱！因此不恰当的 $G$ 点可能会导致我们遭受「小子群攻击」。 为了避免这种风险，建议尽量使用被广泛使用的加密库，而不是自己撸一个。 椭圆曲线的域参数ECC椭圆曲线由一组椭圆曲线域参数描述，如曲线方程参数、场参数和生成点坐标。这些参数在各种密码学标准中指定，你可以网上搜到相应的 RFC 或 NIST 文档。 这些标准定义了一组命名曲线的参数，例如 secp256k1、P-521、brainpoolP512t1 和 SM2. 这些加密标准中描述的有限域上的椭圆曲线得到了密码学家的充分研究和分析，并被认为具有一定的安全强度。 也有一些密码学家（如 Daniel Bernstein）认为，官方密码标准中描述的大多数曲线都是「不安全的」，并定义了他们自己的密码标准，这些标准在更广泛的层面上考虑了 ECC 安全性。 开发人员应该仅使用各项标准文档给出的、经过密码学家充分研究的命名曲线。 secp256k1此曲线被应用在比特币中，它的域参数如下： p (modulus) = 0xFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFEFFFFFC2F n (order; size; the count of all possible EC points) = 0xFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFEBAAEDCE6AF48A03BBFD25E8CD0364141 a (方程 $y^2 ≡ x^3 + a*x + b (\\mod p)$ 中的常数) = 0x0000000000000000000000000000000000000000000000000000000000000000 b (方程 $y^2 ≡ x^3 + a*x + b (\\mod p)$ 中的常数)= 0x0000000000000000000000000000000000000000000000000000000000000007 g (the curve generator point G {x, y}) = (0x79BE667EF9DCBBAC55A06295CE870B07029BFCDB2DCE28D959F2815B16F81798, 0x483ada7726a3c4655da4fbfc0e1108a8fd17b448a68554199c47d08ffb10d4b8) h (cofactor, typically 1) = 01 Edwards 曲线椭圆曲线方程除了我们前面使用的 Weierstrass 形式 $$y^2 = (x^3 + ax + b) \\mod p$$ 外，还可以被写成其他多种形式，这些不同的形式是双有理等价的（别问，我也不懂什么叫「双有理等价」…）。 不同的方程形式在计算机的数值计算上可能会存在区别。 为了性能考虑，ECC 在部分场景下会考虑使用 Edwards 曲线形式进行计算，该方程形式如下： $$ x^{2}+y^{2}=1+dx^{2}y^{2} $$ 画个图长这样： 知名的 Edwards 曲线有： Curve1174 (251-bit) Curve25519 (255-bit) Curve383187 (383-bit) Curve41417 (414-bit) Curve448 (448-bit) E-521 (521-bit) … Curve25519, X25519 和 Ed25519 https://cryptography.io/en/latest/hazmat/primitives/asymmetric/ed25519/ 只要域参数选得好，Edwards 就可以以非常高的性能实现 ECC 密钥交换、数字签名、混合加密方案。 一个例子就是 Curve25519，它是 Edwards 曲线，其 Montgomery 形式的定义如下： $$ y^{2}=x^{3}+486662x^{2}+x $$ 其被定义在有限域 $\\mathbb {F} _{p}$ 上，$p = 2255 - 19$, 其他域参数如下： 阶 n = 2252 + 0x14def9dea2f79cd65812631a5cf5d3ed 辅因子 h = 8 虽然此曲线并未以 Edwards 形式定义，但是它已被证明与如下扭曲 Edwards 曲线（edwards25519）双有理等价： $$ -x^2 + y^2 = 1 + 37095705934669439343138083508754565189542113879843219016388785533085940283555 x^2 y^2 $$ 上面给出的这种 Edwards 形式与前文给出的 Weierstrass 形式完全等价，是专为计算速度优化而设计成这样的。 Curve25519 由 Daniel Bernstein 领导的密码学家团队精心设计，在多个设计和实现层面上达成了非常高的性能，同时不影响安全性。 Curve25519 的构造使其避免了许多潜在的实现缺陷。根据设计，它不受定时攻击的影响，并且它接受任何 32 字节的字符串作为有效的公钥，并且不需要验证。 它能提供 125.8bits 的安全强度（有时称为 ~ 128bits 安全性） Curve25519 的私钥为 251 位，通常编码为 256 位整数（32 个字节，64 个十六进制数字）。 公钥通常也编码为 256 位整数（255 位 y 坐标 + 1 位 x 坐标），这对开发人员来说非常方便。 基于 Curve25519 派生出了名为 X25519 的 ECDH 算法，以及基于 EdDSA 的高速数字签名算法 Ed25519. Curve448, X448 和 Ed448 https://cryptography.io/en/latest/hazmat/primitives/asymmetric/ed448/ Curve448（Curve448-Goldilocks）是一种非扭曲 Edwards 曲线，它的方程定义如下： $$ x^2 + y^2 = 1 - 39081 x^2 y^2 $$ 其被定义在有限域 $\\mathbb {F} _{p}$ 上，$p = 2448 - 2224 - 1$，其他域参数： 阶 n = 2446 - 0x8335dc163bb124b65129c96fde933d8d723a70aadc873d6d54a7bb0d 辅助因子 h = 4 与 Curve25519 一样，Curve448 也等价于前面给出的 Weierstrass 形式，选择 Edwards 形式主要是因为它能显著提升性能。 Curve448 提供 222.8 位的安全强度。 Curve448 的私钥为 446 位，通常编码为 448 位整数（56 个字节，112 个十六进制数字）。 公钥也被编码为 448 位整数。 基于 Curve448 派生出了名为 X448 的 ECDH 算法，以及基于 EdDSA 的高速数字签名算法 Ed448. 该选择哪种椭圆曲线首先，Bernstein 的 SafeCurves 标准列出了符合一组 ECC 安全要求的安","date":"2022-03-09","objectID":"/posts/practical-cryptography-basics-7-asymmetric-key-ciphers/:4:6","series":null,"tags":["Cryptography","密码学","非对称加密","安全","RSA","ECC"],"title":"写给开发人员的实用密码学（七）—— 非对称密钥加密算法 RSA/ECC","uri":"/posts/practical-cryptography-basics-7-asymmetric-key-ciphers/#该选择哪种椭圆曲线"},{"categories":["tech"],"content":" ECIES - 集成加密方案在文章开头我们已经介绍了集成加密方案 (IES)，它在密钥封装机制（KEM）的基础上，添加了密钥派生算法 KDF、消息认证算法 MAC 等其他密码学算法以达成我们对消息的安全性、真实性、完全性的需求。 而 ECIES 也完全类似，是在 ECC + 对称加密算法的基础上，添加了许多其他的密码学算法实现的。 ECIES 是一个加密框架，而不是某种固定的算法。它可以通过插拔不同的算法，形成不同的实现。 比如「secp256k1 + Scrypt + AES-GCM + HMAC-SHA512」。 大概就介绍到这里吧，后续就请在需要用到时自行探索相关的细节咯。 ","date":"2022-03-09","objectID":"/posts/practical-cryptography-basics-7-asymmetric-key-ciphers/:4:7","series":null,"tags":["Cryptography","密码学","非对称加密","安全","RSA","ECC"],"title":"写给开发人员的实用密码学（七）—— 非对称密钥加密算法 RSA/ECC","uri":"/posts/practical-cryptography-basics-7-asymmetric-key-ciphers/#ecies---集成加密方案"},{"categories":["tech"],"content":" 参考 Practical-Cryptography-for-Developers-Book A complete overview of SSL/TLS and its cryptographic system 密码发展史之近现代密码 - 中国国家密码管理局 RFC6090 - Fundamental Elliptic Curve Cryptography Algorithms Which elliptic curve should I use? ","date":"2022-03-09","objectID":"/posts/practical-cryptography-basics-7-asymmetric-key-ciphers/:5:0","series":null,"tags":["Cryptography","密码学","非对称加密","安全","RSA","ECC"],"title":"写给开发人员的实用密码学（七）—— 非对称密钥加密算法 RSA/ECC","uri":"/posts/practical-cryptography-basics-7-asymmetric-key-ciphers/#参考"},{"categories":["tech"],"content":" 本文主要翻译自 Practical-Cryptography-for-Developers-Book，笔者补充了部分代码示例。 《写给开发人员的实用密码学》系列文章目录： 写给开发人员的实用密码学（一）—— 概览 写给开发人员的实用密码学（二）—— 哈希函数 写给开发人员的实用密码学（三）—— MAC 与密钥派生函数 KDF 写给开发人员的实用密码学（四）—— 安全随机数生成器 CSPRNG 写给开发人员的实用密码学（五）—— 密钥交换 DHKE 与完美前向保密 PFS 写给开发人员的实用密码学（六）—— 对称密钥加密算法 写给开发人员的实用密码学（七）—— 非对称密钥加密算法 RSA/ECC 写给开发人员的实用密码学（八）—— 数字证书与 TLS 协议 待续 ","date":"2022-03-06","objectID":"/posts/practical-cryptography-basics-6-symmetric-key-ciphers/:0:0","series":null,"tags":["Cryptography","密码学","对称加密","安全","AES","ChaCha20"],"title":"写给开发人员的实用密码学（六）—— 对称密钥加密算法","uri":"/posts/practical-cryptography-basics-6-symmetric-key-ciphers/#"},{"categories":["tech"],"content":" 零、术语介绍两个常用动词： 加密：cipher 或者 encrypt 解密：decipher 或者 decrypt 另外有几个名词有必要解释： cipher: 指用于加解密的「密码算法」，有时也被直接翻译成「密码」 cryptographic algorithm: 密码学算法，泛指密码学相关的各类算法 ciphertext: 密文，即加密后的信息。对应的词是明文 plaintext password: 这个应该不需要解释，就是我们日常用的各种字符或者数字密码，也可称作口令。 passphrase: 翻译成「密码词组」或者「密碼片語」，通常指用于保护密钥或者其他敏感数据的一个 password 如果你用 ssh/gpg/openssl 等工具生成或使用过密钥，应该对它不陌生。 在密码学里面，最容易搞混的词估计就是「密码」了，cipher/password/passphrase 都可以被翻译成「密码」，需要注意下其中区别。 ","date":"2022-03-06","objectID":"/posts/practical-cryptography-basics-6-symmetric-key-ciphers/:1:0","series":null,"tags":["Cryptography","密码学","对称加密","安全","AES","ChaCha20"],"title":"写给开发人员的实用密码学（六）—— 对称密钥加密算法","uri":"/posts/practical-cryptography-basics-6-symmetric-key-ciphers/#零术语介绍"},{"categories":["tech"],"content":" 一、什么是对称加密在密码学中，有两种加密方案被广泛使用：「对称加密」与「非对称加密」。 对称加密是指，使用相同的密钥进行消息的加密与解密。因为这个特性，我们也称这个密钥为「共享密钥（Shared Secret Key）」，示意图如下： 现代密码学中广泛使用的对称加密算法（ciphers）有：AES（AES-128、AES-192、AES-256）、ChaCha20、Twofish、IDEA、Serpent、Camelia、RC6、CAST 等。 其中绝大多数都是「块密码算法（Block Cipher）」或者叫「分组密码算法」，这种算法一次只能加密固定大小的块（例如 128 位）； 少部分是「流密码算法（Stream Cipher）」，流密码算法将数据逐字节地加密为密文流。 通过使用称为「分组密码工作模式」的技术，可以将「分组密码算法」转换为「流密码算法」。 ","date":"2022-03-06","objectID":"/posts/practical-cryptography-basics-6-symmetric-key-ciphers/:2:0","series":null,"tags":["Cryptography","密码学","对称加密","安全","AES","ChaCha20"],"title":"写给开发人员的实用密码学（六）—— 对称密钥加密算法","uri":"/posts/practical-cryptography-basics-6-symmetric-key-ciphers/#一什么是对称加密"},{"categories":["tech"],"content":" 量子安全性即使计算机进入量子时代，仍然可以沿用当前的对称密码算法。因为大多数现代对称密钥密码算法都是抗量子的（quantum-resistant），这意味当使用长度足够的密钥时，强大的量子计算机无法破坏其安全性。 目前来看 256 位的 AES/Twofish 在很长一段时间内都将是 量子安全 的。 ","date":"2022-03-06","objectID":"/posts/practical-cryptography-basics-6-symmetric-key-ciphers/:2:1","series":null,"tags":["Cryptography","密码学","对称加密","安全","AES","ChaCha20"],"title":"写给开发人员的实用密码学（六）—— 对称密钥加密算法","uri":"/posts/practical-cryptography-basics-6-symmetric-key-ciphers/#量子安全性"},{"categories":["tech"],"content":" 二、对称加密方案的结构我们在第一章「概览」里介绍过，单纯使用数据加密算法只能保证数据的安全性，并不能满足我们对消息真实性、完整性与不可否认性的需求，因此通常我们会将对称加密算法跟其他算法组合成一个「对称加密方案」来使用，这种多个密码学算法组成的「加密方案」能同时保证数据的安全性、真实性、完整性与不可否认性。 一个分组加密方案通常会包含如下几种算法： 将密码转换为密钥的密钥派生算法 KDF（如 Scrypt 或 Argon2）：通过使用 KDF，加密方案可以允许用户使用字符密码作为「Shared Secret Key」，并使密码的破解变得困难和缓慢 分组密码工作模式（用于将分组密码转换为流密码，如 CBC 或 CTR）+ 消息填充算法（如 PKCS7）：分组密码算法（如 AES）需要借助这两种算法，才能加密任意大小的数据 分组密码算法（如 AES）：使用密钥安全地加密固定长度的数据块 大多数流行的对称加密算法，都是分组密码算法 消息认证算法（如HMAC）：用于验证消息的真实性、完整性、不可否认性 而一个流密码加密方案本身就能加密任意长度的数据，因此不需要「分组密码模式」与「消息填充算法」。 如 AES-256-CTR-HMAC-SHA256 就表示一个使用 AES-256 与 Counter 分组模式进行加密，使用 HMAC-SHA256 进行消息认证的加密方案。 其他流行的对称加密方案还有 ChaCha20-Poly1305 和 AES-128-GCM 等，其中 ChaCha20-Poly130 是一个流密码加密方案。我们会在后面单独介绍这两种加密方案。 ","date":"2022-03-06","objectID":"/posts/practical-cryptography-basics-6-symmetric-key-ciphers/:3:0","series":null,"tags":["Cryptography","密码学","对称加密","安全","AES","ChaCha20"],"title":"写给开发人员的实用密码学（六）—— 对称密钥加密算法","uri":"/posts/practical-cryptography-basics-6-symmetric-key-ciphers/#二对称加密方案的结构"},{"categories":["tech"],"content":" 三、分组密码工作模式前面简单介绍了「分组密码工作模式」可以将「分组密码算法」转换为「流密码算法」，从而实现加密任意长度的数据，这里主要就具体介绍下这个分组密码工作模式（下文简称为「分组模式」或者「XXX 模式」）。 加密方案的名称中就带有具体的「分组模式」名称，如： AES-256-GCM - 具有 256 位加密密钥和 GCM 分组模式的 AES 密码 AES-128-CTR - 具有 128 位加密密钥和 CTR 分组模式的 AES 密码 Serpent-128-CBC - 具有 128 位加密密钥和 CBC 分组模式的 Serpent 密码 「分组密码工作模式」背后的主要思想是把明文分成多个长度固定的组，再在这些分组上重复应用分组密码算法进行加密/解密，以实现安全地加密/解密任意长度的数据。 某些分组模式（如 CBC）要求将输入拆分为分组，并使用填充算法（例如添加特殊填充字符）将最末尾的分组填充到块大小。 也有些分组模式（如 CTR、CFB、OFB、CCM、EAX 和 GCM）根本不需要填充，因为它们在每个步骤中，都直接在明文部分和内部密码状态之间执行异或（XOR）运算. 使用「分组模式」加密大量数据的流程基本如下： 初始化加密算法状态（使用加密密钥 + 初始向量 IV） 加密数据的第一个分组 使用加密密钥和其他参数转换加密算法的当前状态 加密下一个分组 再次转换加密状态 再加密下一分组 依此类推，直到处理完所有输入数据 解密的流程跟加密完全类似：先初始化算法，然后依次解密所有分组，中间可能会涉及到加密状态的转换。 下面我们来具体介绍下 CTR 与 GCM 两个常见的分组模式。 ","date":"2022-03-06","objectID":"/posts/practical-cryptography-basics-6-symmetric-key-ciphers/:4:0","series":null,"tags":["Cryptography","密码学","对称加密","安全","AES","ChaCha20"],"title":"写给开发人员的实用密码学（六）—— 对称密钥加密算法","uri":"/posts/practical-cryptography-basics-6-symmetric-key-ciphers/#三分组密码工作模式"},{"categories":["tech"],"content":" 0. 初始向量 IV介绍具体的分组模式前，需要先了解下初始向量 IV（Initialization Vector）这个概念，它有时也被称作 Salt 或者 Nonce。 初始向量 IV 通常是一个随机数，主要作用是往密文中添加随机性，使同样的明文被多次加密也会产生不同的密文，从而确保密文的不可预测性。 IV 的大小应与密码块大小相同，例如 AES、Serpent 和 Camellia 都只支持 128 位密码块，那么它们需要的 IV 也必须也 128 位。 IV 通常无需保密，但是应当足够随机（无法预测），而且不允许重用，应该对每条加密消息使用随机且不可预测的 IV。 一个常见错误是使用相同的对称密钥和相同的 IV 加密多条消息，这使得针对大多数分组模式的各种加密攻击成为可能。 ","date":"2022-03-06","objectID":"/posts/practical-cryptography-basics-6-symmetric-key-ciphers/:4:1","series":null,"tags":["Cryptography","密码学","对称加密","安全","AES","ChaCha20"],"title":"写给开发人员的实用密码学（六）—— 对称密钥加密算法","uri":"/posts/practical-cryptography-basics-6-symmetric-key-ciphers/#0-初始向量-iv"},{"categories":["tech"],"content":" 1. CTR (Counter) 分组模式 参考文档: https://csrc.nist.gov/publications/detail/sp/800-38a/final 下图说明了「CTR 分组工作模式」的加密解密流程，基本上就是将明文/密文拆分成一个个长度固定的分组，然后使用一定的算法进行加密与解密： 可以看到两图中左边的第一个步骤，涉及到三个参数： Nonce，初始向量 IV 的别名，前面已经介绍过了。 Counter: 一个计数器，最常用的 Counter 实现是「从 0 开始，每次计算都自增 1」 Key: 对称加密的密钥 Plaintext: 明文的一个分组。除了最后一个分组外，其他分组的长度应该跟 Key 相同 CTR 模式加解密的算法使用公式来表示如下： $$ \\begin{alignedat}{2} C_i \u0026= P_i \\oplus O_i, \\ \u0026\\text{for } i \u0026= 1, 2 … n-1 \\\\ P_i \u0026= C_i \\oplus O_i, \\ \u0026\\text{for } i \u0026= 1, 2 … n-1 \\\\ O_i \u0026= \\text{CIPH}_{key}(\\text{Nonce} + I_i), \\ \u0026\\text{for } i \u0026= 1, 2 … n-1 \\end{alignedat} $$ 公式的符号说明如下 $C_i$ 表示密文的第 $i$ 个分组 $P_i$ 表示明文的第 $i$ 个 分组 $O_i$ 是一个中间量，第三个公式是它的计算方法 $I_i$ 表示计数器返回的第 $i$ 个值，其长度应与分组的长度相同 $\\text{CIPH}_{key}$ 表示使用密钥 $key$ 的对称加密算法 上面的公式只描述了 $ 0 \\ge i \\le n-1$ 的场景，最后一个分组 $i = n$ 要特殊一些——它的长度可能比 Key 要短。 CTR 模式加解密这最后这个分组时，会直接忽略掉 $O_n$ 末尾多余的 bytes. 这种处理方式使得 CTR 模式不需要使用填充算法对最后一个分组进行填充，而且还使密文跟明文的长度完全一致。 我们假设最后一个分组的长度为 $u$，它的加解密算法描述如下（$MSB_u(O_n)$ 表示取 $O_n$ 的 u 个最高有效位）： $$ \\begin{alignedat}{2} C_{n} \u0026= P_{n} \\oplus {MSB_u}(O_n) \\\\ P_{n} \u0026= C_{n} \\oplus {MSB_u}(O_n)\\\\ O_n \u0026= \\text{CIPH}_{key}(\\text{Nonce} + I_n) \\end{alignedat} $$ 可以看到，因为异或 XOR 的对称性，加密跟解密的算法是完全相同的，直接 XOR $O_i$ 即可。 Python 中最流行的密码学库是 cryptography，requests 的底层曾经就使用了它（新版本已经换成使用标准库 ssl 了），下面我们使用这个库来演示下 AES-256-CTR 算法： # pip install cryptography==36.0.1 import os from cryptography.hazmat.primitives.ciphers import Cipher, algorithms, modes plaintext = b\"this is a test message, hahahahahaha~\" # 使用 32bytes 的 key，即使用算法 AES-256-CTR key = os.urandom(32) # key =\u003e b'\\x96\\xec.\\xc7\\xd5\\x1b/5\\xa1\\x10s\\x9d\\xd5\\x10z\\xdc\\x90\\xb5\\x1cm\"\u003ex\\xfd \\xd5\\xc5\\xaf\\x19\\xd1Z\\xbb' # AES 算法的 block 大小是固定的 128bits，即 16 bytes, IV 长度需要与 block 一致 iv = os.urandom(16) # iv =\u003e b'\\x88[\\xc9\\n`\\xe4\\xc2^\\xaf\\xdc\\x1e\\xfd.c\u003e=' # 1. 发送方加密数据 ## 构建 AES-256-CTR 的 cipher，然后加密数据，得到密文 cipher = Cipher(algorithms.AES(key), modes.CTR(iv)) encryptor = cipher.encryptor() ciphertext = encryptor.update(plaintext) + encryptor.finalize() # ciphertext =\u003e b'\\x9b6(\\x1d\\xfd\\xde\\x96S\\x8b\\x8f\\x90\\xc5}ou\\x9e\\xb1\\xbd\\x9af\\xb8\\xdc\\xec\\xbf\\xa3\"\\x18^\\xac\\x14\\xc8s2*\\x1a\\xcf\\x1d' # 2. 发送方将 iv + ciphertext 发送给接收方 # 3. 接收方解密数据 # 接收方使用自己的 key + 接收到的 iv，构建 cipher，然后解密出原始数据 cipher = Cipher(algorithms.AES(key), modes.CTR(iv)) decryptor = cipher.decryptor() decryptor.update(ciphertext) + decryptor.finalize() 从上面的算法描述能感觉到，CTR 算法还蛮简单的。下面我使用 Python 写一个能够 work 的 CTR 实现： def xor_bytes(a, b): \"\"\"Returns a new byte array with the elements xor'ed. if len(a) != len(b), extra parts are discard. \"\"\" return bytes(i^j for i, j in zip(a, b)) def inc_bytes(a): \"\"\" Returns a new byte array with the value increment by 1 \"\"\" out = list(a) for i in reversed(range(len(out))): if out[i] == 0xFF: out[i] = 0 else: out[i] += 1 break return bytes(out) def split_blocks(message, block_size, require_padding=True): \"\"\" Split `message` with fixed length `block_size` \"\"\" assert len(message) % block_size == 0 or not require_padding return [message[i:i+16] for i in range(0, len(message), block_size)] def encrypt_ctr(block_cipher, plaintext, iv): \"\"\" Encrypts `plaintext` using CTR mode with the given nounce/IV. \"\"\" assert len(iv) == 16 blocks = [] nonce = iv for plaintext_block in split_blocks(plaintext, block_size=16, require_padding=False): # CTR mode encrypt: plaintext_block XOR encrypt(nonce) o = bytes(block_cipher.encrypt(nonce)) block = xor_bytes(plaintext_block, o) # extra parts of `o` are discard in this step blocks.append(block) nonce = inc_bytes(nonce) return b''.join(blocks) # 加密与解密的算法完全一致 decrypt_ctr = encrypt_ctr 接下来验证下算法的正确性： # Python 官方库未提供 AES 实现，因此需要先装下这个库： # pip install pyaes==1.6.1 from pyaes import AES # AES-256-CTR - plaintext key 都与前面的测试代码完全一致 plaintext = b\"this is a test message, hahahahahaha~\" key = b'\\x96\\xec.\\xc7\\xd5\\x1b/5\\xa1\\x10s\\x9d\\xd5\\x10z\\xdc\\x90\\xb5\\x1cm\"\u003ex\\xfd \\xd5\\xc5\\xaf\\x19\\xd1Z\\xbb' # 1. 发送方加密数据 # 首先生成一个随机 IV，为了对比，这里使用前面生成好的数据 iv = b'\\x88[\\xc9\\n`\\xe4\\xc2^\\xaf\\xdc\\x1e\\xfd.c\u003e=' aes_cipher = AES(key) ciphertext = encrypt_ctr(aes_cipher, plaintext, iv) print(\"c","date":"2022-03-06","objectID":"/posts/practical-cryptography-basics-6-symmetric-key-ciphers/:4:2","series":null,"tags":["Cryptography","密码学","对称加密","安全","AES","ChaCha20"],"title":"写给开发人员的实用密码学（六）—— 对称密钥加密算法","uri":"/posts/practical-cryptography-basics-6-symmetric-key-ciphers/#counter_mode"},{"categories":["tech"],"content":" 2. GCM (Galois/Counter) 分组模式GCM (Galois/Counter) 模式在 CTR 模式的基础上，添加了消息认证的功能，而且同时还具有与 CTR 模式相同的并行计算能力。因此相比 CTR 模式，GCM 不仅速度一样快，还能额外提供对消息完整性、真实性的验证能力。 下图直观地解释了 GCM 块模式（Galois/Counter 模式）的工作原理： GCM 模式新增的 Auth Tag，计算起来会有些复杂，我们就直接略过了，对原理感兴趣的可以看下 Galois/Counter_Mode_wiki. ","date":"2022-03-06","objectID":"/posts/practical-cryptography-basics-6-symmetric-key-ciphers/:4:3","series":null,"tags":["Cryptography","密码学","对称加密","安全","AES","ChaCha20"],"title":"写给开发人员的实用密码学（六）—— 对称密钥加密算法","uri":"/posts/practical-cryptography-basics-6-symmetric-key-ciphers/#2-gcm-galoiscounter-分组模式"},{"categories":["tech"],"content":" 3. 如何选用块模式一些 Tips: 常用的安全块模式是 CBC（密码块链接）、CTR（计数器）和 GCM（伽罗瓦/计数器模式），它们需要一个随机（不可预测的）初始化向量 (IV)，也称为 nonce 或 salt 「CTR（Counter）」块模式在大多数情况下是一个不错的选择，因为它具有很强的安全性和并行处理能力，允许任意输入数据长度（无填充）。但它不提供身份验证和完整性，只提供加密 GCM（Galois/Counter Mode）块模式继承了 CTR 模式的所有优点，并增加了加密消息认证能力。GCM 是在对称密码中实现认证加密的快速有效的方法，强烈推荐 CBC 模式在固定大小的分组上工作。因此，在将输入数据拆分为分组后，应使用填充算法使最后一个分组的长度一致。大多数应用程序使用 PKCS7 填充方案或 ANSI X.923. 在某些情况下，CBC 阻塞模式可能容易受到「padding oracle」攻击，因此最好避免使用 CBC 模式 众所周知的不安全块模式是 ECB（电子密码本），它将相等的输入块加密为相等的输出块（无加密扩散能力）。不要使用 ECB 块模式！它可能会危及整个加密方案。 CBC、CTR 和 GCM 模式等大多数块都支持「随机访问」解密。比如在视频播放器中的任意时间偏移处寻找，播放加密的视频流 总之，建议使用 CTR (Counter) 或 GCM (Galois/Counter) 分组模式。 其他的分组在某些情况下可能会有所帮助，但很可能有安全隐患，因此除非你很清楚自己在做什么，否则不要使用其他分组模式！ CTR 和 GCM 加密模式有很多优点：它们是安全的（目前没有已知的重大缺陷），可以加密任意长度的数据而无需填充，可以并行加密和解密分组（在多核 CPU 中）并可以直接解密任意一个密文分组。 因此它们适用于加密加密钱包、文档和流视频（用户可以按时间查找）。 GCM 还提供消息认证，是一般情况下密码块模式的推荐选择。 请注意，GCM、CTR 和其他分组模式会泄漏原始消息的长度，因为它们生成的密文长度与明文消息的长度相同。 如果您想避免泄露原始明文长度，可以在加密前向明文添加一些随机字节（额外的填充数据），并在解密后将其删除。 ","date":"2022-03-06","objectID":"/posts/practical-cryptography-basics-6-symmetric-key-ciphers/:4:4","series":null,"tags":["Cryptography","密码学","对称加密","安全","AES","ChaCha20"],"title":"写给开发人员的实用密码学（六）—— 对称密钥加密算法","uri":"/posts/practical-cryptography-basics-6-symmetric-key-ciphers/#3-如何选用块模式"},{"categories":["tech"],"content":" 四、对称加密算法与对称加密方案前面啰嗦了这么多，下面进入正题：对称加密算法 ","date":"2022-03-06","objectID":"/posts/practical-cryptography-basics-6-symmetric-key-ciphers/:5:0","series":null,"tags":["Cryptography","密码学","对称加密","安全","AES","ChaCha20"],"title":"写给开发人员的实用密码学（六）—— 对称密钥加密算法","uri":"/posts/practical-cryptography-basics-6-symmetric-key-ciphers/#四对称加密算法与对称加密方案"},{"categories":["tech"],"content":" 1. 安全的对称加密算法目前应用最广泛的对称加密算法，是 AES 跟 Salsa20 / ChaCha20 这两个系列。 1. AES (Rijndael) wiki: https://en.wikipedia.org/wiki/Advanced_Encryption_Standard AES（高级加密标准，也称为 Rijndael）是现代 IT 行业中最流行和广泛使用的对称加密算法。AES 被证明是高度安全、快速且标准化的，到目前为止没有发现任何明显的弱点或攻击手段，而且几乎在所有平台上都得到了很好的支持。 AES 是 128 位分组密码，使用 128、192 或 256 位密钥。它通常与分组模式组合成分组加密方案（如 AES-CTR 或 AES-GCM）以处理流数据。 在大多数分组模式中，AES 还需要一个随机的 128 位初始向量 IV。 Rijndael (AES) 算法可免费用于任何用途，而且非常流行。很多站点都选择 AES 作为 TLS 协议的一部分，以实现安全通信。 现代 CPU 硬件基本都在微处理器级别实现了 AES 指令以加速 AES 加密解密操作。 这里有一个纯 Python 的 AES 实现可供参考: AES encryption in pure Python - boppreh 我们在前面的 CTR 分组模式中已经使用 Python 实践了 AES-256-CTR 加密方案。 而实际上更常用的是支持集成身份验证加密（AEAD）的 AES-256-GCM 加密方案，它的优势我们前面已经介绍过了，这里我们使用 Python 演示下如何使用： # pip install cryptography==36.0.1 import os from cryptography.hazmat.primitives.ciphers import ( Cipher, algorithms, modes ) def encrypt(key, plaintext, associated_data): # Generate a random 96-bit IV. iv = os.urandom(12) # Construct an AES-GCM Cipher object with the given key and a # randomly generated IV. encryptor = Cipher( algorithms.AES(key), modes.GCM(iv), ).encryptor() # associated_data will be authenticated but not encrypted, # it must also be passed in on decryption. encryptor.authenticate_additional_data(associated_data) # Encrypt the plaintext and get the associated ciphertext. # GCM does not require padding. ciphertext = encryptor.update(plaintext) + encryptor.finalize() return (iv, ciphertext, encryptor.tag) def decrypt(key, associated_data, iv, ciphertext, tag): # Construct a Cipher object, with the key, iv, and additionally the # GCM tag used for authenticating the message. decryptor = Cipher( algorithms.AES(key), modes.GCM(iv, tag), ).decryptor() # We put associated_data back in or the tag will fail to verify # when we finalize the decryptor. decryptor.authenticate_additional_data(associated_data) # Decryption gets us the authenticated plaintext. # If the tag does not match an InvalidTag exception will be raised. return decryptor.update(ciphertext) + decryptor.finalize() # 接下来进行算法验证 plaintext = b\"this is a paintext, hahahahahaha~\" key = b'\\x96\\xec.\\xc7\\xd5\\x1b/5\\xa1\\x10s\\x9d\\xd5\\x10z\\xdc\\x90\\xb5\\x1cm\"\u003ex\\xfd \\xd5\\xc5\\xaf\\x19\\xd1Z\\xbb' associated_data = b\"authenticated but not encrypted payload\" # 被用于消息认证的关联数据 # 1. 发送方加密消息 iv, ciphertext, tag = encrypt( key, plaintext, associated_data ) # 2. 发送方将 associated_data iv ciphertext tag 打包发送给接收方 # 3. 接收方使用自己的 key 验证并解密数据 descrypt_text = decrypt( key, associated_data, iv, ciphertext, tag ) 2. Salsa20 / ChaCha20 wiki: https://en.wikipedia.org/wiki/Salsa20#ChaCha_variant Salsa20 及其改进的变体 ChaCha（ChaCha8、ChaCha12、ChaCha20）和 XSalsa20 是由密码学家 Daniel Bernstein 设计的现代、快速的对称流密码家族。 Salsa20 密码是对称流密码设计竞赛 eSTREAM（2004-2008）的决赛选手之一，它随后与相关的 BLAKE 哈希函数一起被广泛采用。 Salsa20 及其变体是免版税的，没有专利。 Salsa20 密码将 128 位或 256 位对称密钥 + 随机生成的 64 位随机数（初始向量）和无限长度的数据流作为输入，并生成长度相同的加密数据流作为输出输入流。 ChaCha20-Poly1305Salsa20 应用最为广泛的是认证加密方案：ChaCha20-Poly1305，即组合使用 ChaCha20 与消息认证算法 Poly1305，它们都由密码学家 Bernstein 设计。 ChaCha20-Poly1305 已被证明足够安全，不过跟 GCM 一样它的安全性也依赖于足够随机的初始向量 IV，另外 ChaCha20-Poly1305 也不容易遭受计时攻击。 在没有硬件加速的情况下，ChaCha20 通常比 AES 要快得多（比如在旧的没有硬件加速的移动设备上），这是它最大的优势。 以下是一个 ChaCha20 的 Python 示例： # pip install cryptography==36.0.1 import os from cryptography.hazmat.primitives.ciphers import Cipher, algorithms, modes plaintext = b\"this is a paintext, hahahahahaha~\" key = b'\\x96\\xec.\\xc7\\xd5\\x1b/5\\xa1\\x10s\\x9d\\xd5\\x10z\\xdc\\x90\\xb5\\x1cm\"\u003ex\\xfd \\xd5\\xc5\\xaf\\x19\\xd1Z\\xbb' nonce = os.urandom(16) algorithm = algorithms.ChaCha20(key, nonce) # ChaCha20 是一个流密码，mode 必须为 None cipher = Cipher(algorithm, mode=None) # 1. 加密 encryptor = cipher.encryptor() ct = encryptor.update(plaintext) # 2. 解密 decryptor = cipher.decryptor() decryptor.update(ct) 3. 其他流行的对称加密算法还有一些其他的现代安全对称密码，它们的应用不如 AES 和 ChaCha20 这么广泛，但在程序员和信息安全社区中仍然很流行： Serpent - 安全对称密钥分组密码（密钥大小：128、192 或 256 位），公众所有（Public Domain），完全免费 Twofish - 安全对称密钥分组密码（密钥大小：128、192 或 256 位），公众所有（Public Domain），完全免费 Camellia - 安全对称密钥分组密码（分组大小：128 位；密钥大小：128、192 和 256 位），专利算法，但完全免费 该算法由三菱和日本电信电话（NTT）在 2000 年共同发明 RC5 - 安全对称密","date":"2022-03-06","objectID":"/posts/practical-cryptography-basics-6-symmetric-key-ciphers/:5:1","series":null,"tags":["Cryptography","密码学","对称加密","安全","AES","ChaCha20"],"title":"写给开发人员的实用密码学（六）—— 对称密钥加密算法","uri":"/posts/practical-cryptography-basics-6-symmetric-key-ciphers/#1-安全的对称加密算法"},{"categories":["tech"],"content":" 1. 安全的对称加密算法目前应用最广泛的对称加密算法，是 AES 跟 Salsa20 / ChaCha20 这两个系列。 1. AES (Rijndael) wiki: https://en.wikipedia.org/wiki/Advanced_Encryption_Standard AES（高级加密标准，也称为 Rijndael）是现代 IT 行业中最流行和广泛使用的对称加密算法。AES 被证明是高度安全、快速且标准化的，到目前为止没有发现任何明显的弱点或攻击手段，而且几乎在所有平台上都得到了很好的支持。 AES 是 128 位分组密码，使用 128、192 或 256 位密钥。它通常与分组模式组合成分组加密方案（如 AES-CTR 或 AES-GCM）以处理流数据。 在大多数分组模式中，AES 还需要一个随机的 128 位初始向量 IV。 Rijndael (AES) 算法可免费用于任何用途，而且非常流行。很多站点都选择 AES 作为 TLS 协议的一部分，以实现安全通信。 现代 CPU 硬件基本都在微处理器级别实现了 AES 指令以加速 AES 加密解密操作。 这里有一个纯 Python 的 AES 实现可供参考: AES encryption in pure Python - boppreh 我们在前面的 CTR 分组模式中已经使用 Python 实践了 AES-256-CTR 加密方案。 而实际上更常用的是支持集成身份验证加密（AEAD）的 AES-256-GCM 加密方案，它的优势我们前面已经介绍过了，这里我们使用 Python 演示下如何使用： # pip install cryptography==36.0.1 import os from cryptography.hazmat.primitives.ciphers import ( Cipher, algorithms, modes ) def encrypt(key, plaintext, associated_data): # Generate a random 96-bit IV. iv = os.urandom(12) # Construct an AES-GCM Cipher object with the given key and a # randomly generated IV. encryptor = Cipher( algorithms.AES(key), modes.GCM(iv), ).encryptor() # associated_data will be authenticated but not encrypted, # it must also be passed in on decryption. encryptor.authenticate_additional_data(associated_data) # Encrypt the plaintext and get the associated ciphertext. # GCM does not require padding. ciphertext = encryptor.update(plaintext) + encryptor.finalize() return (iv, ciphertext, encryptor.tag) def decrypt(key, associated_data, iv, ciphertext, tag): # Construct a Cipher object, with the key, iv, and additionally the # GCM tag used for authenticating the message. decryptor = Cipher( algorithms.AES(key), modes.GCM(iv, tag), ).decryptor() # We put associated_data back in or the tag will fail to verify # when we finalize the decryptor. decryptor.authenticate_additional_data(associated_data) # Decryption gets us the authenticated plaintext. # If the tag does not match an InvalidTag exception will be raised. return decryptor.update(ciphertext) + decryptor.finalize() # 接下来进行算法验证 plaintext = b\"this is a paintext, hahahahahaha~\" key = b'\\x96\\xec.\\xc7\\xd5\\x1b/5\\xa1\\x10s\\x9d\\xd5\\x10z\\xdc\\x90\\xb5\\x1cm\"\u003ex\\xfd \\xd5\\xc5\\xaf\\x19\\xd1Z\\xbb' associated_data = b\"authenticated but not encrypted payload\" # 被用于消息认证的关联数据 # 1. 发送方加密消息 iv, ciphertext, tag = encrypt( key, plaintext, associated_data ) # 2. 发送方将 associated_data iv ciphertext tag 打包发送给接收方 # 3. 接收方使用自己的 key 验证并解密数据 descrypt_text = decrypt( key, associated_data, iv, ciphertext, tag ) 2. Salsa20 / ChaCha20 wiki: https://en.wikipedia.org/wiki/Salsa20#ChaCha_variant Salsa20 及其改进的变体 ChaCha（ChaCha8、ChaCha12、ChaCha20）和 XSalsa20 是由密码学家 Daniel Bernstein 设计的现代、快速的对称流密码家族。 Salsa20 密码是对称流密码设计竞赛 eSTREAM（2004-2008）的决赛选手之一，它随后与相关的 BLAKE 哈希函数一起被广泛采用。 Salsa20 及其变体是免版税的，没有专利。 Salsa20 密码将 128 位或 256 位对称密钥 + 随机生成的 64 位随机数（初始向量）和无限长度的数据流作为输入，并生成长度相同的加密数据流作为输出输入流。 ChaCha20-Poly1305Salsa20 应用最为广泛的是认证加密方案：ChaCha20-Poly1305，即组合使用 ChaCha20 与消息认证算法 Poly1305，它们都由密码学家 Bernstein 设计。 ChaCha20-Poly1305 已被证明足够安全，不过跟 GCM 一样它的安全性也依赖于足够随机的初始向量 IV，另外 ChaCha20-Poly1305 也不容易遭受计时攻击。 在没有硬件加速的情况下，ChaCha20 通常比 AES 要快得多（比如在旧的没有硬件加速的移动设备上），这是它最大的优势。 以下是一个 ChaCha20 的 Python 示例： # pip install cryptography==36.0.1 import os from cryptography.hazmat.primitives.ciphers import Cipher, algorithms, modes plaintext = b\"this is a paintext, hahahahahaha~\" key = b'\\x96\\xec.\\xc7\\xd5\\x1b/5\\xa1\\x10s\\x9d\\xd5\\x10z\\xdc\\x90\\xb5\\x1cm\"\u003ex\\xfd \\xd5\\xc5\\xaf\\x19\\xd1Z\\xbb' nonce = os.urandom(16) algorithm = algorithms.ChaCha20(key, nonce) # ChaCha20 是一个流密码，mode 必须为 None cipher = Cipher(algorithm, mode=None) # 1. 加密 encryptor = cipher.encryptor() ct = encryptor.update(plaintext) # 2. 解密 decryptor = cipher.decryptor() decryptor.update(ct) 3. 其他流行的对称加密算法还有一些其他的现代安全对称密码，它们的应用不如 AES 和 ChaCha20 这么广泛，但在程序员和信息安全社区中仍然很流行： Serpent - 安全对称密钥分组密码（密钥大小：128、192 或 256 位），公众所有（Public Domain），完全免费 Twofish - 安全对称密钥分组密码（密钥大小：128、192 或 256 位），公众所有（Public Domain），完全免费 Camellia - 安全对称密钥分组密码（分组大小：128 位；密钥大小：128、192 和 256 位），专利算法，但完全免费 该算法由三菱和日本电信电话（NTT）在 2000 年共同发明 RC5 - 安全对称密","date":"2022-03-06","objectID":"/posts/practical-cryptography-basics-6-symmetric-key-ciphers/:5:1","series":null,"tags":["Cryptography","密码学","对称加密","安全","AES","ChaCha20"],"title":"写给开发人员的实用密码学（六）—— 对称密钥加密算法","uri":"/posts/practical-cryptography-basics-6-symmetric-key-ciphers/#1-aes-rijndael"},{"categories":["tech"],"content":" 1. 安全的对称加密算法目前应用最广泛的对称加密算法，是 AES 跟 Salsa20 / ChaCha20 这两个系列。 1. AES (Rijndael) wiki: https://en.wikipedia.org/wiki/Advanced_Encryption_Standard AES（高级加密标准，也称为 Rijndael）是现代 IT 行业中最流行和广泛使用的对称加密算法。AES 被证明是高度安全、快速且标准化的，到目前为止没有发现任何明显的弱点或攻击手段，而且几乎在所有平台上都得到了很好的支持。 AES 是 128 位分组密码，使用 128、192 或 256 位密钥。它通常与分组模式组合成分组加密方案（如 AES-CTR 或 AES-GCM）以处理流数据。 在大多数分组模式中，AES 还需要一个随机的 128 位初始向量 IV。 Rijndael (AES) 算法可免费用于任何用途，而且非常流行。很多站点都选择 AES 作为 TLS 协议的一部分，以实现安全通信。 现代 CPU 硬件基本都在微处理器级别实现了 AES 指令以加速 AES 加密解密操作。 这里有一个纯 Python 的 AES 实现可供参考: AES encryption in pure Python - boppreh 我们在前面的 CTR 分组模式中已经使用 Python 实践了 AES-256-CTR 加密方案。 而实际上更常用的是支持集成身份验证加密（AEAD）的 AES-256-GCM 加密方案，它的优势我们前面已经介绍过了，这里我们使用 Python 演示下如何使用： # pip install cryptography==36.0.1 import os from cryptography.hazmat.primitives.ciphers import ( Cipher, algorithms, modes ) def encrypt(key, plaintext, associated_data): # Generate a random 96-bit IV. iv = os.urandom(12) # Construct an AES-GCM Cipher object with the given key and a # randomly generated IV. encryptor = Cipher( algorithms.AES(key), modes.GCM(iv), ).encryptor() # associated_data will be authenticated but not encrypted, # it must also be passed in on decryption. encryptor.authenticate_additional_data(associated_data) # Encrypt the plaintext and get the associated ciphertext. # GCM does not require padding. ciphertext = encryptor.update(plaintext) + encryptor.finalize() return (iv, ciphertext, encryptor.tag) def decrypt(key, associated_data, iv, ciphertext, tag): # Construct a Cipher object, with the key, iv, and additionally the # GCM tag used for authenticating the message. decryptor = Cipher( algorithms.AES(key), modes.GCM(iv, tag), ).decryptor() # We put associated_data back in or the tag will fail to verify # when we finalize the decryptor. decryptor.authenticate_additional_data(associated_data) # Decryption gets us the authenticated plaintext. # If the tag does not match an InvalidTag exception will be raised. return decryptor.update(ciphertext) + decryptor.finalize() # 接下来进行算法验证 plaintext = b\"this is a paintext, hahahahahaha~\" key = b'\\x96\\xec.\\xc7\\xd5\\x1b/5\\xa1\\x10s\\x9d\\xd5\\x10z\\xdc\\x90\\xb5\\x1cm\"\u003ex\\xfd \\xd5\\xc5\\xaf\\x19\\xd1Z\\xbb' associated_data = b\"authenticated but not encrypted payload\" # 被用于消息认证的关联数据 # 1. 发送方加密消息 iv, ciphertext, tag = encrypt( key, plaintext, associated_data ) # 2. 发送方将 associated_data iv ciphertext tag 打包发送给接收方 # 3. 接收方使用自己的 key 验证并解密数据 descrypt_text = decrypt( key, associated_data, iv, ciphertext, tag ) 2. Salsa20 / ChaCha20 wiki: https://en.wikipedia.org/wiki/Salsa20#ChaCha_variant Salsa20 及其改进的变体 ChaCha（ChaCha8、ChaCha12、ChaCha20）和 XSalsa20 是由密码学家 Daniel Bernstein 设计的现代、快速的对称流密码家族。 Salsa20 密码是对称流密码设计竞赛 eSTREAM（2004-2008）的决赛选手之一，它随后与相关的 BLAKE 哈希函数一起被广泛采用。 Salsa20 及其变体是免版税的，没有专利。 Salsa20 密码将 128 位或 256 位对称密钥 + 随机生成的 64 位随机数（初始向量）和无限长度的数据流作为输入，并生成长度相同的加密数据流作为输出输入流。 ChaCha20-Poly1305Salsa20 应用最为广泛的是认证加密方案：ChaCha20-Poly1305，即组合使用 ChaCha20 与消息认证算法 Poly1305，它们都由密码学家 Bernstein 设计。 ChaCha20-Poly1305 已被证明足够安全，不过跟 GCM 一样它的安全性也依赖于足够随机的初始向量 IV，另外 ChaCha20-Poly1305 也不容易遭受计时攻击。 在没有硬件加速的情况下，ChaCha20 通常比 AES 要快得多（比如在旧的没有硬件加速的移动设备上），这是它最大的优势。 以下是一个 ChaCha20 的 Python 示例： # pip install cryptography==36.0.1 import os from cryptography.hazmat.primitives.ciphers import Cipher, algorithms, modes plaintext = b\"this is a paintext, hahahahahaha~\" key = b'\\x96\\xec.\\xc7\\xd5\\x1b/5\\xa1\\x10s\\x9d\\xd5\\x10z\\xdc\\x90\\xb5\\x1cm\"\u003ex\\xfd \\xd5\\xc5\\xaf\\x19\\xd1Z\\xbb' nonce = os.urandom(16) algorithm = algorithms.ChaCha20(key, nonce) # ChaCha20 是一个流密码，mode 必须为 None cipher = Cipher(algorithm, mode=None) # 1. 加密 encryptor = cipher.encryptor() ct = encryptor.update(plaintext) # 2. 解密 decryptor = cipher.decryptor() decryptor.update(ct) 3. 其他流行的对称加密算法还有一些其他的现代安全对称密码，它们的应用不如 AES 和 ChaCha20 这么广泛，但在程序员和信息安全社区中仍然很流行： Serpent - 安全对称密钥分组密码（密钥大小：128、192 或 256 位），公众所有（Public Domain），完全免费 Twofish - 安全对称密钥分组密码（密钥大小：128、192 或 256 位），公众所有（Public Domain），完全免费 Camellia - 安全对称密钥分组密码（分组大小：128 位；密钥大小：128、192 和 256 位），专利算法，但完全免费 该算法由三菱和日本电信电话（NTT）在 2000 年共同发明 RC5 - 安全对称密","date":"2022-03-06","objectID":"/posts/practical-cryptography-basics-6-symmetric-key-ciphers/:5:1","series":null,"tags":["Cryptography","密码学","对称加密","安全","AES","ChaCha20"],"title":"写给开发人员的实用密码学（六）—— 对称密钥加密算法","uri":"/posts/practical-cryptography-basics-6-symmetric-key-ciphers/#2-salsa20--chacha20"},{"categories":["tech"],"content":" 1. 安全的对称加密算法目前应用最广泛的对称加密算法，是 AES 跟 Salsa20 / ChaCha20 这两个系列。 1. AES (Rijndael) wiki: https://en.wikipedia.org/wiki/Advanced_Encryption_Standard AES（高级加密标准，也称为 Rijndael）是现代 IT 行业中最流行和广泛使用的对称加密算法。AES 被证明是高度安全、快速且标准化的，到目前为止没有发现任何明显的弱点或攻击手段，而且几乎在所有平台上都得到了很好的支持。 AES 是 128 位分组密码，使用 128、192 或 256 位密钥。它通常与分组模式组合成分组加密方案（如 AES-CTR 或 AES-GCM）以处理流数据。 在大多数分组模式中，AES 还需要一个随机的 128 位初始向量 IV。 Rijndael (AES) 算法可免费用于任何用途，而且非常流行。很多站点都选择 AES 作为 TLS 协议的一部分，以实现安全通信。 现代 CPU 硬件基本都在微处理器级别实现了 AES 指令以加速 AES 加密解密操作。 这里有一个纯 Python 的 AES 实现可供参考: AES encryption in pure Python - boppreh 我们在前面的 CTR 分组模式中已经使用 Python 实践了 AES-256-CTR 加密方案。 而实际上更常用的是支持集成身份验证加密（AEAD）的 AES-256-GCM 加密方案，它的优势我们前面已经介绍过了，这里我们使用 Python 演示下如何使用： # pip install cryptography==36.0.1 import os from cryptography.hazmat.primitives.ciphers import ( Cipher, algorithms, modes ) def encrypt(key, plaintext, associated_data): # Generate a random 96-bit IV. iv = os.urandom(12) # Construct an AES-GCM Cipher object with the given key and a # randomly generated IV. encryptor = Cipher( algorithms.AES(key), modes.GCM(iv), ).encryptor() # associated_data will be authenticated but not encrypted, # it must also be passed in on decryption. encryptor.authenticate_additional_data(associated_data) # Encrypt the plaintext and get the associated ciphertext. # GCM does not require padding. ciphertext = encryptor.update(plaintext) + encryptor.finalize() return (iv, ciphertext, encryptor.tag) def decrypt(key, associated_data, iv, ciphertext, tag): # Construct a Cipher object, with the key, iv, and additionally the # GCM tag used for authenticating the message. decryptor = Cipher( algorithms.AES(key), modes.GCM(iv, tag), ).decryptor() # We put associated_data back in or the tag will fail to verify # when we finalize the decryptor. decryptor.authenticate_additional_data(associated_data) # Decryption gets us the authenticated plaintext. # If the tag does not match an InvalidTag exception will be raised. return decryptor.update(ciphertext) + decryptor.finalize() # 接下来进行算法验证 plaintext = b\"this is a paintext, hahahahahaha~\" key = b'\\x96\\xec.\\xc7\\xd5\\x1b/5\\xa1\\x10s\\x9d\\xd5\\x10z\\xdc\\x90\\xb5\\x1cm\"\u003ex\\xfd \\xd5\\xc5\\xaf\\x19\\xd1Z\\xbb' associated_data = b\"authenticated but not encrypted payload\" # 被用于消息认证的关联数据 # 1. 发送方加密消息 iv, ciphertext, tag = encrypt( key, plaintext, associated_data ) # 2. 发送方将 associated_data iv ciphertext tag 打包发送给接收方 # 3. 接收方使用自己的 key 验证并解密数据 descrypt_text = decrypt( key, associated_data, iv, ciphertext, tag ) 2. Salsa20 / ChaCha20 wiki: https://en.wikipedia.org/wiki/Salsa20#ChaCha_variant Salsa20 及其改进的变体 ChaCha（ChaCha8、ChaCha12、ChaCha20）和 XSalsa20 是由密码学家 Daniel Bernstein 设计的现代、快速的对称流密码家族。 Salsa20 密码是对称流密码设计竞赛 eSTREAM（2004-2008）的决赛选手之一，它随后与相关的 BLAKE 哈希函数一起被广泛采用。 Salsa20 及其变体是免版税的，没有专利。 Salsa20 密码将 128 位或 256 位对称密钥 + 随机生成的 64 位随机数（初始向量）和无限长度的数据流作为输入，并生成长度相同的加密数据流作为输出输入流。 ChaCha20-Poly1305Salsa20 应用最为广泛的是认证加密方案：ChaCha20-Poly1305，即组合使用 ChaCha20 与消息认证算法 Poly1305，它们都由密码学家 Bernstein 设计。 ChaCha20-Poly1305 已被证明足够安全，不过跟 GCM 一样它的安全性也依赖于足够随机的初始向量 IV，另外 ChaCha20-Poly1305 也不容易遭受计时攻击。 在没有硬件加速的情况下，ChaCha20 通常比 AES 要快得多（比如在旧的没有硬件加速的移动设备上），这是它最大的优势。 以下是一个 ChaCha20 的 Python 示例： # pip install cryptography==36.0.1 import os from cryptography.hazmat.primitives.ciphers import Cipher, algorithms, modes plaintext = b\"this is a paintext, hahahahahaha~\" key = b'\\x96\\xec.\\xc7\\xd5\\x1b/5\\xa1\\x10s\\x9d\\xd5\\x10z\\xdc\\x90\\xb5\\x1cm\"\u003ex\\xfd \\xd5\\xc5\\xaf\\x19\\xd1Z\\xbb' nonce = os.urandom(16) algorithm = algorithms.ChaCha20(key, nonce) # ChaCha20 是一个流密码，mode 必须为 None cipher = Cipher(algorithm, mode=None) # 1. 加密 encryptor = cipher.encryptor() ct = encryptor.update(plaintext) # 2. 解密 decryptor = cipher.decryptor() decryptor.update(ct) 3. 其他流行的对称加密算法还有一些其他的现代安全对称密码，它们的应用不如 AES 和 ChaCha20 这么广泛，但在程序员和信息安全社区中仍然很流行： Serpent - 安全对称密钥分组密码（密钥大小：128、192 或 256 位），公众所有（Public Domain），完全免费 Twofish - 安全对称密钥分组密码（密钥大小：128、192 或 256 位），公众所有（Public Domain），完全免费 Camellia - 安全对称密钥分组密码（分组大小：128 位；密钥大小：128、192 和 256 位），专利算法，但完全免费 该算法由三菱和日本电信电话（NTT）在 2000 年共同发明 RC5 - 安全对称密","date":"2022-03-06","objectID":"/posts/practical-cryptography-basics-6-symmetric-key-ciphers/:5:1","series":null,"tags":["Cryptography","密码学","对称加密","安全","AES","ChaCha20"],"title":"写给开发人员的实用密码学（六）—— 对称密钥加密算法","uri":"/posts/practical-cryptography-basics-6-symmetric-key-ciphers/#chacha20-poly1305"},{"categories":["tech"],"content":" 1. 安全的对称加密算法目前应用最广泛的对称加密算法，是 AES 跟 Salsa20 / ChaCha20 这两个系列。 1. AES (Rijndael) wiki: https://en.wikipedia.org/wiki/Advanced_Encryption_Standard AES（高级加密标准，也称为 Rijndael）是现代 IT 行业中最流行和广泛使用的对称加密算法。AES 被证明是高度安全、快速且标准化的，到目前为止没有发现任何明显的弱点或攻击手段，而且几乎在所有平台上都得到了很好的支持。 AES 是 128 位分组密码，使用 128、192 或 256 位密钥。它通常与分组模式组合成分组加密方案（如 AES-CTR 或 AES-GCM）以处理流数据。 在大多数分组模式中，AES 还需要一个随机的 128 位初始向量 IV。 Rijndael (AES) 算法可免费用于任何用途，而且非常流行。很多站点都选择 AES 作为 TLS 协议的一部分，以实现安全通信。 现代 CPU 硬件基本都在微处理器级别实现了 AES 指令以加速 AES 加密解密操作。 这里有一个纯 Python 的 AES 实现可供参考: AES encryption in pure Python - boppreh 我们在前面的 CTR 分组模式中已经使用 Python 实践了 AES-256-CTR 加密方案。 而实际上更常用的是支持集成身份验证加密（AEAD）的 AES-256-GCM 加密方案，它的优势我们前面已经介绍过了，这里我们使用 Python 演示下如何使用： # pip install cryptography==36.0.1 import os from cryptography.hazmat.primitives.ciphers import ( Cipher, algorithms, modes ) def encrypt(key, plaintext, associated_data): # Generate a random 96-bit IV. iv = os.urandom(12) # Construct an AES-GCM Cipher object with the given key and a # randomly generated IV. encryptor = Cipher( algorithms.AES(key), modes.GCM(iv), ).encryptor() # associated_data will be authenticated but not encrypted, # it must also be passed in on decryption. encryptor.authenticate_additional_data(associated_data) # Encrypt the plaintext and get the associated ciphertext. # GCM does not require padding. ciphertext = encryptor.update(plaintext) + encryptor.finalize() return (iv, ciphertext, encryptor.tag) def decrypt(key, associated_data, iv, ciphertext, tag): # Construct a Cipher object, with the key, iv, and additionally the # GCM tag used for authenticating the message. decryptor = Cipher( algorithms.AES(key), modes.GCM(iv, tag), ).decryptor() # We put associated_data back in or the tag will fail to verify # when we finalize the decryptor. decryptor.authenticate_additional_data(associated_data) # Decryption gets us the authenticated plaintext. # If the tag does not match an InvalidTag exception will be raised. return decryptor.update(ciphertext) + decryptor.finalize() # 接下来进行算法验证 plaintext = b\"this is a paintext, hahahahahaha~\" key = b'\\x96\\xec.\\xc7\\xd5\\x1b/5\\xa1\\x10s\\x9d\\xd5\\x10z\\xdc\\x90\\xb5\\x1cm\"\u003ex\\xfd \\xd5\\xc5\\xaf\\x19\\xd1Z\\xbb' associated_data = b\"authenticated but not encrypted payload\" # 被用于消息认证的关联数据 # 1. 发送方加密消息 iv, ciphertext, tag = encrypt( key, plaintext, associated_data ) # 2. 发送方将 associated_data iv ciphertext tag 打包发送给接收方 # 3. 接收方使用自己的 key 验证并解密数据 descrypt_text = decrypt( key, associated_data, iv, ciphertext, tag ) 2. Salsa20 / ChaCha20 wiki: https://en.wikipedia.org/wiki/Salsa20#ChaCha_variant Salsa20 及其改进的变体 ChaCha（ChaCha8、ChaCha12、ChaCha20）和 XSalsa20 是由密码学家 Daniel Bernstein 设计的现代、快速的对称流密码家族。 Salsa20 密码是对称流密码设计竞赛 eSTREAM（2004-2008）的决赛选手之一，它随后与相关的 BLAKE 哈希函数一起被广泛采用。 Salsa20 及其变体是免版税的，没有专利。 Salsa20 密码将 128 位或 256 位对称密钥 + 随机生成的 64 位随机数（初始向量）和无限长度的数据流作为输入，并生成长度相同的加密数据流作为输出输入流。 ChaCha20-Poly1305Salsa20 应用最为广泛的是认证加密方案：ChaCha20-Poly1305，即组合使用 ChaCha20 与消息认证算法 Poly1305，它们都由密码学家 Bernstein 设计。 ChaCha20-Poly1305 已被证明足够安全，不过跟 GCM 一样它的安全性也依赖于足够随机的初始向量 IV，另外 ChaCha20-Poly1305 也不容易遭受计时攻击。 在没有硬件加速的情况下，ChaCha20 通常比 AES 要快得多（比如在旧的没有硬件加速的移动设备上），这是它最大的优势。 以下是一个 ChaCha20 的 Python 示例： # pip install cryptography==36.0.1 import os from cryptography.hazmat.primitives.ciphers import Cipher, algorithms, modes plaintext = b\"this is a paintext, hahahahahaha~\" key = b'\\x96\\xec.\\xc7\\xd5\\x1b/5\\xa1\\x10s\\x9d\\xd5\\x10z\\xdc\\x90\\xb5\\x1cm\"\u003ex\\xfd \\xd5\\xc5\\xaf\\x19\\xd1Z\\xbb' nonce = os.urandom(16) algorithm = algorithms.ChaCha20(key, nonce) # ChaCha20 是一个流密码，mode 必须为 None cipher = Cipher(algorithm, mode=None) # 1. 加密 encryptor = cipher.encryptor() ct = encryptor.update(plaintext) # 2. 解密 decryptor = cipher.decryptor() decryptor.update(ct) 3. 其他流行的对称加密算法还有一些其他的现代安全对称密码，它们的应用不如 AES 和 ChaCha20 这么广泛，但在程序员和信息安全社区中仍然很流行： Serpent - 安全对称密钥分组密码（密钥大小：128、192 或 256 位），公众所有（Public Domain），完全免费 Twofish - 安全对称密钥分组密码（密钥大小：128、192 或 256 位），公众所有（Public Domain），完全免费 Camellia - 安全对称密钥分组密码（分组大小：128 位；密钥大小：128、192 和 256 位），专利算法，但完全免费 该算法由三菱和日本电信电话（NTT）在 2000 年共同发明 RC5 - 安全对称密","date":"2022-03-06","objectID":"/posts/practical-cryptography-basics-6-symmetric-key-ciphers/:5:1","series":null,"tags":["Cryptography","密码学","对称加密","安全","AES","ChaCha20"],"title":"写给开发人员的实用密码学（六）—— 对称密钥加密算法","uri":"/posts/practical-cryptography-basics-6-symmetric-key-ciphers/#3-其他流行的对称加密算法"},{"categories":["tech"],"content":" 2. 不安全的对称加密算法如下这些对称加密算法曾经很流行，但现在被认为是不安全的或有争议的安全性，不建议再使用： DES - 56 位密钥大小，可以被暴力破解 3DES（三重 DES, TDES）- 64 位密码，被认为不安全，已在 2017 年被 NIST 弃用. RC2 - 64 位密码，被认为不安全 RC4 - 流密码，已被破解，网上存在大量它的破解资料 Blowfish - 旧的 64 位密码，已被破坏 Sweet32: Birthday attacks on 64-bit block ciphers in TLS and OpenVPN GOST - 俄罗斯 64 位分组密码，有争议的安全性，被认为有风险 ","date":"2022-03-06","objectID":"/posts/practical-cryptography-basics-6-symmetric-key-ciphers/:5:2","series":null,"tags":["Cryptography","密码学","对称加密","安全","AES","ChaCha20"],"title":"写给开发人员的实用密码学（六）—— 对称密钥加密算法","uri":"/posts/practical-cryptography-basics-6-symmetric-key-ciphers/#2-不安全的对称加密算法"},{"categories":["tech"],"content":" 对称认证加密算法 AE / AEAD我们在前面第三篇文章「MAC 与密钥派生函数 KDF」中介绍过 AE 认证加密及其变体 AEAD. 一些对称加密方案提供集成身份验证加密（AEAD），比如使用了 GCM 分组模式的加密方案 AES-GCM，而其他加密方案（如 AES-CBC 和 AES-CTR）自身不提供身份验证能力，需要额外添加。 最流行的认证加密（AEAD）方案有如下几个，我们在之前已经简单介绍过它们： ChaCha20-Poly1305 具有集成 Poly1305 身份验证器的 ChaCha20 流密码（集成身份验证 AEAD 加密） 使用 256 位密钥和 96 位随机数（初始向量） 极高的性能 在硬件不支持 AES 加速指令时（如路由器、旧手机等硬件上），推荐使用此算法 AES-256-GCM 我们在前面的 GCM 模式一节，使用 Python 实现并验证了这个 AES-256-GCM 加密方案 使用 256 位密钥和 128 位随机数（初始向量） 较高的性能 在硬件支持 AES 加速时（如桌面、服务器等场景），更推荐使用此算法 AES-128-GCM 跟 AES-256-GCM 一样，区别在于它使用 128 位密钥，安全性弱于 ChaCha20-Poly1305 与 AES-256-GCM. 目前被广泛应用在 HTTPS 等多种加密场景下，但是正在慢慢被前面两种方案取代 今天的大多数应用程序应该优先选用上面这些加密方案进行对称加密，而不是自己造轮子。 上述方案是高度安全的、经过验证的、经过良好测试的，并且大多数加密库都已经提供了高效的实现，可以说是开箱即用。 目前应用最广泛的对称加密方案应该是 AES-128-GCM， 而 ChaCha20-Poly1305 因为其极高的性能，也越来越多地被应用在 TLS1.2、TLS1.3、QUIC/HTTP3、Wireguard、SSH 等协议中。 ","date":"2022-03-06","objectID":"/posts/practical-cryptography-basics-6-symmetric-key-ciphers/:5:3","series":null,"tags":["Cryptography","密码学","对称加密","安全","AES","ChaCha20"],"title":"写给开发人员的实用密码学（六）—— 对称密钥加密算法","uri":"/posts/practical-cryptography-basics-6-symmetric-key-ciphers/#对称认证加密算法-ae--aead"},{"categories":["tech"],"content":" 五、AES 算法案例：以太坊钱包加密在这一小节我们研究一个现实中的 AES 应用场景：以太坊区块链的标准加密钱包文件格式。 我们将看到 AES-128-CTR 密码方案如何与 Scrypt 和 MAC 相结合，通过字符密码安全地实现经过身份验证的对称密钥加密。 以太坊 UTC / JSON 钱包在比特币和以太坊等区块链网络中，区块链资产持有者的私钥存储在称为加密钱包的特殊密钥库中。 通常，这些加密钱包是本地硬盘上的文件，并使用字符密码加密。 在以太坊区块链中，加密钱包以一种特殊的加密格式在内部存储，称为「UTC / JSON 钱包（密钥库文件）」或「Web3 秘密存储定义」。 这是一种加密钱包的文件格式，被广泛应用在 geth 和 Parity（以太坊的主要协议实现）、MyEtherWallet（流行的在线客户端以太坊钱包）、MetaMask（广泛使用的浏览器内以太坊钱包）、ethers.js 和 Nethereum 库以及许多其他与以太坊相关的技术和工具中。 以太坊 UTC/JSON 密钥库将加密的私钥、加密数据、加密算法及其参数保存为 JSON 文本文档。 UTC / JSON 钱包的一个示例如下： { \"version\": 3, \"id\": \"07a9f767-93c5-4842-9afd-b3b083659f04\", \"address\": \"aef8cad64d29fcc4ed07629b9e896ebc3160a8d0\", \"Crypto\": { \"ciphertext\": \"99d0e66c67941a08690e48222a58843ef2481e110969325db7ff5284cd3d3093\", \"cipherparams\": { \"iv\": \"7d7fabf8dee2e77f0d7e3ff3b965fc23\" }, \"cipher\": \"aes-128-ctr\", \"kdf\": \"scrypt\", \"kdfparams\": { \"dklen\": 32, \"salt\": \"85ad073989d461c72358ccaea3551f7ecb8e672503cb05c2ee80cfb6b922f4d4\", \"n\": 8192, \"r\": 8, \"p\": 1 }, \"mac\": \"06dcf1cc4bffe1616fafe94a2a7087fd79df444756bb17c93af588c3ab02a913\" } } 上述 json 内容也是认证对称加密的一个典型示例，可以很容易分析出它的一些组成成分： kdf: 用于从字符密码派生出密钥的 KDF 算法名称，这里用的是 scrypt kdfparams: KDF 算法的参数，如迭代参数、盐等… ciphertext: 钱包内容的密文，通常这就是一个被加密的 256 位私钥 cipher + cipherparams: 对称加密算法的名称及参数，这里使用了 AES-128-CTR，并给出了初始向量 IV mac: 由 MAC 算法生成的消息认证码，被用于验证解密密码的正确性 以太坊使用截取派生密钥的一部分，拼接上完整密文，然后进行 keccak-256 哈希运算得到 MAC 值 其他钱包相关的信息 默认情况下，密钥派生函数是 scrypt 并使用的是弱 scrypt 参数（n=8192 成本因子，r=8 块大小，p=1 并行化），因此建议使用长而复杂的密码以避免钱包被暴力解密。 ","date":"2022-03-06","objectID":"/posts/practical-cryptography-basics-6-symmetric-key-ciphers/:6:0","series":null,"tags":["Cryptography","密码学","对称加密","安全","AES","ChaCha20"],"title":"写给开发人员的实用密码学（六）—— 对称密钥加密算法","uri":"/posts/practical-cryptography-basics-6-symmetric-key-ciphers/#五aes-算法案例以太坊钱包加密"},{"categories":["tech"],"content":" 五、AES 算法案例：以太坊钱包加密在这一小节我们研究一个现实中的 AES 应用场景：以太坊区块链的标准加密钱包文件格式。 我们将看到 AES-128-CTR 密码方案如何与 Scrypt 和 MAC 相结合，通过字符密码安全地实现经过身份验证的对称密钥加密。 以太坊 UTC / JSON 钱包在比特币和以太坊等区块链网络中，区块链资产持有者的私钥存储在称为加密钱包的特殊密钥库中。 通常，这些加密钱包是本地硬盘上的文件，并使用字符密码加密。 在以太坊区块链中，加密钱包以一种特殊的加密格式在内部存储，称为「UTC / JSON 钱包（密钥库文件）」或「Web3 秘密存储定义」。 这是一种加密钱包的文件格式，被广泛应用在 geth 和 Parity（以太坊的主要协议实现）、MyEtherWallet（流行的在线客户端以太坊钱包）、MetaMask（广泛使用的浏览器内以太坊钱包）、ethers.js 和 Nethereum 库以及许多其他与以太坊相关的技术和工具中。 以太坊 UTC/JSON 密钥库将加密的私钥、加密数据、加密算法及其参数保存为 JSON 文本文档。 UTC / JSON 钱包的一个示例如下： { \"version\": 3, \"id\": \"07a9f767-93c5-4842-9afd-b3b083659f04\", \"address\": \"aef8cad64d29fcc4ed07629b9e896ebc3160a8d0\", \"Crypto\": { \"ciphertext\": \"99d0e66c67941a08690e48222a58843ef2481e110969325db7ff5284cd3d3093\", \"cipherparams\": { \"iv\": \"7d7fabf8dee2e77f0d7e3ff3b965fc23\" }, \"cipher\": \"aes-128-ctr\", \"kdf\": \"scrypt\", \"kdfparams\": { \"dklen\": 32, \"salt\": \"85ad073989d461c72358ccaea3551f7ecb8e672503cb05c2ee80cfb6b922f4d4\", \"n\": 8192, \"r\": 8, \"p\": 1 }, \"mac\": \"06dcf1cc4bffe1616fafe94a2a7087fd79df444756bb17c93af588c3ab02a913\" } } 上述 json 内容也是认证对称加密的一个典型示例，可以很容易分析出它的一些组成成分： kdf: 用于从字符密码派生出密钥的 KDF 算法名称，这里用的是 scrypt kdfparams: KDF 算法的参数，如迭代参数、盐等… ciphertext: 钱包内容的密文，通常这就是一个被加密的 256 位私钥 cipher + cipherparams: 对称加密算法的名称及参数，这里使用了 AES-128-CTR，并给出了初始向量 IV mac: 由 MAC 算法生成的消息认证码，被用于验证解密密码的正确性 以太坊使用截取派生密钥的一部分，拼接上完整密文，然后进行 keccak-256 哈希运算得到 MAC 值 其他钱包相关的信息 默认情况下，密钥派生函数是 scrypt 并使用的是弱 scrypt 参数（n=8192 成本因子，r=8 块大小，p=1 并行化），因此建议使用长而复杂的密码以避免钱包被暴力解密。 ","date":"2022-03-06","objectID":"/posts/practical-cryptography-basics-6-symmetric-key-ciphers/:6:0","series":null,"tags":["Cryptography","密码学","对称加密","安全","AES","ChaCha20"],"title":"写给开发人员的实用密码学（六）—— 对称密钥加密算法","uri":"/posts/practical-cryptography-basics-6-symmetric-key-ciphers/#以太坊-utc--json-钱包"},{"categories":["tech"],"content":" 参考 Practical-Cryptography-for-Developers-Book A complete overview of SSL/TLS and its cryptographic system AES encryption in pure Python - boppreh Block_cipher_mode_of_operation_wiki Galois/Counter_Mode_wiki ","date":"2022-03-06","objectID":"/posts/practical-cryptography-basics-6-symmetric-key-ciphers/:7:0","series":null,"tags":["Cryptography","密码学","对称加密","安全","AES","ChaCha20"],"title":"写给开发人员的实用密码学（六）—— 对称密钥加密算法","uri":"/posts/practical-cryptography-basics-6-symmetric-key-ciphers/#参考"},{"categories":["life"],"content":" 原文：哪一刻你发现年轻人正在悄悄改变社会？ - 赦己 我的读后感：他的眼里有光！ 之前见过一个特别厉害的面试者，让我觉得，老一辈真的是老一辈了，他放弃了一个月薪一万三，十三薪的工作，他的演讲让我记忆非常深刻，也使得面试官面面相觑。 这个岗位算是万人过独木桥，不仅海内的很多大学生在竞争，海外的很多大学生也在努力，真个过程是这样的：简历筛选-线上一面-线上hr二面-线下主管面试-总裁轮面试（压力轮）。 我们都到了最后的一轮面试,本来就是压力轮面试，但是那天不知道为什么总裁的脾气很暴躁，对他冷嘲热讽，说了一些比较难听的话，大概的意思就是“你还小，以后需要认真学，你们太嫩了”，其实总裁的意思非常明确了，会招他，但是他太嫩需要学很多东西，但是就是他这样大人看小屁孩的感觉惹怒了他，后面他的演讲就是十分高能了，我尽量原文复述。 「你坐在我前面会不会有点点害怕呢？你看看你身边有什么人可以给你参考吗？你没有，你只能战战兢兢如履薄冰，走错一步都是深渊。你知道你在我眼里是什么吗？你只是一个猎物，一个我追逐的、猎杀的的目标，其实你哪里来的自信呢？就凭你是这个公司的总裁吗？来自职级和制度的压力我一概不屑，反而觉得是黔驴技穷，小人做法，我不会服气，只是照做而已。 其实我也很享受被统治的感觉，上一个能统治我的人已经很久了，你知道那种纯粹的实力压服吗？我可以毫无保留地顺从他的任何意见，我从来不怀疑他的任何决定，哪怕行动后面失败了我也觉得他是对的。但是你呢？只是来自制度的威力，你的每一个决定都会遭到我的质疑。 我最讨厌的就是别人和我说，我想让你去做点什么但是你能力还不够，简直瞎扯淡，其实是你能力不够，作为一个管理者，你甚至不知道怎么用我，我如何为你卖命啊？ 我渴望的是在一个稳定的环境默默耕耘，把坏的变成好的，但是前提是我们够团队，你呢？凭你作为一个过来人的经验吗？这些东西经过时间大家都会有的，你还有其他的吗？你真的有能力把我变成你的三头六臂吗？你真的控制得住我吗？」 复述其实没那么精彩了，他支着手目光瞪着总裁的眼睛的时候超级精彩，后面他去了一个对手小公司，相当于这边的市值来了，相差了十倍之多，但是七个月之后再见面已是兵刃交接，他成了六个人团队的小主管，耀武扬威地围着我们总部办公地盘下了一圈广告。 ","date":"2022-03-04","objectID":"/posts/the-thoughtful-youth/:0:0","series":null,"tags":[],"title":"「转」且看有思想的年轻人","uri":"/posts/the-thoughtful-youth/#"},{"categories":["tech"],"content":" 本文主要翻译自 Practical-Cryptography-for-Developers-Book，笔者额外补充了 DHKE/ECDH 的代码示例，以及「PFS 完美前向保密协议 DHE/ECDHE」一节。 《写给开发人员的实用密码学》系列文章目录： 写给开发人员的实用密码学（一）—— 概览 写给开发人员的实用密码学（二）—— 哈希函数 写给开发人员的实用密码学（三）—— MAC 与密钥派生函数 KDF 写给开发人员的实用密码学（四）—— 安全随机数生成器 CSPRNG 写给开发人员的实用密码学（五）—— 密钥交换 DHKE 与完美前向保密 PFS 写给开发人员的实用密码学（六）—— 对称密钥加密算法 写给开发人员的实用密码学（七）—— 非对称密钥加密算法 RSA/ECC 写给开发人员的实用密码学（八）—— 数字证书与 TLS 协议 待续 ","date":"2022-03-01","objectID":"/posts/practical-cryptography-basics-5-key-exchange/:0:0","series":null,"tags":["Cryptography","密码学","密钥交换","安全","DH","DHE","ECDH","ECDHE"],"title":"写给开发人员的实用密码学（五）—— 密钥交换 DHKE 与完美前向保密 PFS","uri":"/posts/practical-cryptography-basics-5-key-exchange/#"},{"categories":["tech"],"content":" 一、前言在密码学中密钥交换是一种协议，功能是在两方之间安全地交换加密密钥，其他任何人都无法获得密钥的副本。通常各种加密通讯协议的第一步都是密钥交换。 密钥交换技术具体来说有两种方案： 密钥协商：协议中的双方都参与了共享密钥的生成，两个代表算法是 Diffie-Hellman (DHKE) 和 Elliptic-Curve Diffie-Hellman (ECDH) 密钥传输：双方中其中一方生成出共享密钥，并通过此方案将共享密钥传输给另一方。密钥传输方案通常都通过公钥密码系统实现。比如在 RSA 密钥交换中，客户端使用它的私钥加密一个随机生成的会话密钥，然后将密文发送给服务端，服务端再使用它的公钥解密出会话密钥。 密钥交换协议无时无刻不在数字世界中运行，在你连接 WiFi 时，或者使用 HTTPS 协议访问一个网站，都会执行密钥交换协议。 密钥交换可以基于匿名的密钥协商协议如 DHKE，一个密码或预共享密钥，一个数字证书等等。有些通讯协议只在开始时交换一次密钥，而有些协议则会随着时间的推移不断地交换密钥。 认证密钥交换（AKE）是一种会同时认证相关方身份的密钥交换协议，比如个人 WiFi 通常就会使用 password-authenticated key agreement (PAKE)，而如果你连接的是公开 WiFi，则会使用匿名密钥交换协议。 目前有许多用于密钥交换的密码算法。其中一些使用公钥密码系统，而另一些则使用更简单的密钥交换方案（如 Diffie-Hellman 密钥交换）；其中有些算法涉及服务器身份验证，也有些涉及客户端身份验证；其中部分算法使用密码，另一部分使用数字证书或其他身份验证机制。下面列举一些知名的密钥交换算法： Diffie-Hellman Key Exchange (DHКЕ) ：传统的、应用最为广泛的密钥交换协议 椭圆曲线 Diffie-Hellman (ECDH) RSA-OAEP 和 RSA-KEM（RSA 密钥传输） PSK（预共享密钥） SRP（安全远程密码协议） FHMQV（Fully Hashed Menezes-Qu-Vanstone） ECMQV（Ellictic-Curve Menezes-Qu-Vanstone） CECPQ1（量子安全密钥协议） ","date":"2022-03-01","objectID":"/posts/practical-cryptography-basics-5-key-exchange/:1:0","series":null,"tags":["Cryptography","密码学","密钥交换","安全","DH","DHE","ECDH","ECDHE"],"title":"写给开发人员的实用密码学（五）—— 密钥交换 DHKE 与完美前向保密 PFS","uri":"/posts/practical-cryptography-basics-5-key-exchange/#一前言"},{"categories":["tech"],"content":" 二、Diffie–Hellman 密钥交换迪菲-赫尔曼密钥交换（Diffie–Hellman Key Exchange）是一种安全协议，它可以让双方在完全没有对方任何预先信息的条件下通过不安全信道安全地协商出一个安全密钥，而且任何窃听者都无法得知密钥信息。 这个密钥可以在后续的通讯中作为对称密钥来加密通讯内容。 DHKE 可以防范嗅探攻击（窃听），但是无法抵挡中间人攻击（中继）。 DHKE 有两种实现方案： 传统的 DHKE 算法：使用离散对数实现 基于椭圆曲线密码学的 ECDH 为了理解 DHKE 如何实现在「大庭广众之下」安全地协商出密钥，我们首先使用色彩混合来形象地解释下它大致的思路。 跟编程语言的 Hello World 一样，密钥交换的解释通常会使用 Alice 跟 Bob 来作为通信双方。 现在他俩想要在公开的信道上，协商出一个秘密色彩出来，但是不希望其他任何人知道这个秘密色彩。他们可以这样做： 分步解释如下： 首先 Alice 跟 Bob 沟通，确定一个初始的色彩，比如黄色。这个沟通不需要保密。 然后，Alice 跟 Bob 分别偷偷地选择出一个自己的秘密色彩，这个就得保密啦。 现在 Alice 跟 Bob，分别将初始色彩跟自己选择的秘密色彩混合，分别得到两个混合色彩。 之后，Alice 跟 Bob 再回到公开信道上，交换双方的混合色彩。 我们假设在仅知道初始色彩跟混合色彩的情况下，很难推导出被混合的秘密色彩。这样第三方就猜不出 Bob 跟 Alice 分别选择了什么秘密色彩了。 最后 Alice 跟 Bob 再分别将自己的秘密色彩，跟对方的混合色彩混合，就得到了最终的秘密色彩。这个最终色彩只有 Alice 跟 Bob 知道，信道上的任何人都无法猜出来。 DHKE 协议也是基于类似的原理，但是使用的是离散对数（discrete logarithms）跟模幂（modular exponentiations）而不是色彩混合。 ","date":"2022-03-01","objectID":"/posts/practical-cryptography-basics-5-key-exchange/:2:0","series":null,"tags":["Cryptography","密码学","密钥交换","安全","DH","DHE","ECDH","ECDHE"],"title":"写给开发人员的实用密码学（五）—— 密钥交换 DHKE 与完美前向保密 PFS","uri":"/posts/practical-cryptography-basics-5-key-exchange/#二diffiehellman-密钥交换"},{"categories":["tech"],"content":" 三、经典 DHKE 协议","date":"2022-03-01","objectID":"/posts/practical-cryptography-basics-5-key-exchange/:3:0","series":null,"tags":["Cryptography","密码学","密钥交换","安全","DH","DHE","ECDH","ECDHE"],"title":"写给开发人员的实用密码学（五）—— 密钥交换 DHKE 与完美前向保密 PFS","uri":"/posts/practical-cryptography-basics-5-key-exchange/#三经典-dhke-协议"},{"categories":["tech"],"content":" 基础数学知识首先介绍下「模幂（modular exponentiations）」，它是指求 $g$ 的 $a$ 次幂模 $p$ 的值 $c$ 的过程，其中 $g$ $a$ $c$ 均为整数，公式如下： $$ g^a \\mod p = c $$ 而「离散对数（discrete logarithms）」，其实就是指模幂的逆运算，它使用如下公式表示： $$ Ind_{g}c \\equiv a {\\pmod {p}} $$ 上述公式，即指在已知整数 $g$，质数 $p$，以及余数（p 的一个原根） $c$ 的情况下，求使前面的模幂等式成立的幂指数 $a$。 已知使用计算机计算上述「模幂」是非常快速的，但是在质数 $p$ 非常大的情况下，求「离散对数」却是非常难的，这就是「离散对数难题」。 然后为了理解 HDKE 的原理，我们还需要了解下模幂运算的一个性质： $$ g^{ab} \\mod p = {g^a \\mod p}^b \\mod p $$ 懂了上面这些基础数学知识，下面就开始介绍 HDKE 算法。 ","date":"2022-03-01","objectID":"/posts/practical-cryptography-basics-5-key-exchange/:3:1","series":null,"tags":["Cryptography","密码学","密钥交换","安全","DH","DHE","ECDH","ECDHE"],"title":"写给开发人员的实用密码学（五）—— 密钥交换 DHKE 与完美前向保密 PFS","uri":"/posts/practical-cryptography-basics-5-key-exchange/#基础数学知识"},{"categories":["tech"],"content":" DHKE 密钥交换流程下面该轮到 Alice 跟 Bob 出场来介绍 DHKE 的过程了，先看图（下面绿色表示非秘密信息，红色表示秘密信息）： Alice 跟 Bob 协定使用两个比较独特的正整数 $p$ 跟 $g$ 假设 $p=23$, $g=5$ Alice 选择一个秘密整数 $a$，计算 $A$$\\ = g^a \\mod p$ 并发送给 Bob 假设 $a=4$，则 $A$$\\ = 5^4 \\mod 23 = 4$ Bob 也选择一个秘密整数 $b$，计算 $B$$\\ = g^b \\mod p$ 并发送给 Alice 假设 $b=3$，则 $B$$\\ = 5^3 \\mod 23 = 10$ Alice 计算 $S_1 = B^a \\mod p$ $S_1 = 10^4 \\mod 23 = 18$ Bob 计算 $S_2 = A^b \\mod p$ $S_2 = 4^3 \\mod 23 = 18$ 已知 $B^a \\mod p = g^{ab} \\mod p = A^b \\mod p$，因此 $S_1 = S_2 = S$ 这样 Alice 跟 Bob 就协商出了密钥 $S$ 因为离散对数的计算非常难，任何窃听者都几乎不可能通过公开的 $p$ $g$ $A$ $B$ 逆推出 $S$ 的值 在最常见的 DHKE 实现中（RFC3526），基数是 $g = 2$，模数 $p$ 是一个 1536 到 8192 比特的大素数。 而整数 $A$ $B$ 通常会使用非常大的数字（1024、2048 或 4096 比特甚至更大）以防范暴力破解。 DHKE 协议基于 Diffie-Hellman 问题的实际难度，这是计算机科学中众所周知的离散对数问题（DLP）的变体，目前还不存在有效的算法。 使用 Python 演示下大概是这样： # pip install cryptography==36.0.1 from cryptography.hazmat.primitives import hashes from cryptography.hazmat.primitives.asymmetric import dh # 1. 双方协商使用两个独特的正整数 g 与 p ## generator =\u003e 即基数 g，通常使用 2, 有时也使用 5 ## key_size =\u003e 模数 p 的长度，通常使用 2048-3096 位（2048 位的安全性正在减弱） params = dh.generate_parameters(generator=2, key_size=2048) param_numbers = params.parameter_numbers() g = param_numbers.g # =\u003e 肯定是 2 p = param_numbers.p # =\u003e 一个 2048 位的整数 print(f\"{g=}, {p=}\") # 2. Alice 生成自己的秘密整数 a 与公开整数 A alice_priv_key = params.generate_private_key() a = alice_priv_key.private_numbers().x A = alice_priv_key.private_numbers().public_numbers.y print(f\"{a=}\") print(f\"{A=}\") # 3. Bob 生成自己的秘密整数 b 与公开整数 B bob_priv_key = params.generate_private_key() b = bob_priv_key.private_numbers().x B = bob_priv_key.private_numbers().public_numbers.y print(f\"{b=}\") print(f\"{B=}\") # 4. Alice 与 Bob 公开交换整数 A 跟 B（即各自的公钥） # 5. Alice 使用 a B 与 p 计算出共享密钥 ## 首先使用 B p g 构造出 bob 的公钥对象（实际上 g 不参与计算） bob_pub_numbers = dh.DHPublicNumbers(B, param_numbers) bob_pub_key = bob_pub_numbers.public_key() ## 计算共享密钥 alice_shared_key = alice_priv_key.exchange(bob_pub_key) # 6. Bob 使用 b A 与 p 计算出共享密钥 ## 首先使用 A p g 构造出 alice 的公钥对象（实际上 g 不参与计算） alice_pub_numbers = dh.DHPublicNumbers(A, param_numbers) alice_pub_key = alice_pub_numbers.public_key() ## 计算共享密钥 bob_shared_key = bob_priv_key.exchange(alice_pub_key) # 两者应该完全相等， Alice 与 Bob 完成第一次密钥交换 alice_shared_key == bob_shared_key # 7. Alice 与 Bob 使用 shared_key 进行对称加密通讯 ","date":"2022-03-01","objectID":"/posts/practical-cryptography-basics-5-key-exchange/:3:2","series":null,"tags":["Cryptography","密码学","密钥交换","安全","DH","DHE","ECDH","ECDHE"],"title":"写给开发人员的实用密码学（五）—— 密钥交换 DHKE 与完美前向保密 PFS","uri":"/posts/practical-cryptography-basics-5-key-exchange/#dhke-密钥交换流程"},{"categories":["tech"],"content":" 四、新一代 ECDH 协议Elliptic-Curve Diffie-Hellman (ECDH) 是一种匿名密钥协商协议，它允许两方，每方都有一个椭圆曲线公钥-私钥对，它的功能也是让双方在完全没有对方任何预先信息的条件下通过不安全信道安全地协商出一个安全密钥。 ECDH 是经典 DHKE 协议的变体，其中模幂计算被椭圆曲线的乘法计算取代，以提高安全性。 ECDH 跟前面介绍的 DHKE 非常相似，只要你理解了椭圆曲线的数学原理，结合前面已经介绍了的 DHKE，基本上可以秒懂。 我会在后面「非对称算法」一文中简单介绍椭圆曲线的数学原理，不过这里也可以先提一下 ECDH 依赖的公式（其中 $a, b$ 为常数，$G$ 为椭圆曲线上的某一点的坐标 $(x, y)$）： $$ (a * G) * b = (b * G) * a $$ 这个公式还是挺直观的吧，感觉小学生也能理解个大概。 下面简单介绍下 ECDH 的流程： Alice 跟 Bob 协商好椭圆曲线的各项参数，以及基点 G，这些参数都是公开的。 Alice 生成一个随机的 ECC 密钥对（公钥：$alicePrivate * G$, 私钥: $alicePrivate$） Bob 生成一个随机的 ECC 密钥对（公钥：$bobPrivate * G$, 私钥: $bobPrivate$） 两人通过不安全的信道交换公钥 Alice 将 Bob 的公钥乘上自己的私钥，得到共享密钥 $sharedKey = (bobPrivate * G) * alicePrivate$ Bob 将 Alice 的公钥乘上自己的私钥，得到共享密钥 $sharedKey = (alicePrivate * G) * bobPrivate$ 因为前面提到的公式，Alice 与 Bob 计算出的共享密钥应该是相等的 这样两方就通过 ECDH 完成了密钥交换。 而 ECDH 的安全性，则由 ECDLP 问题提供保证。 这个问题是说，「通过公开的 $kG$ 以及 $G$ 这两个参数，目前没有有效的手段能快速求解出 $k$ 的值。」 从上面的流程中能看到，公钥就是 ECDLP 中的 $kG$，另外 $G$ 也是公开的，而私钥就是 ECDLP 中的 $k$。 因为 ECDLP 问题的存在，攻击者破解不出 Alice 跟 Bob 的私钥。 代码示例： # pip install tinyec # ECC 曲线库 from tinyec import registry import secrets def compress(pubKey): return hex(pubKey.x) + hex(pubKey.y % 2)[2:] curve = registry.get_curve('brainpoolP256r1') alicePrivKey = secrets.randbelow(curve.field.n) alicePubKey = alicePrivKey * curve.g print(\"Alice public key:\", compress(alicePubKey)) bobPrivKey = secrets.randbelow(curve.field.n) bobPubKey = bobPrivKey * curve.g print(\"Bob public key:\", compress(bobPubKey)) print(\"Now exchange the public keys (e.g. through Internet)\") aliceSharedKey = alicePrivKey * bobPubKey print(\"Alice shared key:\", compress(aliceSharedKey)) bobSharedKey = bobPrivKey * alicePubKey print(\"Bob shared key:\", compress(bobSharedKey)) print(\"Equal shared keys:\", aliceSharedKey == bobSharedKey) ","date":"2022-03-01","objectID":"/posts/practical-cryptography-basics-5-key-exchange/:4:0","series":null,"tags":["Cryptography","密码学","密钥交换","安全","DH","DHE","ECDH","ECDHE"],"title":"写给开发人员的实用密码学（五）—— 密钥交换 DHKE 与完美前向保密 PFS","uri":"/posts/practical-cryptography-basics-5-key-exchange/#四新一代-ecdh-协议"},{"categories":["tech"],"content":" 五、PFS 完美前向保密协议 DHE/ECDHE前面介绍的经典 DHKE 与 ECDH 协议流程，都是在最开始时交换一次密钥，之后就一直使用该密钥通讯。 因此如果密钥被破解，整个会话的所有信息对攻击者而言就完全透明了。 为了进一步提高安全性，密码学家提出了「完全前向保密（Perfect Forward Secrecy，PFS）」的概念，并在 DHKE 与 ECDH 的基础上提出了支持 PFS 的 DHE/ECDHE 协议（末尾的 E 是 ephemeral 的缩写，即指所有的共享密钥都是临时的）。 完全前向保密是指长期使用的主密钥泄漏不会导致过去的会话密钥泄漏，从而保护过去进行的通讯不受密码或密钥在未来暴露的威胁。 下面使用 Python 演示下 DHE 协议的流程（ECDHE 的流程也完全类似）： # pip install cryptography==36.0.1 from cryptography.hazmat.primitives import hashes from cryptography.hazmat.primitives.asymmetric import dh # 1. 双方协商使用两个独特的正整数 g 与 p ## generator =\u003e 即基数 g，通常使用 2, 有时也使用 5 ## key_size =\u003e 模数 p 的长度，通常使用 2048-3096 位（2048 位的安全性正在减弱） params = dh.generate_parameters(generator=2, key_size=2048) param_numbers = params.parameter_numbers() g = param_numbers.g # =\u003e 肯定是 2 p = param_numbers.p # =\u003e 一个 2048 位的整数 print(f\"{g=}, {p=}\") # 2. Alice 生成自己的秘密整数 a 与公开整数 A alice_priv_key = params.generate_private_key() a = alice_priv_key.private_numbers().x A = alice_priv_key.private_numbers().public_numbers.y print(f\"{a=}\") print(f\"{A=}\") # 3. Bob 生成自己的秘密整数 b 与公开整数 B bob_priv_key = params.generate_private_key() b = bob_priv_key.private_numbers().x B = bob_priv_key.private_numbers().public_numbers.y print(f\"{b=}\") print(f\"{B=}\") # 4. Alice 与 Bob 公开交换整数 A 跟 B（即各自的公钥） # 5. Alice 使用 a B 与 p 计算出共享密钥 ## 首先使用 B p g 构造出 bob 的公钥对象（实际上 g 不参与计算） bob_pub_numbers = dh.DHPublicNumbers(B, param_numbers) bob_pub_key = bob_pub_numbers.public_key() ## 计算共享密钥 alice_shared_key = alice_priv_key.exchange(bob_pub_key) # 6. Bob 使用 b A 与 p 计算出共享密钥 ## 首先使用 A p g 构造出 alice 的公钥对象（实际上 g 不参与计算） alice_pub_numbers = dh.DHPublicNumbers(A, param_numbers) alice_pub_key = alice_pub_numbers.public_key() ## 计算共享密钥 bob_shared_key = bob_priv_key.exchange(alice_pub_key) # 上面的流程跟经典 DHKE 完全一致，代码也是从前面 Copy 下来的 # 但是从这里开始，进入 DHE 协议补充的部分 shared_key_1 = bob_shared_key # 第一个共享密钥 # 7. 假设 Bob 现在要发送消息 M_b_1 给 Alice ## 首先 Bob 使用对称加密算法加密消息 M_b M_b_1 = \"Hello Alice, I'm bob~\" C_b_1 = Encrypt(M_b_1, shared_key_1) # Encrypt 是某种对称加密方案的加密算法，如 AES-256-CTR-HMAC-SHA-256 ## 然后 Bob 需要生成一个新的公私钥 b_2 与 B_2（注意 g 与 p 两个参数是不变的） bob_priv_key_2 = parameters.generate_private_key() b_2 = bob_priv_key.private_numbers().x B_2 = bob_priv_key.private_numbers().public_numbers.y print(f\"{b_2=}\") print(f\"{B_2=}\") # 8. Bob 将 C_b_1 与 B_2 一起发送给 Alice # 9. Alice 首先解密数据 C_b_1 得到原始消息 M_b_1 assert M_b_1 == Decrypt(C_b_1, shared_key_1) # Dncrypt 是某种对称加密方案的解密算法，如 AES-256-CTR-HMAC-SHA-256 ## 然后 Alice 也生成新的公私钥 a_2 与 A_2 alice_priv_key_2 = parameters.generate_private_key() ## Alice 使用 a_2 B_2 与 p 计算出新的共享密钥 shared_key_2 bob_pub_numbers_2 = dh.DHPublicNumbers(B_2, param_numbers) bob_pub_key_2 = bob_pub_numbers_2.public_key() shared_key_2 = alice_priv_key_2.exchange(bob_pub_key_2) # 10. Alice 回复 Bob 消息时，使用新共享密钥 shared_key_2 加密消息得到 C_a_1 # 然后将密文 C_a_1 与 A_2 一起发送给 Bob # 11. Bob 使用 b_2 A_2 与 p 计算出共享密钥 shared_key_2 # 然后再使用 shared_key_2 解密数据 # Bob 在下次发送消息时，会生成新的 b_3 与 B_3，将 B_3 随密文一起发送 ## 依次类推 通过上面的代码描述我们应该能理解到，Alice 与 Bob 每次交换数据，实际上都会生成新的临时共享密钥，公钥密钥在每次数据交换时都会更新。 即使攻击者破解了花费了很大的代价破解了其中某一个临时共享密钥 shared_key_k（或者该密钥因为某种原因泄漏了），它也只能解密出其中某一次数据交换的信息 M_b_k，其他所有的消息仍然是保密的，不受此次攻击（或泄漏）的影响。 ","date":"2022-03-01","objectID":"/posts/practical-cryptography-basics-5-key-exchange/:5:0","series":null,"tags":["Cryptography","密码学","密钥交换","安全","DH","DHE","ECDH","ECDHE"],"title":"写给开发人员的实用密码学（五）—— 密钥交换 DHKE 与完美前向保密 PFS","uri":"/posts/practical-cryptography-basics-5-key-exchange/#五pfs-完美前向保密协议-dheecdhe"},{"categories":["tech"],"content":" 参考 Practical-Cryptography-for-Developers-Book A complete overview of SSL/TLS and its cryptographic system ","date":"2022-03-01","objectID":"/posts/practical-cryptography-basics-5-key-exchange/:6:0","series":null,"tags":["Cryptography","密码学","密钥交换","安全","DH","DHE","ECDH","ECDHE"],"title":"写给开发人员的实用密码学（五）—— 密钥交换 DHKE 与完美前向保密 PFS","uri":"/posts/practical-cryptography-basics-5-key-exchange/#参考"},{"categories":["tech"],"content":" 本文主要翻译自 Practical-Cryptography-for-Developers-Book 《写给开发人员的实用密码学》系列文章目录： 写给开发人员的实用密码学（一）—— 概览 写给开发人员的实用密码学（二）—— 哈希函数 写给开发人员的实用密码学（三）—— MAC 与密钥派生函数 KDF 写给开发人员的实用密码学（四）—— 安全随机数生成器 CSPRNG 写给开发人员的实用密码学（五）—— 密钥交换 DHKE 与完美前向保密 PFS 写给开发人员的实用密码学（六）—— 对称密钥加密算法 写给开发人员的实用密码学（七）—— 非对称密钥加密算法 RSA/ECC 写给开发人员的实用密码学（八）—— 数字证书与 TLS 协议 待续 ","date":"2022-03-01","objectID":"/posts/practical-cryptography-basics-4-secure-random-generators/:0:0","series":null,"tags":["Cryptography","密码学","伪随机数","安全","PRNG","CSPRNG"],"title":"写给开发人员的实用密码学（四）—— 安全随机数生成器 CSPRNG","uri":"/posts/practical-cryptography-basics-4-secure-random-generators/#"},{"categories":["tech"],"content":" 一、前言在密码学中，随机性（熵）扮演了一个非常重要的角色，许多密码学算法都要求使用一个不可预测的随机数，只有在生成的随机数不可预测时，这些算法才能保证其安全性。 比如 MAC 算法中的 key 就必须是一个不可预测的值，在这个条件下 MAC 值才是不可伪造的。 另外许多的高性能算法如快速排序、布隆过滤器、蒙特卡洛方法等，都依赖于随机性，如果随机性可以被预测，或者能够找到特定的输入值使这些算法变得特别慢，那黑客就能借此对服务进行 DDoS 攻击，以很小的成本达到让服务不可用的目的。 ","date":"2022-03-01","objectID":"/posts/practical-cryptography-basics-4-secure-random-generators/:1:0","series":null,"tags":["Cryptography","密码学","伪随机数","安全","PRNG","CSPRNG"],"title":"写给开发人员的实用密码学（四）—— 安全随机数生成器 CSPRNG","uri":"/posts/practical-cryptography-basics-4-secure-random-generators/#一前言"},{"categories":["tech"],"content":" 二、PRNG 伪随机数生成器Pseudo-Random Number Generators(PRNG) 是一种数字序列的生成算法，它生成出的数字序列的统计学属性跟真正的随机数序列非常相似，但它生成的伪随机数序列并不是真正的随机数序列！因为该序列完全依赖于提供给 PRNG 的初始值，这个值被称为 PRNG 的种子。 算法流程如下，算法的每次迭代都生成出一个新的伪随机数： 如果输入的初始种子是相同的，PRNG 总是会生成出相同的伪随机数序列，因此 PRNG 也被称为 Deterministic Random Bit Generator (DRBG)，即确定性随机比特生成器。 实际上目前也有所谓的「硬件随机数生成器 TRNG」能生成出真正的随机数，但是因为 PRNG 的高速、低成本、可复现等原因，它仍然被大量使用在现代软件开发中。 PRNG 可用于从一个很小的初始随机性（熵）生成出大量的伪随机性，这被称做「拉伸（Stretching）」。 PRNG 被广泛应用在前面提到的各种依赖随机性的高性能算法以及密码学算法中。 ","date":"2022-03-01","objectID":"/posts/practical-cryptography-basics-4-secure-random-generators/:2:0","series":null,"tags":["Cryptography","密码学","伪随机数","安全","PRNG","CSPRNG"],"title":"写给开发人员的实用密码学（四）—— 安全随机数生成器 CSPRNG","uri":"/posts/practical-cryptography-basics-4-secure-random-generators/#二prng-伪随机数生成器"},{"categories":["tech"],"content":" PRNG 的实现我们在上一篇文章的「MAC 的应用」一节中提到，一个最简单的 PRNG 可以直接使用 MAC 算法实现，用 Python 实现如下： import hmac, hashlib def random_number_generator(seed: bytes, max_num: int): state = seed counter = 0 while True: state = hmac.new(state, bytes(counter), hashlib.sha1).digest() counter += 1 # 这里取余实际上是压缩了信息，某种程度上说，这可以保证内部的真实状态 state 不被逆向出来 yield int.from_bytes(state, byteorder=\"big\") % max_num # 测试下，计算 20 个 100 以内的随机数 gen = random_number_generator(b\"abc\", 100) print([next(gen) for _ in range(20)]) # =\u003e [71, 41, 52, 18, 51, 14, 58, 30, 70, 20, 59, 93, 3, 10, 81, 63, 48, 67, 18, 36] ","date":"2022-03-01","objectID":"/posts/practical-cryptography-basics-4-secure-random-generators/:2:1","series":null,"tags":["Cryptography","密码学","伪随机数","安全","PRNG","CSPRNG"],"title":"写给开发人员的实用密码学（四）—— 安全随机数生成器 CSPRNG","uri":"/posts/practical-cryptography-basics-4-secure-random-generators/#prng-的实现"},{"categories":["tech"],"content":" 三、随机性 - 熵如果初始的 PRNG 种子是完全不可预测的，PRNG 就能保证整个随机序列都不可预测。 因此在 PRNG 中，生成出一个足够随机的种子，就变得非常重要了。 一个最简单的方法，就是收集随机性。对于桌面电脑，随机性可以从鼠标的移动点击、按键事件、网络状况等随机输入来收集。这个事情是由操作系统在内核中处理的，内核会直接为应用程序提供随机数获取的 API，比如 Linux/MacOSX 的 /dev/random 虚拟设备。 如果这个熵的生成有漏洞，就很可能造成严重的问题，一个现实事件就是安卓的 java.security.SecureRandom 漏洞导致安卓用户的比特币钱包失窃。 Python 的 random 库的默认会使用当前时间作为初始 seed，这显然是不够安全的——黑客如果知道你运行程序的大概时间，就能通过遍历的方式暴力破解出你的随机数来！ ","date":"2022-03-01","objectID":"/posts/practical-cryptography-basics-4-secure-random-generators/:3:0","series":null,"tags":["Cryptography","密码学","伪随机数","安全","PRNG","CSPRNG"],"title":"写给开发人员的实用密码学（四）—— 安全随机数生成器 CSPRNG","uri":"/posts/practical-cryptography-basics-4-secure-random-generators/#三随机性---熵"},{"categories":["tech"],"content":" 四、CSPRNG 密码学安全随机数生成器Cryptography Secure Random Number Generators(CSPRNG) 是一种适用于密码学领域的 PRNG，一个 PRNG 如果能够具备如下两个条件，它就是一个 CSPRNG: 能通过「下一比特测试 next-bit test」：即使有人获知了该 PRNG 的 k 位，他也无法使用合理的资源预测第 k+1 位的值 如果攻击者猜出了 PRNG 的内部状态或该状态因某种原因而泄漏，攻击者也无法重建出内部状态泄漏之前生成的所有随机数 有许多的设计都被证明可以用于构造一个 CSPRNG: 基于计数器(CTR)模式下的安全分组密码、流密码或安全散列函数的 CSPRNG 基于数论设计的 CSPRNG，它依靠整数分解问题（IFP）、离散对数问题（DLP）或椭圆曲线离散对数问题（ECDLP）的高难度来确保安全性 CSPRNG 基于加密安全随机性的特殊设计，例如 Yarrow algorithm 和 Fortuna，这俩分别被用于 MacOS 和 FreeBSD. 大多数的 CSPRNG 结合使用来自 OS 的熵与高质量的 PRNG，并且一旦系统生成了新的熵（这可能来自用户输入、磁盘 IO、系统中断、或者硬件 RNG），CSPRNG 会立即使用新的熵来作为 PRNG 新的种子。 这种不断重置 PRNG 种子的行为，使随机数变得非常难以预测。 ","date":"2022-03-01","objectID":"/posts/practical-cryptography-basics-4-secure-random-generators/:4:0","series":null,"tags":["Cryptography","密码学","伪随机数","安全","PRNG","CSPRNG"],"title":"写给开发人员的实用密码学（四）—— 安全随机数生成器 CSPRNG","uri":"/posts/practical-cryptography-basics-4-secure-random-generators/#四csprng-密码学安全随机数生成器"},{"categories":["tech"],"content":" CSPRNG 的用途 加密程序：因为 OS 中熵的收集很缓慢，等待收集到足够多的熵再进行运算是不切实际的，因此很多的加密程序都使用 CSPRNG 来从系统的初始熵生成出足够多的伪随机熵。 其他需要安全随机数的场景 emmmm ","date":"2022-03-01","objectID":"/posts/practical-cryptography-basics-4-secure-random-generators/:4:1","series":null,"tags":["Cryptography","密码学","伪随机数","安全","PRNG","CSPRNG"],"title":"写给开发人员的实用密码学（四）—— 安全随机数生成器 CSPRNG","uri":"/posts/practical-cryptography-basics-4-secure-random-generators/#csprng-的用途"},{"categories":["tech"],"content":" 如何在代码中使用 CSPRNG多数系统都内置了 CSPRNG 算法并提供了内核 API，Unix-like 系统都通过如下两个虚拟设备提供 CSPRNG: /dev/random（受限阻塞随机生成器）: 从这个设备中读取到的是内核熵池中已经收集好的熵，如果熵池空了，此设备会一直阻塞，直到收集到新的环境噪声。 /dev/urandom（不受限非阻塞随机生成器）: 它可能会返回内核熵池中的熵，也可能返回使用「之前收集的熵 + CSPRNG」计算出的安全伪随机数。它不会阻塞。 编程语言的 CSPRNG 接口或库如下： Java: java.security.SecureRandom Python: secrets 库或者 os.urandom() C#: System.Security.Cryptography.RandomNumberGenerator.Create() JavaScript: 客户端可使用 window.crypto.getRandomValues(Uint8Array)，服务端可使用 crypto.randomBytes() 比如使用 Python 实现一个简单但足够安全的随机密码生成器： import secrets import string chars = string.digits + \"your_custom_-content\" + string.ascii_letters def random_string(length: int): \"\"\"生成随机字符串\"\"\" # 注意，这里不应该使用 random 库！而应该使用 secrets code = \"\".join(secrets.choice(chars) for _ in range(length)) return code random_string(24) # =\u003e _rebBfgYs4OtkrPbYtnGmc4n ","date":"2022-03-01","objectID":"/posts/practical-cryptography-basics-4-secure-random-generators/:5:0","series":null,"tags":["Cryptography","密码学","伪随机数","安全","PRNG","CSPRNG"],"title":"写给开发人员的实用密码学（四）—— 安全随机数生成器 CSPRNG","uri":"/posts/practical-cryptography-basics-4-secure-random-generators/#如何在代码中使用-csprng"},{"categories":["tech"],"content":" 参考 Practical-Cryptography-for-Developers-Book ","date":"2022-03-01","objectID":"/posts/practical-cryptography-basics-4-secure-random-generators/:6:0","series":null,"tags":["Cryptography","密码学","伪随机数","安全","PRNG","CSPRNG"],"title":"写给开发人员的实用密码学（四）—— 安全随机数生成器 CSPRNG","uri":"/posts/practical-cryptography-basics-4-secure-random-generators/#参考"},{"categories":["tech"],"content":" 本文主要翻译自 Practical-Cryptography-for-Developers-Book，笔者补充了 HMAC 的 Python 实现以及 scrypt 使用示例。 《写给开发人员的实用密码学》系列文章目录： 写给开发人员的实用密码学（一）—— 概览 写给开发人员的实用密码学（二）—— 哈希函数 写给开发人员的实用密码学（三）—— MAC 与密钥派生函数 KDF 写给开发人员的实用密码学（四）—— 安全随机数生成器 CSPRNG 写给开发人员的实用密码学（五）—— 密钥交换 DHKE 与完美前向保密 PFS 写给开发人员的实用密码学（六）—— 对称密钥加密算法 写给开发人员的实用密码学（七）—— 非对称密钥加密算法 RSA/ECC 写给开发人员的实用密码学（八）—— 数字证书与 TLS 协议 待续 ","date":"2022-03-01","objectID":"/posts/practical-cryptography-basics-3-key-derivation-function/:0:0","series":null,"tags":["Cryptography","Hash","哈希","散列","密码学","安全","MAC","HMAC","KDF","Scrypt"],"title":"写给开发人员的实用密码学（三）—— MAC 与密钥派生函数 KDF","uri":"/posts/practical-cryptography-basics-3-key-derivation-function/#"},{"categories":["tech"],"content":" 一、MAC 消息认证码MAC 消息认证码，即 Message Authentication Code，是用于验证消息的一小段信息。 换句话说，能用它确认消息的真实性——消息来自指定的发件人并且没有被篡改。 MAC 值通过允许验证者（也拥有密钥）检测消息内容的任何更改来保护消息的数据完整性及其真实性。 一个安全的 MAC 函数，跟加密哈希函数非常类似，也拥有如下特性： 快速：计算速度要足够快 确定性：对同样的消息跟密钥，应该总是产生同样的输出 难以分析：对消息或密钥的任何微小改动，都应该使输出完全发生变化 不可逆：从 MAC 值逆向演算出消息跟密钥应该是不可行的。 无碰撞：找到具有相同哈希的两条不同消息应该非常困难（或几乎不可能） 但是 MAC 算法比加密哈希函数多一个输入值：密钥，因此也被称为 keyed hash functions，即「加密钥的哈希函数」。 如下 Python 代码使用 key 跟 消息计算出对应的 HMAC-SHA256 值： import hashlib, hmac, binascii key = b\"key\" msg = b\"some msg\" mac = hmac.new(key, msg, hashlib.sha256).digest() print(f\"HMAC-SHA256({key}, {msg})\", binascii.hexlify(mac).decode('utf8')) # =\u003e HMAC-SHA256(b'key', b'some msg') = 32885b49c8a1009e6d66662f8462e7dd5df769a7b725d1d546574e6d5d6e76ad HMAC 的算法实际上非常简单，我参考 wiki/HMAC 给出的伪码，编写了下面这个 Python 实现，没几行代码，但是完全 work： import hashlib, binascii def xor_bytes(b1, b2): return bytes(a ^ c for a, c in zip(b1, b2)) def my_hmac(key, msg, hash_name): # hash =\u003e (block_size, output_size) # 单位是 bytes，数据来源于 https://en.wikipedia.org/wiki/HMAC hash_size_dict = { \"md5\": (64, 16), \"sha1\": (64, 20), \"sha224\": (64, 28), \"sha256\": (64, 32), # \"sha512/224\": (128, 28), # 这俩算法暂时不清楚在 hashlib 里叫啥名 # \"sha512/256\": (128, 32), \"sha_384\": (128, 48), \"sha_512\": (128, 64), \"sha3_224\": (144, 28), \"sha3_256\": (136, 32), \"sha3_384\": (104, 48), \"sha3_512\": (72, 64), } if hash_name not in hash_size_dict: raise ValueError(\"unknown hash_name\") block_size, output_size = hash_size_dict[hash_name] hash_ = getattr(hashlib, hash_name) # 确保 key 的长度为 block_size block_sized_key = key if len(key) \u003e block_size: block_sized_key = hash_(key).digest() # 用 hash 函数进行压缩 if len(key) \u003c block_size: block_sized_key += b'\\x00' * (block_size - len(key)) # 末尾补 0 o_key_pad = xor_bytes(block_sized_key, (b\"\\x5c\" * block_size)) # Outer padded key i_key_pad = xor_bytes(block_sized_key, (b\"\\x36\" * block_size)) # Inner padded key return hash_(o_key_pad + hash_(i_key_pad + msg).digest()).digest() # 下面验证下 key = b\"key\" msg = b\"some msg\" mac_ = my_hmac(key, msg, \"sha256\") print(f\"HMAC-SHA256({key}, {msg})\", binascii.hexlify(mac_).decode('utf8')) # 输出跟标准库完全一致： # =\u003e HMAC-SHA256(b'key', b'some msg') = 32885b49c8a1009e6d66662f8462e7dd5df769a7b725d1d546574e6d5d6e76ad ","date":"2022-03-01","objectID":"/posts/practical-cryptography-basics-3-key-derivation-function/:1:0","series":null,"tags":["Cryptography","Hash","哈希","散列","密码学","安全","MAC","HMAC","KDF","Scrypt"],"title":"写给开发人员的实用密码学（三）—— MAC 与密钥派生函数 KDF","uri":"/posts/practical-cryptography-basics-3-key-derivation-function/#一mac-消息认证码"},{"categories":["tech"],"content":" MAC 与哈希函数、数字签名的区别上一篇文章提到过，哈希函数只负责生成哈希值，不负责哈希值的可靠传递。 而数字签名呢，跟 MAC 非常相似，但是数字签名使用的是非对称加密系统，更复杂，计算速度也更慢。 MAC 的功能跟数字签名一致，都是验证消息的真实性（authenticity）、完整性（integrity）、不可否认性（non-repudiation），但是 MAC 使用哈希函数或者对称密码系统来做这件事情，速度要更快，算法也更简单。 ","date":"2022-03-01","objectID":"/posts/practical-cryptography-basics-3-key-derivation-function/:1:1","series":null,"tags":["Cryptography","Hash","哈希","散列","密码学","安全","MAC","HMAC","KDF","Scrypt"],"title":"写给开发人员的实用密码学（三）—— MAC 与密钥派生函数 KDF","uri":"/posts/practical-cryptography-basics-3-key-derivation-function/#mac-与哈希函数数字签名的区别"},{"categories":["tech"],"content":" MAC 的应用 1. 验证消息的真实性、完整性这是最简单的一个应用场景，在通信双向都持有一个预共享密钥的前提下，通信时都附带上消息的 MAC 码。 接收方也使用「收到的消息+预共享密钥」计算出 MAC 码，如果跟收到的一致，就说明消息真实无误。 注意这种应用场景中，消息是不保密的！ 2. AE 认证加密 - Authenticated encryption常用的加密方法只能保证数据的保密性，并不能保证数据的完整性。 而这里介绍的 MAC 算法，或者还未介绍的基于非对称加密的数字签名，都只能保证数据的真实性、完整性，不能保证数据被安全传输。 而认证加密，就是将加密算法与 MAC 算法结合使用的一种加密方案。 在确保 MAC 码「强不可伪造」的前提下，首先对数据进行加密，然后计算密文的 MAC 码，再同时传输密文与 MAC 码，就能同时保证数据的保密性、完整性、真实性，这种方法叫 Encrypt-then-MAC, 缩写做 EtM. 接收方在解密前先计算密文的 MAC 码与收到的对比，就能验证密文的完整性与真实性。 AE 有一种更安全的变体——带有关联数据的认证加密 (authenticated encryption with associated data，AEAD)。 AEAD 将「关联数据(Associated Data, AD)」——也称为「附加验证数据（Additional Authenticated Data, AAD）」——绑定到密文和它应该出现的上下文，以便可以检测和拒绝将有效密文“剪切并粘贴”到不同上下文的尝试。 AEAD 用于加密和未加密数据一起使用的场景（例如，在加密的网络协议中），并确保整个数据流经过身份验证和完整性保护。 换句话说，AEAD 增加了检查某些内容的完整性和真实性的能力。 我们会在第六章「对称加密算法」中看到如何通过 Python 使用 AEAD 加密方案 AES-256-GCM. 3. 基于 MAC 的伪随机数生成器MAC 码的另一个用途就是伪随机数生成函数，相比直接使用熵+哈希函数的进行伪随机数计算，MAC 码因为多引入了一个变量 key，理论上它会更安全。 这种场景下，我们称 MAC 使用的密钥为 salt，即盐。 next_seed = MAC(salt, seed) ","date":"2022-03-01","objectID":"/posts/practical-cryptography-basics-3-key-derivation-function/:1:2","series":null,"tags":["Cryptography","Hash","哈希","散列","密码学","安全","MAC","HMAC","KDF","Scrypt"],"title":"写给开发人员的实用密码学（三）—— MAC 与密钥派生函数 KDF","uri":"/posts/practical-cryptography-basics-3-key-derivation-function/#mac-的应用"},{"categories":["tech"],"content":" MAC 的应用 1. 验证消息的真实性、完整性这是最简单的一个应用场景，在通信双向都持有一个预共享密钥的前提下，通信时都附带上消息的 MAC 码。 接收方也使用「收到的消息+预共享密钥」计算出 MAC 码，如果跟收到的一致，就说明消息真实无误。 注意这种应用场景中，消息是不保密的！ 2. AE 认证加密 - Authenticated encryption常用的加密方法只能保证数据的保密性，并不能保证数据的完整性。 而这里介绍的 MAC 算法，或者还未介绍的基于非对称加密的数字签名，都只能保证数据的真实性、完整性，不能保证数据被安全传输。 而认证加密，就是将加密算法与 MAC 算法结合使用的一种加密方案。 在确保 MAC 码「强不可伪造」的前提下，首先对数据进行加密，然后计算密文的 MAC 码，再同时传输密文与 MAC 码，就能同时保证数据的保密性、完整性、真实性，这种方法叫 Encrypt-then-MAC, 缩写做 EtM. 接收方在解密前先计算密文的 MAC 码与收到的对比，就能验证密文的完整性与真实性。 AE 有一种更安全的变体——带有关联数据的认证加密 (authenticated encryption with associated data，AEAD)。 AEAD 将「关联数据(Associated Data, AD)」——也称为「附加验证数据（Additional Authenticated Data, AAD）」——绑定到密文和它应该出现的上下文，以便可以检测和拒绝将有效密文“剪切并粘贴”到不同上下文的尝试。 AEAD 用于加密和未加密数据一起使用的场景（例如，在加密的网络协议中），并确保整个数据流经过身份验证和完整性保护。 换句话说，AEAD 增加了检查某些内容的完整性和真实性的能力。 我们会在第六章「对称加密算法」中看到如何通过 Python 使用 AEAD 加密方案 AES-256-GCM. 3. 基于 MAC 的伪随机数生成器MAC 码的另一个用途就是伪随机数生成函数，相比直接使用熵+哈希函数的进行伪随机数计算，MAC 码因为多引入了一个变量 key，理论上它会更安全。 这种场景下，我们称 MAC 使用的密钥为 salt，即盐。 next_seed = MAC(salt, seed) ","date":"2022-03-01","objectID":"/posts/practical-cryptography-basics-3-key-derivation-function/:1:2","series":null,"tags":["Cryptography","Hash","哈希","散列","密码学","安全","MAC","HMAC","KDF","Scrypt"],"title":"写给开发人员的实用密码学（三）—— MAC 与密钥派生函数 KDF","uri":"/posts/practical-cryptography-basics-3-key-derivation-function/#1-验证消息的真实性完整性"},{"categories":["tech"],"content":" MAC 的应用 1. 验证消息的真实性、完整性这是最简单的一个应用场景，在通信双向都持有一个预共享密钥的前提下，通信时都附带上消息的 MAC 码。 接收方也使用「收到的消息+预共享密钥」计算出 MAC 码，如果跟收到的一致，就说明消息真实无误。 注意这种应用场景中，消息是不保密的！ 2. AE 认证加密 - Authenticated encryption常用的加密方法只能保证数据的保密性，并不能保证数据的完整性。 而这里介绍的 MAC 算法，或者还未介绍的基于非对称加密的数字签名，都只能保证数据的真实性、完整性，不能保证数据被安全传输。 而认证加密，就是将加密算法与 MAC 算法结合使用的一种加密方案。 在确保 MAC 码「强不可伪造」的前提下，首先对数据进行加密，然后计算密文的 MAC 码，再同时传输密文与 MAC 码，就能同时保证数据的保密性、完整性、真实性，这种方法叫 Encrypt-then-MAC, 缩写做 EtM. 接收方在解密前先计算密文的 MAC 码与收到的对比，就能验证密文的完整性与真实性。 AE 有一种更安全的变体——带有关联数据的认证加密 (authenticated encryption with associated data，AEAD)。 AEAD 将「关联数据(Associated Data, AD)」——也称为「附加验证数据（Additional Authenticated Data, AAD）」——绑定到密文和它应该出现的上下文，以便可以检测和拒绝将有效密文“剪切并粘贴”到不同上下文的尝试。 AEAD 用于加密和未加密数据一起使用的场景（例如，在加密的网络协议中），并确保整个数据流经过身份验证和完整性保护。 换句话说，AEAD 增加了检查某些内容的完整性和真实性的能力。 我们会在第六章「对称加密算法」中看到如何通过 Python 使用 AEAD 加密方案 AES-256-GCM. 3. 基于 MAC 的伪随机数生成器MAC 码的另一个用途就是伪随机数生成函数，相比直接使用熵+哈希函数的进行伪随机数计算，MAC 码因为多引入了一个变量 key，理论上它会更安全。 这种场景下，我们称 MAC 使用的密钥为 salt，即盐。 next_seed = MAC(salt, seed) ","date":"2022-03-01","objectID":"/posts/practical-cryptography-basics-3-key-derivation-function/:1:2","series":null,"tags":["Cryptography","Hash","哈希","散列","密码学","安全","MAC","HMAC","KDF","Scrypt"],"title":"写给开发人员的实用密码学（三）—— MAC 与密钥派生函数 KDF","uri":"/posts/practical-cryptography-basics-3-key-derivation-function/#2-ae-认证加密---authenticated-encryption"},{"categories":["tech"],"content":" MAC 的应用 1. 验证消息的真实性、完整性这是最简单的一个应用场景，在通信双向都持有一个预共享密钥的前提下，通信时都附带上消息的 MAC 码。 接收方也使用「收到的消息+预共享密钥」计算出 MAC 码，如果跟收到的一致，就说明消息真实无误。 注意这种应用场景中，消息是不保密的！ 2. AE 认证加密 - Authenticated encryption常用的加密方法只能保证数据的保密性，并不能保证数据的完整性。 而这里介绍的 MAC 算法，或者还未介绍的基于非对称加密的数字签名，都只能保证数据的真实性、完整性，不能保证数据被安全传输。 而认证加密，就是将加密算法与 MAC 算法结合使用的一种加密方案。 在确保 MAC 码「强不可伪造」的前提下，首先对数据进行加密，然后计算密文的 MAC 码，再同时传输密文与 MAC 码，就能同时保证数据的保密性、完整性、真实性，这种方法叫 Encrypt-then-MAC, 缩写做 EtM. 接收方在解密前先计算密文的 MAC 码与收到的对比，就能验证密文的完整性与真实性。 AE 有一种更安全的变体——带有关联数据的认证加密 (authenticated encryption with associated data，AEAD)。 AEAD 将「关联数据(Associated Data, AD)」——也称为「附加验证数据（Additional Authenticated Data, AAD）」——绑定到密文和它应该出现的上下文，以便可以检测和拒绝将有效密文“剪切并粘贴”到不同上下文的尝试。 AEAD 用于加密和未加密数据一起使用的场景（例如，在加密的网络协议中），并确保整个数据流经过身份验证和完整性保护。 换句话说，AEAD 增加了检查某些内容的完整性和真实性的能力。 我们会在第六章「对称加密算法」中看到如何通过 Python 使用 AEAD 加密方案 AES-256-GCM. 3. 基于 MAC 的伪随机数生成器MAC 码的另一个用途就是伪随机数生成函数，相比直接使用熵+哈希函数的进行伪随机数计算，MAC 码因为多引入了一个变量 key，理论上它会更安全。 这种场景下，我们称 MAC 使用的密钥为 salt，即盐。 next_seed = MAC(salt, seed) ","date":"2022-03-01","objectID":"/posts/practical-cryptography-basics-3-key-derivation-function/:1:2","series":null,"tags":["Cryptography","Hash","哈希","散列","密码学","安全","MAC","HMAC","KDF","Scrypt"],"title":"写给开发人员的实用密码学（三）—— MAC 与密钥派生函数 KDF","uri":"/posts/practical-cryptography-basics-3-key-derivation-function/#3-基于-mac-的伪随机数生成器"},{"categories":["tech"],"content":" 二、KDF 密钥派生函数我们都更喜欢使用密码来保护自己的数据而不是二进制的密钥，因为相比之下二进制密钥太难记忆了，字符形式的密码才是符合人类思维习惯的东西。 可对计算机而言就刚好相反了，现代密码学的很多算法都要求输入是一个大的数字，二进制的密钥就是这样一个大的数字。 因此显然我们需要一个将字符密码（Password）转换成密钥（Key）的函数，这就是密钥派生函数 Key Derivation Function. 直接使用 SHA256 之类的加密哈希函数来生成密钥是不安全的，因为为了方便记忆，通常密码并不会很长，绝大多数人的密码长度估计都不超过 15 位。 甚至很多人都在使用非常常见的弱密码，如 123456 admin 生日等等。 这就导致如果直接使用 SHA256 之类的算法，许多密码将很容易被暴力破解、字典攻击、彩虹表攻击等手段猜测出来！ KDF 目前主要从如下三个维度提升 hash 碰撞难度： 时间复杂度：对应 CPU/GPU 计算资源 空间复杂度：对应 Memory 内存资源 并行维度：使用无法分解的算法，锁定只允许单线程运算 主要手段是加盐，以及多次迭代。这种设计方法被称为「密钥拉伸 Key stretching」。 因为它的独特属性，KDF 也被称作慢哈希算法。 目前比较著名的 KDF 算法主要有如下几个： PBKDF2：这是一个非常简单的加密 KDF 算法，目前已经不推荐使用。 Bcrypt：安全性在下降，用得越来越少了。不建议使用。 Scrypt：可以灵活地设定使用的内存大小，在 argon2 不可用时，可使用它。 Argon2：目前最强的密码 Hash 算法，在 2015 年赢得了密码 Hash 竞赛。 如果你正在开发一个新的程序，需要使用到 KDF，建议选用 argon2/scrypt. Python 中最流行的密码学库是 cryptography，requests 的底层曾经就使用了它（新版本已经换成使用标准库 ssl 了），下面我们使用这个库来演示下 Scrypt 算法的使用： # pip install cryptography==36.0.1 import os from cryptography.hazmat.primitives.kdf.scrypt import Scrypt salt = os.urandom(16) # derive kdf = Scrypt( salt=salt, length=32, n=2**14, r=8, p=1, ) key = kdf.derive(b\"my great password\") # verify kdf = Scrypt( salt=salt, length=32, n=2**14, r=8, p=1, ) kdf.verify(b\"my great password\", key) ","date":"2022-03-01","objectID":"/posts/practical-cryptography-basics-3-key-derivation-function/:2:0","series":null,"tags":["Cryptography","Hash","哈希","散列","密码学","安全","MAC","HMAC","KDF","Scrypt"],"title":"写给开发人员的实用密码学（三）—— MAC 与密钥派生函数 KDF","uri":"/posts/practical-cryptography-basics-3-key-derivation-function/#二kdf-密钥派生函数"},{"categories":["tech"],"content":" 参考 Practical-Cryptography-for-Developers-Book A complete overview of SSL/TLS and its cryptographic system ","date":"2022-03-01","objectID":"/posts/practical-cryptography-basics-3-key-derivation-function/:3:0","series":null,"tags":["Cryptography","Hash","哈希","散列","密码学","安全","MAC","HMAC","KDF","Scrypt"],"title":"写给开发人员的实用密码学（三）—— MAC 与密钥派生函数 KDF","uri":"/posts/practical-cryptography-basics-3-key-derivation-function/#参考"},{"categories":["tech"],"content":" 本文主要翻译自 Practical-Cryptography-for-Developers-Book，笔者额外补充了「非加密哈希函数」的简单介绍。 《写给开发人员的实用密码学》系列文章目录： 写给开发人员的实用密码学（一）—— 概览 写给开发人员的实用密码学（二）—— 哈希函数 写给开发人员的实用密码学（三）—— MAC 与密钥派生函数 KDF 写给开发人员的实用密码学（四）—— 安全随机数生成器 CSPRNG 写给开发人员的实用密码学（五）—— 密钥交换 DHKE 与完美前向保密 PFS 写给开发人员的实用密码学（六）—— 对称密钥加密算法 写给开发人员的实用密码学（七）—— 非对称密钥加密算法 RSA/ECC 写给开发人员的实用密码学（八）—— 数字证书与 TLS 协议 待续 ","date":"2022-03-01","objectID":"/posts/practical-cryptography-basics-2-hash/:0:0","series":null,"tags":["Cryptography","Hash","密码学","哈希","散列","安全","SHA-2","SHA-3"],"title":"写给开发人员的实用密码学（二）—— 哈希函数","uri":"/posts/practical-cryptography-basics-2-hash/#"},{"categories":["tech"],"content":" 一、什么是哈希函数哈希函数，或者叫散列函数，是一种从任何一种数据中创建一个数字指纹（也叫数字摘要）的方法，散列函数把数据压缩（或者放大）成一个长度固定的字符串。 哈希函数的输入空间（文本或者二进制数据）是无限大，但是输出空间（一个固定长度的摘要）却是有限的。将「无限」映射到「有限」，不可避免的会有概率不同的输入得到相同的输出，这种情况我们称为碰撞（collision）。 一个简单的哈希函数是直接对输入数据/文本的字节求和。 它会导致大量的碰撞，例如 hello 和 ehllo 将具有相同的哈希值。 更好的哈希函数可以使用这样的方案：它将第一个字节作为状态，然后转换状态（例如，将它乘以像 31 这样的素数），然后将下一个字节添加到状态，然后再次转换状态并添加下一个字节等。 这样的操作可以显着降低碰撞概率并产生更均匀的分布。 ","date":"2022-03-01","objectID":"/posts/practical-cryptography-basics-2-hash/:1:0","series":null,"tags":["Cryptography","Hash","密码学","哈希","散列","安全","SHA-2","SHA-3"],"title":"写给开发人员的实用密码学（二）—— 哈希函数","uri":"/posts/practical-cryptography-basics-2-hash/#一什么是哈希函数"},{"categories":["tech"],"content":" 二、加密哈希函数加密哈希函数（也叫密码学哈希函数）是指一类有特殊属性的哈希函数。 一个好的「加密哈希函数」必须满足抗碰撞（collision-resistant）和不可逆（irreversible）这两个条件。 抗碰撞是指通过统计学方法（彩虹表）很难或几乎不可能猜出哈希值对应的原始数据，而不可逆则是说攻击者很难或几乎不可能从算法层面通过哈希值逆向演算出原始数据。 具体而言，一个理想的加密哈希函数，应当具有如下属性： 快速：计算速度要足够快 确定性：对同样的输入，应该总是产生同样的输出 难以分析：对输入的任何微小改动，都应该使输出完全发生变化 不可逆：从其哈希值逆向演算出输入值应该是不可行的。这意味着没有比暴力破解更好的破解方法 无碰撞：找到具有相同哈希值的两条不同消息应该非常困难（或几乎不可能） 现代加密哈希函数（如 SHA2 和 SHA3）都具有上述几个属性，并被广泛应用在多个领域，各种现代编程语言和平台的标准库中基本都包含这些常用的哈希函数。 ","date":"2022-03-01","objectID":"/posts/practical-cryptography-basics-2-hash/:2:0","series":null,"tags":["Cryptography","Hash","密码学","哈希","散列","安全","SHA-2","SHA-3"],"title":"写给开发人员的实用密码学（二）—— 哈希函数","uri":"/posts/practical-cryptography-basics-2-hash/#二加密哈希函数"},{"categories":["tech"],"content":" 量子安全性现代密码学哈希函数（如 SHA2, SHA3, BLAKE2）都被认为是量子安全的，无惧量子计算机的发展。 ","date":"2022-03-01","objectID":"/posts/practical-cryptography-basics-2-hash/:2:1","series":null,"tags":["Cryptography","Hash","密码学","哈希","散列","安全","SHA-2","SHA-3"],"title":"写给开发人员的实用密码学（二）—— 哈希函数","uri":"/posts/practical-cryptography-basics-2-hash/#量子安全性"},{"categories":["tech"],"content":" 加密哈希函数的应用 1. 数据完整性校验加密哈希函数被广泛用于文件完整性校验。如果你从网上下载的文件计算出的 SHA256 校验和（checksum）跟官方公布的一致，那就说明文件没有损坏。 但是哈希函数自身不能保证文件的真实性，目前来讲，真实性通常是 TLS 协议要保证的，它确保你在 openssl 网站上看到的「SHA256 校验和」真实无误（未被篡改）。 现代网络基本都很难遇到文件损坏的情况了，但是在古早的低速网络中，即使 TCP 跟底层协议已经有多种数据纠错手段，下载完成的文件仍然是有可能损坏的。 这也是以前 rar 压缩格式很流行的原因之一—— rar 压缩文件拥有一定程度上的自我修复能力，传输过程中损坏少量数据，仍然能正常解压。 2. 保存密码加密哈希函数还被用于密码的安全存储，现代系统使用专门设计的安全哈希算法计算用户密码的哈希摘要，保存到数据库中，这样能确保密码的安全性。除了用户自己，没有人清楚该密码的原始数据，即使数据库管理员也只能看到一个哈希摘要。 3. 生成唯一 ID加密哈希函数也被用于为文档或消息生成（绝大多数情况下）唯一的 ID，因此哈希值也被称为数字指纹。 注意这里说的是数字指纹，而非数字签名。 数字签名是与下一篇文章介绍的「MAC」码比较类似的，用于验证消息的真实、完整、认证作者身份的一段数据。 加密哈希函数计算出的哈希值理论上确实有碰撞的概率，但是这个概率实在太小了，因此绝大多数系统（如 Git）都假设哈希函数是无碰撞的（collistion free）。 文档的哈希值可以被用于证明该文档的存在性，或者被当成一个索引，用于从存储系统中提取文档。 使用哈希值作为唯一 ID 的典型例子，Git 版本控制系统（如 3c3be25bc1757ca99aba55d4157596a8ea217698）肯定算一个，比特币地址（如 1BvBMSEYstWetqTFn5Au4m4GFg7xJaNVN2）也算。 4. 伪随机数生成哈希值可以被当作一个随机数看待，生成一个伪随机数的简单流程如下： 通过随机事件得到一个熵（例如键盘点击或鼠标移动），将它作为最初的随机数种子（random seed）。 添加一个 1 到熵中，进行哈希计算得到第一个随机数 再添加一个 2，进行哈希计算得到第二个随机数 以此类推 当然为了确保安全性，实际的加密随机数生成器会比这再复杂一些，我们会在后面的「随机数生成器」一节学习其中细节。 ","date":"2022-03-01","objectID":"/posts/practical-cryptography-basics-2-hash/:2:2","series":null,"tags":["Cryptography","Hash","密码学","哈希","散列","安全","SHA-2","SHA-3"],"title":"写给开发人员的实用密码学（二）—— 哈希函数","uri":"/posts/practical-cryptography-basics-2-hash/#加密哈希函数的应用"},{"categories":["tech"],"content":" 加密哈希函数的应用 1. 数据完整性校验加密哈希函数被广泛用于文件完整性校验。如果你从网上下载的文件计算出的 SHA256 校验和（checksum）跟官方公布的一致，那就说明文件没有损坏。 但是哈希函数自身不能保证文件的真实性，目前来讲，真实性通常是 TLS 协议要保证的，它确保你在 openssl 网站上看到的「SHA256 校验和」真实无误（未被篡改）。 现代网络基本都很难遇到文件损坏的情况了，但是在古早的低速网络中，即使 TCP 跟底层协议已经有多种数据纠错手段，下载完成的文件仍然是有可能损坏的。 这也是以前 rar 压缩格式很流行的原因之一—— rar 压缩文件拥有一定程度上的自我修复能力，传输过程中损坏少量数据，仍然能正常解压。 2. 保存密码加密哈希函数还被用于密码的安全存储，现代系统使用专门设计的安全哈希算法计算用户密码的哈希摘要，保存到数据库中，这样能确保密码的安全性。除了用户自己，没有人清楚该密码的原始数据，即使数据库管理员也只能看到一个哈希摘要。 3. 生成唯一 ID加密哈希函数也被用于为文档或消息生成（绝大多数情况下）唯一的 ID，因此哈希值也被称为数字指纹。 注意这里说的是数字指纹，而非数字签名。 数字签名是与下一篇文章介绍的「MAC」码比较类似的，用于验证消息的真实、完整、认证作者身份的一段数据。 加密哈希函数计算出的哈希值理论上确实有碰撞的概率，但是这个概率实在太小了，因此绝大多数系统（如 Git）都假设哈希函数是无碰撞的（collistion free）。 文档的哈希值可以被用于证明该文档的存在性，或者被当成一个索引，用于从存储系统中提取文档。 使用哈希值作为唯一 ID 的典型例子，Git 版本控制系统（如 3c3be25bc1757ca99aba55d4157596a8ea217698）肯定算一个，比特币地址（如 1BvBMSEYstWetqTFn5Au4m4GFg7xJaNVN2）也算。 4. 伪随机数生成哈希值可以被当作一个随机数看待，生成一个伪随机数的简单流程如下： 通过随机事件得到一个熵（例如键盘点击或鼠标移动），将它作为最初的随机数种子（random seed）。 添加一个 1 到熵中，进行哈希计算得到第一个随机数 再添加一个 2，进行哈希计算得到第二个随机数 以此类推 当然为了确保安全性，实际的加密随机数生成器会比这再复杂一些，我们会在后面的「随机数生成器」一节学习其中细节。 ","date":"2022-03-01","objectID":"/posts/practical-cryptography-basics-2-hash/:2:2","series":null,"tags":["Cryptography","Hash","密码学","哈希","散列","安全","SHA-2","SHA-3"],"title":"写给开发人员的实用密码学（二）—— 哈希函数","uri":"/posts/practical-cryptography-basics-2-hash/#1-数据完整性校验"},{"categories":["tech"],"content":" 加密哈希函数的应用 1. 数据完整性校验加密哈希函数被广泛用于文件完整性校验。如果你从网上下载的文件计算出的 SHA256 校验和（checksum）跟官方公布的一致，那就说明文件没有损坏。 但是哈希函数自身不能保证文件的真实性，目前来讲，真实性通常是 TLS 协议要保证的，它确保你在 openssl 网站上看到的「SHA256 校验和」真实无误（未被篡改）。 现代网络基本都很难遇到文件损坏的情况了，但是在古早的低速网络中，即使 TCP 跟底层协议已经有多种数据纠错手段，下载完成的文件仍然是有可能损坏的。 这也是以前 rar 压缩格式很流行的原因之一—— rar 压缩文件拥有一定程度上的自我修复能力，传输过程中损坏少量数据，仍然能正常解压。 2. 保存密码加密哈希函数还被用于密码的安全存储，现代系统使用专门设计的安全哈希算法计算用户密码的哈希摘要，保存到数据库中，这样能确保密码的安全性。除了用户自己，没有人清楚该密码的原始数据，即使数据库管理员也只能看到一个哈希摘要。 3. 生成唯一 ID加密哈希函数也被用于为文档或消息生成（绝大多数情况下）唯一的 ID，因此哈希值也被称为数字指纹。 注意这里说的是数字指纹，而非数字签名。 数字签名是与下一篇文章介绍的「MAC」码比较类似的，用于验证消息的真实、完整、认证作者身份的一段数据。 加密哈希函数计算出的哈希值理论上确实有碰撞的概率，但是这个概率实在太小了，因此绝大多数系统（如 Git）都假设哈希函数是无碰撞的（collistion free）。 文档的哈希值可以被用于证明该文档的存在性，或者被当成一个索引，用于从存储系统中提取文档。 使用哈希值作为唯一 ID 的典型例子，Git 版本控制系统（如 3c3be25bc1757ca99aba55d4157596a8ea217698）肯定算一个，比特币地址（如 1BvBMSEYstWetqTFn5Au4m4GFg7xJaNVN2）也算。 4. 伪随机数生成哈希值可以被当作一个随机数看待，生成一个伪随机数的简单流程如下： 通过随机事件得到一个熵（例如键盘点击或鼠标移动），将它作为最初的随机数种子（random seed）。 添加一个 1 到熵中，进行哈希计算得到第一个随机数 再添加一个 2，进行哈希计算得到第二个随机数 以此类推 当然为了确保安全性，实际的加密随机数生成器会比这再复杂一些，我们会在后面的「随机数生成器」一节学习其中细节。 ","date":"2022-03-01","objectID":"/posts/practical-cryptography-basics-2-hash/:2:2","series":null,"tags":["Cryptography","Hash","密码学","哈希","散列","安全","SHA-2","SHA-3"],"title":"写给开发人员的实用密码学（二）—— 哈希函数","uri":"/posts/practical-cryptography-basics-2-hash/#2-保存密码"},{"categories":["tech"],"content":" 加密哈希函数的应用 1. 数据完整性校验加密哈希函数被广泛用于文件完整性校验。如果你从网上下载的文件计算出的 SHA256 校验和（checksum）跟官方公布的一致，那就说明文件没有损坏。 但是哈希函数自身不能保证文件的真实性，目前来讲，真实性通常是 TLS 协议要保证的，它确保你在 openssl 网站上看到的「SHA256 校验和」真实无误（未被篡改）。 现代网络基本都很难遇到文件损坏的情况了，但是在古早的低速网络中，即使 TCP 跟底层协议已经有多种数据纠错手段，下载完成的文件仍然是有可能损坏的。 这也是以前 rar 压缩格式很流行的原因之一—— rar 压缩文件拥有一定程度上的自我修复能力，传输过程中损坏少量数据，仍然能正常解压。 2. 保存密码加密哈希函数还被用于密码的安全存储，现代系统使用专门设计的安全哈希算法计算用户密码的哈希摘要，保存到数据库中，这样能确保密码的安全性。除了用户自己，没有人清楚该密码的原始数据，即使数据库管理员也只能看到一个哈希摘要。 3. 生成唯一 ID加密哈希函数也被用于为文档或消息生成（绝大多数情况下）唯一的 ID，因此哈希值也被称为数字指纹。 注意这里说的是数字指纹，而非数字签名。 数字签名是与下一篇文章介绍的「MAC」码比较类似的，用于验证消息的真实、完整、认证作者身份的一段数据。 加密哈希函数计算出的哈希值理论上确实有碰撞的概率，但是这个概率实在太小了，因此绝大多数系统（如 Git）都假设哈希函数是无碰撞的（collistion free）。 文档的哈希值可以被用于证明该文档的存在性，或者被当成一个索引，用于从存储系统中提取文档。 使用哈希值作为唯一 ID 的典型例子，Git 版本控制系统（如 3c3be25bc1757ca99aba55d4157596a8ea217698）肯定算一个，比特币地址（如 1BvBMSEYstWetqTFn5Au4m4GFg7xJaNVN2）也算。 4. 伪随机数生成哈希值可以被当作一个随机数看待，生成一个伪随机数的简单流程如下： 通过随机事件得到一个熵（例如键盘点击或鼠标移动），将它作为最初的随机数种子（random seed）。 添加一个 1 到熵中，进行哈希计算得到第一个随机数 再添加一个 2，进行哈希计算得到第二个随机数 以此类推 当然为了确保安全性，实际的加密随机数生成器会比这再复杂一些，我们会在后面的「随机数生成器」一节学习其中细节。 ","date":"2022-03-01","objectID":"/posts/practical-cryptography-basics-2-hash/:2:2","series":null,"tags":["Cryptography","Hash","密码学","哈希","散列","安全","SHA-2","SHA-3"],"title":"写给开发人员的实用密码学（二）—— 哈希函数","uri":"/posts/practical-cryptography-basics-2-hash/#3-生成唯一-id"},{"categories":["tech"],"content":" 加密哈希函数的应用 1. 数据完整性校验加密哈希函数被广泛用于文件完整性校验。如果你从网上下载的文件计算出的 SHA256 校验和（checksum）跟官方公布的一致，那就说明文件没有损坏。 但是哈希函数自身不能保证文件的真实性，目前来讲，真实性通常是 TLS 协议要保证的，它确保你在 openssl 网站上看到的「SHA256 校验和」真实无误（未被篡改）。 现代网络基本都很难遇到文件损坏的情况了，但是在古早的低速网络中，即使 TCP 跟底层协议已经有多种数据纠错手段，下载完成的文件仍然是有可能损坏的。 这也是以前 rar 压缩格式很流行的原因之一—— rar 压缩文件拥有一定程度上的自我修复能力，传输过程中损坏少量数据，仍然能正常解压。 2. 保存密码加密哈希函数还被用于密码的安全存储，现代系统使用专门设计的安全哈希算法计算用户密码的哈希摘要，保存到数据库中，这样能确保密码的安全性。除了用户自己，没有人清楚该密码的原始数据，即使数据库管理员也只能看到一个哈希摘要。 3. 生成唯一 ID加密哈希函数也被用于为文档或消息生成（绝大多数情况下）唯一的 ID，因此哈希值也被称为数字指纹。 注意这里说的是数字指纹，而非数字签名。 数字签名是与下一篇文章介绍的「MAC」码比较类似的，用于验证消息的真实、完整、认证作者身份的一段数据。 加密哈希函数计算出的哈希值理论上确实有碰撞的概率，但是这个概率实在太小了，因此绝大多数系统（如 Git）都假设哈希函数是无碰撞的（collistion free）。 文档的哈希值可以被用于证明该文档的存在性，或者被当成一个索引，用于从存储系统中提取文档。 使用哈希值作为唯一 ID 的典型例子，Git 版本控制系统（如 3c3be25bc1757ca99aba55d4157596a8ea217698）肯定算一个，比特币地址（如 1BvBMSEYstWetqTFn5Au4m4GFg7xJaNVN2）也算。 4. 伪随机数生成哈希值可以被当作一个随机数看待，生成一个伪随机数的简单流程如下： 通过随机事件得到一个熵（例如键盘点击或鼠标移动），将它作为最初的随机数种子（random seed）。 添加一个 1 到熵中，进行哈希计算得到第一个随机数 再添加一个 2，进行哈希计算得到第二个随机数 以此类推 当然为了确保安全性，实际的加密随机数生成器会比这再复杂一些，我们会在后面的「随机数生成器」一节学习其中细节。 ","date":"2022-03-01","objectID":"/posts/practical-cryptography-basics-2-hash/:2:2","series":null,"tags":["Cryptography","Hash","密码学","哈希","散列","安全","SHA-2","SHA-3"],"title":"写给开发人员的实用密码学（二）—— 哈希函数","uri":"/posts/practical-cryptography-basics-2-hash/#4-伪随机数生成"},{"categories":["tech"],"content":" 安全的加密哈希算法 1. SHA-2, SHA-256, SHA-512SHA-2，即 Secure Hash Algorithm 2，是一组强密码哈希函数，其成本包括：SHA-256（256位哈希）、SHA-384（384位哈希）、SHA-512（512位哈希）等。基于密码概念「Merkle–Damgård 构造」，目前被认为高度安全。 SHA-2 是 SHA-1 的继任者，于 2001 年在美国作为官方加密标准发布。 SHA-2 在软件开发和密码学中被广泛使用，可用于现代商业应用。 其中 SHA-256 被广泛用于 HTTPS 协议、文件完整性校验、比特币区块链等各种场景。 Python 代码示例： import hashlib, binascii text = 'hello' data = text.encode(\"utf8\") sha256hash = hashlib.sha256(data).digest() print(f\"SHA-256({text}) = \", binascii.hexlify(sha256hash).decode(\"utf8\")) sha384hash = hashlib.sha384(data).digest() print(f\"SHA-384({text}) = \", binascii.hexlify(sha384hash).decode(\"utf8\")) sha512hash = hashlib.sha512(data).digest() print(f\"SHA-512({text}) = \", binascii.hexlify(sha512hash).decode(\"utf8\")) 输出如下： SHA-256('hello') = 2cf24dba5fb0a30e26e83b2ac5b9e29e1b161e5c1fa7425e73043362938b9824 SHA-384('hello') = 59e1748777448c69de6b800d7a33bbfb9ff1b463e44354c3553bcdb9c666fa90125a3c79f90397bdf5f6a13de828684f SHA-512('hello') = 9b71d224bd62f3785d96d46ad3ea3d73319bfbc2890caadae2dff72519673ca72323c3d99ba5c11d7c7acc6e14b8c5da0c4663475c2e5c3adef46f73bcdec043 2. 更长的哈希值 == 更高的抗碰撞能力按照设计，哈希函数的输出越长，就有望实现更高的安全性和抗碰撞能力（但也有一些例外）。 一般来说，128 位哈希算法比 256 位哈希算法弱，256 位哈希算法比 512 位哈希算法弱。 因此显然 SHA-512 比 SHA-256 更强。我们可以预期，SHA-512 的碰撞概率要比 SHA-256 更低。 3. SHA-3, SHA3-256, SHA3-512, Keccak-256在输出的哈希长度相同时，SHA-3（及其变体 SHA3-224、SHA3-256、SHA3-384、SHA3-512）被认为拥有比 SHA-2（SHA-224、SHA-256、SHA-384、SHA-512）更高的加密强度。 例如，对于相同的哈希长度（256 位），SHA3-256 提供比 SHA-256 更高的加密强度。 SHA-3 系列函数是 Keccak 哈希家族的代表，它基于密码学概念海绵函数。而 Keccak 是SHA3 NIST 比赛的冠军。 与 SHA-2 不同，SHA-3 系列加密哈希函数不易受到长度拓展攻击 Length extension attack. SHA-3 被认为是高度安全的，并于 2015 年作为美国官方推荐的加密标准发布。 以太坊（Ethereum）区块链中使用的哈希函数 Keccak-256 是 SHA3-256 的变体，在代码中更改了一些常量。 哈希函数 SHAKE128(msg, length) 和 SHAKE256(msg, length) 是 SHA3-256 和 SHA3-512 算法的变体，它们输出消息的长度可以变化。 SHA3 的 Python 代码示例： import hashlib, binascii text = 'hello' data = text.encode(\"utf8\") sha3_256hash = hashlib.sha3_256(data).digest() print(f\"SHA3-256({text}) = \", binascii.hexlify(sha3_256hash).decode(\"utf8\")) sha3_512hash = hashlib.sha3_512(data).digest() print(f\"SHA3-512({text}) = \", binascii.hexlify(sha3_512hash).decode(\"utf8\")) 输出： SHA3-256('hello') = 3338be694f50c5f338814986cdf0686453a888b84f424d792af4b9202398f392 Keccak-256('hello') = 1c8aff950685c2ed4bc3174f3472287b56d9517b9c948127319a09a7a36deac8 SHA3-512('hello') = 75d527c368f2efe848ecf6b073a36767800805e9eef2b1857d5f984f036eb6df891d75f72d9b154518c1cd58835286d1da9a38deba3de98b5a53e5ed78a84976 SHAKE-128('hello', 256) = 4a361de3a0e980a55388df742e9b314bd69d918260d9247768d0221df5262380 SHAKE-256('hello', 160) = 1234075ae4a1e77316cf2d8000974581a343b9eb 4. BLAKE2 / BLAKE2s / BLAKE2bBLAKE / BLAKE2 / BLAKE2s / BLAKE2b 是一系列快速、高度安全的密码学哈希函数，提供 160 位、224 位、256 位、384 位和 512 位摘要大小的计算，在现代密码学中被广泛应用。BLAKE 进入了SHA3 NIST 比赛的决赛。 BLAKE2 函数是 BLAKE 的改进版本。 BLAKE2s（通常为 256 位）是 BLAKE2 实现，针对 32 位微处理器进行了性能优化。 BLAKE2b（通常为 512 位）是 BLAKE2 实现，针对 64 位微处理器进行了性能优化。 BLAKE2 哈希函数具有与 SHA-3 类似的安全强度，但开发人员目前仍然更倾向于使用 SHA2 和 SHA3。 BLAKE 哈希值的 Python 示例： import hashlib, binascii text = 'hello' data = text.encode(\"utf8\") blake2s = hashlib.new('blake2s', data).digest() print(\"BLAKE2s({text}) = \", binascii.hexlify(blake2s).decode(\"utf-8\")) blake2b = hashlib.new('blake2b', data).digest() print(\"BLAKE2b({text}) = \", binascii.hexlify(blake2b).decode(\"utf-8\")) 输出如下： BLAKE2s('hello') = 19213bacc58dee6dbde3ceb9a47cbb330b3d86f8cca8997eb00be456f140ca25 BLAKE2b('hello') = e4cfa39a3d37be31c59609e807970799caa68a19bfaa15135f165085e01d41a65ba1e1b146aeb6bd0092b49eac214c103ccfa3a365954bbbe52f74a2b3620c94 5. RIPEMD-160RIPEMD-160, RIPE Message Digest 是一种安全哈希函数，发布于 1996 年，目前主要被应用在 PGP 和比特币中。 RIPEMD 的 160 位变体在实践中被广泛使用，而 RIPEMD-128、RIPEMD-256 和 RIPEMD-320 等其他变体并不流行，并且它们的安全优势具有争议。 建议优先使用 SHA-2 和 SHA-3 而不是 RIPEMD，因为它们输出的哈希值更长，抗碰撞能力更强。 Python 示例： import hashlib, binascii text = 'hello' data = text.encode(\"utf8\") ripemd160 = hashlib.new('ripemd160', data).digest() print(\"RIPEMD-160({text}) = \", binascii.hexlify(ripemd160).decode(\"utf-8\")) # =\u003e RIPEMD-160({text}) = 108f07b8382412612c048d07d1","date":"2022-03-01","objectID":"/posts/practical-cryptography-basics-2-hash/:2:3","series":null,"tags":["Cryptography","Hash","密码学","哈希","散列","安全","SHA-2","SHA-3"],"title":"写给开发人员的实用密码学（二）—— 哈希函数","uri":"/posts/practical-cryptography-basics-2-hash/#安全的加密哈希算法"},{"categories":["tech"],"content":" 安全的加密哈希算法 1. SHA-2, SHA-256, SHA-512SHA-2，即 Secure Hash Algorithm 2，是一组强密码哈希函数，其成本包括：SHA-256（256位哈希）、SHA-384（384位哈希）、SHA-512（512位哈希）等。基于密码概念「Merkle–Damgård 构造」，目前被认为高度安全。 SHA-2 是 SHA-1 的继任者，于 2001 年在美国作为官方加密标准发布。 SHA-2 在软件开发和密码学中被广泛使用，可用于现代商业应用。 其中 SHA-256 被广泛用于 HTTPS 协议、文件完整性校验、比特币区块链等各种场景。 Python 代码示例： import hashlib, binascii text = 'hello' data = text.encode(\"utf8\") sha256hash = hashlib.sha256(data).digest() print(f\"SHA-256({text}) = \", binascii.hexlify(sha256hash).decode(\"utf8\")) sha384hash = hashlib.sha384(data).digest() print(f\"SHA-384({text}) = \", binascii.hexlify(sha384hash).decode(\"utf8\")) sha512hash = hashlib.sha512(data).digest() print(f\"SHA-512({text}) = \", binascii.hexlify(sha512hash).decode(\"utf8\")) 输出如下： SHA-256('hello') = 2cf24dba5fb0a30e26e83b2ac5b9e29e1b161e5c1fa7425e73043362938b9824 SHA-384('hello') = 59e1748777448c69de6b800d7a33bbfb9ff1b463e44354c3553bcdb9c666fa90125a3c79f90397bdf5f6a13de828684f SHA-512('hello') = 9b71d224bd62f3785d96d46ad3ea3d73319bfbc2890caadae2dff72519673ca72323c3d99ba5c11d7c7acc6e14b8c5da0c4663475c2e5c3adef46f73bcdec043 2. 更长的哈希值 == 更高的抗碰撞能力按照设计，哈希函数的输出越长，就有望实现更高的安全性和抗碰撞能力（但也有一些例外）。 一般来说，128 位哈希算法比 256 位哈希算法弱，256 位哈希算法比 512 位哈希算法弱。 因此显然 SHA-512 比 SHA-256 更强。我们可以预期，SHA-512 的碰撞概率要比 SHA-256 更低。 3. SHA-3, SHA3-256, SHA3-512, Keccak-256在输出的哈希长度相同时，SHA-3（及其变体 SHA3-224、SHA3-256、SHA3-384、SHA3-512）被认为拥有比 SHA-2（SHA-224、SHA-256、SHA-384、SHA-512）更高的加密强度。 例如，对于相同的哈希长度（256 位），SHA3-256 提供比 SHA-256 更高的加密强度。 SHA-3 系列函数是 Keccak 哈希家族的代表，它基于密码学概念海绵函数。而 Keccak 是SHA3 NIST 比赛的冠军。 与 SHA-2 不同，SHA-3 系列加密哈希函数不易受到长度拓展攻击 Length extension attack. SHA-3 被认为是高度安全的，并于 2015 年作为美国官方推荐的加密标准发布。 以太坊（Ethereum）区块链中使用的哈希函数 Keccak-256 是 SHA3-256 的变体，在代码中更改了一些常量。 哈希函数 SHAKE128(msg, length) 和 SHAKE256(msg, length) 是 SHA3-256 和 SHA3-512 算法的变体，它们输出消息的长度可以变化。 SHA3 的 Python 代码示例： import hashlib, binascii text = 'hello' data = text.encode(\"utf8\") sha3_256hash = hashlib.sha3_256(data).digest() print(f\"SHA3-256({text}) = \", binascii.hexlify(sha3_256hash).decode(\"utf8\")) sha3_512hash = hashlib.sha3_512(data).digest() print(f\"SHA3-512({text}) = \", binascii.hexlify(sha3_512hash).decode(\"utf8\")) 输出： SHA3-256('hello') = 3338be694f50c5f338814986cdf0686453a888b84f424d792af4b9202398f392 Keccak-256('hello') = 1c8aff950685c2ed4bc3174f3472287b56d9517b9c948127319a09a7a36deac8 SHA3-512('hello') = 75d527c368f2efe848ecf6b073a36767800805e9eef2b1857d5f984f036eb6df891d75f72d9b154518c1cd58835286d1da9a38deba3de98b5a53e5ed78a84976 SHAKE-128('hello', 256) = 4a361de3a0e980a55388df742e9b314bd69d918260d9247768d0221df5262380 SHAKE-256('hello', 160) = 1234075ae4a1e77316cf2d8000974581a343b9eb 4. BLAKE2 / BLAKE2s / BLAKE2bBLAKE / BLAKE2 / BLAKE2s / BLAKE2b 是一系列快速、高度安全的密码学哈希函数，提供 160 位、224 位、256 位、384 位和 512 位摘要大小的计算，在现代密码学中被广泛应用。BLAKE 进入了SHA3 NIST 比赛的决赛。 BLAKE2 函数是 BLAKE 的改进版本。 BLAKE2s（通常为 256 位）是 BLAKE2 实现，针对 32 位微处理器进行了性能优化。 BLAKE2b（通常为 512 位）是 BLAKE2 实现，针对 64 位微处理器进行了性能优化。 BLAKE2 哈希函数具有与 SHA-3 类似的安全强度，但开发人员目前仍然更倾向于使用 SHA2 和 SHA3。 BLAKE 哈希值的 Python 示例： import hashlib, binascii text = 'hello' data = text.encode(\"utf8\") blake2s = hashlib.new('blake2s', data).digest() print(\"BLAKE2s({text}) = \", binascii.hexlify(blake2s).decode(\"utf-8\")) blake2b = hashlib.new('blake2b', data).digest() print(\"BLAKE2b({text}) = \", binascii.hexlify(blake2b).decode(\"utf-8\")) 输出如下： BLAKE2s('hello') = 19213bacc58dee6dbde3ceb9a47cbb330b3d86f8cca8997eb00be456f140ca25 BLAKE2b('hello') = e4cfa39a3d37be31c59609e807970799caa68a19bfaa15135f165085e01d41a65ba1e1b146aeb6bd0092b49eac214c103ccfa3a365954bbbe52f74a2b3620c94 5. RIPEMD-160RIPEMD-160, RIPE Message Digest 是一种安全哈希函数，发布于 1996 年，目前主要被应用在 PGP 和比特币中。 RIPEMD 的 160 位变体在实践中被广泛使用，而 RIPEMD-128、RIPEMD-256 和 RIPEMD-320 等其他变体并不流行，并且它们的安全优势具有争议。 建议优先使用 SHA-2 和 SHA-3 而不是 RIPEMD，因为它们输出的哈希值更长，抗碰撞能力更强。 Python 示例： import hashlib, binascii text = 'hello' data = text.encode(\"utf8\") ripemd160 = hashlib.new('ripemd160', data).digest() print(\"RIPEMD-160({text}) = \", binascii.hexlify(ripemd160).decode(\"utf-8\")) # =\u003e RIPEMD-160({text}) = 108f07b8382412612c048d07d1","date":"2022-03-01","objectID":"/posts/practical-cryptography-basics-2-hash/:2:3","series":null,"tags":["Cryptography","Hash","密码学","哈希","散列","安全","SHA-2","SHA-3"],"title":"写给开发人员的实用密码学（二）—— 哈希函数","uri":"/posts/practical-cryptography-basics-2-hash/#1-sha-2-sha-256-sha-512"},{"categories":["tech"],"content":" 安全的加密哈希算法 1. SHA-2, SHA-256, SHA-512SHA-2，即 Secure Hash Algorithm 2，是一组强密码哈希函数，其成本包括：SHA-256（256位哈希）、SHA-384（384位哈希）、SHA-512（512位哈希）等。基于密码概念「Merkle–Damgård 构造」，目前被认为高度安全。 SHA-2 是 SHA-1 的继任者，于 2001 年在美国作为官方加密标准发布。 SHA-2 在软件开发和密码学中被广泛使用，可用于现代商业应用。 其中 SHA-256 被广泛用于 HTTPS 协议、文件完整性校验、比特币区块链等各种场景。 Python 代码示例： import hashlib, binascii text = 'hello' data = text.encode(\"utf8\") sha256hash = hashlib.sha256(data).digest() print(f\"SHA-256({text}) = \", binascii.hexlify(sha256hash).decode(\"utf8\")) sha384hash = hashlib.sha384(data).digest() print(f\"SHA-384({text}) = \", binascii.hexlify(sha384hash).decode(\"utf8\")) sha512hash = hashlib.sha512(data).digest() print(f\"SHA-512({text}) = \", binascii.hexlify(sha512hash).decode(\"utf8\")) 输出如下： SHA-256('hello') = 2cf24dba5fb0a30e26e83b2ac5b9e29e1b161e5c1fa7425e73043362938b9824 SHA-384('hello') = 59e1748777448c69de6b800d7a33bbfb9ff1b463e44354c3553bcdb9c666fa90125a3c79f90397bdf5f6a13de828684f SHA-512('hello') = 9b71d224bd62f3785d96d46ad3ea3d73319bfbc2890caadae2dff72519673ca72323c3d99ba5c11d7c7acc6e14b8c5da0c4663475c2e5c3adef46f73bcdec043 2. 更长的哈希值 == 更高的抗碰撞能力按照设计，哈希函数的输出越长，就有望实现更高的安全性和抗碰撞能力（但也有一些例外）。 一般来说，128 位哈希算法比 256 位哈希算法弱，256 位哈希算法比 512 位哈希算法弱。 因此显然 SHA-512 比 SHA-256 更强。我们可以预期，SHA-512 的碰撞概率要比 SHA-256 更低。 3. SHA-3, SHA3-256, SHA3-512, Keccak-256在输出的哈希长度相同时，SHA-3（及其变体 SHA3-224、SHA3-256、SHA3-384、SHA3-512）被认为拥有比 SHA-2（SHA-224、SHA-256、SHA-384、SHA-512）更高的加密强度。 例如，对于相同的哈希长度（256 位），SHA3-256 提供比 SHA-256 更高的加密强度。 SHA-3 系列函数是 Keccak 哈希家族的代表，它基于密码学概念海绵函数。而 Keccak 是SHA3 NIST 比赛的冠军。 与 SHA-2 不同，SHA-3 系列加密哈希函数不易受到长度拓展攻击 Length extension attack. SHA-3 被认为是高度安全的，并于 2015 年作为美国官方推荐的加密标准发布。 以太坊（Ethereum）区块链中使用的哈希函数 Keccak-256 是 SHA3-256 的变体，在代码中更改了一些常量。 哈希函数 SHAKE128(msg, length) 和 SHAKE256(msg, length) 是 SHA3-256 和 SHA3-512 算法的变体，它们输出消息的长度可以变化。 SHA3 的 Python 代码示例： import hashlib, binascii text = 'hello' data = text.encode(\"utf8\") sha3_256hash = hashlib.sha3_256(data).digest() print(f\"SHA3-256({text}) = \", binascii.hexlify(sha3_256hash).decode(\"utf8\")) sha3_512hash = hashlib.sha3_512(data).digest() print(f\"SHA3-512({text}) = \", binascii.hexlify(sha3_512hash).decode(\"utf8\")) 输出： SHA3-256('hello') = 3338be694f50c5f338814986cdf0686453a888b84f424d792af4b9202398f392 Keccak-256('hello') = 1c8aff950685c2ed4bc3174f3472287b56d9517b9c948127319a09a7a36deac8 SHA3-512('hello') = 75d527c368f2efe848ecf6b073a36767800805e9eef2b1857d5f984f036eb6df891d75f72d9b154518c1cd58835286d1da9a38deba3de98b5a53e5ed78a84976 SHAKE-128('hello', 256) = 4a361de3a0e980a55388df742e9b314bd69d918260d9247768d0221df5262380 SHAKE-256('hello', 160) = 1234075ae4a1e77316cf2d8000974581a343b9eb 4. BLAKE2 / BLAKE2s / BLAKE2bBLAKE / BLAKE2 / BLAKE2s / BLAKE2b 是一系列快速、高度安全的密码学哈希函数，提供 160 位、224 位、256 位、384 位和 512 位摘要大小的计算，在现代密码学中被广泛应用。BLAKE 进入了SHA3 NIST 比赛的决赛。 BLAKE2 函数是 BLAKE 的改进版本。 BLAKE2s（通常为 256 位）是 BLAKE2 实现，针对 32 位微处理器进行了性能优化。 BLAKE2b（通常为 512 位）是 BLAKE2 实现，针对 64 位微处理器进行了性能优化。 BLAKE2 哈希函数具有与 SHA-3 类似的安全强度，但开发人员目前仍然更倾向于使用 SHA2 和 SHA3。 BLAKE 哈希值的 Python 示例： import hashlib, binascii text = 'hello' data = text.encode(\"utf8\") blake2s = hashlib.new('blake2s', data).digest() print(\"BLAKE2s({text}) = \", binascii.hexlify(blake2s).decode(\"utf-8\")) blake2b = hashlib.new('blake2b', data).digest() print(\"BLAKE2b({text}) = \", binascii.hexlify(blake2b).decode(\"utf-8\")) 输出如下： BLAKE2s('hello') = 19213bacc58dee6dbde3ceb9a47cbb330b3d86f8cca8997eb00be456f140ca25 BLAKE2b('hello') = e4cfa39a3d37be31c59609e807970799caa68a19bfaa15135f165085e01d41a65ba1e1b146aeb6bd0092b49eac214c103ccfa3a365954bbbe52f74a2b3620c94 5. RIPEMD-160RIPEMD-160, RIPE Message Digest 是一种安全哈希函数，发布于 1996 年，目前主要被应用在 PGP 和比特币中。 RIPEMD 的 160 位变体在实践中被广泛使用，而 RIPEMD-128、RIPEMD-256 和 RIPEMD-320 等其他变体并不流行，并且它们的安全优势具有争议。 建议优先使用 SHA-2 和 SHA-3 而不是 RIPEMD，因为它们输出的哈希值更长，抗碰撞能力更强。 Python 示例： import hashlib, binascii text = 'hello' data = text.encode(\"utf8\") ripemd160 = hashlib.new('ripemd160', data).digest() print(\"RIPEMD-160({text}) = \", binascii.hexlify(ripemd160).decode(\"utf-8\")) # =\u003e RIPEMD-160({text}) = 108f07b8382412612c048d07d1","date":"2022-03-01","objectID":"/posts/practical-cryptography-basics-2-hash/:2:3","series":null,"tags":["Cryptography","Hash","密码学","哈希","散列","安全","SHA-2","SHA-3"],"title":"写给开发人员的实用密码学（二）—— 哈希函数","uri":"/posts/practical-cryptography-basics-2-hash/#2-更长的哈希值--更高的抗碰撞能力"},{"categories":["tech"],"content":" 安全的加密哈希算法 1. SHA-2, SHA-256, SHA-512SHA-2，即 Secure Hash Algorithm 2，是一组强密码哈希函数，其成本包括：SHA-256（256位哈希）、SHA-384（384位哈希）、SHA-512（512位哈希）等。基于密码概念「Merkle–Damgård 构造」，目前被认为高度安全。 SHA-2 是 SHA-1 的继任者，于 2001 年在美国作为官方加密标准发布。 SHA-2 在软件开发和密码学中被广泛使用，可用于现代商业应用。 其中 SHA-256 被广泛用于 HTTPS 协议、文件完整性校验、比特币区块链等各种场景。 Python 代码示例： import hashlib, binascii text = 'hello' data = text.encode(\"utf8\") sha256hash = hashlib.sha256(data).digest() print(f\"SHA-256({text}) = \", binascii.hexlify(sha256hash).decode(\"utf8\")) sha384hash = hashlib.sha384(data).digest() print(f\"SHA-384({text}) = \", binascii.hexlify(sha384hash).decode(\"utf8\")) sha512hash = hashlib.sha512(data).digest() print(f\"SHA-512({text}) = \", binascii.hexlify(sha512hash).decode(\"utf8\")) 输出如下： SHA-256('hello') = 2cf24dba5fb0a30e26e83b2ac5b9e29e1b161e5c1fa7425e73043362938b9824 SHA-384('hello') = 59e1748777448c69de6b800d7a33bbfb9ff1b463e44354c3553bcdb9c666fa90125a3c79f90397bdf5f6a13de828684f SHA-512('hello') = 9b71d224bd62f3785d96d46ad3ea3d73319bfbc2890caadae2dff72519673ca72323c3d99ba5c11d7c7acc6e14b8c5da0c4663475c2e5c3adef46f73bcdec043 2. 更长的哈希值 == 更高的抗碰撞能力按照设计，哈希函数的输出越长，就有望实现更高的安全性和抗碰撞能力（但也有一些例外）。 一般来说，128 位哈希算法比 256 位哈希算法弱，256 位哈希算法比 512 位哈希算法弱。 因此显然 SHA-512 比 SHA-256 更强。我们可以预期，SHA-512 的碰撞概率要比 SHA-256 更低。 3. SHA-3, SHA3-256, SHA3-512, Keccak-256在输出的哈希长度相同时，SHA-3（及其变体 SHA3-224、SHA3-256、SHA3-384、SHA3-512）被认为拥有比 SHA-2（SHA-224、SHA-256、SHA-384、SHA-512）更高的加密强度。 例如，对于相同的哈希长度（256 位），SHA3-256 提供比 SHA-256 更高的加密强度。 SHA-3 系列函数是 Keccak 哈希家族的代表，它基于密码学概念海绵函数。而 Keccak 是SHA3 NIST 比赛的冠军。 与 SHA-2 不同，SHA-3 系列加密哈希函数不易受到长度拓展攻击 Length extension attack. SHA-3 被认为是高度安全的，并于 2015 年作为美国官方推荐的加密标准发布。 以太坊（Ethereum）区块链中使用的哈希函数 Keccak-256 是 SHA3-256 的变体，在代码中更改了一些常量。 哈希函数 SHAKE128(msg, length) 和 SHAKE256(msg, length) 是 SHA3-256 和 SHA3-512 算法的变体，它们输出消息的长度可以变化。 SHA3 的 Python 代码示例： import hashlib, binascii text = 'hello' data = text.encode(\"utf8\") sha3_256hash = hashlib.sha3_256(data).digest() print(f\"SHA3-256({text}) = \", binascii.hexlify(sha3_256hash).decode(\"utf8\")) sha3_512hash = hashlib.sha3_512(data).digest() print(f\"SHA3-512({text}) = \", binascii.hexlify(sha3_512hash).decode(\"utf8\")) 输出： SHA3-256('hello') = 3338be694f50c5f338814986cdf0686453a888b84f424d792af4b9202398f392 Keccak-256('hello') = 1c8aff950685c2ed4bc3174f3472287b56d9517b9c948127319a09a7a36deac8 SHA3-512('hello') = 75d527c368f2efe848ecf6b073a36767800805e9eef2b1857d5f984f036eb6df891d75f72d9b154518c1cd58835286d1da9a38deba3de98b5a53e5ed78a84976 SHAKE-128('hello', 256) = 4a361de3a0e980a55388df742e9b314bd69d918260d9247768d0221df5262380 SHAKE-256('hello', 160) = 1234075ae4a1e77316cf2d8000974581a343b9eb 4. BLAKE2 / BLAKE2s / BLAKE2bBLAKE / BLAKE2 / BLAKE2s / BLAKE2b 是一系列快速、高度安全的密码学哈希函数，提供 160 位、224 位、256 位、384 位和 512 位摘要大小的计算，在现代密码学中被广泛应用。BLAKE 进入了SHA3 NIST 比赛的决赛。 BLAKE2 函数是 BLAKE 的改进版本。 BLAKE2s（通常为 256 位）是 BLAKE2 实现，针对 32 位微处理器进行了性能优化。 BLAKE2b（通常为 512 位）是 BLAKE2 实现，针对 64 位微处理器进行了性能优化。 BLAKE2 哈希函数具有与 SHA-3 类似的安全强度，但开发人员目前仍然更倾向于使用 SHA2 和 SHA3。 BLAKE 哈希值的 Python 示例： import hashlib, binascii text = 'hello' data = text.encode(\"utf8\") blake2s = hashlib.new('blake2s', data).digest() print(\"BLAKE2s({text}) = \", binascii.hexlify(blake2s).decode(\"utf-8\")) blake2b = hashlib.new('blake2b', data).digest() print(\"BLAKE2b({text}) = \", binascii.hexlify(blake2b).decode(\"utf-8\")) 输出如下： BLAKE2s('hello') = 19213bacc58dee6dbde3ceb9a47cbb330b3d86f8cca8997eb00be456f140ca25 BLAKE2b('hello') = e4cfa39a3d37be31c59609e807970799caa68a19bfaa15135f165085e01d41a65ba1e1b146aeb6bd0092b49eac214c103ccfa3a365954bbbe52f74a2b3620c94 5. RIPEMD-160RIPEMD-160, RIPE Message Digest 是一种安全哈希函数，发布于 1996 年，目前主要被应用在 PGP 和比特币中。 RIPEMD 的 160 位变体在实践中被广泛使用，而 RIPEMD-128、RIPEMD-256 和 RIPEMD-320 等其他变体并不流行，并且它们的安全优势具有争议。 建议优先使用 SHA-2 和 SHA-3 而不是 RIPEMD，因为它们输出的哈希值更长，抗碰撞能力更强。 Python 示例： import hashlib, binascii text = 'hello' data = text.encode(\"utf8\") ripemd160 = hashlib.new('ripemd160', data).digest() print(\"RIPEMD-160({text}) = \", binascii.hexlify(ripemd160).decode(\"utf-8\")) # =\u003e RIPEMD-160({text}) = 108f07b8382412612c048d07d1","date":"2022-03-01","objectID":"/posts/practical-cryptography-basics-2-hash/:2:3","series":null,"tags":["Cryptography","Hash","密码学","哈希","散列","安全","SHA-2","SHA-3"],"title":"写给开发人员的实用密码学（二）—— 哈希函数","uri":"/posts/practical-cryptography-basics-2-hash/#3-sha-3-sha3-256-sha3-512-keccak-256"},{"categories":["tech"],"content":" 安全的加密哈希算法 1. SHA-2, SHA-256, SHA-512SHA-2，即 Secure Hash Algorithm 2，是一组强密码哈希函数，其成本包括：SHA-256（256位哈希）、SHA-384（384位哈希）、SHA-512（512位哈希）等。基于密码概念「Merkle–Damgård 构造」，目前被认为高度安全。 SHA-2 是 SHA-1 的继任者，于 2001 年在美国作为官方加密标准发布。 SHA-2 在软件开发和密码学中被广泛使用，可用于现代商业应用。 其中 SHA-256 被广泛用于 HTTPS 协议、文件完整性校验、比特币区块链等各种场景。 Python 代码示例： import hashlib, binascii text = 'hello' data = text.encode(\"utf8\") sha256hash = hashlib.sha256(data).digest() print(f\"SHA-256({text}) = \", binascii.hexlify(sha256hash).decode(\"utf8\")) sha384hash = hashlib.sha384(data).digest() print(f\"SHA-384({text}) = \", binascii.hexlify(sha384hash).decode(\"utf8\")) sha512hash = hashlib.sha512(data).digest() print(f\"SHA-512({text}) = \", binascii.hexlify(sha512hash).decode(\"utf8\")) 输出如下： SHA-256('hello') = 2cf24dba5fb0a30e26e83b2ac5b9e29e1b161e5c1fa7425e73043362938b9824 SHA-384('hello') = 59e1748777448c69de6b800d7a33bbfb9ff1b463e44354c3553bcdb9c666fa90125a3c79f90397bdf5f6a13de828684f SHA-512('hello') = 9b71d224bd62f3785d96d46ad3ea3d73319bfbc2890caadae2dff72519673ca72323c3d99ba5c11d7c7acc6e14b8c5da0c4663475c2e5c3adef46f73bcdec043 2. 更长的哈希值 == 更高的抗碰撞能力按照设计，哈希函数的输出越长，就有望实现更高的安全性和抗碰撞能力（但也有一些例外）。 一般来说，128 位哈希算法比 256 位哈希算法弱，256 位哈希算法比 512 位哈希算法弱。 因此显然 SHA-512 比 SHA-256 更强。我们可以预期，SHA-512 的碰撞概率要比 SHA-256 更低。 3. SHA-3, SHA3-256, SHA3-512, Keccak-256在输出的哈希长度相同时，SHA-3（及其变体 SHA3-224、SHA3-256、SHA3-384、SHA3-512）被认为拥有比 SHA-2（SHA-224、SHA-256、SHA-384、SHA-512）更高的加密强度。 例如，对于相同的哈希长度（256 位），SHA3-256 提供比 SHA-256 更高的加密强度。 SHA-3 系列函数是 Keccak 哈希家族的代表，它基于密码学概念海绵函数。而 Keccak 是SHA3 NIST 比赛的冠军。 与 SHA-2 不同，SHA-3 系列加密哈希函数不易受到长度拓展攻击 Length extension attack. SHA-3 被认为是高度安全的，并于 2015 年作为美国官方推荐的加密标准发布。 以太坊（Ethereum）区块链中使用的哈希函数 Keccak-256 是 SHA3-256 的变体，在代码中更改了一些常量。 哈希函数 SHAKE128(msg, length) 和 SHAKE256(msg, length) 是 SHA3-256 和 SHA3-512 算法的变体，它们输出消息的长度可以变化。 SHA3 的 Python 代码示例： import hashlib, binascii text = 'hello' data = text.encode(\"utf8\") sha3_256hash = hashlib.sha3_256(data).digest() print(f\"SHA3-256({text}) = \", binascii.hexlify(sha3_256hash).decode(\"utf8\")) sha3_512hash = hashlib.sha3_512(data).digest() print(f\"SHA3-512({text}) = \", binascii.hexlify(sha3_512hash).decode(\"utf8\")) 输出： SHA3-256('hello') = 3338be694f50c5f338814986cdf0686453a888b84f424d792af4b9202398f392 Keccak-256('hello') = 1c8aff950685c2ed4bc3174f3472287b56d9517b9c948127319a09a7a36deac8 SHA3-512('hello') = 75d527c368f2efe848ecf6b073a36767800805e9eef2b1857d5f984f036eb6df891d75f72d9b154518c1cd58835286d1da9a38deba3de98b5a53e5ed78a84976 SHAKE-128('hello', 256) = 4a361de3a0e980a55388df742e9b314bd69d918260d9247768d0221df5262380 SHAKE-256('hello', 160) = 1234075ae4a1e77316cf2d8000974581a343b9eb 4. BLAKE2 / BLAKE2s / BLAKE2bBLAKE / BLAKE2 / BLAKE2s / BLAKE2b 是一系列快速、高度安全的密码学哈希函数，提供 160 位、224 位、256 位、384 位和 512 位摘要大小的计算，在现代密码学中被广泛应用。BLAKE 进入了SHA3 NIST 比赛的决赛。 BLAKE2 函数是 BLAKE 的改进版本。 BLAKE2s（通常为 256 位）是 BLAKE2 实现，针对 32 位微处理器进行了性能优化。 BLAKE2b（通常为 512 位）是 BLAKE2 实现，针对 64 位微处理器进行了性能优化。 BLAKE2 哈希函数具有与 SHA-3 类似的安全强度，但开发人员目前仍然更倾向于使用 SHA2 和 SHA3。 BLAKE 哈希值的 Python 示例： import hashlib, binascii text = 'hello' data = text.encode(\"utf8\") blake2s = hashlib.new('blake2s', data).digest() print(\"BLAKE2s({text}) = \", binascii.hexlify(blake2s).decode(\"utf-8\")) blake2b = hashlib.new('blake2b', data).digest() print(\"BLAKE2b({text}) = \", binascii.hexlify(blake2b).decode(\"utf-8\")) 输出如下： BLAKE2s('hello') = 19213bacc58dee6dbde3ceb9a47cbb330b3d86f8cca8997eb00be456f140ca25 BLAKE2b('hello') = e4cfa39a3d37be31c59609e807970799caa68a19bfaa15135f165085e01d41a65ba1e1b146aeb6bd0092b49eac214c103ccfa3a365954bbbe52f74a2b3620c94 5. RIPEMD-160RIPEMD-160, RIPE Message Digest 是一种安全哈希函数，发布于 1996 年，目前主要被应用在 PGP 和比特币中。 RIPEMD 的 160 位变体在实践中被广泛使用，而 RIPEMD-128、RIPEMD-256 和 RIPEMD-320 等其他变体并不流行，并且它们的安全优势具有争议。 建议优先使用 SHA-2 和 SHA-3 而不是 RIPEMD，因为它们输出的哈希值更长，抗碰撞能力更强。 Python 示例： import hashlib, binascii text = 'hello' data = text.encode(\"utf8\") ripemd160 = hashlib.new('ripemd160', data).digest() print(\"RIPEMD-160({text}) = \", binascii.hexlify(ripemd160).decode(\"utf-8\")) # =\u003e RIPEMD-160({text}) = 108f07b8382412612c048d07d1","date":"2022-03-01","objectID":"/posts/practical-cryptography-basics-2-hash/:2:3","series":null,"tags":["Cryptography","Hash","密码学","哈希","散列","安全","SHA-2","SHA-3"],"title":"写给开发人员的实用密码学（二）—— 哈希函数","uri":"/posts/practical-cryptography-basics-2-hash/#4-blake2--blake2s--blake2b"},{"categories":["tech"],"content":" 安全的加密哈希算法 1. SHA-2, SHA-256, SHA-512SHA-2，即 Secure Hash Algorithm 2，是一组强密码哈希函数，其成本包括：SHA-256（256位哈希）、SHA-384（384位哈希）、SHA-512（512位哈希）等。基于密码概念「Merkle–Damgård 构造」，目前被认为高度安全。 SHA-2 是 SHA-1 的继任者，于 2001 年在美国作为官方加密标准发布。 SHA-2 在软件开发和密码学中被广泛使用，可用于现代商业应用。 其中 SHA-256 被广泛用于 HTTPS 协议、文件完整性校验、比特币区块链等各种场景。 Python 代码示例： import hashlib, binascii text = 'hello' data = text.encode(\"utf8\") sha256hash = hashlib.sha256(data).digest() print(f\"SHA-256({text}) = \", binascii.hexlify(sha256hash).decode(\"utf8\")) sha384hash = hashlib.sha384(data).digest() print(f\"SHA-384({text}) = \", binascii.hexlify(sha384hash).decode(\"utf8\")) sha512hash = hashlib.sha512(data).digest() print(f\"SHA-512({text}) = \", binascii.hexlify(sha512hash).decode(\"utf8\")) 输出如下： SHA-256('hello') = 2cf24dba5fb0a30e26e83b2ac5b9e29e1b161e5c1fa7425e73043362938b9824 SHA-384('hello') = 59e1748777448c69de6b800d7a33bbfb9ff1b463e44354c3553bcdb9c666fa90125a3c79f90397bdf5f6a13de828684f SHA-512('hello') = 9b71d224bd62f3785d96d46ad3ea3d73319bfbc2890caadae2dff72519673ca72323c3d99ba5c11d7c7acc6e14b8c5da0c4663475c2e5c3adef46f73bcdec043 2. 更长的哈希值 == 更高的抗碰撞能力按照设计，哈希函数的输出越长，就有望实现更高的安全性和抗碰撞能力（但也有一些例外）。 一般来说，128 位哈希算法比 256 位哈希算法弱，256 位哈希算法比 512 位哈希算法弱。 因此显然 SHA-512 比 SHA-256 更强。我们可以预期，SHA-512 的碰撞概率要比 SHA-256 更低。 3. SHA-3, SHA3-256, SHA3-512, Keccak-256在输出的哈希长度相同时，SHA-3（及其变体 SHA3-224、SHA3-256、SHA3-384、SHA3-512）被认为拥有比 SHA-2（SHA-224、SHA-256、SHA-384、SHA-512）更高的加密强度。 例如，对于相同的哈希长度（256 位），SHA3-256 提供比 SHA-256 更高的加密强度。 SHA-3 系列函数是 Keccak 哈希家族的代表，它基于密码学概念海绵函数。而 Keccak 是SHA3 NIST 比赛的冠军。 与 SHA-2 不同，SHA-3 系列加密哈希函数不易受到长度拓展攻击 Length extension attack. SHA-3 被认为是高度安全的，并于 2015 年作为美国官方推荐的加密标准发布。 以太坊（Ethereum）区块链中使用的哈希函数 Keccak-256 是 SHA3-256 的变体，在代码中更改了一些常量。 哈希函数 SHAKE128(msg, length) 和 SHAKE256(msg, length) 是 SHA3-256 和 SHA3-512 算法的变体，它们输出消息的长度可以变化。 SHA3 的 Python 代码示例： import hashlib, binascii text = 'hello' data = text.encode(\"utf8\") sha3_256hash = hashlib.sha3_256(data).digest() print(f\"SHA3-256({text}) = \", binascii.hexlify(sha3_256hash).decode(\"utf8\")) sha3_512hash = hashlib.sha3_512(data).digest() print(f\"SHA3-512({text}) = \", binascii.hexlify(sha3_512hash).decode(\"utf8\")) 输出： SHA3-256('hello') = 3338be694f50c5f338814986cdf0686453a888b84f424d792af4b9202398f392 Keccak-256('hello') = 1c8aff950685c2ed4bc3174f3472287b56d9517b9c948127319a09a7a36deac8 SHA3-512('hello') = 75d527c368f2efe848ecf6b073a36767800805e9eef2b1857d5f984f036eb6df891d75f72d9b154518c1cd58835286d1da9a38deba3de98b5a53e5ed78a84976 SHAKE-128('hello', 256) = 4a361de3a0e980a55388df742e9b314bd69d918260d9247768d0221df5262380 SHAKE-256('hello', 160) = 1234075ae4a1e77316cf2d8000974581a343b9eb 4. BLAKE2 / BLAKE2s / BLAKE2bBLAKE / BLAKE2 / BLAKE2s / BLAKE2b 是一系列快速、高度安全的密码学哈希函数，提供 160 位、224 位、256 位、384 位和 512 位摘要大小的计算，在现代密码学中被广泛应用。BLAKE 进入了SHA3 NIST 比赛的决赛。 BLAKE2 函数是 BLAKE 的改进版本。 BLAKE2s（通常为 256 位）是 BLAKE2 实现，针对 32 位微处理器进行了性能优化。 BLAKE2b（通常为 512 位）是 BLAKE2 实现，针对 64 位微处理器进行了性能优化。 BLAKE2 哈希函数具有与 SHA-3 类似的安全强度，但开发人员目前仍然更倾向于使用 SHA2 和 SHA3。 BLAKE 哈希值的 Python 示例： import hashlib, binascii text = 'hello' data = text.encode(\"utf8\") blake2s = hashlib.new('blake2s', data).digest() print(\"BLAKE2s({text}) = \", binascii.hexlify(blake2s).decode(\"utf-8\")) blake2b = hashlib.new('blake2b', data).digest() print(\"BLAKE2b({text}) = \", binascii.hexlify(blake2b).decode(\"utf-8\")) 输出如下： BLAKE2s('hello') = 19213bacc58dee6dbde3ceb9a47cbb330b3d86f8cca8997eb00be456f140ca25 BLAKE2b('hello') = e4cfa39a3d37be31c59609e807970799caa68a19bfaa15135f165085e01d41a65ba1e1b146aeb6bd0092b49eac214c103ccfa3a365954bbbe52f74a2b3620c94 5. RIPEMD-160RIPEMD-160, RIPE Message Digest 是一种安全哈希函数，发布于 1996 年，目前主要被应用在 PGP 和比特币中。 RIPEMD 的 160 位变体在实践中被广泛使用，而 RIPEMD-128、RIPEMD-256 和 RIPEMD-320 等其他变体并不流行，并且它们的安全优势具有争议。 建议优先使用 SHA-2 和 SHA-3 而不是 RIPEMD，因为它们输出的哈希值更长，抗碰撞能力更强。 Python 示例： import hashlib, binascii text = 'hello' data = text.encode(\"utf8\") ripemd160 = hashlib.new('ripemd160', data).digest() print(\"RIPEMD-160({text}) = \", binascii.hexlify(ripemd160).decode(\"utf-8\")) # =\u003e RIPEMD-160({text}) = 108f07b8382412612c048d07d1","date":"2022-03-01","objectID":"/posts/practical-cryptography-basics-2-hash/:2:3","series":null,"tags":["Cryptography","Hash","密码学","哈希","散列","安全","SHA-2","SHA-3"],"title":"写给开发人员的实用密码学（二）—— 哈希函数","uri":"/posts/practical-cryptography-basics-2-hash/#5-ripemd-160"},{"categories":["tech"],"content":" 安全的加密哈希算法 1. SHA-2, SHA-256, SHA-512SHA-2，即 Secure Hash Algorithm 2，是一组强密码哈希函数，其成本包括：SHA-256（256位哈希）、SHA-384（384位哈希）、SHA-512（512位哈希）等。基于密码概念「Merkle–Damgård 构造」，目前被认为高度安全。 SHA-2 是 SHA-1 的继任者，于 2001 年在美国作为官方加密标准发布。 SHA-2 在软件开发和密码学中被广泛使用，可用于现代商业应用。 其中 SHA-256 被广泛用于 HTTPS 协议、文件完整性校验、比特币区块链等各种场景。 Python 代码示例： import hashlib, binascii text = 'hello' data = text.encode(\"utf8\") sha256hash = hashlib.sha256(data).digest() print(f\"SHA-256({text}) = \", binascii.hexlify(sha256hash).decode(\"utf8\")) sha384hash = hashlib.sha384(data).digest() print(f\"SHA-384({text}) = \", binascii.hexlify(sha384hash).decode(\"utf8\")) sha512hash = hashlib.sha512(data).digest() print(f\"SHA-512({text}) = \", binascii.hexlify(sha512hash).decode(\"utf8\")) 输出如下： SHA-256('hello') = 2cf24dba5fb0a30e26e83b2ac5b9e29e1b161e5c1fa7425e73043362938b9824 SHA-384('hello') = 59e1748777448c69de6b800d7a33bbfb9ff1b463e44354c3553bcdb9c666fa90125a3c79f90397bdf5f6a13de828684f SHA-512('hello') = 9b71d224bd62f3785d96d46ad3ea3d73319bfbc2890caadae2dff72519673ca72323c3d99ba5c11d7c7acc6e14b8c5da0c4663475c2e5c3adef46f73bcdec043 2. 更长的哈希值 == 更高的抗碰撞能力按照设计，哈希函数的输出越长，就有望实现更高的安全性和抗碰撞能力（但也有一些例外）。 一般来说，128 位哈希算法比 256 位哈希算法弱，256 位哈希算法比 512 位哈希算法弱。 因此显然 SHA-512 比 SHA-256 更强。我们可以预期，SHA-512 的碰撞概率要比 SHA-256 更低。 3. SHA-3, SHA3-256, SHA3-512, Keccak-256在输出的哈希长度相同时，SHA-3（及其变体 SHA3-224、SHA3-256、SHA3-384、SHA3-512）被认为拥有比 SHA-2（SHA-224、SHA-256、SHA-384、SHA-512）更高的加密强度。 例如，对于相同的哈希长度（256 位），SHA3-256 提供比 SHA-256 更高的加密强度。 SHA-3 系列函数是 Keccak 哈希家族的代表，它基于密码学概念海绵函数。而 Keccak 是SHA3 NIST 比赛的冠军。 与 SHA-2 不同，SHA-3 系列加密哈希函数不易受到长度拓展攻击 Length extension attack. SHA-3 被认为是高度安全的，并于 2015 年作为美国官方推荐的加密标准发布。 以太坊（Ethereum）区块链中使用的哈希函数 Keccak-256 是 SHA3-256 的变体，在代码中更改了一些常量。 哈希函数 SHAKE128(msg, length) 和 SHAKE256(msg, length) 是 SHA3-256 和 SHA3-512 算法的变体，它们输出消息的长度可以变化。 SHA3 的 Python 代码示例： import hashlib, binascii text = 'hello' data = text.encode(\"utf8\") sha3_256hash = hashlib.sha3_256(data).digest() print(f\"SHA3-256({text}) = \", binascii.hexlify(sha3_256hash).decode(\"utf8\")) sha3_512hash = hashlib.sha3_512(data).digest() print(f\"SHA3-512({text}) = \", binascii.hexlify(sha3_512hash).decode(\"utf8\")) 输出： SHA3-256('hello') = 3338be694f50c5f338814986cdf0686453a888b84f424d792af4b9202398f392 Keccak-256('hello') = 1c8aff950685c2ed4bc3174f3472287b56d9517b9c948127319a09a7a36deac8 SHA3-512('hello') = 75d527c368f2efe848ecf6b073a36767800805e9eef2b1857d5f984f036eb6df891d75f72d9b154518c1cd58835286d1da9a38deba3de98b5a53e5ed78a84976 SHAKE-128('hello', 256) = 4a361de3a0e980a55388df742e9b314bd69d918260d9247768d0221df5262380 SHAKE-256('hello', 160) = 1234075ae4a1e77316cf2d8000974581a343b9eb 4. BLAKE2 / BLAKE2s / BLAKE2bBLAKE / BLAKE2 / BLAKE2s / BLAKE2b 是一系列快速、高度安全的密码学哈希函数，提供 160 位、224 位、256 位、384 位和 512 位摘要大小的计算，在现代密码学中被广泛应用。BLAKE 进入了SHA3 NIST 比赛的决赛。 BLAKE2 函数是 BLAKE 的改进版本。 BLAKE2s（通常为 256 位）是 BLAKE2 实现，针对 32 位微处理器进行了性能优化。 BLAKE2b（通常为 512 位）是 BLAKE2 实现，针对 64 位微处理器进行了性能优化。 BLAKE2 哈希函数具有与 SHA-3 类似的安全强度，但开发人员目前仍然更倾向于使用 SHA2 和 SHA3。 BLAKE 哈希值的 Python 示例： import hashlib, binascii text = 'hello' data = text.encode(\"utf8\") blake2s = hashlib.new('blake2s', data).digest() print(\"BLAKE2s({text}) = \", binascii.hexlify(blake2s).decode(\"utf-8\")) blake2b = hashlib.new('blake2b', data).digest() print(\"BLAKE2b({text}) = \", binascii.hexlify(blake2b).decode(\"utf-8\")) 输出如下： BLAKE2s('hello') = 19213bacc58dee6dbde3ceb9a47cbb330b3d86f8cca8997eb00be456f140ca25 BLAKE2b('hello') = e4cfa39a3d37be31c59609e807970799caa68a19bfaa15135f165085e01d41a65ba1e1b146aeb6bd0092b49eac214c103ccfa3a365954bbbe52f74a2b3620c94 5. RIPEMD-160RIPEMD-160, RIPE Message Digest 是一种安全哈希函数，发布于 1996 年，目前主要被应用在 PGP 和比特币中。 RIPEMD 的 160 位变体在实践中被广泛使用，而 RIPEMD-128、RIPEMD-256 和 RIPEMD-320 等其他变体并不流行，并且它们的安全优势具有争议。 建议优先使用 SHA-2 和 SHA-3 而不是 RIPEMD，因为它们输出的哈希值更长，抗碰撞能力更强。 Python 示例： import hashlib, binascii text = 'hello' data = text.encode(\"utf8\") ripemd160 = hashlib.new('ripemd160', data).digest() print(\"RIPEMD-160({text}) = \", binascii.hexlify(ripemd160).decode(\"utf-8\")) # =\u003e RIPEMD-160({text}) = 108f07b8382412612c048d07d1","date":"2022-03-01","objectID":"/posts/practical-cryptography-basics-2-hash/:2:3","series":null,"tags":["Cryptography","Hash","密码学","哈希","散列","安全","SHA-2","SHA-3"],"title":"写给开发人员的实用密码学（二）—— 哈希函数","uri":"/posts/practical-cryptography-basics-2-hash/#6-其他安全哈希算法"},{"categories":["tech"],"content":" 不安全的加密哈希算法一些老一代的加密哈希算法，如 MD5, SHA-0 和 SHA-1 被认为是不安全的，并且都存在已被发现的加密漏洞（碰撞）。不要使用 MD5、SHA-0 和 SHA-1！这些哈希函数都已被证明不够安全。 使用这些不安全的哈希算法，可能会导致数字签名被伪造、密码泄漏等严重问题！ 另外也请避免使用以下被认为不安全或安全性有争议的哈希算法： MD2, MD4, MD5, SHA-0, SHA-1, Panama, HAVAL（有争议的安全性，在 HAVAL-128 上发现了碰撞），Tiger（有争议，已发现其弱点），SipHash（它属于非加密哈希函数）。 ","date":"2022-03-01","objectID":"/posts/practical-cryptography-basics-2-hash/:2:4","series":null,"tags":["Cryptography","Hash","密码学","哈希","散列","安全","SHA-2","SHA-3"],"title":"写给开发人员的实用密码学（二）—— 哈希函数","uri":"/posts/practical-cryptography-basics-2-hash/#不安全的加密哈希算法"},{"categories":["tech"],"content":" PoW 工作量证明哈希函数区块链中的 Proof-of-Work 工作量证明挖矿算法使用了一类特殊的哈希函数，这些函数是计算密集型和内存密集型的。 这些哈希函数被设计成需要消耗大量计算资源和大量内存，并且很难在硬件设备（例如集成电路或矿机）中实现，也就难以设计专用硬件来加速计算。这种哈希函数被称为抗 ASIC（ASIC-resistant）。 大部分工作量证明（Proof-of-Work）算法，都是要求计算出一个比特定值（称为挖掘难度）更大的哈希值。 因为哈希值是不可预测的，为了找出符合条件的哈希值，矿工需要计算数十亿个不同的哈希值，再从中找出最大的那个。 比如，一个工作量证明问题可能会被定义成这样：已有常数 x，要求找到一个数 p，使 hash(x + p) 的前十个比特都为 0. 有许多哈希函数是专为工作量证明挖掘算法设计的，例如 ETHash、Equihash、CryptoNight 和 Cookoo Cycle. 这些哈希函数的计算速度很慢，通常使用 GPU 硬件（如 NVIDIA GTX 1080 等显卡）或强大的 CPU 硬件（如 Intel Core i7-8700K）和大量快速 RAM 内存（如 DDR4 芯片）来执行这类算法。 这些挖矿算法的目标是通过刺激小型矿工（家庭用户和小型矿场）来最大限度地减少挖矿的集中化，并限制挖矿行业中高级玩家们（他们有能力建造巨型挖矿设施和数据中心）的力量。 与少数的高玩相比，大量小玩家意味着更好的去中心化。 目前大型虚拟货币挖矿公司手中的主要武器是 ASIC 矿机，因此，现代加密货币通常会要求使用「抗 ASIC 哈希算法」或「权益证明（proof-of-stake）共识协议」进行「工作量证明挖矿」，以限制这部分高级玩家，达成更好的去中心化。 因为工作量证明算法需要消耗大量能源，不够环保，以太坊等区块链已经声明未来将会升级到权益证明（Proof-of-S）这类更环保的算法。不过这里我们只关注 PoW 如何基于哈希函数实现的，不讨论这个。 1. ETHash这里简要说明下以太坊区块链中使用的 ETHash 工作量证明挖掘哈希函数背后的思想。 ETHash 是以太坊区块链中的工作量证明哈希函数。它是内存密集型哈希函数（需要大量 RAM 才能快速计算），因此它被认为是抗 ASIC 的。 ETHash 的工作流程： 基于直到当前区块的整个链，为每个区块计算一个「种子」 从种子中计算出一个 16 MB 的伪随机缓存 从缓存中提取 1 GB 数据集以用于挖掘 挖掘涉及将数据集的随机切片一起进行哈希 更多信息参见 eth.wiki - ethash 2. Equihash简要解释一下 Zcash、Bitcoin Gold 和其他一些区块链中使用的 Equihash 工作量证明挖掘哈希函数背后的思想。 Equihash 是 Zcash 和 Bitcoin Gold 区块链中的工作量证明哈希函数。它是内存密集型哈希函数（需要大量 RAM 才能进行快速计算），因此它被认为是抗 ASIC 的。 Equihash 的工作流程： 基于直到当前区块的整个链，使用 BLAKE2b 计算出 50 MB 哈希数据集 在生成的哈希数据集上解决「广义生日问题」（从 2097152 中挑选 512 个不同的字符串，使得它们的二进制 XOR 为零）。已知最佳的解决方案（瓦格纳算法）在指数时间内运行，因此它需要大量的内存密集型和计算密集型计算 对前面得到的结果，进行双 SHA256 计算得到最终结果，即 SHA256(SHA256(solution)) 更多信息参见 https://github.com/tromp/equihash ","date":"2022-03-01","objectID":"/posts/practical-cryptography-basics-2-hash/:2:5","series":null,"tags":["Cryptography","Hash","密码学","哈希","散列","安全","SHA-2","SHA-3"],"title":"写给开发人员的实用密码学（二）—— 哈希函数","uri":"/posts/practical-cryptography-basics-2-hash/#pow-工作量证明哈希函数"},{"categories":["tech"],"content":" PoW 工作量证明哈希函数区块链中的 Proof-of-Work 工作量证明挖矿算法使用了一类特殊的哈希函数，这些函数是计算密集型和内存密集型的。 这些哈希函数被设计成需要消耗大量计算资源和大量内存，并且很难在硬件设备（例如集成电路或矿机）中实现，也就难以设计专用硬件来加速计算。这种哈希函数被称为抗 ASIC（ASIC-resistant）。 大部分工作量证明（Proof-of-Work）算法，都是要求计算出一个比特定值（称为挖掘难度）更大的哈希值。 因为哈希值是不可预测的，为了找出符合条件的哈希值，矿工需要计算数十亿个不同的哈希值，再从中找出最大的那个。 比如，一个工作量证明问题可能会被定义成这样：已有常数 x，要求找到一个数 p，使 hash(x + p) 的前十个比特都为 0. 有许多哈希函数是专为工作量证明挖掘算法设计的，例如 ETHash、Equihash、CryptoNight 和 Cookoo Cycle. 这些哈希函数的计算速度很慢，通常使用 GPU 硬件（如 NVIDIA GTX 1080 等显卡）或强大的 CPU 硬件（如 Intel Core i7-8700K）和大量快速 RAM 内存（如 DDR4 芯片）来执行这类算法。 这些挖矿算法的目标是通过刺激小型矿工（家庭用户和小型矿场）来最大限度地减少挖矿的集中化，并限制挖矿行业中高级玩家们（他们有能力建造巨型挖矿设施和数据中心）的力量。 与少数的高玩相比，大量小玩家意味着更好的去中心化。 目前大型虚拟货币挖矿公司手中的主要武器是 ASIC 矿机，因此，现代加密货币通常会要求使用「抗 ASIC 哈希算法」或「权益证明（proof-of-stake）共识协议」进行「工作量证明挖矿」，以限制这部分高级玩家，达成更好的去中心化。 因为工作量证明算法需要消耗大量能源，不够环保，以太坊等区块链已经声明未来将会升级到权益证明（Proof-of-S）这类更环保的算法。不过这里我们只关注 PoW 如何基于哈希函数实现的，不讨论这个。 1. ETHash这里简要说明下以太坊区块链中使用的 ETHash 工作量证明挖掘哈希函数背后的思想。 ETHash 是以太坊区块链中的工作量证明哈希函数。它是内存密集型哈希函数（需要大量 RAM 才能快速计算），因此它被认为是抗 ASIC 的。 ETHash 的工作流程： 基于直到当前区块的整个链，为每个区块计算一个「种子」 从种子中计算出一个 16 MB 的伪随机缓存 从缓存中提取 1 GB 数据集以用于挖掘 挖掘涉及将数据集的随机切片一起进行哈希 更多信息参见 eth.wiki - ethash 2. Equihash简要解释一下 Zcash、Bitcoin Gold 和其他一些区块链中使用的 Equihash 工作量证明挖掘哈希函数背后的思想。 Equihash 是 Zcash 和 Bitcoin Gold 区块链中的工作量证明哈希函数。它是内存密集型哈希函数（需要大量 RAM 才能进行快速计算），因此它被认为是抗 ASIC 的。 Equihash 的工作流程： 基于直到当前区块的整个链，使用 BLAKE2b 计算出 50 MB 哈希数据集 在生成的哈希数据集上解决「广义生日问题」（从 2097152 中挑选 512 个不同的字符串，使得它们的二进制 XOR 为零）。已知最佳的解决方案（瓦格纳算法）在指数时间内运行，因此它需要大量的内存密集型和计算密集型计算 对前面得到的结果，进行双 SHA256 计算得到最终结果，即 SHA256(SHA256(solution)) 更多信息参见 https://github.com/tromp/equihash ","date":"2022-03-01","objectID":"/posts/practical-cryptography-basics-2-hash/:2:5","series":null,"tags":["Cryptography","Hash","密码学","哈希","散列","安全","SHA-2","SHA-3"],"title":"写给开发人员的实用密码学（二）—— 哈希函数","uri":"/posts/practical-cryptography-basics-2-hash/#1-ethash"},{"categories":["tech"],"content":" PoW 工作量证明哈希函数区块链中的 Proof-of-Work 工作量证明挖矿算法使用了一类特殊的哈希函数，这些函数是计算密集型和内存密集型的。 这些哈希函数被设计成需要消耗大量计算资源和大量内存，并且很难在硬件设备（例如集成电路或矿机）中实现，也就难以设计专用硬件来加速计算。这种哈希函数被称为抗 ASIC（ASIC-resistant）。 大部分工作量证明（Proof-of-Work）算法，都是要求计算出一个比特定值（称为挖掘难度）更大的哈希值。 因为哈希值是不可预测的，为了找出符合条件的哈希值，矿工需要计算数十亿个不同的哈希值，再从中找出最大的那个。 比如，一个工作量证明问题可能会被定义成这样：已有常数 x，要求找到一个数 p，使 hash(x + p) 的前十个比特都为 0. 有许多哈希函数是专为工作量证明挖掘算法设计的，例如 ETHash、Equihash、CryptoNight 和 Cookoo Cycle. 这些哈希函数的计算速度很慢，通常使用 GPU 硬件（如 NVIDIA GTX 1080 等显卡）或强大的 CPU 硬件（如 Intel Core i7-8700K）和大量快速 RAM 内存（如 DDR4 芯片）来执行这类算法。 这些挖矿算法的目标是通过刺激小型矿工（家庭用户和小型矿场）来最大限度地减少挖矿的集中化，并限制挖矿行业中高级玩家们（他们有能力建造巨型挖矿设施和数据中心）的力量。 与少数的高玩相比，大量小玩家意味着更好的去中心化。 目前大型虚拟货币挖矿公司手中的主要武器是 ASIC 矿机，因此，现代加密货币通常会要求使用「抗 ASIC 哈希算法」或「权益证明（proof-of-stake）共识协议」进行「工作量证明挖矿」，以限制这部分高级玩家，达成更好的去中心化。 因为工作量证明算法需要消耗大量能源，不够环保，以太坊等区块链已经声明未来将会升级到权益证明（Proof-of-S）这类更环保的算法。不过这里我们只关注 PoW 如何基于哈希函数实现的，不讨论这个。 1. ETHash这里简要说明下以太坊区块链中使用的 ETHash 工作量证明挖掘哈希函数背后的思想。 ETHash 是以太坊区块链中的工作量证明哈希函数。它是内存密集型哈希函数（需要大量 RAM 才能快速计算），因此它被认为是抗 ASIC 的。 ETHash 的工作流程： 基于直到当前区块的整个链，为每个区块计算一个「种子」 从种子中计算出一个 16 MB 的伪随机缓存 从缓存中提取 1 GB 数据集以用于挖掘 挖掘涉及将数据集的随机切片一起进行哈希 更多信息参见 eth.wiki - ethash 2. Equihash简要解释一下 Zcash、Bitcoin Gold 和其他一些区块链中使用的 Equihash 工作量证明挖掘哈希函数背后的思想。 Equihash 是 Zcash 和 Bitcoin Gold 区块链中的工作量证明哈希函数。它是内存密集型哈希函数（需要大量 RAM 才能进行快速计算），因此它被认为是抗 ASIC 的。 Equihash 的工作流程： 基于直到当前区块的整个链，使用 BLAKE2b 计算出 50 MB 哈希数据集 在生成的哈希数据集上解决「广义生日问题」（从 2097152 中挑选 512 个不同的字符串，使得它们的二进制 XOR 为零）。已知最佳的解决方案（瓦格纳算法）在指数时间内运行，因此它需要大量的内存密集型和计算密集型计算 对前面得到的结果，进行双 SHA256 计算得到最终结果，即 SHA256(SHA256(solution)) 更多信息参见 https://github.com/tromp/equihash ","date":"2022-03-01","objectID":"/posts/practical-cryptography-basics-2-hash/:2:5","series":null,"tags":["Cryptography","Hash","密码学","哈希","散列","安全","SHA-2","SHA-3"],"title":"写给开发人员的实用密码学（二）—— 哈希函数","uri":"/posts/practical-cryptography-basics-2-hash/#2-equihash"},{"categories":["tech"],"content":" 三、非加密哈希函数加密哈希函数非常看重「加密」，为了实现更高的安全强度，费了非常多的心思、也付出了很多代价。 但是实际应用中很多场景是不需要这么高的安全性的，相反可能会对速度、随机均匀性等有更高的要求。 这就催生出了很多「非加密哈希函数」。 非加密哈希函数的应用场景有很多： 哈希表 Hash Table: 在很多语言中也被称为 map/dict，它使用的算法很简单，通常就是把对象的各种属性不断乘个质数（比如 31）再相加，哈希空间会随着表的变化而变化。这里最希望的是数据的分布足够均匀。 一致性哈希：目的是解决分布式缓存的问题。在移除或者添加一个服务器时，能够尽可能小地改变已存在的服务请求与处理请求服务器之间的映射关系。 高性能哈希算法：SipHash MurMurHash3 等，使用它们的目的可能是对数据进行快速去重，要求就是足够快。 有时我们甚至可能不太在意哈希碰撞的概率。 也有的场景输入是有限的，这时我们可能会希望哈希函数具有可逆性。 总之非加密哈希函数也有非常多的应用，但不是本文的主题。 这里就不详细介绍了，有兴趣的朋友们可以自行寻找其他资源。 ","date":"2022-03-01","objectID":"/posts/practical-cryptography-basics-2-hash/:3:0","series":null,"tags":["Cryptography","Hash","密码学","哈希","散列","安全","SHA-2","SHA-3"],"title":"写给开发人员的实用密码学（二）—— 哈希函数","uri":"/posts/practical-cryptography-basics-2-hash/#三非加密哈希函数"},{"categories":["tech"],"content":" 参考 Practical-Cryptography-for-Developers-Book 漫谈非加密哈希算法 开发中常见的一些Hash函数（一） ","date":"2022-03-01","objectID":"/posts/practical-cryptography-basics-2-hash/:4:0","series":null,"tags":["Cryptography","Hash","密码学","哈希","散列","安全","SHA-2","SHA-3"],"title":"写给开发人员的实用密码学（二）—— 哈希函数","uri":"/posts/practical-cryptography-basics-2-hash/#参考"},{"categories":["tech"],"content":" 本文主要翻译自 Practical-Cryptography-for-Developers-Book 《写给开发人员的实用密码学》系列文章目录： 写给开发人员的实用密码学（一）—— 概览 写给开发人员的实用密码学（二）—— 哈希函数 写给开发人员的实用密码学（三）—— MAC 与密钥派生函数 KDF 写给开发人员的实用密码学（四）—— 安全随机数生成器 CSPRNG 写给开发人员的实用密码学（五）—— 密钥交换 DHKE 与完美前向保密 PFS 写给开发人员的实用密码学（六）—— 对称密钥加密算法 写给开发人员的实用密码学（七）—— 非对称密钥加密算法 RSA/ECC 写给开发人员的实用密码学（八）—— 数字证书与 TLS 协议 待续 ","date":"2022-03-01","objectID":"/posts/practical-cryptography-basics-1/:0:0","series":null,"tags":["Cryptography","Hash","KDF","密码学","安全","哈希","加解密","签名"],"title":"写给开发人员的实用密码学（一）—— 概览","uri":"/posts/practical-cryptography-basics-1/#"},{"categories":["tech"],"content":" 零、前言你是软件开发人员吗？有时你会需要在日常工作中使用哈希、加密或数字签名等密码学工具吗？ 你认为密码学很复杂，充满了数学知识，而且只适合书呆子吗？ 不，不是这样滴，每个开发人员都可以学习如何使用加密算法。 从开发人员的角度理解密码学概念不需要你是一个厉害的数学家。 本书将尽量以最浅显的方式教你应用密码学的基础知识，而且包含大量循序渐进的代码示例和实践练习——就像你学习 Web 开发、数据库或 APP 一样。 没错，如果你能够学会 Web 开发或 RESTful 服务，那么你也完全可以学会实用密码学。这就像学习一个新的 API 或一个新的 Web 开发框架，只要掌握了概念 + 加密库 API + 工具 + 最佳实践，你就学会了实用密码学~ 从本书中，你将学习如何使用密码算法和密码系统，如哈希、MAC 码和密钥派生函数 (KDF)、随机生成器、密钥交换协议、对称密码算法、加密方案、非对称密码系统、公钥密码学、椭圆曲线、数字签名和量子安全加密算法，以及现代加密工具和库。 ","date":"2022-03-01","objectID":"/posts/practical-cryptography-basics-1/:1:0","series":null,"tags":["Cryptography","Hash","KDF","密码学","安全","哈希","加解密","签名"],"title":"写给开发人员的实用密码学（一）—— 概览","uri":"/posts/practical-cryptography-basics-1/#零前言"},{"categories":["tech"],"content":" 一、现代密码学概览密码学已经从第一代广泛应用的密码学算法（比如已经退役的 MD5 跟 DES），发展到现代密码学算法（如 SHA-3, Argon2 以及 ChaCha20）。 让我们首先跟一些基本的密码学概念混个脸熟： 哈希函数，如 SHA-256, SHA3, RIPEMD 等 散列消息认证码 HMAC 密钥派生函数 KDF，如 Scrypt 密钥交换算法，如 Diffie-Hellman 密钥交换协议 对称密钥加密方案，如 AES-256-CTR-HMAC-SHA-256 使用公私钥的非对称密钥加密方案，如 RSA 和 ECC, secp256k1 曲线跟 Ed25519 密码系统 数字签名算法，如 ECDSA 熵（entropy）与安全随机数生成 量子安全密码学 上述这些概念涉及到技术被广泛应用在 IT 领域，如果你有过一些开发经验，可能会很熟悉其中部分名词。 如果不熟也没任何关系，本书的目的就是帮你搞清楚这些概念。 这个系列的文章会按上面给出的顺序，依次介绍这些密码学概念以及如何在日常开发中使用它们。 不过在开始学习之前，我们先来了解一下什么是密码学，以及密码学的几大用途。 ","date":"2022-03-01","objectID":"/posts/practical-cryptography-basics-1/:2:0","series":null,"tags":["Cryptography","Hash","KDF","密码学","安全","哈希","加解密","签名"],"title":"写给开发人员的实用密码学（一）—— 概览","uri":"/posts/practical-cryptography-basics-1/#一现代密码学概览"},{"categories":["tech"],"content":" 二、什么是密码学密码学（Cryptography）是提供信息安全和保护的科学。 它在我们的数字世界中无处不在，当你打开网站时、发送电子邮件时、连接到 WiFi 网络时，使用账号密码登录 APP 时、使用二步认证验证码认证身份时，都有涉及到密码学相关技术。 因此开发人员应该对密码学有基本的了解，以避免写出不安全的代码。 至少也得知道如何使用密码算法和密码库，了解哈希、对称密码算法、非对称密码算法（cipher）与加密方案这些概念，知晓数字签名及其背后的密码系统和算法。 ","date":"2022-03-01","objectID":"/posts/practical-cryptography-basics-1/:3:0","series":null,"tags":["Cryptography","Hash","KDF","密码学","安全","哈希","加解密","签名"],"title":"写给开发人员的实用密码学（一）—— 概览","uri":"/posts/practical-cryptography-basics-1/#二什么是密码学"},{"categories":["tech"],"content":" 三、密码学的用途","date":"2022-03-01","objectID":"/posts/practical-cryptography-basics-1/:4:0","series":null,"tags":["Cryptography","Hash","KDF","密码学","安全","哈希","加解密","签名"],"title":"写给开发人员的实用密码学（一）—— 概览","uri":"/posts/practical-cryptography-basics-1/#三密码学的用途"},{"categories":["tech"],"content":" 1. 加密与密钥密码学的一大用途，就是进行数据的安全存储和安全传输。 这可能涉及使用对称或非对称加密方案加密和解密数据，其中一个或多个密钥用于将数据从明文转换为加密形式或者相反。 对称加密（如 AES、Twofish 和 ChaCha20）使用相同的密钥（一个密钥）来加密和解密消息， 而非对称加密使用公钥密码系统（如 RSA 或 ECC）和密钥对（两个密钥）来进行这两项操作。 单纯使用加密算法是不够的，这是因为有的加密算法只能按块进行加密，而且很多加密算法并不能保证密文的真实性、完整性。 因此现实中我们通常会使用加密方案进行数据的加密解密。加密方案是结合了加密算法、消息认证或数字签名算法、块密码模式等多种算法，能同时保证数据的安全性、真实性、完整性的一套加密方案，如 AES-256-CTR-HMAC-SHA-256、ChaCha20-Poly1305 或 ECIES-secp256k1-AES-128-GCM。 后面我们会学到，加密方案的名称就是使用到的各种密码算法名称的组合。 ","date":"2022-03-01","objectID":"/posts/practical-cryptography-basics-1/:4:1","series":null,"tags":["Cryptography","Hash","KDF","密码学","安全","哈希","加解密","签名"],"title":"写给开发人员的实用密码学（一）—— 概览","uri":"/posts/practical-cryptography-basics-1/#1-加密与密钥"},{"categories":["tech"],"content":" 2. 数字签名与消息认证密码学提供了保证消息真实性（authenticity）、完整性（integrity）和不可否认性（non-repudiation）的方法：数字签名算法与消息认证（MAC）算法。 大多数数字签名算法（如 DSA、ECDSA 和 EdDSA）使用非对称密钥对（私钥和公钥）干这个活：消息由私钥签名，签名由相应的公钥验证。 在银行系统中，数字签名用于签署和批准付款。 在区块链签名交易中，用户可以将区块链资产从一个地址转移到另一个地址，确保转移操作的真实、完整、不可否认。 消息认证算法（如 HMAC）和消息认证码（MAC 码）也是密码学的一部分。MAC 跟数字签名的功能实际上是一致的，区别在于 MAC 使用哈希算法或者对称加密系统。 ","date":"2022-03-01","objectID":"/posts/practical-cryptography-basics-1/:4:2","series":null,"tags":["Cryptography","Hash","KDF","密码学","安全","哈希","加解密","签名"],"title":"写给开发人员的实用密码学（一）—— 概览","uri":"/posts/practical-cryptography-basics-1/#2-数字签名与消息认证"},{"categories":["tech"],"content":" 3. 安全随机数密码学的另一个部分，是熵（entropy，不可预测的随机性）和随机数的安全生成（例如使用 CSPRNG）。 安全随机数理论上是不可预测的，开发人员需要关心的是你使用的随机数生成器是否足够安全。 很多编程语言中被广泛使用的随机数生成器都是不安全的（比如 Python 的 random 库），如果你在对安全有严格要求的场景下使用了这种不安全的随机生成器，可能会黑客被预测到它生成的随机数，导致系统或者 APP 被黑客入侵。 ","date":"2022-03-01","objectID":"/posts/practical-cryptography-basics-1/:4:3","series":null,"tags":["Cryptography","Hash","KDF","密码学","安全","哈希","加解密","签名"],"title":"写给开发人员的实用密码学（一）—— 概览","uri":"/posts/practical-cryptography-basics-1/#3-安全随机数"},{"categories":["tech"],"content":" 4. 密钥交换密码学定义了密钥交换算法（如 Diffie-Hellman 密钥交换和 ECDH）和密钥构建方案，用于在需要安全传输消息的两方之间安全地构建加密密钥。 这种算法通常在两方之间建立新的安全连接时执行，例如当你打开一个现代 HTTPS 网站或连接到 WiFi 网络时。 ","date":"2022-03-01","objectID":"/posts/practical-cryptography-basics-1/:4:4","series":null,"tags":["Cryptography","Hash","KDF","密码学","安全","哈希","加解密","签名"],"title":"写给开发人员的实用密码学（一）—— 概览","uri":"/posts/practical-cryptography-basics-1/#4-密钥交换"},{"categories":["tech"],"content":" 5. 加密哈希与 Password 哈希密码学提供了加密哈希函数（如 SHA-3 和 BLAKE2）将消息转换为消息摘要/数字指纹（固定长度的散列），确保无法逆向出原始消息，并且几乎不可能找到具有相同哈希值的两条不同消息。 例如，在区块链系统中，哈希用于生成区块链地址、交易 ID 以及许多其他算法和协议。在 Git 中，加密哈希用于为文件和提交生成唯一 ID。 而密钥派生函数（如 Scrypt 和 Argon2）通过从基于文本的 Password 安全地派生出哈希值（或密钥），并且这种算法还通过注入随机参数（盐）和使用大量迭代和计算资源使密码破解速度变慢。 密码学也被用于密钥（一个非常大的、保密的数字）的生成。 因为人类只擅长记忆字符形式的 Password/Passphrases，而各种需要加密算法需要的密钥，都是一个非常大的、保密的数字。 ","date":"2022-03-01","objectID":"/posts/practical-cryptography-basics-1/:5:0","series":null,"tags":["Cryptography","Hash","KDF","密码学","安全","哈希","加解密","签名"],"title":"写给开发人员的实用密码学（一）—— 概览","uri":"/posts/practical-cryptography-basics-1/#5-加密哈希与-password-哈希"},{"categories":["tech"],"content":" 四、混淆与扩散在密码学当中，香农提出的混淆（confusion）与扩散（diffusion）是设计安全密码学算法的两个原则。 混淆使密文和对称加密中密钥的映射关系变得尽可能的复杂，使之难以分析。 如果使用了混淆，那么输出密文中的每个比特位都应该依赖于密钥和输入数据的多个部分，确保两者无法建立直接映射。 混淆常用的方法是「替换」与「排列」。 「扩散」将明文的统计结构扩散到大量密文中，隐藏明文与密文之间的统计学关系。 使单个明文或密钥位的影响尽可能扩大到更多的密文中去，确保改变输入中的任意一位都应该导致输出中大约一半的位发生变化，反过来改变输出密文的任一位，明文中大约一半的位也必须发生变化。 扩散常用的方法是「置换」。 这两个原则被包含在大多数散列函数、MAC 算法、随机数生成器、对称和非对称密码算法中。 ","date":"2022-03-01","objectID":"/posts/practical-cryptography-basics-1/:6:0","series":null,"tags":["Cryptography","Hash","KDF","密码学","安全","哈希","加解密","签名"],"title":"写给开发人员的实用密码学（一）—— 概览","uri":"/posts/practical-cryptography-basics-1/#四混淆与扩散"},{"categories":["tech"],"content":" 五、密码库说了这么多，作为一个程序员，我学习密码学的目的，只是了解如何在编程语言中使用现代密码库，并从中挑选合适的算法、使用合适的参数。 程序员经常会自嘲日常复制粘贴，但是在编写涉及到密码学的代码时，一定要谨慎处理！盲目地从 Internet 复制/粘贴代码或遵循博客中的示例可能会导致安全问题；曾经安全的代码、算法或者最佳实践，随着时间的推移也可能变得不再安全。 本系列文章的后续部分，会分别介绍上述密码学概念，并使用 Python 演示其用法，其他语言的写法网上也很容易找到。 ","date":"2022-03-01","objectID":"/posts/practical-cryptography-basics-1/:7:0","series":null,"tags":["Cryptography","Hash","KDF","密码学","安全","哈希","加解密","签名"],"title":"写给开发人员的实用密码学（一）—— 概览","uri":"/posts/practical-cryptography-basics-1/#五密码库"},{"categories":["life"],"content":" 本文转载自朋友写的 写给优秀程序员看的马拉松指南🏃 - Chuanyi，读下来感觉写得超棒超正能量，征得他同意后转载过来分享下嘿嘿~ 文中术语：PB(Personal Best) PW(Personal Worst) BQ(Boston Qualify) 4 月 11 日，气温 16-19 度，东风。前两天看预报，说可能会有雷阵雨，当天看预报，雷阵雨又延后到了下午两点多，真是天公作美啊。早上八点气温非常舒适，我预感到我又要 PB 了。 今年仙马的路线非常平稳，起伏不到三十米。正是农历草长莺飞二月天，沿途春意盎然，令人心情愉悦。只是十公里处的折返点非常恼人，拐弯后跑出几十米就立刻折返再次拐弯，减速到 0 并再次加速浪费了数秒。我知道此举是为了凑距离，但是完全可以取消这个折返点，延长终点。路线上的这个凸起，不禁让我联想到 Ph.D 的使命。 一名博士的使命 仙马四年，声名鹊起，一路摘得铜牌、银牌、金牌，今年又被“世界田径“正式列为标牌赛事，这些年出圈的努力和成果都蕴含在这个尖尖上了，我如此解读，组委会应该没有意见吧。 4 月 11 日早上，东风三级，起步向东，有些逆风，不利。吹面不寒杨柳风，逆风带来了凉爽，一路汗水都被吹干，全身上下始终都保持着干燥舒适。 第一公里计划是五分配，但是太过于兴奋，有些失控，但仍然压着 440；后五公里状态来临，逐渐将配速提到并保持在 430；六公里多迎来一个南北走向的下坡，借着势能的释放，配速拉到 410 以内。由南向北，春风拂面，夹道樱花，落英缤纷，我踩着碳板，好像踏着粉色的云霞。此时手表却一直在耳机里提醒我配速过高，机器终归是无情的，不懂风月，难知我心；七到十六公里折返点终于开始顺风，此时我稳定了 4:25 上下配速，喝了两次水，感觉还不错，并没有什么痛苦。十二公里，为了防止临近终点力量不足，我掏出一个柠檬味能量胶，迅速挤在嘴里，并在后面的水站取了一杯水。到了十三公里外已经能看见折返的第一梯队了；十六公里折返开始一个长达三公里的缓坡，中途听见一位大哥在和同伴谈论后面如何如何难跑。以我的状态看来，我不以为然，缓坡没让我失速太多。十九公里路过我的母校北门，门口有我校传统艺能舞龙舞狮，不过PB 目标不允许我驻足拍一张，有些遗憾；越跑越欢，转眼二十公里，前方是熟悉的校友团服，追上前去看，是张书记，打了招呼后，我便全力冲刺，最终成绩 133，意料之外，情理之中，冬天堆有氧的效果体现了。 青春仙林，大爱仙马。17 年首届，门外汉，门外看；18年入门，陈子豪（2016 年南京市大学生运动会 1500 米冠军田径小霸王2016年南京市运会1500m一骑绝尘！）带我们在仙林校区开始练，一圈刚好就是 5km 的绝佳跑场。19 年终于参加了我的第一次仙马，去年仙马因疫情停办一年，时隔一年后再次回归。故地重游，取得 PB，这段仙马记忆永远不会斑驳。 仙马成绩证书 第七场半程了, 成绩一路提升，156 =\u003e 149 =\u003e 147 =\u003e null =\u003e 157 =\u003e 138 =\u003e 133，每次的进步都会令我无比激动。 生活好似一个湖泊，平水如鉴，岁月静好，固然优雅；但不流动的水是容易腐败的，需要一些外来的扰动，狂风骤雨之下，浊浪翻滚，却也注入了全新的生命力。每次 PB 都是我对这平静生活狂风暴雨一般的拷问，我的生命力不应该只局限在钢筋水泥之间。 ","date":"2022-02-26","objectID":"/posts/likenttt-2021-04-11-xianlin-half-marathon-1_33_12/:0:0","series":null,"tags":["长跑","马拉松","运动"],"title":"「转」仙马赛记——我又 PB 了","uri":"/posts/likenttt-2021-04-11-xianlin-half-marathon-1_33_12/#"},{"categories":["life"],"content":" 本文转载自朋友写的 写给优秀程序员看的马拉松指南🏃 - Chuanyi，读下来感觉写得超棒超正能量，征得他同意后转载过来分享下嘿嘿~ 本文所描述的广州马拉松赛事时间为 2020-12-13 文中术语：PB(Personal Best) PW(Personal Worst) BQ(Boston Qualify) ","date":"2022-02-26","objectID":"/posts/likenttt-2020-12-13-guangzhou-marathon-3_30_15/:0:0","series":null,"tags":["长跑","马拉松","运动"],"title":"「转」MIRT出征广马——首次摸到330的边儿","uri":"/posts/likenttt-2020-12-13-guangzhou-marathon-3_30_15/#"},{"categories":["life"],"content":" 赛前计划目标成绩：3:37 平均配速：5:08 前21km 5:13 加减 5s 后21km 5:03 加减 5s 实际：3:30:15 ","date":"2022-02-26","objectID":"/posts/likenttt-2020-12-13-guangzhou-marathon-3_30_15/:1:0","series":null,"tags":["长跑","马拉松","运动"],"title":"「转」MIRT出征广马——首次摸到330的边儿","uri":"/posts/likenttt-2020-12-13-guangzhou-marathon-3_30_15/#赛前计划"},{"categories":["life"],"content":" 补给能量胶6支 绿灰绿灰黄红 服用时机 颜色 赛前5分钟 绿 10 灰 20 绿 30 灰 35 黄 40 红 ","date":"2022-02-26","objectID":"/posts/likenttt-2020-12-13-guangzhou-marathon-3_30_15/:2:0","series":null,"tags":["长跑","马拉松","运动"],"title":"「转」MIRT出征广马——首次摸到330的边儿","uri":"/posts/likenttt-2020-12-13-guangzhou-marathon-3_30_15/#补给"},{"categories":["life"],"content":" 赛中简记我和兴勇师兄一起从 C 区出发，按照 500 配速跑，观察状态。 九公里处追上旦哥，旦哥此次担任 345 Pacer ，赛前他向我们许诺，如果能追上他就能摸一摸他的光头。说实话，我对这颗光头是垂涎已久，心想这是罕有的机会，舍我其谁。当我远远地望见飘动的气球时，我就忍不住兴奋的呼喊旦哥，渐渐距离迫近到数米，旦哥心照不宣地兑现承诺，主动伸过他的头，让我摸。紧实的光头满是汗水，很滑，竟没有一丝头发，既不扎手也没有阻滞感，像极了一颗剥了壳的鸡蛋，我一个程序员也不禁为之动容，设计师（旦哥是一名声音设计师）这么伤头发吗，幸亏我没入这行。师兄决意一路跟着旦哥，我遂和他分别，去追赶 330 兔子，配速始终稳定在 450～500。 此后至 38km 之间配速稳定在这个区间里，看看风景，胡思乱想。猎德大桥是一个不小的挑战，迂回冲坡上引桥，下桥减速绕弯弯，起伏之间容易跑崩，但我始终平稳。下了桥是一个超长的折返，双向车道被绿化带切开，木棉（也许是合欢）一字排开，树干跟保龄球一样臃肿粗壮，草地上洒满了粉的花，白的穗，这南国的冬天竟然好似江南的樱花季。去程左前方已经稀稀落落地有人折回，能拉开这么多距离，是精英选手无疑了，他们跑姿大多都很美观，服装、配件、摆臂、踏步令我欣赏了好一阵。但是也有一些跑者，跑姿不那么具有观赏性的，用力过度，姿势僵硬，力量运用地不太经济，近似一种暴力美学了，我认为他们中有一些人只是暂时领先而已。后来我也折返了，此时再往左前方看，人群开始密集了起来，这种视角仿佛和原来的自己打了个照面。目光数次和几个聚集的配速员集团相遇，我试图在人群中分辨出我的队友们，却始终搜索无获，我数度怀疑是不是我已经走神儿错过了他们。21km 附近几个隧道也是不小的挑战，U型隧道起伏大，下坡要适当利用势能但也要避免心率过高，上坡要适当减速增加抓地力，地面湿滑，摩擦系数减小，要防止滑倒。过了半程以后，广州塔近在咫尺，仰之弥高，我的精神还很轻松，决定钻之弥坚。 今天是国家公祭日，十点，脑子里想到南京城此时应该鸣笛的，不禁热泪盈眶，随后心中默哀了一分多钟。昭昭前事，惕惕后人。永矢弗谖，祈愿和平。 天气预报显示今日气温在 21～22 度，但湿度较大，体感温度高于 22 度。好在穿的是背心，体表散热面积大，心率始终控制在 175 以下。每逢水站喝一小杯水，每五公里吃两片盐丸，按着计划吃能量胶。得益于以上种种努力，前 38 公里都还轻松，甚至游刃有余。但是第 39 公里我开始感到疲惫，感到厌倦，看看手表，发现心率已经到达 189，心下想这是终点前跑崩的前兆啊。一个多月前无锡马拉松折戟的经历还令我心有余悸。第 39 和 40 公里分别跑出了 521 和 523 的配速，心里很慌。到达水站后，再度补充盐丸和能量胶，并不断给自己做心里建设工作：这是难得的机遇，跑团南下首秀，集团给了莫大支持，跑团组委会也做了大量筹备；年初 PB(Personal Best)，年尾 PW(Personal Worst)，有点虎头蛇尾，接下来，坚持鏖战，后悔几天，放弃躺倒，懊悔半年。吃完最后一支柠檬味的能量胶，喝饱了水，重新出发。最后两公里跑回 451 配速，安全完赛并 PB。冲过终点，顿时感到一种超然🤯的轻快，广州是一座充满希望的城市！不过差 15 秒就能达到广马的 BQ(Boston Qualify)，仍然有一丝遗憾。但无论如何，取得这样的成绩我已经很满意了。 我很庆幸，从学生时代就培养起来的长跑爱好，可以陪伴我走进职业生涯并顽强保持至今，一路上我收获一群互相砥砺支持的好朋友。易方达基金的 slogan：乐于在长跑中取得胜利，这也是广马重要收获之一。胜利，指挑战并超越过去的自己，beat yesterday。寄予希望，并不断挑战超越，这是一种痛并快乐着的幸福。长跑如此，生活职业亦如是。 ","date":"2022-02-26","objectID":"/posts/likenttt-2020-12-13-guangzhou-marathon-3_30_15/:3:0","series":null,"tags":["长跑","马拉松","运动"],"title":"「转」MIRT出征广马——首次摸到330的边儿","uri":"/posts/likenttt-2020-12-13-guangzhou-marathon-3_30_15/#赛中简记"},{"categories":["life"],"content":" 技术总结 广马当天高达二十多摄氏度，通常来说，这样的温度并不特别友好，无疑，这丰富了我高温作战的经验。及时补给，盐丸和能量胶，如果天热尤其要补水和盐丸。后来和旦哥复盘，我抱怨道中途补充了九次水，后面几乎逢水站必停。旦哥说：如果你不补水，你怎么保证后面不会崩呢？我恍然大悟，吃到第个三馒头饱了，功劳绝不只是第三个馒头的，前两个馒头亦是关键。 预先规划好时间和配速。根据目的估算并严格执行。如果是跑成绩，不要高估，避免过度透支能力，也不要低估，一直躺在舒适的成绩上。 烟雾弹还是要放的，这是赛前乐趣所在，烟雾弹放出去了，进可凡学，退可务实。 训练需要注意提高体能和心理承受阈值。体能耗尽，其后是心有余而力不足；心态崩溃，无心再战，先前努力便俱付东流。 ","date":"2022-02-26","objectID":"/posts/likenttt-2020-12-13-guangzhou-marathon-3_30_15/:3:1","series":null,"tags":["长跑","马拉松","运动"],"title":"「转」MIRT出征广马——首次摸到330的边儿","uri":"/posts/likenttt-2020-12-13-guangzhou-marathon-3_30_15/#技术总结"},{"categories":["tech"],"content":" 个人笔记，只会列出我自己容易忘掉的命令，方便查阅。 内容比较多，适合当参考手册用。可能不太适合从头读到尾… 本文主要介绍 Linux 命令，顺带介绍下 Windows/MacOSX. ","date":"2022-02-13","objectID":"/posts/common-commands-for-various-operating-systems/:0:0","series":null,"tags":["Linux","MacOSX","Windows","CLI","Powershell","Shell","tmux","rsync","vim","awk"],"title":"Linux/Windows/MacOSX 系统常用命令集锦","uri":"/posts/common-commands-for-various-operating-systems/#"},{"categories":["tech"],"content":" 一、Linux","date":"2022-02-13","objectID":"/posts/common-commands-for-various-operating-systems/:1:0","series":null,"tags":["Linux","MacOSX","Windows","CLI","Powershell","Shell","tmux","rsync","vim","awk"],"title":"Linux/Windows/MacOSX 系统常用命令集锦","uri":"/posts/common-commands-for-various-operating-systems/#一linux"},{"categories":["tech"],"content":" 1. 后台运行 # 1. 后台运行命令 nohup python xxx.py \u0026 也可以使用 tmux，tmux 提供的 session 功能比 nohup 更好用，后面会介绍 tmux ","date":"2022-02-13","objectID":"/posts/common-commands-for-various-operating-systems/:1:1","series":null,"tags":["Linux","MacOSX","Windows","CLI","Powershell","Shell","tmux","rsync","vim","awk"],"title":"Linux/Windows/MacOSX 系统常用命令集锦","uri":"/posts/common-commands-for-various-operating-systems/#1-后台运行"},{"categories":["tech"],"content":" 2. 查找替换 sed/awksed 常用命令： ## 只在目录中所有的 .py 和 .dart 文件中递归搜索字符\"main()\" grep \"main()\" . -r --include *.{py, dart} ## 在 .js 文件中搜索关键字 xxxxx 并仅展示关键字前后 40 个字符（用在 .js 等被压缩过的文本文件上很有效） cat *.js | grep -o -P '.{0,40}xxxxx.{0,40}' ## 1） 全文搜索并替换 ### -i --in-place 原地替换（修改原文件） ### -i=SUFFIX 替换后的文件添加 SUFFIX 这个后缀 ### -r 使用拓展的正则表达式，注意此正则不支持 \\d\\w\\s 等语法，必须使用 [0-9] [a-zA-Z] 等来替换！！！ sed -ri \"s/pattern_str/replace_str/g\" `grep \"key_pattern\" 'path_pattern' -rl` ## 2）文件名搜索，替换文件内容 sed -ri \"s/pattern_str/replace_str/g\" `find . -name \"pattern\"` ## 3）批量转换大小写 # 将当前文件夹内，所有的 gitlab URL 都转换成小写 # \\L 转小写 \\U 转大写 sed -ri 's@http://GITLAB.*.git@\\L\u0026@g' `find . -name pubspec*` ## 4) 拷贝文件，并且保持文件夹结构（--parents 表示保持文件夹结构） cp --parents `find \u003csrc-dir\u003e -name *.py` \u003cdst-dir\u003e awk 用于按列处理文本，它比 sed 更强大更复杂，常用命令： ## 1. 单独选出第 1 列的文本 cat xxx.txt | awk -F '{print $1}' | head ## 2. 可以使用 -F 指定分隔符，打印出多列 awk -F ',' '{print $1,$2}'| head ## 3. 打印出行数 cat log_test | awk '{print NR,$1}' | more ## 4. if 判断语句 cat log_test | awk '{if($11\u003e300) print($1,$11)}' cat log_test | awk '{print $11}' | sort -n | uniq -c # 求和 cat data|awk '{sum+=$1} END {print \"Sum = \", sum}' # 求平均 cat data|awk '{sum+=$1} END {print \"Average = \", sum/NR}' # 求最大值 cat data|awk 'BEGIN {max = 0} {if ($1\u003emax) max=$1 fi} END {print \"Max=\", max}' # 求最小值（min的初始值设置一个超大数即可） awk 'BEGIN {min = 1999999} {if ($1\u003cmin) min=$1 fi} END {print \"Min=\", min}' ","date":"2022-02-13","objectID":"/posts/common-commands-for-various-operating-systems/:1:2","series":null,"tags":["Linux","MacOSX","Windows","CLI","Powershell","Shell","tmux","rsync","vim","awk"],"title":"Linux/Windows/MacOSX 系统常用命令集锦","uri":"/posts/common-commands-for-various-operating-systems/#2-查找替换-sedawk"},{"categories":["tech"],"content":" 3. 压缩相关 # 直接 cat 压缩文件的内容 zcat xxx.gz | more # gzip xzcat xxx.xz | more # xz tar -axvf xxx.tar.* # 通过后缀识别压缩格式，智能解压 更多命令参见 常见压缩格式的区别，及 Linux 下的压缩相关指令 ","date":"2022-02-13","objectID":"/posts/common-commands-for-various-operating-systems/:1:3","series":null,"tags":["Linux","MacOSX","Windows","CLI","Powershell","Shell","tmux","rsync","vim","awk"],"title":"Linux/Windows/MacOSX 系统常用命令集锦","uri":"/posts/common-commands-for-various-operating-systems/#3-压缩相关"},{"categories":["tech"],"content":" 4. 文件拷贝与同步各种 Linux 发行版都自带 scp/ssh，这两个工具功能简单，一般够用。 另外就是更强大也更复杂的 rsync，部分发行版会自带 rsync。 下面分别介绍下。 1. ssh/scp # 如果使用 ssh 命令进行文件传输，可安装 pv 命令查看传输速度（pipeviewer） ## ubuntu sudo apt-get install pv ## centos sudo yum install epel-release sudo yum install pv ## 1)从本地上传到服务器 ### 使用 ssh 的好处是流式传输不会占用目标机器的存储空间，适合传输可能引起空间不足的大文件，并在目标机器上实时处理该文件。 cat \u003cfilename\u003e | pv | ssh \u003cuser\u003e@\u003chost\u003e -p 22 \"cat - \u003e \u003cnew-filename\u003e\" tar cz \u003cfilename or foldername or glob\u003e | pv | ssh \u003cuser\u003e@\u003chost\u003e -p 22 \"tar xz\" # 压缩传输 ## scp 命令比 ssh 命令更简洁（但是不适合用于传文件夹，它会破坏文件的权限设置，把文件夹弄得一团糟） scp -P 22 \u003cfilename\u003e \u003cuser\u003e@\u003chost\u003e:\u003cfolder-name or filename\u003e # 通过 scp 传输，传文件夹时记得添加 -r 参数（recursive） ## 2) 从服务器下载到本地 ssh \u003cuser\u003e@\u003chost\u003e -p 22 \"tar cz \u003cfilename or foldername or glob\u003e\" | pv | tar xz # 压缩传输 scp -P 22 \u003cuser\u003e@\u003chost\u003e:\u003cfolder-name or filename\u003e \u003cfilename\u003e # 通过 scp 传输，传文件夹时记得添加 -r 参数（recursive） 2. rsyncrsync 的功能其实和前面的 scp/(tar+ssh) 是一样的，将文件从一个地方拷贝到另一个地方。 区别在于它只做增量同步，在多次拷贝文件时，只拷贝（同步）修改过的部分，很多场景下可以大大加快拷贝/备份速度。 rsync 的常用命令： # 将一个文件夹归档、压缩，并通过 ssh 协议（默认）同步到另一个地方 # -a, --archive # 归档模式，保留文件的所有元信息，等同于 `-rlptgoD` # -r, --recursive # 递归复制文件夹，`-a` 隐含了这个参数，通常都用 -a。 # -v, --verbose # 输出详细信息 # --progress # 显示传输进度 # -z, --compress # 传输文件时进行压缩 rsync -avz --progress src host:dest rsync -avz --progress -e \"ssh -p225\" /path/src user@host:dest # 使用非默认的 ssh 端口进行传输 rsync -avz --progress -e \"ssh -i id_xxx\" /path/src user@host:dest # 使用指定的私钥连接 ssh 服务端，其他各种 ssh 参数都可以在这里指定 # --exclude 排除掉某些不需要的文件(夹) rsync -avz --progress --exclude \"foor/bar\" src user@host:dest # 有时我们希望在同步数据时修改文件的 user/group # --chown # 设置文件的 user:group，必须与 `-og`/`--owner --group` 同时使用！（`-a` 隐含了 `-og`） rsync -avz --progress --chown=root:root src user@host:dest # 传输时修改 user/group 为 root # 详细说明 src 和 dest 的位置 rsync -avz --progress path/src user@host:/tmp # 将 src 拷贝到远程主机的 /tmp 中（得到 /tmp/src） ## 注意 src 结尾有 / rsync -avz --progress path/src/ user@host:/tmp/src # 将 src 目录中的文件拷贝到远程主机的 /tmp/src 目录中（同样得到 /tmp/src） # 有时候我们在传输文件时不希望保留文件的元信息 # rsync 默认不会删除 dest 中多余的文件，使用 --delete 可让 rsync 删除这部分无关的文件 # 对 src 文件夹进行完全镜像，保证两个文件夹的内容一模一样，不多不少 rsync -avz --progress --delete src user@host:dest # 也可以使用 --ignore-existing 让 rsync 忽略掉 dest 已经存在的文件。就是只同步新增的文件。 rsync -avz --progress --ignore-existing src user@host:dest 另外也有使用双冒号 :: 分隔的传输命令，这种命令使用 rsync 协议进行传输，要求目标主机启用 rsync-daemon。用得会比 ssh 少一些，暂时不做介绍。 rsync 详细文档参见 https://rsync.samba.org/documentation.html，或者 man rsync. ","date":"2022-02-13","objectID":"/posts/common-commands-for-various-operating-systems/:1:4","series":null,"tags":["Linux","MacOSX","Windows","CLI","Powershell","Shell","tmux","rsync","vim","awk"],"title":"Linux/Windows/MacOSX 系统常用命令集锦","uri":"/posts/common-commands-for-various-operating-systems/#4-文件拷贝与同步"},{"categories":["tech"],"content":" 4. 文件拷贝与同步各种 Linux 发行版都自带 scp/ssh，这两个工具功能简单，一般够用。 另外就是更强大也更复杂的 rsync，部分发行版会自带 rsync。 下面分别介绍下。 1. ssh/scp # 如果使用 ssh 命令进行文件传输，可安装 pv 命令查看传输速度（pipeviewer） ## ubuntu sudo apt-get install pv ## centos sudo yum install epel-release sudo yum install pv ## 1)从本地上传到服务器 ### 使用 ssh 的好处是流式传输不会占用目标机器的存储空间，适合传输可能引起空间不足的大文件，并在目标机器上实时处理该文件。 cat | pv | ssh @ -p 22 \"cat - \u003e \" tar cz | pv | ssh @ -p 22 \"tar xz\" # 压缩传输 ## scp 命令比 ssh 命令更简洁（但是不适合用于传文件夹，它会破坏文件的权限设置，把文件夹弄得一团糟） scp -P 22 @: # 通过 scp 传输，传文件夹时记得添加 -r 参数（recursive） ## 2) 从服务器下载到本地 ssh @ -p 22 \"tar cz \" | pv | tar xz # 压缩传输 scp -P 22 @: # 通过 scp 传输，传文件夹时记得添加 -r 参数（recursive） 2. rsyncrsync 的功能其实和前面的 scp/(tar+ssh) 是一样的，将文件从一个地方拷贝到另一个地方。 区别在于它只做增量同步，在多次拷贝文件时，只拷贝（同步）修改过的部分，很多场景下可以大大加快拷贝/备份速度。 rsync 的常用命令： # 将一个文件夹归档、压缩，并通过 ssh 协议（默认）同步到另一个地方 # -a, --archive # 归档模式，保留文件的所有元信息，等同于 `-rlptgoD` # -r, --recursive # 递归复制文件夹，`-a` 隐含了这个参数，通常都用 -a。 # -v, --verbose # 输出详细信息 # --progress # 显示传输进度 # -z, --compress # 传输文件时进行压缩 rsync -avz --progress src host:dest rsync -avz --progress -e \"ssh -p225\" /path/src user@host:dest # 使用非默认的 ssh 端口进行传输 rsync -avz --progress -e \"ssh -i id_xxx\" /path/src user@host:dest # 使用指定的私钥连接 ssh 服务端，其他各种 ssh 参数都可以在这里指定 # --exclude 排除掉某些不需要的文件(夹) rsync -avz --progress --exclude \"foor/bar\" src user@host:dest # 有时我们希望在同步数据时修改文件的 user/group # --chown # 设置文件的 user:group，必须与 `-og`/`--owner --group` 同时使用！（`-a` 隐含了 `-og`） rsync -avz --progress --chown=root:root src user@host:dest # 传输时修改 user/group 为 root # 详细说明 src 和 dest 的位置 rsync -avz --progress path/src user@host:/tmp # 将 src 拷贝到远程主机的 /tmp 中（得到 /tmp/src） ## 注意 src 结尾有 / rsync -avz --progress path/src/ user@host:/tmp/src # 将 src 目录中的文件拷贝到远程主机的 /tmp/src 目录中（同样得到 /tmp/src） # 有时候我们在传输文件时不希望保留文件的元信息 # rsync 默认不会删除 dest 中多余的文件，使用 --delete 可让 rsync 删除这部分无关的文件 # 对 src 文件夹进行完全镜像，保证两个文件夹的内容一模一样，不多不少 rsync -avz --progress --delete src user@host:dest # 也可以使用 --ignore-existing 让 rsync 忽略掉 dest 已经存在的文件。就是只同步新增的文件。 rsync -avz --progress --ignore-existing src user@host:dest 另外也有使用双冒号 :: 分隔的传输命令，这种命令使用 rsync 协议进行传输，要求目标主机启用 rsync-daemon。用得会比 ssh 少一些，暂时不做介绍。 rsync 详细文档参见 https://rsync.samba.org/documentation.html，或者 man rsync. ","date":"2022-02-13","objectID":"/posts/common-commands-for-various-operating-systems/:1:4","series":null,"tags":["Linux","MacOSX","Windows","CLI","Powershell","Shell","tmux","rsync","vim","awk"],"title":"Linux/Windows/MacOSX 系统常用命令集锦","uri":"/posts/common-commands-for-various-operating-systems/#1-sshscp"},{"categories":["tech"],"content":" 4. 文件拷贝与同步各种 Linux 发行版都自带 scp/ssh，这两个工具功能简单，一般够用。 另外就是更强大也更复杂的 rsync，部分发行版会自带 rsync。 下面分别介绍下。 1. ssh/scp # 如果使用 ssh 命令进行文件传输，可安装 pv 命令查看传输速度（pipeviewer） ## ubuntu sudo apt-get install pv ## centos sudo yum install epel-release sudo yum install pv ## 1)从本地上传到服务器 ### 使用 ssh 的好处是流式传输不会占用目标机器的存储空间，适合传输可能引起空间不足的大文件，并在目标机器上实时处理该文件。 cat | pv | ssh @ -p 22 \"cat - \u003e \" tar cz | pv | ssh @ -p 22 \"tar xz\" # 压缩传输 ## scp 命令比 ssh 命令更简洁（但是不适合用于传文件夹，它会破坏文件的权限设置，把文件夹弄得一团糟） scp -P 22 @: # 通过 scp 传输，传文件夹时记得添加 -r 参数（recursive） ## 2) 从服务器下载到本地 ssh @ -p 22 \"tar cz \" | pv | tar xz # 压缩传输 scp -P 22 @: # 通过 scp 传输，传文件夹时记得添加 -r 参数（recursive） 2. rsyncrsync 的功能其实和前面的 scp/(tar+ssh) 是一样的，将文件从一个地方拷贝到另一个地方。 区别在于它只做增量同步，在多次拷贝文件时，只拷贝（同步）修改过的部分，很多场景下可以大大加快拷贝/备份速度。 rsync 的常用命令： # 将一个文件夹归档、压缩，并通过 ssh 协议（默认）同步到另一个地方 # -a, --archive # 归档模式，保留文件的所有元信息，等同于 `-rlptgoD` # -r, --recursive # 递归复制文件夹，`-a` 隐含了这个参数，通常都用 -a。 # -v, --verbose # 输出详细信息 # --progress # 显示传输进度 # -z, --compress # 传输文件时进行压缩 rsync -avz --progress src host:dest rsync -avz --progress -e \"ssh -p225\" /path/src user@host:dest # 使用非默认的 ssh 端口进行传输 rsync -avz --progress -e \"ssh -i id_xxx\" /path/src user@host:dest # 使用指定的私钥连接 ssh 服务端，其他各种 ssh 参数都可以在这里指定 # --exclude 排除掉某些不需要的文件(夹) rsync -avz --progress --exclude \"foor/bar\" src user@host:dest # 有时我们希望在同步数据时修改文件的 user/group # --chown # 设置文件的 user:group，必须与 `-og`/`--owner --group` 同时使用！（`-a` 隐含了 `-og`） rsync -avz --progress --chown=root:root src user@host:dest # 传输时修改 user/group 为 root # 详细说明 src 和 dest 的位置 rsync -avz --progress path/src user@host:/tmp # 将 src 拷贝到远程主机的 /tmp 中（得到 /tmp/src） ## 注意 src 结尾有 / rsync -avz --progress path/src/ user@host:/tmp/src # 将 src 目录中的文件拷贝到远程主机的 /tmp/src 目录中（同样得到 /tmp/src） # 有时候我们在传输文件时不希望保留文件的元信息 # rsync 默认不会删除 dest 中多余的文件，使用 --delete 可让 rsync 删除这部分无关的文件 # 对 src 文件夹进行完全镜像，保证两个文件夹的内容一模一样，不多不少 rsync -avz --progress --delete src user@host:dest # 也可以使用 --ignore-existing 让 rsync 忽略掉 dest 已经存在的文件。就是只同步新增的文件。 rsync -avz --progress --ignore-existing src user@host:dest 另外也有使用双冒号 :: 分隔的传输命令，这种命令使用 rsync 协议进行传输，要求目标主机启用 rsync-daemon。用得会比 ssh 少一些，暂时不做介绍。 rsync 详细文档参见 https://rsync.samba.org/documentation.html，或者 man rsync. ","date":"2022-02-13","objectID":"/posts/common-commands-for-various-operating-systems/:1:4","series":null,"tags":["Linux","MacOSX","Windows","CLI","Powershell","Shell","tmux","rsync","vim","awk"],"title":"Linux/Windows/MacOSX 系统常用命令集锦","uri":"/posts/common-commands-for-various-operating-systems/#2-rsync"},{"categories":["tech"],"content":" 5. Tmux 输入 tmux 启动一个 tmux 会话。（或者用 tmux new -s \u003csession-name\u003e 启动一个命名会话） 输入 python xxx.py，python 进程开始运行。 按快捷键 ctrl+b，然后再按一下 d 脱离(detatch)当前会话。此时 python 进程进入后台运行，关闭当前终端对 python 进程没有影响。 输入 tmux ls 可以查看当前正在后台运行的会话。（命名会话会显示名称，否则只显示 id） 通过 tmux attach -t \u003csession-name/id\u003e 重新接入后台会话。 缩写 tmux a -t \u003csession\u003e 或者通过 tmux kill-session -t \u003csession-name/id\u003e 杀死一个后台会话。 常用快捷键： # prefix 表示 `ctrl`+`b` # pane 的切分与选择 prefix \" # 在下方新建一个 pane prefix % # 在右侧新建一个 pane prefix `方向键` # 光标移动到指定方向的 pane 中 # 使用方向键滚动窗口内容 prefix [ # 进入翻页模式，可使用 page up/down，或者方向键来浏览 pane 的内容 # 使用鼠标滚轮来滚动窗口内容（也可以把此命令添加到 `~/.tmux.conf` 中使它永久生效） prefix `:` 然后输入 `set-window-option -g mode-mouse on` # （调整 pane 大小）将当前的 pane 向给定的方向扩容 5 行或者 5 列 # 按住 ALT 时快速重复敲击「方向键」，能快速调整，否则就得从 prefix 开始重新输入 prefix `Alt` + `方向键` # 将当前窗格全屏显示，第二次使用此命令，会将窗格还原 prefix z # 交换 pane 的位置 prefix { # 当前窗格与上一个窗格交换位置 prefix } # 当前窗格与下一个窗格交换位置 # session 相关操作 prefix s # 查看 session 列表，并通过方向键选择 session prefix `number` # 通过数字标签选择 session # window 相关操作（关系：每个 session 可以包含多个 window，每个 window 里面又可以有多个 pane） prefix c # 新建 window prefix w # 通过数字标签选择 window 参考文档： https://github.com/tmux/tmux/wiki/Getting-Started https://www.ruanyifeng.com/blog/2019/10/tmux.html ","date":"2022-02-13","objectID":"/posts/common-commands-for-various-operating-systems/:1:5","series":null,"tags":["Linux","MacOSX","Windows","CLI","Powershell","Shell","tmux","rsync","vim","awk"],"title":"Linux/Windows/MacOSX 系统常用命令集锦","uri":"/posts/common-commands-for-various-operating-systems/#5-tmux"},{"categories":["tech"],"content":" 6. Bash Shell 基础目标：能使用 shell 编写 10 行以内的脚本。更长的脚本可以使用 Python 编写，就没必要折腾 Shell 了。 1. For 循环单行 for 循环，有时候很有用： # 数字枚举 for i in $(seq 1 5); do echo $i; done # sh/bash 都支持 for i in {1..5}; do echo $i; done # sh 不支持此语法 # 文件枚举，可使用 glob 语法进行文件匹配 for f in *; do echo $f; done for f in /etc/*.py; do echo $f; done # 使用 find 进行文件枚举 for f in $(find . -name *.py); do echo $f; done 单行 for 循环加几个换行就得到多行 for 循环，格式如下：写脚本用得到，不过更建议用 python: for i in $(seq 1 5) do echo $i done # sh/bash 都支持 2. if 语句 # 单行 if 语句 if [ true ]; then \u003ccommand\u003e; fi # if else if [ expression ] then Statement(s) to be executed if expression is true else Statement(s) to be executed if expression is not true fi 3. Shell脚本中的set指令，比如set -x 和 set -e参见：Shell脚本中的set指令，比如set -x 和 set -e 4. 其他资料 shell_scripts: 实用 shell 小脚本 ","date":"2022-02-13","objectID":"/posts/common-commands-for-various-operating-systems/:1:6","series":null,"tags":["Linux","MacOSX","Windows","CLI","Powershell","Shell","tmux","rsync","vim","awk"],"title":"Linux/Windows/MacOSX 系统常用命令集锦","uri":"/posts/common-commands-for-various-operating-systems/#6-bash-shell-基础"},{"categories":["tech"],"content":" 6. Bash Shell 基础目标：能使用 shell 编写 10 行以内的脚本。更长的脚本可以使用 Python 编写，就没必要折腾 Shell 了。 1. For 循环单行 for 循环，有时候很有用： # 数字枚举 for i in $(seq 1 5); do echo $i; done # sh/bash 都支持 for i in {1..5}; do echo $i; done # sh 不支持此语法 # 文件枚举，可使用 glob 语法进行文件匹配 for f in *; do echo $f; done for f in /etc/*.py; do echo $f; done # 使用 find 进行文件枚举 for f in $(find . -name *.py); do echo $f; done 单行 for 循环加几个换行就得到多行 for 循环，格式如下：写脚本用得到，不过更建议用 python: for i in $(seq 1 5) do echo $i done # sh/bash 都支持 2. if 语句 # 单行 if 语句 if [ true ]; then ; fi # if else if [ expression ] then Statement(s) to be executed if expression is true else Statement(s) to be executed if expression is not true fi 3. Shell脚本中的set指令，比如set -x 和 set -e参见：Shell脚本中的set指令，比如set -x 和 set -e 4. 其他资料 shell_scripts: 实用 shell 小脚本 ","date":"2022-02-13","objectID":"/posts/common-commands-for-various-operating-systems/:1:6","series":null,"tags":["Linux","MacOSX","Windows","CLI","Powershell","Shell","tmux","rsync","vim","awk"],"title":"Linux/Windows/MacOSX 系统常用命令集锦","uri":"/posts/common-commands-for-various-operating-systems/#1-for-循环"},{"categories":["tech"],"content":" 6. Bash Shell 基础目标：能使用 shell 编写 10 行以内的脚本。更长的脚本可以使用 Python 编写，就没必要折腾 Shell 了。 1. For 循环单行 for 循环，有时候很有用： # 数字枚举 for i in $(seq 1 5); do echo $i; done # sh/bash 都支持 for i in {1..5}; do echo $i; done # sh 不支持此语法 # 文件枚举，可使用 glob 语法进行文件匹配 for f in *; do echo $f; done for f in /etc/*.py; do echo $f; done # 使用 find 进行文件枚举 for f in $(find . -name *.py); do echo $f; done 单行 for 循环加几个换行就得到多行 for 循环，格式如下：写脚本用得到，不过更建议用 python: for i in $(seq 1 5) do echo $i done # sh/bash 都支持 2. if 语句 # 单行 if 语句 if [ true ]; then ; fi # if else if [ expression ] then Statement(s) to be executed if expression is true else Statement(s) to be executed if expression is not true fi 3. Shell脚本中的set指令，比如set -x 和 set -e参见：Shell脚本中的set指令，比如set -x 和 set -e 4. 其他资料 shell_scripts: 实用 shell 小脚本 ","date":"2022-02-13","objectID":"/posts/common-commands-for-various-operating-systems/:1:6","series":null,"tags":["Linux","MacOSX","Windows","CLI","Powershell","Shell","tmux","rsync","vim","awk"],"title":"Linux/Windows/MacOSX 系统常用命令集锦","uri":"/posts/common-commands-for-various-operating-systems/#2-if-语句"},{"categories":["tech"],"content":" 6. Bash Shell 基础目标：能使用 shell 编写 10 行以内的脚本。更长的脚本可以使用 Python 编写，就没必要折腾 Shell 了。 1. For 循环单行 for 循环，有时候很有用： # 数字枚举 for i in $(seq 1 5); do echo $i; done # sh/bash 都支持 for i in {1..5}; do echo $i; done # sh 不支持此语法 # 文件枚举，可使用 glob 语法进行文件匹配 for f in *; do echo $f; done for f in /etc/*.py; do echo $f; done # 使用 find 进行文件枚举 for f in $(find . -name *.py); do echo $f; done 单行 for 循环加几个换行就得到多行 for 循环，格式如下：写脚本用得到，不过更建议用 python: for i in $(seq 1 5) do echo $i done # sh/bash 都支持 2. if 语句 # 单行 if 语句 if [ true ]; then ; fi # if else if [ expression ] then Statement(s) to be executed if expression is true else Statement(s) to be executed if expression is not true fi 3. Shell脚本中的set指令，比如set -x 和 set -e参见：Shell脚本中的set指令，比如set -x 和 set -e 4. 其他资料 shell_scripts: 实用 shell 小脚本 ","date":"2022-02-13","objectID":"/posts/common-commands-for-various-operating-systems/:1:6","series":null,"tags":["Linux","MacOSX","Windows","CLI","Powershell","Shell","tmux","rsync","vim","awk"],"title":"Linux/Windows/MacOSX 系统常用命令集锦","uri":"/posts/common-commands-for-various-operating-systems/#3-shell脚本中的set指令比如set--x-和-set--e"},{"categories":["tech"],"content":" 6. Bash Shell 基础目标：能使用 shell 编写 10 行以内的脚本。更长的脚本可以使用 Python 编写，就没必要折腾 Shell 了。 1. For 循环单行 for 循环，有时候很有用： # 数字枚举 for i in $(seq 1 5); do echo $i; done # sh/bash 都支持 for i in {1..5}; do echo $i; done # sh 不支持此语法 # 文件枚举，可使用 glob 语法进行文件匹配 for f in *; do echo $f; done for f in /etc/*.py; do echo $f; done # 使用 find 进行文件枚举 for f in $(find . -name *.py); do echo $f; done 单行 for 循环加几个换行就得到多行 for 循环，格式如下：写脚本用得到，不过更建议用 python: for i in $(seq 1 5) do echo $i done # sh/bash 都支持 2. if 语句 # 单行 if 语句 if [ true ]; then ; fi # if else if [ expression ] then Statement(s) to be executed if expression is true else Statement(s) to be executed if expression is not true fi 3. Shell脚本中的set指令，比如set -x 和 set -e参见：Shell脚本中的set指令，比如set -x 和 set -e 4. 其他资料 shell_scripts: 实用 shell 小脚本 ","date":"2022-02-13","objectID":"/posts/common-commands-for-various-operating-systems/:1:6","series":null,"tags":["Linux","MacOSX","Windows","CLI","Powershell","Shell","tmux","rsync","vim","awk"],"title":"Linux/Windows/MacOSX 系统常用命令集锦","uri":"/posts/common-commands-for-various-operating-systems/#4-其他资料"},{"categories":["tech"],"content":" 7. socket 连接查询 - ss/netcat/lsof查看 socket 信息可以帮我们回答下列问题： 我的程序是不是真的在监听我指定的端口？ 我的程序是在监听 127.0.0.1（本机），还是在监听 0.0.0.0（整个网络） 进程们分别在使用哪些端口？ 我的连接数是否达到了上限？ 现在较新版本的 Ubuntu 和 CentOS 都已经使用 iproute2 替换掉了 net-tools， 如果你还需要使用陈旧的 route netstat 等命令，需要手动安装 net-tools。 我们可以使用 ss(socket statistics) 或者 netstat 命令来查看 socket 信息: # 查看 socket 连接的统计信息 # 主要统计处于各种状态的 tcp sockets 数量，以及其他 sockets 的统计信息 ss --summary ss -s # 缩写 # 查看哪个进程在监听 80 端口 # --listening 列出所有正在被监听的 socket # --processes 显示出每个 socket 对应的 process 名称和 pid # --numeric 直接打印数字端口号（不解析协议名称） ss --listening --processes --numeric | grep 80 ss -nlp | grep 80 # 缩写 ss -lp | grep http # 解析协议名称，然后通过协议名搜索监听 ## 使用过时的 netstat ### -t tcp ### -u udp netstat -tunlp | grep \":80\" # 查看 sshd 当前使用的端口号 ss --listening --processes | grep sshd ## 使用过时的 netstat netstat -tunlp | grep \u003cpid\u003e # pid 通过 ps 命令获得 # 列出所有的 tcp sockets，包括所有的 socket 状态 ss --tcp --all # 只列出正在 listen 的 socket ss --listening # 列出所有 ESTABLISHED 的 socket（默认行为） ss # 统计 TCP 连接数 ss | grep ESTAB | wc -l # 列出所有 ESTABLISHED 的 socket，并且给出连接的计时器 ss --options # 查看所有来自 192.168.5 的 sockets ss dst 192.168.1.5 # 查看本机与服务器 192.168.1.100 建立的 sockets ss src 192.168.1.5 TCP 连接数受 Linux 文件描述符上限控制，可以通过如下方法查看已用文件句柄的数量。 # 已用文件描述符数量 lsof | wc -l # 文件描述符上限 ulimit -n ","date":"2022-02-13","objectID":"/posts/common-commands-for-various-operating-systems/:1:7","series":null,"tags":["Linux","MacOSX","Windows","CLI","Powershell","Shell","tmux","rsync","vim","awk"],"title":"Linux/Windows/MacOSX 系统常用命令集锦","uri":"/posts/common-commands-for-various-operating-systems/#socket-commands"},{"categories":["tech"],"content":" 8. 其他网络相关命令主要是 iproute2 dhclient lsof 等 # 查看路由表 routel # 旧的 net-tools 包中的命令 ip route ls # iproute2 提供的新命令 # DHCP，先释放旧租约，再建立新租约 sudo dhclient -r eth0 \u0026\u0026 sudo dhclient eth0 # 查看 DHCP 租期 cat /var/lib/dhcp/dhcpd.leases # 清理 DNS 缓存 ## 1. 如果你使用的是 systemd-resolve，使用此命令 sudo systemd-resolve --flush-caches sudo systemd-resolve --statistics # 查看缓存状态 ## 2. 如果使用的是 dnsmasq，使用此命令 sudo systemctl restart dnsmasq sudo killall -HUP dnsmasq # 直接发送 HUP 信号也可以 ","date":"2022-02-13","objectID":"/posts/common-commands-for-various-operating-systems/:1:8","series":null,"tags":["Linux","MacOSX","Windows","CLI","Powershell","Shell","tmux","rsync","vim","awk"],"title":"Linux/Windows/MacOSX 系统常用命令集锦","uri":"/posts/common-commands-for-various-operating-systems/#8-其他网络相关命令"},{"categories":["tech"],"content":" 9. 容器网络诊断 - nsenterDocker 容器有自己的 namespace，直接通过宿主机的 ss 命令是查看不到容器的 socket 信息的。 比较直观的方法是直接通过 docker exec 在容器中通过 ss 命令。但是这要求容器中必须自带 ss 等程序，有的精简镜像可能不会自带它。 通过 nsenter 可以直接进入到容器的指定 namespace 中，这样就能直接查询容器网络相关的信息了。 docker ps | grep xxx echo CONTAINER=xxx # 容器名称或 ID # 1. 查询到容器对应的 pid PID=$(docker inspect --format {{.State.Pid}} $CONTAINER) # 2. nsenter 通过 pid 进入容器的 network namespace，执行 ss 查看 socket 信息 nsenter --target $PID --net ss -s nsenter 这个工具貌似是 docker 自带的或者是系统内置命令，只要装了 docker，ubuntu/centos 都可以直接使用这个命令。 nsenter 是一个进入名字空间的工具，功能不仅仅局限在「网络诊断」，还有更多用法。 ","date":"2022-02-13","objectID":"/posts/common-commands-for-various-operating-systems/:1:9","series":null,"tags":["Linux","MacOSX","Windows","CLI","Powershell","Shell","tmux","rsync","vim","awk"],"title":"Linux/Windows/MacOSX 系统常用命令集锦","uri":"/posts/common-commands-for-various-operating-systems/#9-容器网络诊断---nsenter"},{"categories":["tech"],"content":" 10. 用户与群组 ## 查看用户属于哪些群组 groups \u003cuser-name\u003e # 方法一 id \u003cusername\u003e # 方法二，它会额外列出 gid/uid cat /etc/group | grep \u003cuser-name\u003e # 方法三，直接查看配置 ## 查看群组中有哪些用户，第一列是群组，最后一列是用户名 cat /etc/group | grep \u003cgroup-name\u003e ","date":"2022-02-13","objectID":"/posts/common-commands-for-various-operating-systems/:1:10","series":null,"tags":["Linux","MacOSX","Windows","CLI","Powershell","Shell","tmux","rsync","vim","awk"],"title":"Linux/Windows/MacOSX 系统常用命令集锦","uri":"/posts/common-commands-for-various-operating-systems/#10-用户与群组"},{"categories":["tech"],"content":" 二、PowershellPowershell 是微软推出的一款新一代 shell，它的特点之一是，命令都有一致的命名规则：谓词-名词， 谓词表示动作：Get/Set/Stop/Start 等，名词指示操作对象：Service/Member/ChildItem/Command 等。 这样的命名格式使我们可以很容易地猜测到自己需要的命令的名称。 为了使用方便，powershell 还提供了一些常用命令的缩写，并且添加了大量类似 Linux 命令的别名。 还有就是，Windows 默认不区分字母大小写，日常使用可以全部小写。 ","date":"2022-02-13","objectID":"/posts/common-commands-for-various-operating-systems/:2:0","series":null,"tags":["Linux","MacOSX","Windows","CLI","Powershell","Shell","tmux","rsync","vim","awk"],"title":"Linux/Windows/MacOSX 系统常用命令集锦","uri":"/posts/common-commands-for-various-operating-systems/#二powershell"},{"categories":["tech"],"content":" 1. 实用命令 # 删除文件/文件夹 remove-item xxx -confirm ri xxx # 别名1 rm xxx # 别名2 rmdir xxx # etc... # 复制 copy-item xxx xx -r cp -r xxx xx # 显示工作目录 get-location gl pwd # 切换工作目录 set-location xxx sl xxx cd xxx # 查看环境变量 get-childitem env: gci env: gci env:PATH # 查看 PATH 变量 $env:XXX=\"value\" # 临时设置环境变量 $env:Path += \";SomeRandomPath\" # 临时在 Path 末尾添加新路径 ## 以下三行命令只对 windows 有效，linux 下无效 [Environment]::SetEnvironmentVariable(\"XXX\", $env:XXX + \";value\", [EnvironmentVariableTarget]::User) # 修改当前用户的环境变量（永久），只对新进程有效 [Environment]::SetEnvironmentVariable(\"XXX\", \"value\", [EnvironmentVariableTarget]::Machine) # 给这台电脑设置环境变量（永久），只对新进程有效，需要管理员权限 [Environment]::SetEnvironmentVariable(\"XXX\", $env:XXX + \";value\", \"User\") # target 也可用字符串指定 # 删除文件/文件夹 rm xxx # 删除文件夹时会进入交互界面，按提示输入就行。 # 查看命名位置（类似 Linux Shell 的 which） get-command xxx gcm xxx # 通过关键字查找 powershell 命令 gcm | select-string \u003ckeyword\u003e # 通过关键字查找 powershell 命令和环境变量中的程序，比较慢 gcm * | select-string \u003ckeyword\u003e # 查看别名对应的真实命令 get-alias # 类似 linux 的 find/ls 命令 get-childitem -Recurse -Include *.py gci -r -i *.py # 清空终端的输出 clear-host clear # 查看文件内容 get-content xx.py | more get-content xx.py | out-host -paging cat xx.py gc xx.py # 字符串搜索，不能对对象使用 # 类似 linux 的 grep 命令 cat xxx.log | select-string \u003cpattern\u003e gci env: | out-string -stream | select-string \u003cpattern\u003e # 需要先使用 out-string 将对象转换成 string gci env: | where-object {$_.Name -like \u003cpattern\u003e} # 计算输出的行数/对象个数 gci env: | measure-object gci env: | measure # 这是缩写 # 关机/重启 stop-computer restart-computer # windows 计算 hash 值 # 功能等同于 linux 下的 sha256sum/sha1sum/sha512sum/md5sum Get-FileHash -Path /path/to/file -Algorithm SHA256 Get-FileHash -Path /path/to/file -Algorithm SHA256 | Format-List # 用 format 修改格式化效果 # base64 编解码 [Convert]::ToBase64String([Text.Encoding]::UTF8.GetBytes(\"xxx\")) # base64 编码 [Text.Encoding]::UTF8.GetString([Convert]::FromBase64String(\"eHh4\")) # 解码 另外 windows 同样自带 ssh/scp 命令，参数也和 linux 一致 ","date":"2022-02-13","objectID":"/posts/common-commands-for-various-operating-systems/:2:1","series":null,"tags":["Linux","MacOSX","Windows","CLI","Powershell","Shell","tmux","rsync","vim","awk"],"title":"Linux/Windows/MacOSX 系统常用命令集锦","uri":"/posts/common-commands-for-various-operating-systems/#1-实用命令"},{"categories":["tech"],"content":" 2. 进程相关命令 # 查看所有进程 get-process | more ps | more # 别名 # 查找某进程（替代掉 tasklist） get-process -name exp*,power* # 使用正则查找进程 get-process | select-string \u003cpattern\u003e # 效果同上 # 通过 id 杀掉某进程（替代掉 taskkill） # 也可以通过 -Name 用正则匹配进程 stop-process \u003cpid\u003e kill \u003cpid\u003e # 别名 ","date":"2022-02-13","objectID":"/posts/common-commands-for-various-operating-systems/:2:2","series":null,"tags":["Linux","MacOSX","Windows","CLI","Powershell","Shell","tmux","rsync","vim","awk"],"title":"Linux/Windows/MacOSX 系统常用命令集锦","uri":"/posts/common-commands-for-various-operating-systems/#2-进程相关命令"},{"categories":["tech"],"content":" 3. 网络相关命令 ## 1. dns 相关(dns-client) Clear-DnsClientCache # 清除 dns 缓存（替换掉 `ipconfig /flushdns`） Get-DnsClientCache # 查看 dns 缓存 Resolve-DnsName baidu.com # 解析域名 # 更新 DHCP 租约 ipconfig /renew ## 2. TCP/IP 相关命令 Get-Command Get-Net* # 查看所有 TCP/IP 相关的命令 Get-NetIPAddress # 查看 IP 地址 Get-NetIPInterface # 查看 IP 接口 Get-NetRoute # 查看路由表 Get-NetNeighbor # 获取链路层 MAC 地址缓存 Get-NetTCPConnection # 查看 TCP 连接 ### 也可以对 TCP/IP 的 IP 地址、接口、路由表进行增删改 New-NetRoute Remove-NetNeighbor # 清除 MAC 地址缓存 ","date":"2022-02-13","objectID":"/posts/common-commands-for-various-operating-systems/:2:3","series":null,"tags":["Linux","MacOSX","Windows","CLI","Powershell","Shell","tmux","rsync","vim","awk"],"title":"Linux/Windows/MacOSX 系统常用命令集锦","uri":"/posts/common-commands-for-various-operating-systems/#3-网络相关命令"},{"categories":["tech"],"content":" 4. socket 信息查询 - netstatWindows 系统和 macOS 一样，也没有 ss，但是自带 netstat，该命令和 Linux 下的 netstat 有一定差别，具体使用方法如下： netstat -? # 查看使用帮助，很清晰易懂 # 查看那个进程在监听 80 端口，最后一列是进程的 Pid netstat -ano | findstr 80 # windows 命令 netstat -ano | select-string 80 # powershell 命令，就是把 findstr 替换成 select-string # 不仅列出 Pid，还给出 Pid 对应的可执行文件名称（需要管理员权限） netstat -ano -b | select-string 80 # powershell 命令 # 列出所有 ESTABLISHED 的 socket（默认行为） netstat # 列出所有正在监听的端口 netstat -ano | findstr LISTENING # 只列出 TCP 连接 netstat -ano -p TCP # 查看路由表 route -? # 查看使用帮助，很清晰易懂 route print # 查看所有路由信息 route print -4 # 仅 ipv4 比如我们遇到端口占用问题时，就可以通过上述命令查找到端口对应的 Pid，然后使用 kill \u003cPid\u003e 命令（powershell stop-process 的别名）杀死对应的进程。 ","date":"2022-02-13","objectID":"/posts/common-commands-for-various-operating-systems/:2:4","series":null,"tags":["Linux","MacOSX","Windows","CLI","Powershell","Shell","tmux","rsync","vim","awk"],"title":"Linux/Windows/MacOSX 系统常用命令集锦","uri":"/posts/common-commands-for-various-operating-systems/#4-socket-信息查询---netstat"},{"categories":["tech"],"content":" 三、Mac OS XMac OS X 系统也是 unix-like 系统，也使用 zsh/bash，因此大部分命令基本都跟 Linux 没啥区别，可以直接参考前面 Linux 一节的内容。 但是要注意一些坑： macos 自带的 tar 并不是 gnutar，命令使用方式不一样！ 解决：brew install gnu-tar，安装好后通过 gtar 调用，参数就跟 linux 一致了。 网络相关的命令区别较大，后面会详细介绍。 MacOSX 使用 launchpad 作为系统服务管理器，跟 systemd 区别很大。 ","date":"2022-02-13","objectID":"/posts/common-commands-for-various-operating-systems/:3:0","series":null,"tags":["Linux","MacOSX","Windows","CLI","Powershell","Shell","tmux","rsync","vim","awk"],"title":"Linux/Windows/MacOSX 系统常用命令集锦","uri":"/posts/common-commands-for-various-operating-systems/#三mac-os-x"},{"categories":["tech"],"content":" 1. 查看 socket 信息Mac OS X 系统目前没有 ss，但是自带 netstat，该命令和 Linux 下的 netstat 有一定差别，而且还很慢，还不能显示 pid. 所以 stackoverflow 上更推荐使用 lsof，几条常用命令记录如下 # -n 表示不显示主机名 # -P 表示不显示端口俗称 # 不加 sudo 只能查看以当前用户运行的程序 # 通用格式： sudo lsof -nP -iTCP:端口号 -sTCP:LISTEN # 查看所有 tcp 连接 lsof -nP -iTCP # 查看所有监听端口相关的信息（command/pid） lsof -nP -iTCP -sTCP:LISTEN ","date":"2022-02-13","objectID":"/posts/common-commands-for-various-operating-systems/:3:1","series":null,"tags":["Linux","MacOSX","Windows","CLI","Powershell","Shell","tmux","rsync","vim","awk"],"title":"Linux/Windows/MacOSX 系统常用命令集锦","uri":"/posts/common-commands-for-various-operating-systems/#1-查看-socket-信息"},{"categories":["tech"],"content":" 2. 其他网络相关命令清理 DNS 缓存： # macos 10.10+ sudo dscacheutil -flushcache sudo killall -HUP mDNSResponder # 其他版本请自己网上搜... # 查看所有网络接口及相关参数（ip/mac/type...） ifconfig # 查看路由表 netstat -nr ","date":"2022-02-13","objectID":"/posts/common-commands-for-various-operating-systems/:3:2","series":null,"tags":["Linux","MacOSX","Windows","CLI","Powershell","Shell","tmux","rsync","vim","awk"],"title":"Linux/Windows/MacOSX 系统常用命令集锦","uri":"/posts/common-commands-for-various-operating-systems/#2-其他网络相关命令"},{"categories":["tech"],"content":" 四、跨平台程序","date":"2022-02-13","objectID":"/posts/common-commands-for-various-operating-systems/:4:0","series":null,"tags":["Linux","MacOSX","Windows","CLI","Powershell","Shell","tmux","rsync","vim","awk"],"title":"Linux/Windows/MacOSX 系统常用命令集锦","uri":"/posts/common-commands-for-various-operating-systems/#四跨平台程序"},{"categories":["tech"],"content":" 1. vim 常用技巧移动： 0/^ 回到行首，$ 去到行末 w 跳到下个单词的首部，e 跳到下个单词末尾 也能使用 2w 2e 这种命令按单词数量跳转 删除 d 或修改 c: dw 删除单词，d2w 删除两个单词 d$ 删除到行末，d0/d^ 删除到行首 d(/d{/d[[ 删除到文件首部，d)/d}/d]] 删除到文件末尾 r 替换一个字符，R 持续往后替换 多行修改多行插入，主要用于加注释之类的： 光标停留在你需要插入文本的地方 ctrl+v 进入 visual block 模式，选中多行 输入 I，进入编辑模式 输入 # 注释或者其他字符，但是注意不能输入换行符！也不能删除？ 按两下 Esc，依次退出 Insert 和 visual block 模式，就插入成功了 多行删除： v 进入 visual 模式，在第一行，选中你想要删除的文本块 或者也可以先进入 visual block 模式，再通过左右方向键选择文本。 ctrl+v 进入 visual block 模式，选中多行 visual block 的特点是它是垂直选择，而 visual 模式是段落选择 按 d 键就能删除被选中的所有内容。 多行替换（基本和 sed 一致）多行行首插入注释符号 # :1,6 s/^/#/g :2,$ s/^/#/g 注：此为2行至尾行 :% s/^/#/g 注：此为所有行 这里使用了正则表达式 ^ 匹配行首，改成 $ 就可在行尾进行批量修改。 此外，它的分隔符也不仅限于 \\，也可以用 @ 等符号，方便阅读。比如： :1,6 s@^@#@g :2,$ s@^@#@g 注：此为2行至尾行 :% s@^@#@g 注：此为所有行 使用 vim 的这个正则匹配功能，不仅能进行插入，也能完成删除、替换的功能。 将选中部分写入到文件 首先按 v 进入 visual 模式，选中需要的内容 按 :，应该会显示 :'\u003c,'\u003e，表示对选中部分进行操作 输入内容 w new.txt，此时显示效果应该是 :'\u003c,'\u003ew new.txt 回车就能完成文件写入 问题：在 vim 中粘贴 yaml 时缩进会变得一团糟解决方法：在命令模式下输入 :set paste 进入粘贴模式，然后再粘贴 yaml 内容。 注意行首可能会丢失几个字符，需要手动补上。 ","date":"2022-02-13","objectID":"/posts/common-commands-for-various-operating-systems/:4:1","series":null,"tags":["Linux","MacOSX","Windows","CLI","Powershell","Shell","tmux","rsync","vim","awk"],"title":"Linux/Windows/MacOSX 系统常用命令集锦","uri":"/posts/common-commands-for-various-operating-systems/#1-vim"},{"categories":["tech"],"content":" 1. vim 常用技巧移动： 0/^ 回到行首，$ 去到行末 w 跳到下个单词的首部，e 跳到下个单词末尾 也能使用 2w 2e 这种命令按单词数量跳转 删除 d 或修改 c: dw 删除单词，d2w 删除两个单词 d$ 删除到行末，d0/d^ 删除到行首 d(/d{/d[[ 删除到文件首部，d)/d}/d]] 删除到文件末尾 r 替换一个字符，R 持续往后替换 多行修改多行插入，主要用于加注释之类的： 光标停留在你需要插入文本的地方 ctrl+v 进入 visual block 模式，选中多行 输入 I，进入编辑模式 输入 # 注释或者其他字符，但是注意不能输入换行符！也不能删除？ 按两下 Esc，依次退出 Insert 和 visual block 模式，就插入成功了 多行删除： v 进入 visual 模式，在第一行，选中你想要删除的文本块 或者也可以先进入 visual block 模式，再通过左右方向键选择文本。 ctrl+v 进入 visual block 模式，选中多行 visual block 的特点是它是垂直选择，而 visual 模式是段落选择 按 d 键就能删除被选中的所有内容。 多行替换（基本和 sed 一致）多行行首插入注释符号 # :1,6 s/^/#/g :2,$ s/^/#/g 注：此为2行至尾行 :% s/^/#/g 注：此为所有行 这里使用了正则表达式 ^ 匹配行首，改成 $ 就可在行尾进行批量修改。 此外，它的分隔符也不仅限于 \\，也可以用 @ 等符号，方便阅读。比如： :1,6 s@^@#@g :2,$ s@^@#@g 注：此为2行至尾行 :% s@^@#@g 注：此为所有行 使用 vim 的这个正则匹配功能，不仅能进行插入，也能完成删除、替换的功能。 将选中部分写入到文件 首先按 v 进入 visual 模式，选中需要的内容 按 :，应该会显示 :'\u003c,'\u003e，表示对选中部分进行操作 输入内容 w new.txt，此时显示效果应该是 :'\u003c,'\u003ew new.txt 回车就能完成文件写入 问题：在 vim 中粘贴 yaml 时缩进会变得一团糟解决方法：在命令模式下输入 :set paste 进入粘贴模式，然后再粘贴 yaml 内容。 注意行首可能会丢失几个字符，需要手动补上。 ","date":"2022-02-13","objectID":"/posts/common-commands-for-various-operating-systems/:4:1","series":null,"tags":["Linux","MacOSX","Windows","CLI","Powershell","Shell","tmux","rsync","vim","awk"],"title":"Linux/Windows/MacOSX 系统常用命令集锦","uri":"/posts/common-commands-for-various-operating-systems/#常用技巧"},{"categories":["tech"],"content":" 1. vim 常用技巧移动： 0/^ 回到行首，$ 去到行末 w 跳到下个单词的首部，e 跳到下个单词末尾 也能使用 2w 2e 这种命令按单词数量跳转 删除 d 或修改 c: dw 删除单词，d2w 删除两个单词 d$ 删除到行末，d0/d^ 删除到行首 d(/d{/d[[ 删除到文件首部，d)/d}/d]] 删除到文件末尾 r 替换一个字符，R 持续往后替换 多行修改多行插入，主要用于加注释之类的： 光标停留在你需要插入文本的地方 ctrl+v 进入 visual block 模式，选中多行 输入 I，进入编辑模式 输入 # 注释或者其他字符，但是注意不能输入换行符！也不能删除？ 按两下 Esc，依次退出 Insert 和 visual block 模式，就插入成功了 多行删除： v 进入 visual 模式，在第一行，选中你想要删除的文本块 或者也可以先进入 visual block 模式，再通过左右方向键选择文本。 ctrl+v 进入 visual block 模式，选中多行 visual block 的特点是它是垂直选择，而 visual 模式是段落选择 按 d 键就能删除被选中的所有内容。 多行替换（基本和 sed 一致）多行行首插入注释符号 # :1,6 s/^/#/g :2,$ s/^/#/g 注：此为2行至尾行 :% s/^/#/g 注：此为所有行 这里使用了正则表达式 ^ 匹配行首，改成 $ 就可在行尾进行批量修改。 此外，它的分隔符也不仅限于 \\，也可以用 @ 等符号，方便阅读。比如： :1,6 s@^@#@g :2,$ s@^@#@g 注：此为2行至尾行 :% s@^@#@g 注：此为所有行 使用 vim 的这个正则匹配功能，不仅能进行插入，也能完成删除、替换的功能。 将选中部分写入到文件 首先按 v 进入 visual 模式，选中需要的内容 按 :，应该会显示 :'\u003c,'\u003e，表示对选中部分进行操作 输入内容 w new.txt，此时显示效果应该是 :'\u003c,'\u003ew new.txt 回车就能完成文件写入 问题：在 vim 中粘贴 yaml 时缩进会变得一团糟解决方法：在命令模式下输入 :set paste 进入粘贴模式，然后再粘贴 yaml 内容。 注意行首可能会丢失几个字符，需要手动补上。 ","date":"2022-02-13","objectID":"/posts/common-commands-for-various-operating-systems/:4:1","series":null,"tags":["Linux","MacOSX","Windows","CLI","Powershell","Shell","tmux","rsync","vim","awk"],"title":"Linux/Windows/MacOSX 系统常用命令集锦","uri":"/posts/common-commands-for-various-operating-systems/#多行修改"},{"categories":["tech"],"content":" 1. vim 常用技巧移动： 0/^ 回到行首，$ 去到行末 w 跳到下个单词的首部，e 跳到下个单词末尾 也能使用 2w 2e 这种命令按单词数量跳转 删除 d 或修改 c: dw 删除单词，d2w 删除两个单词 d$ 删除到行末，d0/d^ 删除到行首 d(/d{/d[[ 删除到文件首部，d)/d}/d]] 删除到文件末尾 r 替换一个字符，R 持续往后替换 多行修改多行插入，主要用于加注释之类的： 光标停留在你需要插入文本的地方 ctrl+v 进入 visual block 模式，选中多行 输入 I，进入编辑模式 输入 # 注释或者其他字符，但是注意不能输入换行符！也不能删除？ 按两下 Esc，依次退出 Insert 和 visual block 模式，就插入成功了 多行删除： v 进入 visual 模式，在第一行，选中你想要删除的文本块 或者也可以先进入 visual block 模式，再通过左右方向键选择文本。 ctrl+v 进入 visual block 模式，选中多行 visual block 的特点是它是垂直选择，而 visual 模式是段落选择 按 d 键就能删除被选中的所有内容。 多行替换（基本和 sed 一致）多行行首插入注释符号 # :1,6 s/^/#/g :2,$ s/^/#/g 注：此为2行至尾行 :% s/^/#/g 注：此为所有行 这里使用了正则表达式 ^ 匹配行首，改成 $ 就可在行尾进行批量修改。 此外，它的分隔符也不仅限于 \\，也可以用 @ 等符号，方便阅读。比如： :1,6 s@^@#@g :2,$ s@^@#@g 注：此为2行至尾行 :% s@^@#@g 注：此为所有行 使用 vim 的这个正则匹配功能，不仅能进行插入，也能完成删除、替换的功能。 将选中部分写入到文件 首先按 v 进入 visual 模式，选中需要的内容 按 :，应该会显示 :'\u003c,'\u003e，表示对选中部分进行操作 输入内容 w new.txt，此时显示效果应该是 :'\u003c,'\u003ew new.txt 回车就能完成文件写入 问题：在 vim 中粘贴 yaml 时缩进会变得一团糟解决方法：在命令模式下输入 :set paste 进入粘贴模式，然后再粘贴 yaml 内容。 注意行首可能会丢失几个字符，需要手动补上。 ","date":"2022-02-13","objectID":"/posts/common-commands-for-various-operating-systems/:4:1","series":null,"tags":["Linux","MacOSX","Windows","CLI","Powershell","Shell","tmux","rsync","vim","awk"],"title":"Linux/Windows/MacOSX 系统常用命令集锦","uri":"/posts/common-commands-for-various-operating-systems/#多行替换基本和-sed-一致"},{"categories":["tech"],"content":" 1. vim 常用技巧移动： 0/^ 回到行首，$ 去到行末 w 跳到下个单词的首部，e 跳到下个单词末尾 也能使用 2w 2e 这种命令按单词数量跳转 删除 d 或修改 c: dw 删除单词，d2w 删除两个单词 d$ 删除到行末，d0/d^ 删除到行首 d(/d{/d[[ 删除到文件首部，d)/d}/d]] 删除到文件末尾 r 替换一个字符，R 持续往后替换 多行修改多行插入，主要用于加注释之类的： 光标停留在你需要插入文本的地方 ctrl+v 进入 visual block 模式，选中多行 输入 I，进入编辑模式 输入 # 注释或者其他字符，但是注意不能输入换行符！也不能删除？ 按两下 Esc，依次退出 Insert 和 visual block 模式，就插入成功了 多行删除： v 进入 visual 模式，在第一行，选中你想要删除的文本块 或者也可以先进入 visual block 模式，再通过左右方向键选择文本。 ctrl+v 进入 visual block 模式，选中多行 visual block 的特点是它是垂直选择，而 visual 模式是段落选择 按 d 键就能删除被选中的所有内容。 多行替换（基本和 sed 一致）多行行首插入注释符号 # :1,6 s/^/#/g :2,$ s/^/#/g 注：此为2行至尾行 :% s/^/#/g 注：此为所有行 这里使用了正则表达式 ^ 匹配行首，改成 $ 就可在行尾进行批量修改。 此外，它的分隔符也不仅限于 \\，也可以用 @ 等符号，方便阅读。比如： :1,6 s@^@#@g :2,$ s@^@#@g 注：此为2行至尾行 :% s@^@#@g 注：此为所有行 使用 vim 的这个正则匹配功能，不仅能进行插入，也能完成删除、替换的功能。 将选中部分写入到文件 首先按 v 进入 visual 模式，选中需要的内容 按 :，应该会显示 :'\u003c,'\u003e，表示对选中部分进行操作 输入内容 w new.txt，此时显示效果应该是 :'\u003c,'\u003ew new.txt 回车就能完成文件写入 问题：在 vim 中粘贴 yaml 时缩进会变得一团糟解决方法：在命令模式下输入 :set paste 进入粘贴模式，然后再粘贴 yaml 内容。 注意行首可能会丢失几个字符，需要手动补上。 ","date":"2022-02-13","objectID":"/posts/common-commands-for-various-operating-systems/:4:1","series":null,"tags":["Linux","MacOSX","Windows","CLI","Powershell","Shell","tmux","rsync","vim","awk"],"title":"Linux/Windows/MacOSX 系统常用命令集锦","uri":"/posts/common-commands-for-various-operating-systems/#将选中部分写入到文件"},{"categories":["tech"],"content":" 1. vim 常用技巧移动： 0/^ 回到行首，$ 去到行末 w 跳到下个单词的首部，e 跳到下个单词末尾 也能使用 2w 2e 这种命令按单词数量跳转 删除 d 或修改 c: dw 删除单词，d2w 删除两个单词 d$ 删除到行末，d0/d^ 删除到行首 d(/d{/d[[ 删除到文件首部，d)/d}/d]] 删除到文件末尾 r 替换一个字符，R 持续往后替换 多行修改多行插入，主要用于加注释之类的： 光标停留在你需要插入文本的地方 ctrl+v 进入 visual block 模式，选中多行 输入 I，进入编辑模式 输入 # 注释或者其他字符，但是注意不能输入换行符！也不能删除？ 按两下 Esc，依次退出 Insert 和 visual block 模式，就插入成功了 多行删除： v 进入 visual 模式，在第一行，选中你想要删除的文本块 或者也可以先进入 visual block 模式，再通过左右方向键选择文本。 ctrl+v 进入 visual block 模式，选中多行 visual block 的特点是它是垂直选择，而 visual 模式是段落选择 按 d 键就能删除被选中的所有内容。 多行替换（基本和 sed 一致）多行行首插入注释符号 # :1,6 s/^/#/g :2,$ s/^/#/g 注：此为2行至尾行 :% s/^/#/g 注：此为所有行 这里使用了正则表达式 ^ 匹配行首，改成 $ 就可在行尾进行批量修改。 此外，它的分隔符也不仅限于 \\，也可以用 @ 等符号，方便阅读。比如： :1,6 s@^@#@g :2,$ s@^@#@g 注：此为2行至尾行 :% s@^@#@g 注：此为所有行 使用 vim 的这个正则匹配功能，不仅能进行插入，也能完成删除、替换的功能。 将选中部分写入到文件 首先按 v 进入 visual 模式，选中需要的内容 按 :，应该会显示 :'\u003c,'\u003e，表示对选中部分进行操作 输入内容 w new.txt，此时显示效果应该是 :'\u003c,'\u003ew new.txt 回车就能完成文件写入 问题：在 vim 中粘贴 yaml 时缩进会变得一团糟解决方法：在命令模式下输入 :set paste 进入粘贴模式，然后再粘贴 yaml 内容。 注意行首可能会丢失几个字符，需要手动补上。 ","date":"2022-02-13","objectID":"/posts/common-commands-for-various-operating-systems/:4:1","series":null,"tags":["Linux","MacOSX","Windows","CLI","Powershell","Shell","tmux","rsync","vim","awk"],"title":"Linux/Windows/MacOSX 系统常用命令集锦","uri":"/posts/common-commands-for-various-operating-systems/#问题在-vim-中粘贴-yaml-时缩进会变得一团糟"},{"categories":["tech"],"content":" 参考 如何在 Linux 中查看进程占用的端口号 github - nsenter 使用 lsof 代替 Mac OS X 中的 netstat 查看占用端口的程序 aws 常用命令 ","date":"2022-02-13","objectID":"/posts/common-commands-for-various-operating-systems/:5:0","series":null,"tags":["Linux","MacOSX","Windows","CLI","Powershell","Shell","tmux","rsync","vim","awk"],"title":"Linux/Windows/MacOSX 系统常用命令集锦","uri":"/posts/common-commands-for-various-operating-systems/#参考"},{"categories":["tech"],"content":" 个人笔记，不保证正确。 内容比较多，建议参照目录浏览。 ","date":"2022-02-13","objectID":"/posts/python-tips-and-tricks/:0:0","series":null,"tags":["Python","Tips","Tricks","常见错误"],"title":"Python 实用技巧与常见错误集锦","uri":"/posts/python-tips-and-tricks/#"},{"categories":["tech"],"content":" 一、标准库","date":"2022-02-13","objectID":"/posts/python-tips-and-tricks/:1:0","series":null,"tags":["Python","Tips","Tricks","常见错误"],"title":"Python 实用技巧与常见错误集锦","uri":"/posts/python-tips-and-tricks/#一标准库"},{"categories":["tech"],"content":" 1. 文件路径 - pathlib提供了 OS 无关的文件路径抽象，可以完全替代旧的 os.path 和 glob. 学会了 pathlib.Path，你就会了 Python 处理文件路径的所有功能。 1. 路径解析与拼接 from pathlib import Path data_folder = Path(\"./source_data/text_files/\") data_file = data_folder / \"raw_data.txt\" # Path 重载了 / 操作符，路径拼接超级方便 # 路径的解析 data_file.parent # 获取父路径，这里的结果就是 data_folder data_foler.parent # 会返回 Path(\"source_data\") data_file.parents[1] # 即获取到 data_file 的上上层目录，结果和上面一样是 Path(\"source_data\") data_file.parents[2] # 上上上层目录，Path(\".\") dara_file.name # 文件名 \"raw_data.txt\" dara_file.suffix # 文件的后缀（最末尾的）\".txt\"，还可用 suffixes 获取所有后缀 data_file.stem # 去除掉最末尾的后缀后（只去除一个），剩下的文件名：raw_data # 替换文件名或者文件后缀 data_file.with_name(\"test.txt\") # 变成 .../test.txt data_file.with_suffix(\".pdf\") # 变成 .../raw_data.pdf # 当前路径与另一路径 的相对路径 data_file.relative_to(data_folder) # PosixPath('raw_data.txt') 2. pathlib 常用函数 if not data_folder.exists(): data_folder.mkdir(parents=True) # 直接创建文件夹，如果父文件夹不存在，也自动创建 if not filename.exists(): # 文件是否存在 filename.touch() # 直接创建空文件，或者用 filename.open() 直接获取文件句柄 # 路径类型判断 if data_file.is_file(): # 是文件 print(data_file, \"is a file\") elif data_file.is_dir(): # 是文件夹 for child in p.iterdir(): # 通过 Path.iterdir() 迭代文件夹中的内容 print(child) # 路径解析 # 获取文件的绝对路径（符号链接也会被解析到真正的文件） filename.resolve() # 在不区分大小写的系统上（Windows），这个函数也会将大小写转换成实际的形式。 # 可以直接获取 Home 路径或者当前路径 Path.home() / \"file.txt\" # 有时需要以 home 为 base path 来构建文件路径 Path.cwd() / \"file.txt\" # 或者基于当前路径构建 还有很多其它的实用函数，可在使用中慢慢探索。 3. glob 通配符pathlib 也提供了 glob 支持，也就是广泛用在路径匹配上的一种简化正则表达式。 data_file.match(glob_pattern) # 返回 True 或 False，表示文件路径与给出的 glob pattern 是否匹配 for py_file in data_folder.glob(\"*/*.py\"): # 匹配当前路径下的子文件夹中的 py 文件，会返回一个可迭代对象 print(py_file) # 反向匹配，相当于 glob 模式开头添加 \"**/\" for py_file in data_folder.glob(\"**/*.py\"): # 匹配当前路径下的所有 py 文件（所有子文件夹也会被搜索），返回一个可迭代对象 print(py_file) glob 中的 * 表示任意字符，而 ** 则表示任意层目录。（在大型文件树上使用 ** 速度会很慢！） ","date":"2022-02-13","objectID":"/posts/python-tips-and-tricks/:1:1","series":null,"tags":["Python","Tips","Tricks","常见错误"],"title":"Python 实用技巧与常见错误集锦","uri":"/posts/python-tips-and-tricks/#1-文件路径---pathlib"},{"categories":["tech"],"content":" 1. 文件路径 - pathlib提供了 OS 无关的文件路径抽象，可以完全替代旧的 os.path 和 glob. 学会了 pathlib.Path，你就会了 Python 处理文件路径的所有功能。 1. 路径解析与拼接 from pathlib import Path data_folder = Path(\"./source_data/text_files/\") data_file = data_folder / \"raw_data.txt\" # Path 重载了 / 操作符，路径拼接超级方便 # 路径的解析 data_file.parent # 获取父路径，这里的结果就是 data_folder data_foler.parent # 会返回 Path(\"source_data\") data_file.parents[1] # 即获取到 data_file 的上上层目录，结果和上面一样是 Path(\"source_data\") data_file.parents[2] # 上上上层目录，Path(\".\") dara_file.name # 文件名 \"raw_data.txt\" dara_file.suffix # 文件的后缀（最末尾的）\".txt\"，还可用 suffixes 获取所有后缀 data_file.stem # 去除掉最末尾的后缀后（只去除一个），剩下的文件名：raw_data # 替换文件名或者文件后缀 data_file.with_name(\"test.txt\") # 变成 .../test.txt data_file.with_suffix(\".pdf\") # 变成 .../raw_data.pdf # 当前路径与另一路径 的相对路径 data_file.relative_to(data_folder) # PosixPath('raw_data.txt') 2. pathlib 常用函数 if not data_folder.exists(): data_folder.mkdir(parents=True) # 直接创建文件夹，如果父文件夹不存在，也自动创建 if not filename.exists(): # 文件是否存在 filename.touch() # 直接创建空文件，或者用 filename.open() 直接获取文件句柄 # 路径类型判断 if data_file.is_file(): # 是文件 print(data_file, \"is a file\") elif data_file.is_dir(): # 是文件夹 for child in p.iterdir(): # 通过 Path.iterdir() 迭代文件夹中的内容 print(child) # 路径解析 # 获取文件的绝对路径（符号链接也会被解析到真正的文件） filename.resolve() # 在不区分大小写的系统上（Windows），这个函数也会将大小写转换成实际的形式。 # 可以直接获取 Home 路径或者当前路径 Path.home() / \"file.txt\" # 有时需要以 home 为 base path 来构建文件路径 Path.cwd() / \"file.txt\" # 或者基于当前路径构建 还有很多其它的实用函数，可在使用中慢慢探索。 3. glob 通配符pathlib 也提供了 glob 支持，也就是广泛用在路径匹配上的一种简化正则表达式。 data_file.match(glob_pattern) # 返回 True 或 False，表示文件路径与给出的 glob pattern 是否匹配 for py_file in data_folder.glob(\"*/*.py\"): # 匹配当前路径下的子文件夹中的 py 文件，会返回一个可迭代对象 print(py_file) # 反向匹配，相当于 glob 模式开头添加 \"**/\" for py_file in data_folder.glob(\"**/*.py\"): # 匹配当前路径下的所有 py 文件（所有子文件夹也会被搜索），返回一个可迭代对象 print(py_file) glob 中的 * 表示任意字符，而 ** 则表示任意层目录。（在大型文件树上使用 ** 速度会很慢！） ","date":"2022-02-13","objectID":"/posts/python-tips-and-tricks/:1:1","series":null,"tags":["Python","Tips","Tricks","常见错误"],"title":"Python 实用技巧与常见错误集锦","uri":"/posts/python-tips-and-tricks/#1-路径解析与拼接"},{"categories":["tech"],"content":" 1. 文件路径 - pathlib提供了 OS 无关的文件路径抽象，可以完全替代旧的 os.path 和 glob. 学会了 pathlib.Path，你就会了 Python 处理文件路径的所有功能。 1. 路径解析与拼接 from pathlib import Path data_folder = Path(\"./source_data/text_files/\") data_file = data_folder / \"raw_data.txt\" # Path 重载了 / 操作符，路径拼接超级方便 # 路径的解析 data_file.parent # 获取父路径，这里的结果就是 data_folder data_foler.parent # 会返回 Path(\"source_data\") data_file.parents[1] # 即获取到 data_file 的上上层目录，结果和上面一样是 Path(\"source_data\") data_file.parents[2] # 上上上层目录，Path(\".\") dara_file.name # 文件名 \"raw_data.txt\" dara_file.suffix # 文件的后缀（最末尾的）\".txt\"，还可用 suffixes 获取所有后缀 data_file.stem # 去除掉最末尾的后缀后（只去除一个），剩下的文件名：raw_data # 替换文件名或者文件后缀 data_file.with_name(\"test.txt\") # 变成 .../test.txt data_file.with_suffix(\".pdf\") # 变成 .../raw_data.pdf # 当前路径与另一路径 的相对路径 data_file.relative_to(data_folder) # PosixPath('raw_data.txt') 2. pathlib 常用函数 if not data_folder.exists(): data_folder.mkdir(parents=True) # 直接创建文件夹，如果父文件夹不存在，也自动创建 if not filename.exists(): # 文件是否存在 filename.touch() # 直接创建空文件，或者用 filename.open() 直接获取文件句柄 # 路径类型判断 if data_file.is_file(): # 是文件 print(data_file, \"is a file\") elif data_file.is_dir(): # 是文件夹 for child in p.iterdir(): # 通过 Path.iterdir() 迭代文件夹中的内容 print(child) # 路径解析 # 获取文件的绝对路径（符号链接也会被解析到真正的文件） filename.resolve() # 在不区分大小写的系统上（Windows），这个函数也会将大小写转换成实际的形式。 # 可以直接获取 Home 路径或者当前路径 Path.home() / \"file.txt\" # 有时需要以 home 为 base path 来构建文件路径 Path.cwd() / \"file.txt\" # 或者基于当前路径构建 还有很多其它的实用函数，可在使用中慢慢探索。 3. glob 通配符pathlib 也提供了 glob 支持，也就是广泛用在路径匹配上的一种简化正则表达式。 data_file.match(glob_pattern) # 返回 True 或 False，表示文件路径与给出的 glob pattern 是否匹配 for py_file in data_folder.glob(\"*/*.py\"): # 匹配当前路径下的子文件夹中的 py 文件，会返回一个可迭代对象 print(py_file) # 反向匹配，相当于 glob 模式开头添加 \"**/\" for py_file in data_folder.glob(\"**/*.py\"): # 匹配当前路径下的所有 py 文件（所有子文件夹也会被搜索），返回一个可迭代对象 print(py_file) glob 中的 * 表示任意字符，而 ** 则表示任意层目录。（在大型文件树上使用 ** 速度会很慢！） ","date":"2022-02-13","objectID":"/posts/python-tips-and-tricks/:1:1","series":null,"tags":["Python","Tips","Tricks","常见错误"],"title":"Python 实用技巧与常见错误集锦","uri":"/posts/python-tips-and-tricks/#2-pathlib-常用函数"},{"categories":["tech"],"content":" 1. 文件路径 - pathlib提供了 OS 无关的文件路径抽象，可以完全替代旧的 os.path 和 glob. 学会了 pathlib.Path，你就会了 Python 处理文件路径的所有功能。 1. 路径解析与拼接 from pathlib import Path data_folder = Path(\"./source_data/text_files/\") data_file = data_folder / \"raw_data.txt\" # Path 重载了 / 操作符，路径拼接超级方便 # 路径的解析 data_file.parent # 获取父路径，这里的结果就是 data_folder data_foler.parent # 会返回 Path(\"source_data\") data_file.parents[1] # 即获取到 data_file 的上上层目录，结果和上面一样是 Path(\"source_data\") data_file.parents[2] # 上上上层目录，Path(\".\") dara_file.name # 文件名 \"raw_data.txt\" dara_file.suffix # 文件的后缀（最末尾的）\".txt\"，还可用 suffixes 获取所有后缀 data_file.stem # 去除掉最末尾的后缀后（只去除一个），剩下的文件名：raw_data # 替换文件名或者文件后缀 data_file.with_name(\"test.txt\") # 变成 .../test.txt data_file.with_suffix(\".pdf\") # 变成 .../raw_data.pdf # 当前路径与另一路径 的相对路径 data_file.relative_to(data_folder) # PosixPath('raw_data.txt') 2. pathlib 常用函数 if not data_folder.exists(): data_folder.mkdir(parents=True) # 直接创建文件夹，如果父文件夹不存在，也自动创建 if not filename.exists(): # 文件是否存在 filename.touch() # 直接创建空文件，或者用 filename.open() 直接获取文件句柄 # 路径类型判断 if data_file.is_file(): # 是文件 print(data_file, \"is a file\") elif data_file.is_dir(): # 是文件夹 for child in p.iterdir(): # 通过 Path.iterdir() 迭代文件夹中的内容 print(child) # 路径解析 # 获取文件的绝对路径（符号链接也会被解析到真正的文件） filename.resolve() # 在不区分大小写的系统上（Windows），这个函数也会将大小写转换成实际的形式。 # 可以直接获取 Home 路径或者当前路径 Path.home() / \"file.txt\" # 有时需要以 home 为 base path 来构建文件路径 Path.cwd() / \"file.txt\" # 或者基于当前路径构建 还有很多其它的实用函数，可在使用中慢慢探索。 3. glob 通配符pathlib 也提供了 glob 支持，也就是广泛用在路径匹配上的一种简化正则表达式。 data_file.match(glob_pattern) # 返回 True 或 False，表示文件路径与给出的 glob pattern 是否匹配 for py_file in data_folder.glob(\"*/*.py\"): # 匹配当前路径下的子文件夹中的 py 文件，会返回一个可迭代对象 print(py_file) # 反向匹配，相当于 glob 模式开头添加 \"**/\" for py_file in data_folder.glob(\"**/*.py\"): # 匹配当前路径下的所有 py 文件（所有子文件夹也会被搜索），返回一个可迭代对象 print(py_file) glob 中的 * 表示任意字符，而 ** 则表示任意层目录。（在大型文件树上使用 ** 速度会很慢！） ","date":"2022-02-13","objectID":"/posts/python-tips-and-tricks/:1:1","series":null,"tags":["Python","Tips","Tricks","常见错误"],"title":"Python 实用技巧与常见错误集锦","uri":"/posts/python-tips-and-tricks/#3-glob-通配符"},{"categories":["tech"],"content":" 2. 时间日期处理python3 在时间日期处理方面，有标准库 datetime 跟 calender，也有流行的第三方库 arrow 跟 maya. 标准库 datetime 有时候不太方便，比如没有提供解析 iso 格式的函数。 另外就是用标准库时，经常需要自定义格式化串。 相比之下，maya 和 arrow 这两个第三方库会方便很多。 不过第三方库并不是任何时候都可用，这里只介绍标准库 datetime 的用法，maya/arrow 请自行查找官方文档学习。 1. 获取当前时间 import time import datetime as dt # 1. 获取当前时间的时间戳 time.time() # 直接调用 c api，因此速度很快: 1582315203.537061 utcnow = dt.datetime.utcnow() # 当前的世界标准时间: datetime.datetime(2020, 2, 22, 4, 0, 3, 537061) utcnow.timestamp() # 将标准时转换成时间戳：datetime =\u003e 1582315203.537061 # 2. UTC 世界标准时间 time.gmtime() #输出为： time.struct_time(tm_year=2019, tm_mon=6, tm_mday=23, # tm_hour=3, tm_min=49, tm_sec=17, # tm_wday=6, tm_yday=174, tm_isdst=0) # 这实际上是一个命名元组 # 3. 构建一个指定的 datetime 实例 time_1997 = dt.datetime(year=1997, month=1, day=1) # =\u003e datetime.datetime(1997, 1, 1, 0, 0) dt.datetime(year=1997, month=1, day=1, minute=11) # =\u003e datetime.datetime(1997, 1, 1, 0, 11) 2. 时间日期的修改与运算 # 0. 日期的修改（修改年月时分秒） utcnow.replace(day=11) # =\u003e datetime.datetime(2020, 2, 11, 4, 0, 3, 537061) 修改 day utcnow.replace(hour=11) # =\u003e datetime.datetime(2020, 2, 22, 11, 0, 3, 537061) 修改 hour # 1. 日期与时间 date_utcnow = utcnow.date() # =\u003e datetime.date(2020, 2, 22) 年月日 time_utcnow = utcnow.time() # =\u003e datetime.time(4, 0, 3, 537061) 时分秒 # 2. 联结时间和日期（date 和 time 不能用加法联结） dt.datetime.combine(date_utcnow, time_utcnow) # =\u003e datetime.datetime(2020, 2, 22, 4, 0, 3, 537061) # 3. 日期的运算 # 3.1 datetime 之间只能计算时间差（减法），不能进行其他运算 utcnow - time_1997 # =\u003e datetime.timedelta(days=8452, seconds=14403, microseconds=537061) # 3.2 使用 timedelta 进行时间的增减 days_step = dt.timedelta(days=1) # 注意参数是复数形式 time_1997 + days_step # =\u003e datetime.datetime(1997, 1, 2, 0, 0) time_1997 - days_step # =\u003e datetime.datetime(1996, 12, 31, 0, 0) # 3.3 timedelta 之间也可以进行加减法 hours_step = dt.timedelta(hours=1) # =\u003e datetime.timedelta(seconds=3600) days_step + hours_step # =\u003e datetime.timedelta(days=1, seconds=3600) days_step - hours_step # =\u003e datetime.timedelta(seconds=82800) hours_step - days_step # =\u003e datetime.timedelta(days=-1, seconds=3600) # 3.4 timedelta 还可以按比例增减（与数字进行乘除法） hours_step * 2 # =\u003e datetime.timedelta(seconds=7200) days_step * -2 # =\u003e datetime.timedelta(days=-2) hours_step * 1.1 # =\u003e datetime.timedelta(seconds=3960) 3. 时间日期的格式化与解析先介绍下常用的格式化字符串： 普通格式 - ‘%Y-%m-%d %H:%M:%S’ =\u003e ‘2020-02-22 04:00:03’ ISO 格式 - ‘%Y-%m-%dT%H:%M:%S.%fZ’ =\u003e ‘2020-02-22T04:00:03.537061Z’ 带时区的格式 - ‘%Y-%m-%dT%H:%M:%S%Z’ =\u003e 2022-02-10T00:48:52UTC+08:00 需要时间对象自身有时区属性才行！否则格式化时会忽略 %Z 另外再介绍下 Python 两个时间格式化与解析函数的命名： strftime: 即 string formate time strptime: 即 string parse time # 1. 将时间格式化成字符串 # 1.1 将 datetime 格式化为 iso 标准格式 utcnow.isoformat() # =\u003e '2020-02-22T04:00:03.537061' utcnow.strftime('%Y-%m-%dT%H:%M:%S.%fZ') # =\u003e '2020-02-22T04:00:03.537061Z' utcnow.date().strftime('%Y-%m-%dT%H:%M:%S.%fZ') # =\u003e '2020-02-22T00:00:00.000000Z' # 1.2 将 time.struct_time 格式化为日期字符串（貌似不支持 iso，可能是精度不够） time.strftime('%Y-%m-%dT%H:%M:%S', gm) # =\u003e '2020-02-22T04:00:03' # 1.3 将 datetime 格式化成指定格式 utcnow.strftime('%Y-%m-%d %H:%M:%S') # =\u003e '2020-02-22 04:00:03' # 2. 解析时间字符串 # 2.1 解析 iso 格式的时间字符串，手动指定格式（注意 %f 只对应六位小数，对9位小数它无能为力。。） dt.datetime.strptime('2020-02-22T04:00:03.537061Z', '%Y-%m-%dT%H:%M:%S.%fZ') # =\u003e datetime.datetime(2020, 2, 22, 4, 0, 3, 537061) # 2.2 解析 iso 格式的时间字符串(需要 python 3.7+) dt.datetime.fromisoformat('2020-02-22T04:00:03.537061') # =\u003e datetime.datetime(2020, 2, 22, 4, 0, 3, 537061) dt.date.fromisoformat('2020-02-22') # =\u003e datetime.date(2020, 2, 22) dt.time.fromisoformat(\"04:00:03.537061\") # =\u003e datetime.time(4, 0, 3, 537061) # 2.3 解析指定格式的字符串 dt.datetime.strptime('2020-02-22 04:00:03', '%Y-%m-%d %H:%M:%S') # =\u003e datetime.datetime(2020, 2, 22, 4, 0, 3) 4. 时区转换与日期格式化 # 上海时区：东八区 utc+8 tz_shanghai = dt.timezone(dt.timedelta(hours=8)) now_shanghai = dt.datetime.now(tz=tz_shanghai) now_shanghai.strftime('%Y-%m-%dT%H:%M:%S%Z') # =\u003e 2022-02-10T00:48:52UTC+08:00 ","date":"2022-02-13","objectID":"/posts/python-tips-and-tricks/:1:2","series":null,"tags":["Python","Tips","Tricks","常见错误"],"title":"Python 实用技巧与常见错误集锦","uri":"/posts/python-tips-and-tricks/#2-时间日期处理"},{"categories":["tech"],"content":" 2. 时间日期处理python3 在时间日期处理方面，有标准库 datetime 跟 calender，也有流行的第三方库 arrow 跟 maya. 标准库 datetime 有时候不太方便，比如没有提供解析 iso 格式的函数。 另外就是用标准库时，经常需要自定义格式化串。 相比之下，maya 和 arrow 这两个第三方库会方便很多。 不过第三方库并不是任何时候都可用，这里只介绍标准库 datetime 的用法，maya/arrow 请自行查找官方文档学习。 1. 获取当前时间 import time import datetime as dt # 1. 获取当前时间的时间戳 time.time() # 直接调用 c api，因此速度很快: 1582315203.537061 utcnow = dt.datetime.utcnow() # 当前的世界标准时间: datetime.datetime(2020, 2, 22, 4, 0, 3, 537061) utcnow.timestamp() # 将标准时转换成时间戳：datetime =\u003e 1582315203.537061 # 2. UTC 世界标准时间 time.gmtime() #输出为： time.struct_time(tm_year=2019, tm_mon=6, tm_mday=23, # tm_hour=3, tm_min=49, tm_sec=17, # tm_wday=6, tm_yday=174, tm_isdst=0) # 这实际上是一个命名元组 # 3. 构建一个指定的 datetime 实例 time_1997 = dt.datetime(year=1997, month=1, day=1) # =\u003e datetime.datetime(1997, 1, 1, 0, 0) dt.datetime(year=1997, month=1, day=1, minute=11) # =\u003e datetime.datetime(1997, 1, 1, 0, 11) 2. 时间日期的修改与运算 # 0. 日期的修改（修改年月时分秒） utcnow.replace(day=11) # =\u003e datetime.datetime(2020, 2, 11, 4, 0, 3, 537061) 修改 day utcnow.replace(hour=11) # =\u003e datetime.datetime(2020, 2, 22, 11, 0, 3, 537061) 修改 hour # 1. 日期与时间 date_utcnow = utcnow.date() # =\u003e datetime.date(2020, 2, 22) 年月日 time_utcnow = utcnow.time() # =\u003e datetime.time(4, 0, 3, 537061) 时分秒 # 2. 联结时间和日期（date 和 time 不能用加法联结） dt.datetime.combine(date_utcnow, time_utcnow) # =\u003e datetime.datetime(2020, 2, 22, 4, 0, 3, 537061) # 3. 日期的运算 # 3.1 datetime 之间只能计算时间差（减法），不能进行其他运算 utcnow - time_1997 # =\u003e datetime.timedelta(days=8452, seconds=14403, microseconds=537061) # 3.2 使用 timedelta 进行时间的增减 days_step = dt.timedelta(days=1) # 注意参数是复数形式 time_1997 + days_step # =\u003e datetime.datetime(1997, 1, 2, 0, 0) time_1997 - days_step # =\u003e datetime.datetime(1996, 12, 31, 0, 0) # 3.3 timedelta 之间也可以进行加减法 hours_step = dt.timedelta(hours=1) # =\u003e datetime.timedelta(seconds=3600) days_step + hours_step # =\u003e datetime.timedelta(days=1, seconds=3600) days_step - hours_step # =\u003e datetime.timedelta(seconds=82800) hours_step - days_step # =\u003e datetime.timedelta(days=-1, seconds=3600) # 3.4 timedelta 还可以按比例增减（与数字进行乘除法） hours_step * 2 # =\u003e datetime.timedelta(seconds=7200) days_step * -2 # =\u003e datetime.timedelta(days=-2) hours_step * 1.1 # =\u003e datetime.timedelta(seconds=3960) 3. 时间日期的格式化与解析先介绍下常用的格式化字符串： 普通格式 - ‘%Y-%m-%d %H:%M:%S’ =\u003e ‘2020-02-22 04:00:03’ ISO 格式 - ‘%Y-%m-%dT%H:%M:%S.%fZ’ =\u003e ‘2020-02-22T04:00:03.537061Z’ 带时区的格式 - ‘%Y-%m-%dT%H:%M:%S%Z’ =\u003e 2022-02-10T00:48:52UTC+08:00 需要时间对象自身有时区属性才行！否则格式化时会忽略 %Z 另外再介绍下 Python 两个时间格式化与解析函数的命名： strftime: 即 string formate time strptime: 即 string parse time # 1. 将时间格式化成字符串 # 1.1 将 datetime 格式化为 iso 标准格式 utcnow.isoformat() # =\u003e '2020-02-22T04:00:03.537061' utcnow.strftime('%Y-%m-%dT%H:%M:%S.%fZ') # =\u003e '2020-02-22T04:00:03.537061Z' utcnow.date().strftime('%Y-%m-%dT%H:%M:%S.%fZ') # =\u003e '2020-02-22T00:00:00.000000Z' # 1.2 将 time.struct_time 格式化为日期字符串（貌似不支持 iso，可能是精度不够） time.strftime('%Y-%m-%dT%H:%M:%S', gm) # =\u003e '2020-02-22T04:00:03' # 1.3 将 datetime 格式化成指定格式 utcnow.strftime('%Y-%m-%d %H:%M:%S') # =\u003e '2020-02-22 04:00:03' # 2. 解析时间字符串 # 2.1 解析 iso 格式的时间字符串，手动指定格式（注意 %f 只对应六位小数，对9位小数它无能为力。。） dt.datetime.strptime('2020-02-22T04:00:03.537061Z', '%Y-%m-%dT%H:%M:%S.%fZ') # =\u003e datetime.datetime(2020, 2, 22, 4, 0, 3, 537061) # 2.2 解析 iso 格式的时间字符串(需要 python 3.7+) dt.datetime.fromisoformat('2020-02-22T04:00:03.537061') # =\u003e datetime.datetime(2020, 2, 22, 4, 0, 3, 537061) dt.date.fromisoformat('2020-02-22') # =\u003e datetime.date(2020, 2, 22) dt.time.fromisoformat(\"04:00:03.537061\") # =\u003e datetime.time(4, 0, 3, 537061) # 2.3 解析指定格式的字符串 dt.datetime.strptime('2020-02-22 04:00:03', '%Y-%m-%d %H:%M:%S') # =\u003e datetime.datetime(2020, 2, 22, 4, 0, 3) 4. 时区转换与日期格式化 # 上海时区：东八区 utc+8 tz_shanghai = dt.timezone(dt.timedelta(hours=8)) now_shanghai = dt.datetime.now(tz=tz_shanghai) now_shanghai.strftime('%Y-%m-%dT%H:%M:%S%Z') # =\u003e 2022-02-10T00:48:52UTC+08:00 ","date":"2022-02-13","objectID":"/posts/python-tips-and-tricks/:1:2","series":null,"tags":["Python","Tips","Tricks","常见错误"],"title":"Python 实用技巧与常见错误集锦","uri":"/posts/python-tips-and-tricks/#1-获取当前时间"},{"categories":["tech"],"content":" 2. 时间日期处理python3 在时间日期处理方面，有标准库 datetime 跟 calender，也有流行的第三方库 arrow 跟 maya. 标准库 datetime 有时候不太方便，比如没有提供解析 iso 格式的函数。 另外就是用标准库时，经常需要自定义格式化串。 相比之下，maya 和 arrow 这两个第三方库会方便很多。 不过第三方库并不是任何时候都可用，这里只介绍标准库 datetime 的用法，maya/arrow 请自行查找官方文档学习。 1. 获取当前时间 import time import datetime as dt # 1. 获取当前时间的时间戳 time.time() # 直接调用 c api，因此速度很快: 1582315203.537061 utcnow = dt.datetime.utcnow() # 当前的世界标准时间: datetime.datetime(2020, 2, 22, 4, 0, 3, 537061) utcnow.timestamp() # 将标准时转换成时间戳：datetime =\u003e 1582315203.537061 # 2. UTC 世界标准时间 time.gmtime() #输出为： time.struct_time(tm_year=2019, tm_mon=6, tm_mday=23, # tm_hour=3, tm_min=49, tm_sec=17, # tm_wday=6, tm_yday=174, tm_isdst=0) # 这实际上是一个命名元组 # 3. 构建一个指定的 datetime 实例 time_1997 = dt.datetime(year=1997, month=1, day=1) # =\u003e datetime.datetime(1997, 1, 1, 0, 0) dt.datetime(year=1997, month=1, day=1, minute=11) # =\u003e datetime.datetime(1997, 1, 1, 0, 11) 2. 时间日期的修改与运算 # 0. 日期的修改（修改年月时分秒） utcnow.replace(day=11) # =\u003e datetime.datetime(2020, 2, 11, 4, 0, 3, 537061) 修改 day utcnow.replace(hour=11) # =\u003e datetime.datetime(2020, 2, 22, 11, 0, 3, 537061) 修改 hour # 1. 日期与时间 date_utcnow = utcnow.date() # =\u003e datetime.date(2020, 2, 22) 年月日 time_utcnow = utcnow.time() # =\u003e datetime.time(4, 0, 3, 537061) 时分秒 # 2. 联结时间和日期（date 和 time 不能用加法联结） dt.datetime.combine(date_utcnow, time_utcnow) # =\u003e datetime.datetime(2020, 2, 22, 4, 0, 3, 537061) # 3. 日期的运算 # 3.1 datetime 之间只能计算时间差（减法），不能进行其他运算 utcnow - time_1997 # =\u003e datetime.timedelta(days=8452, seconds=14403, microseconds=537061) # 3.2 使用 timedelta 进行时间的增减 days_step = dt.timedelta(days=1) # 注意参数是复数形式 time_1997 + days_step # =\u003e datetime.datetime(1997, 1, 2, 0, 0) time_1997 - days_step # =\u003e datetime.datetime(1996, 12, 31, 0, 0) # 3.3 timedelta 之间也可以进行加减法 hours_step = dt.timedelta(hours=1) # =\u003e datetime.timedelta(seconds=3600) days_step + hours_step # =\u003e datetime.timedelta(days=1, seconds=3600) days_step - hours_step # =\u003e datetime.timedelta(seconds=82800) hours_step - days_step # =\u003e datetime.timedelta(days=-1, seconds=3600) # 3.4 timedelta 还可以按比例增减（与数字进行乘除法） hours_step * 2 # =\u003e datetime.timedelta(seconds=7200) days_step * -2 # =\u003e datetime.timedelta(days=-2) hours_step * 1.1 # =\u003e datetime.timedelta(seconds=3960) 3. 时间日期的格式化与解析先介绍下常用的格式化字符串： 普通格式 - ‘%Y-%m-%d %H:%M:%S’ =\u003e ‘2020-02-22 04:00:03’ ISO 格式 - ‘%Y-%m-%dT%H:%M:%S.%fZ’ =\u003e ‘2020-02-22T04:00:03.537061Z’ 带时区的格式 - ‘%Y-%m-%dT%H:%M:%S%Z’ =\u003e 2022-02-10T00:48:52UTC+08:00 需要时间对象自身有时区属性才行！否则格式化时会忽略 %Z 另外再介绍下 Python 两个时间格式化与解析函数的命名： strftime: 即 string formate time strptime: 即 string parse time # 1. 将时间格式化成字符串 # 1.1 将 datetime 格式化为 iso 标准格式 utcnow.isoformat() # =\u003e '2020-02-22T04:00:03.537061' utcnow.strftime('%Y-%m-%dT%H:%M:%S.%fZ') # =\u003e '2020-02-22T04:00:03.537061Z' utcnow.date().strftime('%Y-%m-%dT%H:%M:%S.%fZ') # =\u003e '2020-02-22T00:00:00.000000Z' # 1.2 将 time.struct_time 格式化为日期字符串（貌似不支持 iso，可能是精度不够） time.strftime('%Y-%m-%dT%H:%M:%S', gm) # =\u003e '2020-02-22T04:00:03' # 1.3 将 datetime 格式化成指定格式 utcnow.strftime('%Y-%m-%d %H:%M:%S') # =\u003e '2020-02-22 04:00:03' # 2. 解析时间字符串 # 2.1 解析 iso 格式的时间字符串，手动指定格式（注意 %f 只对应六位小数，对9位小数它无能为力。。） dt.datetime.strptime('2020-02-22T04:00:03.537061Z', '%Y-%m-%dT%H:%M:%S.%fZ') # =\u003e datetime.datetime(2020, 2, 22, 4, 0, 3, 537061) # 2.2 解析 iso 格式的时间字符串(需要 python 3.7+) dt.datetime.fromisoformat('2020-02-22T04:00:03.537061') # =\u003e datetime.datetime(2020, 2, 22, 4, 0, 3, 537061) dt.date.fromisoformat('2020-02-22') # =\u003e datetime.date(2020, 2, 22) dt.time.fromisoformat(\"04:00:03.537061\") # =\u003e datetime.time(4, 0, 3, 537061) # 2.3 解析指定格式的字符串 dt.datetime.strptime('2020-02-22 04:00:03', '%Y-%m-%d %H:%M:%S') # =\u003e datetime.datetime(2020, 2, 22, 4, 0, 3) 4. 时区转换与日期格式化 # 上海时区：东八区 utc+8 tz_shanghai = dt.timezone(dt.timedelta(hours=8)) now_shanghai = dt.datetime.now(tz=tz_shanghai) now_shanghai.strftime('%Y-%m-%dT%H:%M:%S%Z') # =\u003e 2022-02-10T00:48:52UTC+08:00 ","date":"2022-02-13","objectID":"/posts/python-tips-and-tricks/:1:2","series":null,"tags":["Python","Tips","Tricks","常见错误"],"title":"Python 实用技巧与常见错误集锦","uri":"/posts/python-tips-and-tricks/#2-时间日期的修改与运算"},{"categories":["tech"],"content":" 2. 时间日期处理python3 在时间日期处理方面，有标准库 datetime 跟 calender，也有流行的第三方库 arrow 跟 maya. 标准库 datetime 有时候不太方便，比如没有提供解析 iso 格式的函数。 另外就是用标准库时，经常需要自定义格式化串。 相比之下，maya 和 arrow 这两个第三方库会方便很多。 不过第三方库并不是任何时候都可用，这里只介绍标准库 datetime 的用法，maya/arrow 请自行查找官方文档学习。 1. 获取当前时间 import time import datetime as dt # 1. 获取当前时间的时间戳 time.time() # 直接调用 c api，因此速度很快: 1582315203.537061 utcnow = dt.datetime.utcnow() # 当前的世界标准时间: datetime.datetime(2020, 2, 22, 4, 0, 3, 537061) utcnow.timestamp() # 将标准时转换成时间戳：datetime =\u003e 1582315203.537061 # 2. UTC 世界标准时间 time.gmtime() #输出为： time.struct_time(tm_year=2019, tm_mon=6, tm_mday=23, # tm_hour=3, tm_min=49, tm_sec=17, # tm_wday=6, tm_yday=174, tm_isdst=0) # 这实际上是一个命名元组 # 3. 构建一个指定的 datetime 实例 time_1997 = dt.datetime(year=1997, month=1, day=1) # =\u003e datetime.datetime(1997, 1, 1, 0, 0) dt.datetime(year=1997, month=1, day=1, minute=11) # =\u003e datetime.datetime(1997, 1, 1, 0, 11) 2. 时间日期的修改与运算 # 0. 日期的修改（修改年月时分秒） utcnow.replace(day=11) # =\u003e datetime.datetime(2020, 2, 11, 4, 0, 3, 537061) 修改 day utcnow.replace(hour=11) # =\u003e datetime.datetime(2020, 2, 22, 11, 0, 3, 537061) 修改 hour # 1. 日期与时间 date_utcnow = utcnow.date() # =\u003e datetime.date(2020, 2, 22) 年月日 time_utcnow = utcnow.time() # =\u003e datetime.time(4, 0, 3, 537061) 时分秒 # 2. 联结时间和日期（date 和 time 不能用加法联结） dt.datetime.combine(date_utcnow, time_utcnow) # =\u003e datetime.datetime(2020, 2, 22, 4, 0, 3, 537061) # 3. 日期的运算 # 3.1 datetime 之间只能计算时间差（减法），不能进行其他运算 utcnow - time_1997 # =\u003e datetime.timedelta(days=8452, seconds=14403, microseconds=537061) # 3.2 使用 timedelta 进行时间的增减 days_step = dt.timedelta(days=1) # 注意参数是复数形式 time_1997 + days_step # =\u003e datetime.datetime(1997, 1, 2, 0, 0) time_1997 - days_step # =\u003e datetime.datetime(1996, 12, 31, 0, 0) # 3.3 timedelta 之间也可以进行加减法 hours_step = dt.timedelta(hours=1) # =\u003e datetime.timedelta(seconds=3600) days_step + hours_step # =\u003e datetime.timedelta(days=1, seconds=3600) days_step - hours_step # =\u003e datetime.timedelta(seconds=82800) hours_step - days_step # =\u003e datetime.timedelta(days=-1, seconds=3600) # 3.4 timedelta 还可以按比例增减（与数字进行乘除法） hours_step * 2 # =\u003e datetime.timedelta(seconds=7200) days_step * -2 # =\u003e datetime.timedelta(days=-2) hours_step * 1.1 # =\u003e datetime.timedelta(seconds=3960) 3. 时间日期的格式化与解析先介绍下常用的格式化字符串： 普通格式 - ‘%Y-%m-%d %H:%M:%S’ =\u003e ‘2020-02-22 04:00:03’ ISO 格式 - ‘%Y-%m-%dT%H:%M:%S.%fZ’ =\u003e ‘2020-02-22T04:00:03.537061Z’ 带时区的格式 - ‘%Y-%m-%dT%H:%M:%S%Z’ =\u003e 2022-02-10T00:48:52UTC+08:00 需要时间对象自身有时区属性才行！否则格式化时会忽略 %Z 另外再介绍下 Python 两个时间格式化与解析函数的命名： strftime: 即 string formate time strptime: 即 string parse time # 1. 将时间格式化成字符串 # 1.1 将 datetime 格式化为 iso 标准格式 utcnow.isoformat() # =\u003e '2020-02-22T04:00:03.537061' utcnow.strftime('%Y-%m-%dT%H:%M:%S.%fZ') # =\u003e '2020-02-22T04:00:03.537061Z' utcnow.date().strftime('%Y-%m-%dT%H:%M:%S.%fZ') # =\u003e '2020-02-22T00:00:00.000000Z' # 1.2 将 time.struct_time 格式化为日期字符串（貌似不支持 iso，可能是精度不够） time.strftime('%Y-%m-%dT%H:%M:%S', gm) # =\u003e '2020-02-22T04:00:03' # 1.3 将 datetime 格式化成指定格式 utcnow.strftime('%Y-%m-%d %H:%M:%S') # =\u003e '2020-02-22 04:00:03' # 2. 解析时间字符串 # 2.1 解析 iso 格式的时间字符串，手动指定格式（注意 %f 只对应六位小数，对9位小数它无能为力。。） dt.datetime.strptime('2020-02-22T04:00:03.537061Z', '%Y-%m-%dT%H:%M:%S.%fZ') # =\u003e datetime.datetime(2020, 2, 22, 4, 0, 3, 537061) # 2.2 解析 iso 格式的时间字符串(需要 python 3.7+) dt.datetime.fromisoformat('2020-02-22T04:00:03.537061') # =\u003e datetime.datetime(2020, 2, 22, 4, 0, 3, 537061) dt.date.fromisoformat('2020-02-22') # =\u003e datetime.date(2020, 2, 22) dt.time.fromisoformat(\"04:00:03.537061\") # =\u003e datetime.time(4, 0, 3, 537061) # 2.3 解析指定格式的字符串 dt.datetime.strptime('2020-02-22 04:00:03', '%Y-%m-%d %H:%M:%S') # =\u003e datetime.datetime(2020, 2, 22, 4, 0, 3) 4. 时区转换与日期格式化 # 上海时区：东八区 utc+8 tz_shanghai = dt.timezone(dt.timedelta(hours=8)) now_shanghai = dt.datetime.now(tz=tz_shanghai) now_shanghai.strftime('%Y-%m-%dT%H:%M:%S%Z') # =\u003e 2022-02-10T00:48:52UTC+08:00 ","date":"2022-02-13","objectID":"/posts/python-tips-and-tricks/:1:2","series":null,"tags":["Python","Tips","Tricks","常见错误"],"title":"Python 实用技巧与常见错误集锦","uri":"/posts/python-tips-and-tricks/#3-时间日期的格式化与解析"},{"categories":["tech"],"content":" 2. 时间日期处理python3 在时间日期处理方面，有标准库 datetime 跟 calender，也有流行的第三方库 arrow 跟 maya. 标准库 datetime 有时候不太方便，比如没有提供解析 iso 格式的函数。 另外就是用标准库时，经常需要自定义格式化串。 相比之下，maya 和 arrow 这两个第三方库会方便很多。 不过第三方库并不是任何时候都可用，这里只介绍标准库 datetime 的用法，maya/arrow 请自行查找官方文档学习。 1. 获取当前时间 import time import datetime as dt # 1. 获取当前时间的时间戳 time.time() # 直接调用 c api，因此速度很快: 1582315203.537061 utcnow = dt.datetime.utcnow() # 当前的世界标准时间: datetime.datetime(2020, 2, 22, 4, 0, 3, 537061) utcnow.timestamp() # 将标准时转换成时间戳：datetime =\u003e 1582315203.537061 # 2. UTC 世界标准时间 time.gmtime() #输出为： time.struct_time(tm_year=2019, tm_mon=6, tm_mday=23, # tm_hour=3, tm_min=49, tm_sec=17, # tm_wday=6, tm_yday=174, tm_isdst=0) # 这实际上是一个命名元组 # 3. 构建一个指定的 datetime 实例 time_1997 = dt.datetime(year=1997, month=1, day=1) # =\u003e datetime.datetime(1997, 1, 1, 0, 0) dt.datetime(year=1997, month=1, day=1, minute=11) # =\u003e datetime.datetime(1997, 1, 1, 0, 11) 2. 时间日期的修改与运算 # 0. 日期的修改（修改年月时分秒） utcnow.replace(day=11) # =\u003e datetime.datetime(2020, 2, 11, 4, 0, 3, 537061) 修改 day utcnow.replace(hour=11) # =\u003e datetime.datetime(2020, 2, 22, 11, 0, 3, 537061) 修改 hour # 1. 日期与时间 date_utcnow = utcnow.date() # =\u003e datetime.date(2020, 2, 22) 年月日 time_utcnow = utcnow.time() # =\u003e datetime.time(4, 0, 3, 537061) 时分秒 # 2. 联结时间和日期（date 和 time 不能用加法联结） dt.datetime.combine(date_utcnow, time_utcnow) # =\u003e datetime.datetime(2020, 2, 22, 4, 0, 3, 537061) # 3. 日期的运算 # 3.1 datetime 之间只能计算时间差（减法），不能进行其他运算 utcnow - time_1997 # =\u003e datetime.timedelta(days=8452, seconds=14403, microseconds=537061) # 3.2 使用 timedelta 进行时间的增减 days_step = dt.timedelta(days=1) # 注意参数是复数形式 time_1997 + days_step # =\u003e datetime.datetime(1997, 1, 2, 0, 0) time_1997 - days_step # =\u003e datetime.datetime(1996, 12, 31, 0, 0) # 3.3 timedelta 之间也可以进行加减法 hours_step = dt.timedelta(hours=1) # =\u003e datetime.timedelta(seconds=3600) days_step + hours_step # =\u003e datetime.timedelta(days=1, seconds=3600) days_step - hours_step # =\u003e datetime.timedelta(seconds=82800) hours_step - days_step # =\u003e datetime.timedelta(days=-1, seconds=3600) # 3.4 timedelta 还可以按比例增减（与数字进行乘除法） hours_step * 2 # =\u003e datetime.timedelta(seconds=7200) days_step * -2 # =\u003e datetime.timedelta(days=-2) hours_step * 1.1 # =\u003e datetime.timedelta(seconds=3960) 3. 时间日期的格式化与解析先介绍下常用的格式化字符串： 普通格式 - ‘%Y-%m-%d %H:%M:%S’ =\u003e ‘2020-02-22 04:00:03’ ISO 格式 - ‘%Y-%m-%dT%H:%M:%S.%fZ’ =\u003e ‘2020-02-22T04:00:03.537061Z’ 带时区的格式 - ‘%Y-%m-%dT%H:%M:%S%Z’ =\u003e 2022-02-10T00:48:52UTC+08:00 需要时间对象自身有时区属性才行！否则格式化时会忽略 %Z 另外再介绍下 Python 两个时间格式化与解析函数的命名： strftime: 即 string formate time strptime: 即 string parse time # 1. 将时间格式化成字符串 # 1.1 将 datetime 格式化为 iso 标准格式 utcnow.isoformat() # =\u003e '2020-02-22T04:00:03.537061' utcnow.strftime('%Y-%m-%dT%H:%M:%S.%fZ') # =\u003e '2020-02-22T04:00:03.537061Z' utcnow.date().strftime('%Y-%m-%dT%H:%M:%S.%fZ') # =\u003e '2020-02-22T00:00:00.000000Z' # 1.2 将 time.struct_time 格式化为日期字符串（貌似不支持 iso，可能是精度不够） time.strftime('%Y-%m-%dT%H:%M:%S', gm) # =\u003e '2020-02-22T04:00:03' # 1.3 将 datetime 格式化成指定格式 utcnow.strftime('%Y-%m-%d %H:%M:%S') # =\u003e '2020-02-22 04:00:03' # 2. 解析时间字符串 # 2.1 解析 iso 格式的时间字符串，手动指定格式（注意 %f 只对应六位小数，对9位小数它无能为力。。） dt.datetime.strptime('2020-02-22T04:00:03.537061Z', '%Y-%m-%dT%H:%M:%S.%fZ') # =\u003e datetime.datetime(2020, 2, 22, 4, 0, 3, 537061) # 2.2 解析 iso 格式的时间字符串(需要 python 3.7+) dt.datetime.fromisoformat('2020-02-22T04:00:03.537061') # =\u003e datetime.datetime(2020, 2, 22, 4, 0, 3, 537061) dt.date.fromisoformat('2020-02-22') # =\u003e datetime.date(2020, 2, 22) dt.time.fromisoformat(\"04:00:03.537061\") # =\u003e datetime.time(4, 0, 3, 537061) # 2.3 解析指定格式的字符串 dt.datetime.strptime('2020-02-22 04:00:03', '%Y-%m-%d %H:%M:%S') # =\u003e datetime.datetime(2020, 2, 22, 4, 0, 3) 4. 时区转换与日期格式化 # 上海时区：东八区 utc+8 tz_shanghai = dt.timezone(dt.timedelta(hours=8)) now_shanghai = dt.datetime.now(tz=tz_shanghai) now_shanghai.strftime('%Y-%m-%dT%H:%M:%S%Z') # =\u003e 2022-02-10T00:48:52UTC+08:00 ","date":"2022-02-13","objectID":"/posts/python-tips-and-tricks/:1:2","series":null,"tags":["Python","Tips","Tricks","常见错误"],"title":"Python 实用技巧与常见错误集锦","uri":"/posts/python-tips-and-tricks/#4-时区转换与日期格式化"},{"categories":["tech"],"content":" 3. 排序常用库 - operatoroperator 模块包含四种类型的方法： 1. operator.itemgetter经常被用于 sorted/max/mix/itertools.groupby 等 使用方法： # itemgetter f = itemgetter(2) f(r) # return r[2] # 还能一次获取多个值，像 numpy 那样索引 f2 = itemgetter(2,4,5) f2(r) # return (r[2], r[4], r[5]) # 或者使用 slice 切片 s = itemgetter(slice(2, None)) s[r] # return r[2:] # dict 索引也能用 d = itemgetter('rank', 'name') d[r] # return d['rank'], d['name'] 用途： # 用于指定用于比较大小的属性 key = itemgetter(1) sorted(iterable, key=key) # 使用 iterable[1] 对 iterable 进行排序 max(iterable, key=key) # 找出最大的元素，使用 iterable[1] 做比较 # 用于高级切片（比如像 numpy 那样的，指定只获取某几列） s = itemgetter(1,3,4) matrix = [[0,1,2,3,4], [1,2,3,4,5]] map(s, matrix) # list 后得到 [(1, 3, 4), (2,4,5)] 2. operator.attrgetter可用于动态获取对象的属性，与直接用 getattr() 不同的是，它可以嵌套访问属性。 # 嵌套访问属性 att = attrgetter(\"a.b.c\") att(obj) # return obj.a.b.c # 和 itemgetter 一样，也可以一次获取多个属性 att = attrgetter(\"a.b.c\", \"x.y\") att(obj) # return (obj.a.b.c, obj.x.y) # 不嵌套的话，用 getattr 就行 getattr(obj, \"a\") # return obj.a 这里可以回顾一下类的两个魔法函数： __getattr__: 当被访问的属性不存在时，这个方法会被调用，它的返回值会成为对象的该属性。 用于动态生成实例的属性/函数 __getattribute__: 与 __getattr__ 唯一的差别在于，访问对象的任何属性，都会直接调用这个方法，不管属性存不存在。 3. operator.methodcaller可用于调用函数，它和 attrgetter 很像，差别在于 attrgetter 只是返回指定的属性，而 methodcaller 会直接把指定的属性当成函数调用，然后返回结果。 举例 f = methodcaller('name', 'foo', bar=1) f(b) # returns b.name('foo', bar=1) 4. 各种操作符对应的函数operator.add、operator.sub、operator.mul、operator.div 等等，函数式编程有时需要用到。 ","date":"2022-02-13","objectID":"/posts/python-tips-and-tricks/:1:3","series":null,"tags":["Python","Tips","Tricks","常见错误"],"title":"Python 实用技巧与常见错误集锦","uri":"/posts/python-tips-and-tricks/#3-排序常用库---operator"},{"categories":["tech"],"content":" 3. 排序常用库 - operatoroperator 模块包含四种类型的方法： 1. operator.itemgetter经常被用于 sorted/max/mix/itertools.groupby 等 使用方法： # itemgetter f = itemgetter(2) f(r) # return r[2] # 还能一次获取多个值，像 numpy 那样索引 f2 = itemgetter(2,4,5) f2(r) # return (r[2], r[4], r[5]) # 或者使用 slice 切片 s = itemgetter(slice(2, None)) s[r] # return r[2:] # dict 索引也能用 d = itemgetter('rank', 'name') d[r] # return d['rank'], d['name'] 用途： # 用于指定用于比较大小的属性 key = itemgetter(1) sorted(iterable, key=key) # 使用 iterable[1] 对 iterable 进行排序 max(iterable, key=key) # 找出最大的元素，使用 iterable[1] 做比较 # 用于高级切片（比如像 numpy 那样的，指定只获取某几列） s = itemgetter(1,3,4) matrix = [[0,1,2,3,4], [1,2,3,4,5]] map(s, matrix) # list 后得到 [(1, 3, 4), (2,4,5)] 2. operator.attrgetter可用于动态获取对象的属性，与直接用 getattr() 不同的是，它可以嵌套访问属性。 # 嵌套访问属性 att = attrgetter(\"a.b.c\") att(obj) # return obj.a.b.c # 和 itemgetter 一样，也可以一次获取多个属性 att = attrgetter(\"a.b.c\", \"x.y\") att(obj) # return (obj.a.b.c, obj.x.y) # 不嵌套的话，用 getattr 就行 getattr(obj, \"a\") # return obj.a 这里可以回顾一下类的两个魔法函数： __getattr__: 当被访问的属性不存在时，这个方法会被调用，它的返回值会成为对象的该属性。 用于动态生成实例的属性/函数 __getattribute__: 与 __getattr__ 唯一的差别在于，访问对象的任何属性，都会直接调用这个方法，不管属性存不存在。 3. operator.methodcaller可用于调用函数，它和 attrgetter 很像，差别在于 attrgetter 只是返回指定的属性，而 methodcaller 会直接把指定的属性当成函数调用，然后返回结果。 举例 f = methodcaller('name', 'foo', bar=1) f(b) # returns b.name('foo', bar=1) 4. 各种操作符对应的函数operator.add、operator.sub、operator.mul、operator.div 等等，函数式编程有时需要用到。 ","date":"2022-02-13","objectID":"/posts/python-tips-and-tricks/:1:3","series":null,"tags":["Python","Tips","Tricks","常见错误"],"title":"Python 实用技巧与常见错误集锦","uri":"/posts/python-tips-and-tricks/#1-operatoritemgetter"},{"categories":["tech"],"content":" 3. 排序常用库 - operatoroperator 模块包含四种类型的方法： 1. operator.itemgetter经常被用于 sorted/max/mix/itertools.groupby 等 使用方法： # itemgetter f = itemgetter(2) f(r) # return r[2] # 还能一次获取多个值，像 numpy 那样索引 f2 = itemgetter(2,4,5) f2(r) # return (r[2], r[4], r[5]) # 或者使用 slice 切片 s = itemgetter(slice(2, None)) s[r] # return r[2:] # dict 索引也能用 d = itemgetter('rank', 'name') d[r] # return d['rank'], d['name'] 用途： # 用于指定用于比较大小的属性 key = itemgetter(1) sorted(iterable, key=key) # 使用 iterable[1] 对 iterable 进行排序 max(iterable, key=key) # 找出最大的元素，使用 iterable[1] 做比较 # 用于高级切片（比如像 numpy 那样的，指定只获取某几列） s = itemgetter(1,3,4) matrix = [[0,1,2,3,4], [1,2,3,4,5]] map(s, matrix) # list 后得到 [(1, 3, 4), (2,4,5)] 2. operator.attrgetter可用于动态获取对象的属性，与直接用 getattr() 不同的是，它可以嵌套访问属性。 # 嵌套访问属性 att = attrgetter(\"a.b.c\") att(obj) # return obj.a.b.c # 和 itemgetter 一样，也可以一次获取多个属性 att = attrgetter(\"a.b.c\", \"x.y\") att(obj) # return (obj.a.b.c, obj.x.y) # 不嵌套的话，用 getattr 就行 getattr(obj, \"a\") # return obj.a 这里可以回顾一下类的两个魔法函数： __getattr__: 当被访问的属性不存在时，这个方法会被调用，它的返回值会成为对象的该属性。 用于动态生成实例的属性/函数 __getattribute__: 与 __getattr__ 唯一的差别在于，访问对象的任何属性，都会直接调用这个方法，不管属性存不存在。 3. operator.methodcaller可用于调用函数，它和 attrgetter 很像，差别在于 attrgetter 只是返回指定的属性，而 methodcaller 会直接把指定的属性当成函数调用，然后返回结果。 举例 f = methodcaller('name', 'foo', bar=1) f(b) # returns b.name('foo', bar=1) 4. 各种操作符对应的函数operator.add、operator.sub、operator.mul、operator.div 等等，函数式编程有时需要用到。 ","date":"2022-02-13","objectID":"/posts/python-tips-and-tricks/:1:3","series":null,"tags":["Python","Tips","Tricks","常见错误"],"title":"Python 实用技巧与常见错误集锦","uri":"/posts/python-tips-and-tricks/#2-operatorattrgetter"},{"categories":["tech"],"content":" 3. 排序常用库 - operatoroperator 模块包含四种类型的方法： 1. operator.itemgetter经常被用于 sorted/max/mix/itertools.groupby 等 使用方法： # itemgetter f = itemgetter(2) f(r) # return r[2] # 还能一次获取多个值，像 numpy 那样索引 f2 = itemgetter(2,4,5) f2(r) # return (r[2], r[4], r[5]) # 或者使用 slice 切片 s = itemgetter(slice(2, None)) s[r] # return r[2:] # dict 索引也能用 d = itemgetter('rank', 'name') d[r] # return d['rank'], d['name'] 用途： # 用于指定用于比较大小的属性 key = itemgetter(1) sorted(iterable, key=key) # 使用 iterable[1] 对 iterable 进行排序 max(iterable, key=key) # 找出最大的元素，使用 iterable[1] 做比较 # 用于高级切片（比如像 numpy 那样的，指定只获取某几列） s = itemgetter(1,3,4) matrix = [[0,1,2,3,4], [1,2,3,4,5]] map(s, matrix) # list 后得到 [(1, 3, 4), (2,4,5)] 2. operator.attrgetter可用于动态获取对象的属性，与直接用 getattr() 不同的是，它可以嵌套访问属性。 # 嵌套访问属性 att = attrgetter(\"a.b.c\") att(obj) # return obj.a.b.c # 和 itemgetter 一样，也可以一次获取多个属性 att = attrgetter(\"a.b.c\", \"x.y\") att(obj) # return (obj.a.b.c, obj.x.y) # 不嵌套的话，用 getattr 就行 getattr(obj, \"a\") # return obj.a 这里可以回顾一下类的两个魔法函数： __getattr__: 当被访问的属性不存在时，这个方法会被调用，它的返回值会成为对象的该属性。 用于动态生成实例的属性/函数 __getattribute__: 与 __getattr__ 唯一的差别在于，访问对象的任何属性，都会直接调用这个方法，不管属性存不存在。 3. operator.methodcaller可用于调用函数，它和 attrgetter 很像，差别在于 attrgetter 只是返回指定的属性，而 methodcaller 会直接把指定的属性当成函数调用，然后返回结果。 举例 f = methodcaller('name', 'foo', bar=1) f(b) # returns b.name('foo', bar=1) 4. 各种操作符对应的函数operator.add、operator.sub、operator.mul、operator.div 等等，函数式编程有时需要用到。 ","date":"2022-02-13","objectID":"/posts/python-tips-and-tricks/:1:3","series":null,"tags":["Python","Tips","Tricks","常见错误"],"title":"Python 实用技巧与常见错误集锦","uri":"/posts/python-tips-and-tricks/#3-operatormethodcaller"},{"categories":["tech"],"content":" 3. 排序常用库 - operatoroperator 模块包含四种类型的方法： 1. operator.itemgetter经常被用于 sorted/max/mix/itertools.groupby 等 使用方法： # itemgetter f = itemgetter(2) f(r) # return r[2] # 还能一次获取多个值，像 numpy 那样索引 f2 = itemgetter(2,4,5) f2(r) # return (r[2], r[4], r[5]) # 或者使用 slice 切片 s = itemgetter(slice(2, None)) s[r] # return r[2:] # dict 索引也能用 d = itemgetter('rank', 'name') d[r] # return d['rank'], d['name'] 用途： # 用于指定用于比较大小的属性 key = itemgetter(1) sorted(iterable, key=key) # 使用 iterable[1] 对 iterable 进行排序 max(iterable, key=key) # 找出最大的元素，使用 iterable[1] 做比较 # 用于高级切片（比如像 numpy 那样的，指定只获取某几列） s = itemgetter(1,3,4) matrix = [[0,1,2,3,4], [1,2,3,4,5]] map(s, matrix) # list 后得到 [(1, 3, 4), (2,4,5)] 2. operator.attrgetter可用于动态获取对象的属性，与直接用 getattr() 不同的是，它可以嵌套访问属性。 # 嵌套访问属性 att = attrgetter(\"a.b.c\") att(obj) # return obj.a.b.c # 和 itemgetter 一样，也可以一次获取多个属性 att = attrgetter(\"a.b.c\", \"x.y\") att(obj) # return (obj.a.b.c, obj.x.y) # 不嵌套的话，用 getattr 就行 getattr(obj, \"a\") # return obj.a 这里可以回顾一下类的两个魔法函数： __getattr__: 当被访问的属性不存在时，这个方法会被调用，它的返回值会成为对象的该属性。 用于动态生成实例的属性/函数 __getattribute__: 与 __getattr__ 唯一的差别在于，访问对象的任何属性，都会直接调用这个方法，不管属性存不存在。 3. operator.methodcaller可用于调用函数，它和 attrgetter 很像，差别在于 attrgetter 只是返回指定的属性，而 methodcaller 会直接把指定的属性当成函数调用，然后返回结果。 举例 f = methodcaller('name', 'foo', bar=1) f(b) # returns b.name('foo', bar=1) 4. 各种操作符对应的函数operator.add、operator.sub、operator.mul、operator.div 等等，函数式编程有时需要用到。 ","date":"2022-02-13","objectID":"/posts/python-tips-and-tricks/:1:3","series":null,"tags":["Python","Tips","Tricks","常见错误"],"title":"Python 实用技巧与常见错误集锦","uri":"/posts/python-tips-and-tricks/#4-各种操作符对应的函数"},{"categories":["tech"],"content":" 4. itertoolsitertools 提供了许多针对可迭代对象的实用函数 方法很多，基本不可能一次全记住。还是要用到时多查吧。大致记住有提供哪些功能，需要用到时能想起可以查这个模块就行。 1. 无限迭代器 count(start=0, step=1): 从 start 开始，每次迭代时，返回值都加一个 step 默认返回序列为 0 1 2 3… cycle(iterable): 不断循环迭代 iterable repeat(element, times=None): 默认永远返回 element。（如果 times 不为 None，就迭代 times 后结束） 2. 排列组合迭代器 product(p1, p2, …, repeat=1)：p1, p2… 的元素的笛卡尔积，相当于多层 for 循环 repeat 指参数重复次数，比如 \u003e\u003e\u003e from itertools import product \u003e\u003e\u003e r = product([1, 2], [3, 4], [5, 6]) # 重复一次，也就是 (p1, p2, p3) 的笛卡尔积 \u003e\u003e\u003e pprint(list(r)) [(1, 3, 5), (1, 3, 6), (1, 4, 5), (1, 4, 6), (2, 3, 5), (2, 3, 6), (2, 4, 5), (2, 4, 6)] \u003e\u003e\u003e r2 = product([1, 2], [3, 4], [5, 6], repeat=2) # 重复两次，即 (p1, p2, p3, p1, p2, p3) 的笛卡尔积 \u003e\u003e\u003e pprint(list(r2)) [(1, 3, 5, 1, 3, 5), (1, 3, 5, 1, 3, 6), (1, 3, 5, 1, 4, 5), (1, 3, 5, 1, 4, 6), (1, 3, 5, 2, 3, 5), ... permutations(p[, r])：p 中元素，长度为 r 的所有可能的排列。相当于 product 去重后的结果。 combinations(p, r)：既然有排列，当然就有组合了。 3. 其他 zip_longest(*iterables, fillvalue=None)：和 zip 的差别在于，缺失的元素它会用 fillvalue 补全，而不是直接结束。 takewhile() dropwhile() groupby() 等等等，用得到的时候再查了。。。 ","date":"2022-02-13","objectID":"/posts/python-tips-and-tricks/:1:4","series":null,"tags":["Python","Tips","Tricks","常见错误"],"title":"Python 实用技巧与常见错误集锦","uri":"/posts/python-tips-and-tricks/#4-itertools"},{"categories":["tech"],"content":" 4. itertoolsitertools 提供了许多针对可迭代对象的实用函数 方法很多，基本不可能一次全记住。还是要用到时多查吧。大致记住有提供哪些功能，需要用到时能想起可以查这个模块就行。 1. 无限迭代器 count(start=0, step=1): 从 start 开始，每次迭代时，返回值都加一个 step 默认返回序列为 0 1 2 3… cycle(iterable): 不断循环迭代 iterable repeat(element, times=None): 默认永远返回 element。（如果 times 不为 None，就迭代 times 后结束） 2. 排列组合迭代器 product(p1, p2, …, repeat=1)：p1, p2… 的元素的笛卡尔积，相当于多层 for 循环 repeat 指参数重复次数，比如 \u003e\u003e\u003e from itertools import product \u003e\u003e\u003e r = product([1, 2], [3, 4], [5, 6]) # 重复一次，也就是 (p1, p2, p3) 的笛卡尔积 \u003e\u003e\u003e pprint(list(r)) [(1, 3, 5), (1, 3, 6), (1, 4, 5), (1, 4, 6), (2, 3, 5), (2, 3, 6), (2, 4, 5), (2, 4, 6)] \u003e\u003e\u003e r2 = product([1, 2], [3, 4], [5, 6], repeat=2) # 重复两次，即 (p1, p2, p3, p1, p2, p3) 的笛卡尔积 \u003e\u003e\u003e pprint(list(r2)) [(1, 3, 5, 1, 3, 5), (1, 3, 5, 1, 3, 6), (1, 3, 5, 1, 4, 5), (1, 3, 5, 1, 4, 6), (1, 3, 5, 2, 3, 5), ... permutations(p[, r])：p 中元素，长度为 r 的所有可能的排列。相当于 product 去重后的结果。 combinations(p, r)：既然有排列，当然就有组合了。 3. 其他 zip_longest(*iterables, fillvalue=None)：和 zip 的差别在于，缺失的元素它会用 fillvalue 补全，而不是直接结束。 takewhile() dropwhile() groupby() 等等等，用得到的时候再查了。。。 ","date":"2022-02-13","objectID":"/posts/python-tips-and-tricks/:1:4","series":null,"tags":["Python","Tips","Tricks","常见错误"],"title":"Python 实用技巧与常见错误集锦","uri":"/posts/python-tips-and-tricks/#1-无限迭代器"},{"categories":["tech"],"content":" 4. itertoolsitertools 提供了许多针对可迭代对象的实用函数 方法很多，基本不可能一次全记住。还是要用到时多查吧。大致记住有提供哪些功能，需要用到时能想起可以查这个模块就行。 1. 无限迭代器 count(start=0, step=1): 从 start 开始，每次迭代时，返回值都加一个 step 默认返回序列为 0 1 2 3… cycle(iterable): 不断循环迭代 iterable repeat(element, times=None): 默认永远返回 element。（如果 times 不为 None，就迭代 times 后结束） 2. 排列组合迭代器 product(p1, p2, …, repeat=1)：p1, p2… 的元素的笛卡尔积，相当于多层 for 循环 repeat 指参数重复次数，比如 \u003e\u003e\u003e from itertools import product \u003e\u003e\u003e r = product([1, 2], [3, 4], [5, 6]) # 重复一次，也就是 (p1, p2, p3) 的笛卡尔积 \u003e\u003e\u003e pprint(list(r)) [(1, 3, 5), (1, 3, 6), (1, 4, 5), (1, 4, 6), (2, 3, 5), (2, 3, 6), (2, 4, 5), (2, 4, 6)] \u003e\u003e\u003e r2 = product([1, 2], [3, 4], [5, 6], repeat=2) # 重复两次，即 (p1, p2, p3, p1, p2, p3) 的笛卡尔积 \u003e\u003e\u003e pprint(list(r2)) [(1, 3, 5, 1, 3, 5), (1, 3, 5, 1, 3, 6), (1, 3, 5, 1, 4, 5), (1, 3, 5, 1, 4, 6), (1, 3, 5, 2, 3, 5), ... permutations(p[, r])：p 中元素，长度为 r 的所有可能的排列。相当于 product 去重后的结果。 combinations(p, r)：既然有排列，当然就有组合了。 3. 其他 zip_longest(*iterables, fillvalue=None)：和 zip 的差别在于，缺失的元素它会用 fillvalue 补全，而不是直接结束。 takewhile() dropwhile() groupby() 等等等，用得到的时候再查了。。。 ","date":"2022-02-13","objectID":"/posts/python-tips-and-tricks/:1:4","series":null,"tags":["Python","Tips","Tricks","常见错误"],"title":"Python 实用技巧与常见错误集锦","uri":"/posts/python-tips-and-tricks/#2-排列组合迭代器"},{"categories":["tech"],"content":" 4. itertoolsitertools 提供了许多针对可迭代对象的实用函数 方法很多，基本不可能一次全记住。还是要用到时多查吧。大致记住有提供哪些功能，需要用到时能想起可以查这个模块就行。 1. 无限迭代器 count(start=0, step=1): 从 start 开始，每次迭代时，返回值都加一个 step 默认返回序列为 0 1 2 3… cycle(iterable): 不断循环迭代 iterable repeat(element, times=None): 默认永远返回 element。（如果 times 不为 None，就迭代 times 后结束） 2. 排列组合迭代器 product(p1, p2, …, repeat=1)：p1, p2… 的元素的笛卡尔积，相当于多层 for 循环 repeat 指参数重复次数，比如 \u003e\u003e\u003e from itertools import product \u003e\u003e\u003e r = product([1, 2], [3, 4], [5, 6]) # 重复一次，也就是 (p1, p2, p3) 的笛卡尔积 \u003e\u003e\u003e pprint(list(r)) [(1, 3, 5), (1, 3, 6), (1, 4, 5), (1, 4, 6), (2, 3, 5), (2, 3, 6), (2, 4, 5), (2, 4, 6)] \u003e\u003e\u003e r2 = product([1, 2], [3, 4], [5, 6], repeat=2) # 重复两次，即 (p1, p2, p3, p1, p2, p3) 的笛卡尔积 \u003e\u003e\u003e pprint(list(r2)) [(1, 3, 5, 1, 3, 5), (1, 3, 5, 1, 3, 6), (1, 3, 5, 1, 4, 5), (1, 3, 5, 1, 4, 6), (1, 3, 5, 2, 3, 5), ... permutations(p[, r])：p 中元素，长度为 r 的所有可能的排列。相当于 product 去重后的结果。 combinations(p, r)：既然有排列，当然就有组合了。 3. 其他 zip_longest(*iterables, fillvalue=None)：和 zip 的差别在于，缺失的元素它会用 fillvalue 补全，而不是直接结束。 takewhile() dropwhile() groupby() 等等等，用得到的时候再查了。。。 ","date":"2022-02-13","objectID":"/posts/python-tips-and-tricks/:1:4","series":null,"tags":["Python","Tips","Tricks","常见错误"],"title":"Python 实用技巧与常见错误集锦","uri":"/posts/python-tips-and-tricks/#3-其他"},{"categories":["tech"],"content":" 5. collections提供了一些实用的高级数据结构（容器） defaultdict：这个感觉是最常用的，可以给定 key 的默认值 Counter：方便、快速的计数器。常用于分类统计 deque：一个线程安全的双端队列 OrderedDict：有时候会需要有序字典 namedtuple：命名元组，有时用于参数传递。与 tuple 的差别是它提供了关键字参数和通过名字访问属性的功能 ChainMap：将多个 map 连接（chain）在一起，提供一个统一的视图。因为是视图，所以原来的 map 不会被影响。 ","date":"2022-02-13","objectID":"/posts/python-tips-and-tricks/:1:5","series":null,"tags":["Python","Tips","Tricks","常见错误"],"title":"Python 实用技巧与常见错误集锦","uri":"/posts/python-tips-and-tricks/#5-collections"},{"categories":["tech"],"content":" 6. 常用函数装饰器 functoolsfunctools 提供了几个有时很有用的函数和装饰器 1. @functools.wraps这个装饰器用于使装饰器 copy 被装饰的对象的 __module__, __name__, __qualname__, __annotations__ and __doc__ 属性，这样装饰器就显得更加透明。 from functools import wraps def my_decorator(f): @wraps(f) def wrapper(*args, **kwds): print('Calling decorated function') return f(*args, **kwds) return wrapper # 用了 wraps，wrapper 会复制 f 的各种文档属性 @my_decorator def func(xx): \"\"\" this is func's docstring\"\"\" print(\"this is func~\") 如果不用 wraps 的话，因为实际上返回的是 wrapper，被装饰对象的这些文档属性都会丢失。（比如 docstring） 因此在使用 wrapper 装饰器时，添加 @wraps() 装饰器是个好习惯。 2. functools.partial这个感觉和高等数学的偏函数很像：比如函数 z = f(x, y) 有 x 和 y 两个变量，现在把 x 看作常数，就可以对 y 进行求导运算。 而 python 的 partial 也差不多，不过它不是把 x 看作常数，而是先给定 x 的值。用法如下： from functools import partial basetwo = partial(int, base=2) # 先给定 int 函数的 base 参数为 2 basetwo.__doc__ = 'Convert base 2 string to an int.' # 如果需要文档，可以添加 __doc__ 属性 basetwo('10010') # return 18 此外，还有个 partialmethod 函数，待了解 3. @functools.lru_cache(maxsize=128, typed=False)如果某方法可能被频繁调用（使用相同的参数），而且它的结果在一定时间内不会改变。可以用 lru_cache 装饰它，减少运算量或 IO 操作。 from functools import lru_cache # 缓存最近的（least recently used，lru） 64 次参数不同的调用结果。 @lru_cache(maxsize=64) def my_sum(x): # 后续的调用中，如果参数能匹配到缓存，就直接返回缓存结果 return sum(x) 比如用递归计算斐波那契数列，数值较低的参数会被频繁使用，于是可以用 lru_cache 来缓存它们。 或者爬取网页，可能会需要频繁爬取一个变化不快的网页，这时完全可以用 cache 缓存。 但是它不能控制缓存失效时间，因此不能用于 Web 系统的缓存。还是得自己写个简单的装饰器，把缓存存到 redis 里并设置 expires。或者直接用 Flask 或 Django 的 caching 插件。 4. @functools.singledispatch单重派发，即根据函数的第一个参数的类型，来决定调用哪一个同名函数。 @singledispatch def parse(arg): # 首先定义一个默认函数 print('没有合适的类型被调用') # 如果参数类型没有匹配上，就调用这个默认函数 @parse.register(type(None)) # 第一个参数为 None def _(arg): print('出现 None 了') @parse.register(int) # 第一个参数为整数 def _(arg): print('这次输入的是整数') @parse.register def _(arg: list): # python3.7 开始，可以直接用类型注解来标注第一个参数的类型 print('这次输入的是列表') 画外：有单重派发，自然就有多重派发，Julia 语言就支持多重派发，即根据函数所有参数的类型，来决定调用哪一个同名函数。 Julia 语言根本没有类这个定义，类型的所有方法都是通过多重派发来定义的。 其他 @functools.total_ordering：用于自动生成比较函数。 functools.cmp_to_key(func)：用于将老式的比较函数，转换成新式的 key 函数。 ","date":"2022-02-13","objectID":"/posts/python-tips-and-tricks/:1:6","series":null,"tags":["Python","Tips","Tricks","常见错误"],"title":"Python 实用技巧与常见错误集锦","uri":"/posts/python-tips-and-tricks/#6-常用函数装饰器-functools"},{"categories":["tech"],"content":" 6. 常用函数装饰器 functoolsfunctools 提供了几个有时很有用的函数和装饰器 1. @functools.wraps这个装饰器用于使装饰器 copy 被装饰的对象的 __module__, __name__, __qualname__, __annotations__ and __doc__ 属性，这样装饰器就显得更加透明。 from functools import wraps def my_decorator(f): @wraps(f) def wrapper(*args, **kwds): print('Calling decorated function') return f(*args, **kwds) return wrapper # 用了 wraps，wrapper 会复制 f 的各种文档属性 @my_decorator def func(xx): \"\"\" this is func's docstring\"\"\" print(\"this is func~\") 如果不用 wraps 的话，因为实际上返回的是 wrapper，被装饰对象的这些文档属性都会丢失。（比如 docstring） 因此在使用 wrapper 装饰器时，添加 @wraps() 装饰器是个好习惯。 2. functools.partial这个感觉和高等数学的偏函数很像：比如函数 z = f(x, y) 有 x 和 y 两个变量，现在把 x 看作常数，就可以对 y 进行求导运算。 而 python 的 partial 也差不多，不过它不是把 x 看作常数，而是先给定 x 的值。用法如下： from functools import partial basetwo = partial(int, base=2) # 先给定 int 函数的 base 参数为 2 basetwo.__doc__ = 'Convert base 2 string to an int.' # 如果需要文档，可以添加 __doc__ 属性 basetwo('10010') # return 18 此外，还有个 partialmethod 函数，待了解 3. @functools.lru_cache(maxsize=128, typed=False)如果某方法可能被频繁调用（使用相同的参数），而且它的结果在一定时间内不会改变。可以用 lru_cache 装饰它，减少运算量或 IO 操作。 from functools import lru_cache # 缓存最近的（least recently used，lru） 64 次参数不同的调用结果。 @lru_cache(maxsize=64) def my_sum(x): # 后续的调用中，如果参数能匹配到缓存，就直接返回缓存结果 return sum(x) 比如用递归计算斐波那契数列，数值较低的参数会被频繁使用，于是可以用 lru_cache 来缓存它们。 或者爬取网页，可能会需要频繁爬取一个变化不快的网页，这时完全可以用 cache 缓存。 但是它不能控制缓存失效时间，因此不能用于 Web 系统的缓存。还是得自己写个简单的装饰器，把缓存存到 redis 里并设置 expires。或者直接用 Flask 或 Django 的 caching 插件。 4. @functools.singledispatch单重派发，即根据函数的第一个参数的类型，来决定调用哪一个同名函数。 @singledispatch def parse(arg): # 首先定义一个默认函数 print('没有合适的类型被调用') # 如果参数类型没有匹配上，就调用这个默认函数 @parse.register(type(None)) # 第一个参数为 None def _(arg): print('出现 None 了') @parse.register(int) # 第一个参数为整数 def _(arg): print('这次输入的是整数') @parse.register def _(arg: list): # python3.7 开始，可以直接用类型注解来标注第一个参数的类型 print('这次输入的是列表') 画外：有单重派发，自然就有多重派发，Julia 语言就支持多重派发，即根据函数所有参数的类型，来决定调用哪一个同名函数。 Julia 语言根本没有类这个定义，类型的所有方法都是通过多重派发来定义的。 其他 @functools.total_ordering：用于自动生成比较函数。 functools.cmp_to_key(func)：用于将老式的比较函数，转换成新式的 key 函数。 ","date":"2022-02-13","objectID":"/posts/python-tips-and-tricks/:1:6","series":null,"tags":["Python","Tips","Tricks","常见错误"],"title":"Python 实用技巧与常见错误集锦","uri":"/posts/python-tips-and-tricks/#1-functoolswraps"},{"categories":["tech"],"content":" 6. 常用函数装饰器 functoolsfunctools 提供了几个有时很有用的函数和装饰器 1. @functools.wraps这个装饰器用于使装饰器 copy 被装饰的对象的 __module__, __name__, __qualname__, __annotations__ and __doc__ 属性，这样装饰器就显得更加透明。 from functools import wraps def my_decorator(f): @wraps(f) def wrapper(*args, **kwds): print('Calling decorated function') return f(*args, **kwds) return wrapper # 用了 wraps，wrapper 会复制 f 的各种文档属性 @my_decorator def func(xx): \"\"\" this is func's docstring\"\"\" print(\"this is func~\") 如果不用 wraps 的话，因为实际上返回的是 wrapper，被装饰对象的这些文档属性都会丢失。（比如 docstring） 因此在使用 wrapper 装饰器时，添加 @wraps() 装饰器是个好习惯。 2. functools.partial这个感觉和高等数学的偏函数很像：比如函数 z = f(x, y) 有 x 和 y 两个变量，现在把 x 看作常数，就可以对 y 进行求导运算。 而 python 的 partial 也差不多，不过它不是把 x 看作常数，而是先给定 x 的值。用法如下： from functools import partial basetwo = partial(int, base=2) # 先给定 int 函数的 base 参数为 2 basetwo.__doc__ = 'Convert base 2 string to an int.' # 如果需要文档，可以添加 __doc__ 属性 basetwo('10010') # return 18 此外，还有个 partialmethod 函数，待了解 3. @functools.lru_cache(maxsize=128, typed=False)如果某方法可能被频繁调用（使用相同的参数），而且它的结果在一定时间内不会改变。可以用 lru_cache 装饰它，减少运算量或 IO 操作。 from functools import lru_cache # 缓存最近的（least recently used，lru） 64 次参数不同的调用结果。 @lru_cache(maxsize=64) def my_sum(x): # 后续的调用中，如果参数能匹配到缓存，就直接返回缓存结果 return sum(x) 比如用递归计算斐波那契数列，数值较低的参数会被频繁使用，于是可以用 lru_cache 来缓存它们。 或者爬取网页，可能会需要频繁爬取一个变化不快的网页，这时完全可以用 cache 缓存。 但是它不能控制缓存失效时间，因此不能用于 Web 系统的缓存。还是得自己写个简单的装饰器，把缓存存到 redis 里并设置 expires。或者直接用 Flask 或 Django 的 caching 插件。 4. @functools.singledispatch单重派发，即根据函数的第一个参数的类型，来决定调用哪一个同名函数。 @singledispatch def parse(arg): # 首先定义一个默认函数 print('没有合适的类型被调用') # 如果参数类型没有匹配上，就调用这个默认函数 @parse.register(type(None)) # 第一个参数为 None def _(arg): print('出现 None 了') @parse.register(int) # 第一个参数为整数 def _(arg): print('这次输入的是整数') @parse.register def _(arg: list): # python3.7 开始，可以直接用类型注解来标注第一个参数的类型 print('这次输入的是列表') 画外：有单重派发，自然就有多重派发，Julia 语言就支持多重派发，即根据函数所有参数的类型，来决定调用哪一个同名函数。 Julia 语言根本没有类这个定义，类型的所有方法都是通过多重派发来定义的。 其他 @functools.total_ordering：用于自动生成比较函数。 functools.cmp_to_key(func)：用于将老式的比较函数，转换成新式的 key 函数。 ","date":"2022-02-13","objectID":"/posts/python-tips-and-tricks/:1:6","series":null,"tags":["Python","Tips","Tricks","常见错误"],"title":"Python 实用技巧与常见错误集锦","uri":"/posts/python-tips-and-tricks/#2-functoolspartial"},{"categories":["tech"],"content":" 6. 常用函数装饰器 functoolsfunctools 提供了几个有时很有用的函数和装饰器 1. @functools.wraps这个装饰器用于使装饰器 copy 被装饰的对象的 __module__, __name__, __qualname__, __annotations__ and __doc__ 属性，这样装饰器就显得更加透明。 from functools import wraps def my_decorator(f): @wraps(f) def wrapper(*args, **kwds): print('Calling decorated function') return f(*args, **kwds) return wrapper # 用了 wraps，wrapper 会复制 f 的各种文档属性 @my_decorator def func(xx): \"\"\" this is func's docstring\"\"\" print(\"this is func~\") 如果不用 wraps 的话，因为实际上返回的是 wrapper，被装饰对象的这些文档属性都会丢失。（比如 docstring） 因此在使用 wrapper 装饰器时，添加 @wraps() 装饰器是个好习惯。 2. functools.partial这个感觉和高等数学的偏函数很像：比如函数 z = f(x, y) 有 x 和 y 两个变量，现在把 x 看作常数，就可以对 y 进行求导运算。 而 python 的 partial 也差不多，不过它不是把 x 看作常数，而是先给定 x 的值。用法如下： from functools import partial basetwo = partial(int, base=2) # 先给定 int 函数的 base 参数为 2 basetwo.__doc__ = 'Convert base 2 string to an int.' # 如果需要文档，可以添加 __doc__ 属性 basetwo('10010') # return 18 此外，还有个 partialmethod 函数，待了解 3. @functools.lru_cache(maxsize=128, typed=False)如果某方法可能被频繁调用（使用相同的参数），而且它的结果在一定时间内不会改变。可以用 lru_cache 装饰它，减少运算量或 IO 操作。 from functools import lru_cache # 缓存最近的（least recently used，lru） 64 次参数不同的调用结果。 @lru_cache(maxsize=64) def my_sum(x): # 后续的调用中，如果参数能匹配到缓存，就直接返回缓存结果 return sum(x) 比如用递归计算斐波那契数列，数值较低的参数会被频繁使用，于是可以用 lru_cache 来缓存它们。 或者爬取网页，可能会需要频繁爬取一个变化不快的网页，这时完全可以用 cache 缓存。 但是它不能控制缓存失效时间，因此不能用于 Web 系统的缓存。还是得自己写个简单的装饰器，把缓存存到 redis 里并设置 expires。或者直接用 Flask 或 Django 的 caching 插件。 4. @functools.singledispatch单重派发，即根据函数的第一个参数的类型，来决定调用哪一个同名函数。 @singledispatch def parse(arg): # 首先定义一个默认函数 print('没有合适的类型被调用') # 如果参数类型没有匹配上，就调用这个默认函数 @parse.register(type(None)) # 第一个参数为 None def _(arg): print('出现 None 了') @parse.register(int) # 第一个参数为整数 def _(arg): print('这次输入的是整数') @parse.register def _(arg: list): # python3.7 开始，可以直接用类型注解来标注第一个参数的类型 print('这次输入的是列表') 画外：有单重派发，自然就有多重派发，Julia 语言就支持多重派发，即根据函数所有参数的类型，来决定调用哪一个同名函数。 Julia 语言根本没有类这个定义，类型的所有方法都是通过多重派发来定义的。 其他 @functools.total_ordering：用于自动生成比较函数。 functools.cmp_to_key(func)：用于将老式的比较函数，转换成新式的 key 函数。 ","date":"2022-02-13","objectID":"/posts/python-tips-and-tricks/:1:6","series":null,"tags":["Python","Tips","Tricks","常见错误"],"title":"Python 实用技巧与常见错误集锦","uri":"/posts/python-tips-and-tricks/#3-functoolslru_cachemaxsize128-typedfalse"},{"categories":["tech"],"content":" 6. 常用函数装饰器 functoolsfunctools 提供了几个有时很有用的函数和装饰器 1. @functools.wraps这个装饰器用于使装饰器 copy 被装饰的对象的 __module__, __name__, __qualname__, __annotations__ and __doc__ 属性，这样装饰器就显得更加透明。 from functools import wraps def my_decorator(f): @wraps(f) def wrapper(*args, **kwds): print('Calling decorated function') return f(*args, **kwds) return wrapper # 用了 wraps，wrapper 会复制 f 的各种文档属性 @my_decorator def func(xx): \"\"\" this is func's docstring\"\"\" print(\"this is func~\") 如果不用 wraps 的话，因为实际上返回的是 wrapper，被装饰对象的这些文档属性都会丢失。（比如 docstring） 因此在使用 wrapper 装饰器时，添加 @wraps() 装饰器是个好习惯。 2. functools.partial这个感觉和高等数学的偏函数很像：比如函数 z = f(x, y) 有 x 和 y 两个变量，现在把 x 看作常数，就可以对 y 进行求导运算。 而 python 的 partial 也差不多，不过它不是把 x 看作常数，而是先给定 x 的值。用法如下： from functools import partial basetwo = partial(int, base=2) # 先给定 int 函数的 base 参数为 2 basetwo.__doc__ = 'Convert base 2 string to an int.' # 如果需要文档，可以添加 __doc__ 属性 basetwo('10010') # return 18 此外，还有个 partialmethod 函数，待了解 3. @functools.lru_cache(maxsize=128, typed=False)如果某方法可能被频繁调用（使用相同的参数），而且它的结果在一定时间内不会改变。可以用 lru_cache 装饰它，减少运算量或 IO 操作。 from functools import lru_cache # 缓存最近的（least recently used，lru） 64 次参数不同的调用结果。 @lru_cache(maxsize=64) def my_sum(x): # 后续的调用中，如果参数能匹配到缓存，就直接返回缓存结果 return sum(x) 比如用递归计算斐波那契数列，数值较低的参数会被频繁使用，于是可以用 lru_cache 来缓存它们。 或者爬取网页，可能会需要频繁爬取一个变化不快的网页，这时完全可以用 cache 缓存。 但是它不能控制缓存失效时间，因此不能用于 Web 系统的缓存。还是得自己写个简单的装饰器，把缓存存到 redis 里并设置 expires。或者直接用 Flask 或 Django 的 caching 插件。 4. @functools.singledispatch单重派发，即根据函数的第一个参数的类型，来决定调用哪一个同名函数。 @singledispatch def parse(arg): # 首先定义一个默认函数 print('没有合适的类型被调用') # 如果参数类型没有匹配上，就调用这个默认函数 @parse.register(type(None)) # 第一个参数为 None def _(arg): print('出现 None 了') @parse.register(int) # 第一个参数为整数 def _(arg): print('这次输入的是整数') @parse.register def _(arg: list): # python3.7 开始，可以直接用类型注解来标注第一个参数的类型 print('这次输入的是列表') 画外：有单重派发，自然就有多重派发，Julia 语言就支持多重派发，即根据函数所有参数的类型，来决定调用哪一个同名函数。 Julia 语言根本没有类这个定义，类型的所有方法都是通过多重派发来定义的。 其他 @functools.total_ordering：用于自动生成比较函数。 functools.cmp_to_key(func)：用于将老式的比较函数，转换成新式的 key 函数。 ","date":"2022-02-13","objectID":"/posts/python-tips-and-tricks/:1:6","series":null,"tags":["Python","Tips","Tricks","常见错误"],"title":"Python 实用技巧与常见错误集锦","uri":"/posts/python-tips-and-tricks/#4-functoolssingledispatch"},{"categories":["tech"],"content":" 6. 常用函数装饰器 functoolsfunctools 提供了几个有时很有用的函数和装饰器 1. @functools.wraps这个装饰器用于使装饰器 copy 被装饰的对象的 __module__, __name__, __qualname__, __annotations__ and __doc__ 属性，这样装饰器就显得更加透明。 from functools import wraps def my_decorator(f): @wraps(f) def wrapper(*args, **kwds): print('Calling decorated function') return f(*args, **kwds) return wrapper # 用了 wraps，wrapper 会复制 f 的各种文档属性 @my_decorator def func(xx): \"\"\" this is func's docstring\"\"\" print(\"this is func~\") 如果不用 wraps 的话，因为实际上返回的是 wrapper，被装饰对象的这些文档属性都会丢失。（比如 docstring） 因此在使用 wrapper 装饰器时，添加 @wraps() 装饰器是个好习惯。 2. functools.partial这个感觉和高等数学的偏函数很像：比如函数 z = f(x, y) 有 x 和 y 两个变量，现在把 x 看作常数，就可以对 y 进行求导运算。 而 python 的 partial 也差不多，不过它不是把 x 看作常数，而是先给定 x 的值。用法如下： from functools import partial basetwo = partial(int, base=2) # 先给定 int 函数的 base 参数为 2 basetwo.__doc__ = 'Convert base 2 string to an int.' # 如果需要文档，可以添加 __doc__ 属性 basetwo('10010') # return 18 此外，还有个 partialmethod 函数，待了解 3. @functools.lru_cache(maxsize=128, typed=False)如果某方法可能被频繁调用（使用相同的参数），而且它的结果在一定时间内不会改变。可以用 lru_cache 装饰它，减少运算量或 IO 操作。 from functools import lru_cache # 缓存最近的（least recently used，lru） 64 次参数不同的调用结果。 @lru_cache(maxsize=64) def my_sum(x): # 后续的调用中，如果参数能匹配到缓存，就直接返回缓存结果 return sum(x) 比如用递归计算斐波那契数列，数值较低的参数会被频繁使用，于是可以用 lru_cache 来缓存它们。 或者爬取网页，可能会需要频繁爬取一个变化不快的网页，这时完全可以用 cache 缓存。 但是它不能控制缓存失效时间，因此不能用于 Web 系统的缓存。还是得自己写个简单的装饰器，把缓存存到 redis 里并设置 expires。或者直接用 Flask 或 Django 的 caching 插件。 4. @functools.singledispatch单重派发，即根据函数的第一个参数的类型，来决定调用哪一个同名函数。 @singledispatch def parse(arg): # 首先定义一个默认函数 print('没有合适的类型被调用') # 如果参数类型没有匹配上，就调用这个默认函数 @parse.register(type(None)) # 第一个参数为 None def _(arg): print('出现 None 了') @parse.register(int) # 第一个参数为整数 def _(arg): print('这次输入的是整数') @parse.register def _(arg: list): # python3.7 开始，可以直接用类型注解来标注第一个参数的类型 print('这次输入的是列表') 画外：有单重派发，自然就有多重派发，Julia 语言就支持多重派发，即根据函数所有参数的类型，来决定调用哪一个同名函数。 Julia 语言根本没有类这个定义，类型的所有方法都是通过多重派发来定义的。 其他 @functools.total_ordering：用于自动生成比较函数。 functools.cmp_to_key(func)：用于将老式的比较函数，转换成新式的 key 函数。 ","date":"2022-02-13","objectID":"/posts/python-tips-and-tricks/:1:6","series":null,"tags":["Python","Tips","Tricks","常见错误"],"title":"Python 实用技巧与常见错误集锦","uri":"/posts/python-tips-and-tricks/#其他"},{"categories":["tech"],"content":" 7. 上下文管理 - contextlib即实现使用 with 语句进行自定义的上下文管理。 1. 使用 __enter__ 和 __exit__Java 使用 try 来自动管理资源，只要实现了 AutoCloseable 接口，就可以部分摆脱手动 colse 的地狱了。 而 Python，则是定义了两个 Protocol：__enter__ 和 __exit__. 下面是一个 open 的模拟实现： class OpenContext(object): def __init__(self, filename, mode): # 调用 open(filename, mode) 返回一个实例 self.fp = open(filename, mode) def __enter__(self): # 用 with 管理 __init__ 返回的实例时，with 会自动调用这个方法 return self.fp # 退出 with 代码块时，会自动调用这个方法。 def __exit__(self, exc_type, exc_value, traceback): self.fp.close() # 这里先构造了 OpenContext 实例，然后用 with 管理该实例 with OpenContext('/tmp/a', 'a') as f: f.write('hello world') 这里唯一有点复杂的，就是 __exit__ 方法。和 Java 一样，__exit__ 相当于 try - catch - finally 的 finally 代码块，在发生异常时，它也会被调用。 当没有异常发生时，__exit__ 的三个参数 exc_type, exc_value, traceback 都为 None，而当发生异常时，它们就对应异常的详细信息。 发生异常时， __exit__ 的返回值将被用于决定是否向外层抛出该异常，返回 True 则抛出，返回 False 则抑制（swallow it）。 Note 1：Python 3.6 提供了 async with 异步上下文管理器，它的 Protocol 和同步的 with 完全类似，是 __aenter__ 和 __aexit__ 两个方法。 Note 2：与 Java 相同，with 支持同时管理多个资源，因此可以直接写 with open(x) as a, open(y) as b: 这样的形式。 2. 推荐：contextlib 2.1 @contextlib.contextmanager对于简单的 with 资源管理，编写一个类可能会显得比较繁琐，为此 contextlib 提供了一个方便的装饰器 @contextlib.contextmanager 用来简化代码。 使用它，上面的 OpenContext 可以改写成这样： from contextlib import contextmanager @contextmanager def make_open_context(filename, mode): fp = open(filename, mode) try: yield fp # 没错，这是一个生成器函数 finally: fp.close() with make_open_context('/tmp/a', 'a') as f: f.write('hello world') 使用 contextmanager 装饰一个生成器函数，yield 之前的代码对应 __enter__，finally 代码块就对应 __exit__. Note：同样，也有异步版本的装饰器 @contextlib.asynccontextmanager 2.2 contextlib.closing(thing)用于将原本不支持 with 管理的资源，包装成一个 Context 对象。 from contextlib import closing from urllib.request import urlopen with closing(urlopen('http://www.python.org')) as page: for line in page: print(line) # closing 等同于 from contextlib import contextmanager @contextmanager def closing(thing): try: yield thing finally: thing.close() # 就是添加了一个自动 close 的功能 2.3 contextlib.suppress(*exceptions)使 with 管理器抑制代码块内任何被指定的异常： from contextlib import suppress with suppress(FileNotFoundError): os.remove('somefile.tmp') # 等同于 try: os.remove('somefile.tmp') except FileNotFoundError: pass 2.4 contextlib.redirect_stdout(new_target)将 with 代码块内的 stdout 重定向到指定的 target（可用于收集 stdout 的输出） f = io.StringIO() with redirect_stdout(f): # 将输出直接写入到 StringIO help(pow) s = f.getvalue() # 或者直接写入到文件 with open('help.txt', 'w') as f: with redirect_stdout(f): help(pow) redirect_stdout 函数返回的 Context 是可重入的（ reentrant），可以重复使用。 ","date":"2022-02-13","objectID":"/posts/python-tips-and-tricks/:1:7","series":null,"tags":["Python","Tips","Tricks","常见错误"],"title":"Python 实用技巧与常见错误集锦","uri":"/posts/python-tips-and-tricks/#7-上下文管理---contextlib"},{"categories":["tech"],"content":" 7. 上下文管理 - contextlib即实现使用 with 语句进行自定义的上下文管理。 1. 使用 __enter__ 和 __exit__Java 使用 try 来自动管理资源，只要实现了 AutoCloseable 接口，就可以部分摆脱手动 colse 的地狱了。 而 Python，则是定义了两个 Protocol：__enter__ 和 __exit__. 下面是一个 open 的模拟实现： class OpenContext(object): def __init__(self, filename, mode): # 调用 open(filename, mode) 返回一个实例 self.fp = open(filename, mode) def __enter__(self): # 用 with 管理 __init__ 返回的实例时，with 会自动调用这个方法 return self.fp # 退出 with 代码块时，会自动调用这个方法。 def __exit__(self, exc_type, exc_value, traceback): self.fp.close() # 这里先构造了 OpenContext 实例，然后用 with 管理该实例 with OpenContext('/tmp/a', 'a') as f: f.write('hello world') 这里唯一有点复杂的，就是 __exit__ 方法。和 Java 一样，__exit__ 相当于 try - catch - finally 的 finally 代码块，在发生异常时，它也会被调用。 当没有异常发生时，__exit__ 的三个参数 exc_type, exc_value, traceback 都为 None，而当发生异常时，它们就对应异常的详细信息。 发生异常时， __exit__ 的返回值将被用于决定是否向外层抛出该异常，返回 True 则抛出，返回 False 则抑制（swallow it）。 Note 1：Python 3.6 提供了 async with 异步上下文管理器，它的 Protocol 和同步的 with 完全类似，是 __aenter__ 和 __aexit__ 两个方法。 Note 2：与 Java 相同，with 支持同时管理多个资源，因此可以直接写 with open(x) as a, open(y) as b: 这样的形式。 2. 推荐：contextlib 2.1 @contextlib.contextmanager对于简单的 with 资源管理，编写一个类可能会显得比较繁琐，为此 contextlib 提供了一个方便的装饰器 @contextlib.contextmanager 用来简化代码。 使用它，上面的 OpenContext 可以改写成这样： from contextlib import contextmanager @contextmanager def make_open_context(filename, mode): fp = open(filename, mode) try: yield fp # 没错，这是一个生成器函数 finally: fp.close() with make_open_context('/tmp/a', 'a') as f: f.write('hello world') 使用 contextmanager 装饰一个生成器函数，yield 之前的代码对应 __enter__，finally 代码块就对应 __exit__. Note：同样，也有异步版本的装饰器 @contextlib.asynccontextmanager 2.2 contextlib.closing(thing)用于将原本不支持 with 管理的资源，包装成一个 Context 对象。 from contextlib import closing from urllib.request import urlopen with closing(urlopen('http://www.python.org')) as page: for line in page: print(line) # closing 等同于 from contextlib import contextmanager @contextmanager def closing(thing): try: yield thing finally: thing.close() # 就是添加了一个自动 close 的功能 2.3 contextlib.suppress(*exceptions)使 with 管理器抑制代码块内任何被指定的异常： from contextlib import suppress with suppress(FileNotFoundError): os.remove('somefile.tmp') # 等同于 try: os.remove('somefile.tmp') except FileNotFoundError: pass 2.4 contextlib.redirect_stdout(new_target)将 with 代码块内的 stdout 重定向到指定的 target（可用于收集 stdout 的输出） f = io.StringIO() with redirect_stdout(f): # 将输出直接写入到 StringIO help(pow) s = f.getvalue() # 或者直接写入到文件 with open('help.txt', 'w') as f: with redirect_stdout(f): help(pow) redirect_stdout 函数返回的 Context 是可重入的（ reentrant），可以重复使用。 ","date":"2022-02-13","objectID":"/posts/python-tips-and-tricks/:1:7","series":null,"tags":["Python","Tips","Tricks","常见错误"],"title":"Python 实用技巧与常见错误集锦","uri":"/posts/python-tips-and-tricks/#1-使用-__enter__-和-__exit__"},{"categories":["tech"],"content":" 7. 上下文管理 - contextlib即实现使用 with 语句进行自定义的上下文管理。 1. 使用 __enter__ 和 __exit__Java 使用 try 来自动管理资源，只要实现了 AutoCloseable 接口，就可以部分摆脱手动 colse 的地狱了。 而 Python，则是定义了两个 Protocol：__enter__ 和 __exit__. 下面是一个 open 的模拟实现： class OpenContext(object): def __init__(self, filename, mode): # 调用 open(filename, mode) 返回一个实例 self.fp = open(filename, mode) def __enter__(self): # 用 with 管理 __init__ 返回的实例时，with 会自动调用这个方法 return self.fp # 退出 with 代码块时，会自动调用这个方法。 def __exit__(self, exc_type, exc_value, traceback): self.fp.close() # 这里先构造了 OpenContext 实例，然后用 with 管理该实例 with OpenContext('/tmp/a', 'a') as f: f.write('hello world') 这里唯一有点复杂的，就是 __exit__ 方法。和 Java 一样，__exit__ 相当于 try - catch - finally 的 finally 代码块，在发生异常时，它也会被调用。 当没有异常发生时，__exit__ 的三个参数 exc_type, exc_value, traceback 都为 None，而当发生异常时，它们就对应异常的详细信息。 发生异常时， __exit__ 的返回值将被用于决定是否向外层抛出该异常，返回 True 则抛出，返回 False 则抑制（swallow it）。 Note 1：Python 3.6 提供了 async with 异步上下文管理器，它的 Protocol 和同步的 with 完全类似，是 __aenter__ 和 __aexit__ 两个方法。 Note 2：与 Java 相同，with 支持同时管理多个资源，因此可以直接写 with open(x) as a, open(y) as b: 这样的形式。 2. 推荐：contextlib 2.1 @contextlib.contextmanager对于简单的 with 资源管理，编写一个类可能会显得比较繁琐，为此 contextlib 提供了一个方便的装饰器 @contextlib.contextmanager 用来简化代码。 使用它，上面的 OpenContext 可以改写成这样： from contextlib import contextmanager @contextmanager def make_open_context(filename, mode): fp = open(filename, mode) try: yield fp # 没错，这是一个生成器函数 finally: fp.close() with make_open_context('/tmp/a', 'a') as f: f.write('hello world') 使用 contextmanager 装饰一个生成器函数，yield 之前的代码对应 __enter__，finally 代码块就对应 __exit__. Note：同样，也有异步版本的装饰器 @contextlib.asynccontextmanager 2.2 contextlib.closing(thing)用于将原本不支持 with 管理的资源，包装成一个 Context 对象。 from contextlib import closing from urllib.request import urlopen with closing(urlopen('http://www.python.org')) as page: for line in page: print(line) # closing 等同于 from contextlib import contextmanager @contextmanager def closing(thing): try: yield thing finally: thing.close() # 就是添加了一个自动 close 的功能 2.3 contextlib.suppress(*exceptions)使 with 管理器抑制代码块内任何被指定的异常： from contextlib import suppress with suppress(FileNotFoundError): os.remove('somefile.tmp') # 等同于 try: os.remove('somefile.tmp') except FileNotFoundError: pass 2.4 contextlib.redirect_stdout(new_target)将 with 代码块内的 stdout 重定向到指定的 target（可用于收集 stdout 的输出） f = io.StringIO() with redirect_stdout(f): # 将输出直接写入到 StringIO help(pow) s = f.getvalue() # 或者直接写入到文件 with open('help.txt', 'w') as f: with redirect_stdout(f): help(pow) redirect_stdout 函数返回的 Context 是可重入的（ reentrant），可以重复使用。 ","date":"2022-02-13","objectID":"/posts/python-tips-and-tricks/:1:7","series":null,"tags":["Python","Tips","Tricks","常见错误"],"title":"Python 实用技巧与常见错误集锦","uri":"/posts/python-tips-and-tricks/#2-推荐contextlib"},{"categories":["tech"],"content":" 7. 上下文管理 - contextlib即实现使用 with 语句进行自定义的上下文管理。 1. 使用 __enter__ 和 __exit__Java 使用 try 来自动管理资源，只要实现了 AutoCloseable 接口，就可以部分摆脱手动 colse 的地狱了。 而 Python，则是定义了两个 Protocol：__enter__ 和 __exit__. 下面是一个 open 的模拟实现： class OpenContext(object): def __init__(self, filename, mode): # 调用 open(filename, mode) 返回一个实例 self.fp = open(filename, mode) def __enter__(self): # 用 with 管理 __init__ 返回的实例时，with 会自动调用这个方法 return self.fp # 退出 with 代码块时，会自动调用这个方法。 def __exit__(self, exc_type, exc_value, traceback): self.fp.close() # 这里先构造了 OpenContext 实例，然后用 with 管理该实例 with OpenContext('/tmp/a', 'a') as f: f.write('hello world') 这里唯一有点复杂的，就是 __exit__ 方法。和 Java 一样，__exit__ 相当于 try - catch - finally 的 finally 代码块，在发生异常时，它也会被调用。 当没有异常发生时，__exit__ 的三个参数 exc_type, exc_value, traceback 都为 None，而当发生异常时，它们就对应异常的详细信息。 发生异常时， __exit__ 的返回值将被用于决定是否向外层抛出该异常，返回 True 则抛出，返回 False 则抑制（swallow it）。 Note 1：Python 3.6 提供了 async with 异步上下文管理器，它的 Protocol 和同步的 with 完全类似，是 __aenter__ 和 __aexit__ 两个方法。 Note 2：与 Java 相同，with 支持同时管理多个资源，因此可以直接写 with open(x) as a, open(y) as b: 这样的形式。 2. 推荐：contextlib 2.1 @contextlib.contextmanager对于简单的 with 资源管理，编写一个类可能会显得比较繁琐，为此 contextlib 提供了一个方便的装饰器 @contextlib.contextmanager 用来简化代码。 使用它，上面的 OpenContext 可以改写成这样： from contextlib import contextmanager @contextmanager def make_open_context(filename, mode): fp = open(filename, mode) try: yield fp # 没错，这是一个生成器函数 finally: fp.close() with make_open_context('/tmp/a', 'a') as f: f.write('hello world') 使用 contextmanager 装饰一个生成器函数，yield 之前的代码对应 __enter__，finally 代码块就对应 __exit__. Note：同样，也有异步版本的装饰器 @contextlib.asynccontextmanager 2.2 contextlib.closing(thing)用于将原本不支持 with 管理的资源，包装成一个 Context 对象。 from contextlib import closing from urllib.request import urlopen with closing(urlopen('http://www.python.org')) as page: for line in page: print(line) # closing 等同于 from contextlib import contextmanager @contextmanager def closing(thing): try: yield thing finally: thing.close() # 就是添加了一个自动 close 的功能 2.3 contextlib.suppress(*exceptions)使 with 管理器抑制代码块内任何被指定的异常： from contextlib import suppress with suppress(FileNotFoundError): os.remove('somefile.tmp') # 等同于 try: os.remove('somefile.tmp') except FileNotFoundError: pass 2.4 contextlib.redirect_stdout(new_target)将 with 代码块内的 stdout 重定向到指定的 target（可用于收集 stdout 的输出） f = io.StringIO() with redirect_stdout(f): # 将输出直接写入到 StringIO help(pow) s = f.getvalue() # 或者直接写入到文件 with open('help.txt', 'w') as f: with redirect_stdout(f): help(pow) redirect_stdout 函数返回的 Context 是可重入的（ reentrant），可以重复使用。 ","date":"2022-02-13","objectID":"/posts/python-tips-and-tricks/:1:7","series":null,"tags":["Python","Tips","Tricks","常见错误"],"title":"Python 实用技巧与常见错误集锦","uri":"/posts/python-tips-and-tricks/#21-contextlibcontextmanager"},{"categories":["tech"],"content":" 7. 上下文管理 - contextlib即实现使用 with 语句进行自定义的上下文管理。 1. 使用 __enter__ 和 __exit__Java 使用 try 来自动管理资源，只要实现了 AutoCloseable 接口，就可以部分摆脱手动 colse 的地狱了。 而 Python，则是定义了两个 Protocol：__enter__ 和 __exit__. 下面是一个 open 的模拟实现： class OpenContext(object): def __init__(self, filename, mode): # 调用 open(filename, mode) 返回一个实例 self.fp = open(filename, mode) def __enter__(self): # 用 with 管理 __init__ 返回的实例时，with 会自动调用这个方法 return self.fp # 退出 with 代码块时，会自动调用这个方法。 def __exit__(self, exc_type, exc_value, traceback): self.fp.close() # 这里先构造了 OpenContext 实例，然后用 with 管理该实例 with OpenContext('/tmp/a', 'a') as f: f.write('hello world') 这里唯一有点复杂的，就是 __exit__ 方法。和 Java 一样，__exit__ 相当于 try - catch - finally 的 finally 代码块，在发生异常时，它也会被调用。 当没有异常发生时，__exit__ 的三个参数 exc_type, exc_value, traceback 都为 None，而当发生异常时，它们就对应异常的详细信息。 发生异常时， __exit__ 的返回值将被用于决定是否向外层抛出该异常，返回 True 则抛出，返回 False 则抑制（swallow it）。 Note 1：Python 3.6 提供了 async with 异步上下文管理器，它的 Protocol 和同步的 with 完全类似，是 __aenter__ 和 __aexit__ 两个方法。 Note 2：与 Java 相同，with 支持同时管理多个资源，因此可以直接写 with open(x) as a, open(y) as b: 这样的形式。 2. 推荐：contextlib 2.1 @contextlib.contextmanager对于简单的 with 资源管理，编写一个类可能会显得比较繁琐，为此 contextlib 提供了一个方便的装饰器 @contextlib.contextmanager 用来简化代码。 使用它，上面的 OpenContext 可以改写成这样： from contextlib import contextmanager @contextmanager def make_open_context(filename, mode): fp = open(filename, mode) try: yield fp # 没错，这是一个生成器函数 finally: fp.close() with make_open_context('/tmp/a', 'a') as f: f.write('hello world') 使用 contextmanager 装饰一个生成器函数，yield 之前的代码对应 __enter__，finally 代码块就对应 __exit__. Note：同样，也有异步版本的装饰器 @contextlib.asynccontextmanager 2.2 contextlib.closing(thing)用于将原本不支持 with 管理的资源，包装成一个 Context 对象。 from contextlib import closing from urllib.request import urlopen with closing(urlopen('http://www.python.org')) as page: for line in page: print(line) # closing 等同于 from contextlib import contextmanager @contextmanager def closing(thing): try: yield thing finally: thing.close() # 就是添加了一个自动 close 的功能 2.3 contextlib.suppress(*exceptions)使 with 管理器抑制代码块内任何被指定的异常： from contextlib import suppress with suppress(FileNotFoundError): os.remove('somefile.tmp') # 等同于 try: os.remove('somefile.tmp') except FileNotFoundError: pass 2.4 contextlib.redirect_stdout(new_target)将 with 代码块内的 stdout 重定向到指定的 target（可用于收集 stdout 的输出） f = io.StringIO() with redirect_stdout(f): # 将输出直接写入到 StringIO help(pow) s = f.getvalue() # 或者直接写入到文件 with open('help.txt', 'w') as f: with redirect_stdout(f): help(pow) redirect_stdout 函数返回的 Context 是可重入的（ reentrant），可以重复使用。 ","date":"2022-02-13","objectID":"/posts/python-tips-and-tricks/:1:7","series":null,"tags":["Python","Tips","Tricks","常见错误"],"title":"Python 实用技巧与常见错误集锦","uri":"/posts/python-tips-and-tricks/#22-contextlibclosingthing"},{"categories":["tech"],"content":" 7. 上下文管理 - contextlib即实现使用 with 语句进行自定义的上下文管理。 1. 使用 __enter__ 和 __exit__Java 使用 try 来自动管理资源，只要实现了 AutoCloseable 接口，就可以部分摆脱手动 colse 的地狱了。 而 Python，则是定义了两个 Protocol：__enter__ 和 __exit__. 下面是一个 open 的模拟实现： class OpenContext(object): def __init__(self, filename, mode): # 调用 open(filename, mode) 返回一个实例 self.fp = open(filename, mode) def __enter__(self): # 用 with 管理 __init__ 返回的实例时，with 会自动调用这个方法 return self.fp # 退出 with 代码块时，会自动调用这个方法。 def __exit__(self, exc_type, exc_value, traceback): self.fp.close() # 这里先构造了 OpenContext 实例，然后用 with 管理该实例 with OpenContext('/tmp/a', 'a') as f: f.write('hello world') 这里唯一有点复杂的，就是 __exit__ 方法。和 Java 一样，__exit__ 相当于 try - catch - finally 的 finally 代码块，在发生异常时，它也会被调用。 当没有异常发生时，__exit__ 的三个参数 exc_type, exc_value, traceback 都为 None，而当发生异常时，它们就对应异常的详细信息。 发生异常时， __exit__ 的返回值将被用于决定是否向外层抛出该异常，返回 True 则抛出，返回 False 则抑制（swallow it）。 Note 1：Python 3.6 提供了 async with 异步上下文管理器，它的 Protocol 和同步的 with 完全类似，是 __aenter__ 和 __aexit__ 两个方法。 Note 2：与 Java 相同，with 支持同时管理多个资源，因此可以直接写 with open(x) as a, open(y) as b: 这样的形式。 2. 推荐：contextlib 2.1 @contextlib.contextmanager对于简单的 with 资源管理，编写一个类可能会显得比较繁琐，为此 contextlib 提供了一个方便的装饰器 @contextlib.contextmanager 用来简化代码。 使用它，上面的 OpenContext 可以改写成这样： from contextlib import contextmanager @contextmanager def make_open_context(filename, mode): fp = open(filename, mode) try: yield fp # 没错，这是一个生成器函数 finally: fp.close() with make_open_context('/tmp/a', 'a') as f: f.write('hello world') 使用 contextmanager 装饰一个生成器函数，yield 之前的代码对应 __enter__，finally 代码块就对应 __exit__. Note：同样，也有异步版本的装饰器 @contextlib.asynccontextmanager 2.2 contextlib.closing(thing)用于将原本不支持 with 管理的资源，包装成一个 Context 对象。 from contextlib import closing from urllib.request import urlopen with closing(urlopen('http://www.python.org')) as page: for line in page: print(line) # closing 等同于 from contextlib import contextmanager @contextmanager def closing(thing): try: yield thing finally: thing.close() # 就是添加了一个自动 close 的功能 2.3 contextlib.suppress(*exceptions)使 with 管理器抑制代码块内任何被指定的异常： from contextlib import suppress with suppress(FileNotFoundError): os.remove('somefile.tmp') # 等同于 try: os.remove('somefile.tmp') except FileNotFoundError: pass 2.4 contextlib.redirect_stdout(new_target)将 with 代码块内的 stdout 重定向到指定的 target（可用于收集 stdout 的输出） f = io.StringIO() with redirect_stdout(f): # 将输出直接写入到 StringIO help(pow) s = f.getvalue() # 或者直接写入到文件 with open('help.txt', 'w') as f: with redirect_stdout(f): help(pow) redirect_stdout 函数返回的 Context 是可重入的（ reentrant），可以重复使用。 ","date":"2022-02-13","objectID":"/posts/python-tips-and-tricks/:1:7","series":null,"tags":["Python","Tips","Tricks","常见错误"],"title":"Python 实用技巧与常见错误集锦","uri":"/posts/python-tips-and-tricks/#23-contextlibsuppressexceptions"},{"categories":["tech"],"content":" 7. 上下文管理 - contextlib即实现使用 with 语句进行自定义的上下文管理。 1. 使用 __enter__ 和 __exit__Java 使用 try 来自动管理资源，只要实现了 AutoCloseable 接口，就可以部分摆脱手动 colse 的地狱了。 而 Python，则是定义了两个 Protocol：__enter__ 和 __exit__. 下面是一个 open 的模拟实现： class OpenContext(object): def __init__(self, filename, mode): # 调用 open(filename, mode) 返回一个实例 self.fp = open(filename, mode) def __enter__(self): # 用 with 管理 __init__ 返回的实例时，with 会自动调用这个方法 return self.fp # 退出 with 代码块时，会自动调用这个方法。 def __exit__(self, exc_type, exc_value, traceback): self.fp.close() # 这里先构造了 OpenContext 实例，然后用 with 管理该实例 with OpenContext('/tmp/a', 'a') as f: f.write('hello world') 这里唯一有点复杂的，就是 __exit__ 方法。和 Java 一样，__exit__ 相当于 try - catch - finally 的 finally 代码块，在发生异常时，它也会被调用。 当没有异常发生时，__exit__ 的三个参数 exc_type, exc_value, traceback 都为 None，而当发生异常时，它们就对应异常的详细信息。 发生异常时， __exit__ 的返回值将被用于决定是否向外层抛出该异常，返回 True 则抛出，返回 False 则抑制（swallow it）。 Note 1：Python 3.6 提供了 async with 异步上下文管理器，它的 Protocol 和同步的 with 完全类似，是 __aenter__ 和 __aexit__ 两个方法。 Note 2：与 Java 相同，with 支持同时管理多个资源，因此可以直接写 with open(x) as a, open(y) as b: 这样的形式。 2. 推荐：contextlib 2.1 @contextlib.contextmanager对于简单的 with 资源管理，编写一个类可能会显得比较繁琐，为此 contextlib 提供了一个方便的装饰器 @contextlib.contextmanager 用来简化代码。 使用它，上面的 OpenContext 可以改写成这样： from contextlib import contextmanager @contextmanager def make_open_context(filename, mode): fp = open(filename, mode) try: yield fp # 没错，这是一个生成器函数 finally: fp.close() with make_open_context('/tmp/a', 'a') as f: f.write('hello world') 使用 contextmanager 装饰一个生成器函数，yield 之前的代码对应 __enter__，finally 代码块就对应 __exit__. Note：同样，也有异步版本的装饰器 @contextlib.asynccontextmanager 2.2 contextlib.closing(thing)用于将原本不支持 with 管理的资源，包装成一个 Context 对象。 from contextlib import closing from urllib.request import urlopen with closing(urlopen('http://www.python.org')) as page: for line in page: print(line) # closing 等同于 from contextlib import contextmanager @contextmanager def closing(thing): try: yield thing finally: thing.close() # 就是添加了一个自动 close 的功能 2.3 contextlib.suppress(*exceptions)使 with 管理器抑制代码块内任何被指定的异常： from contextlib import suppress with suppress(FileNotFoundError): os.remove('somefile.tmp') # 等同于 try: os.remove('somefile.tmp') except FileNotFoundError: pass 2.4 contextlib.redirect_stdout(new_target)将 with 代码块内的 stdout 重定向到指定的 target（可用于收集 stdout 的输出） f = io.StringIO() with redirect_stdout(f): # 将输出直接写入到 StringIO help(pow) s = f.getvalue() # 或者直接写入到文件 with open('help.txt', 'w') as f: with redirect_stdout(f): help(pow) redirect_stdout 函数返回的 Context 是可重入的（ reentrant），可以重复使用。 ","date":"2022-02-13","objectID":"/posts/python-tips-and-tricks/:1:7","series":null,"tags":["Python","Tips","Tricks","常见错误"],"title":"Python 实用技巧与常见错误集锦","uri":"/posts/python-tips-and-tricks/#24-contextlibredirect_stdoutnew_target"},{"categories":["tech"],"content":" 二、实用代码片段","date":"2022-02-13","objectID":"/posts/python-tips-and-tricks/:2:0","series":null,"tags":["Python","Tips","Tricks","常见错误"],"title":"Python 实用技巧与常见错误集锦","uri":"/posts/python-tips-and-tricks/#二实用代码片段"},{"categories":["tech"],"content":" 1. 元素分组/group数据处理中一个常见的操作，是将列表中的元素，依次每 k 个分作一组。 下面的函数使用非常简洁的代码实现了元素分组的功能： from itertools import zip_longest def group_each(a, size: int, longest=False): \"\"\" 将一个可迭代对象 a 内的元素, 每 size 个分为一组 group_each([1,2,3,4], 2) -\u003e [(1,2), (3,4)] \"\"\" iterators = [iter(a)] * size # 将新构造的 iterator 复制 size 次（浅复制） func_zip = zip_longest if longest else zip return func_zip(*iterators) # 然后 zip a = \"abcdefghijk\" list(group_each(a, 3)) # =\u003e [('a', 'b', 'c'), ('d', 'e', 'f'), ('g', 'h', 'i')] list(group_each(a, 3, longest=True)) # =\u003e [('a', 'b', 'c'), ('d', 'e', 'f'), ('g', 'h', 'i'), ('j', 'k', None)] 这个函数还可以进一步简化为 zip(*[iter(a)] * 3)，如果没想到浅复制（Shallow Copy）特性的话，会很难理解它的逻辑。 此外，如果某个 size 比较常用（比如 2），还可以用 partial 封装一下： from functools import partial # 每两个分一组 group_each_2 = partial(group_each, size=2) # 等同于 group_each_2 = lambda a: group_each(a, 2) a = \"abcde\" list(group_each_2(a)) # =\u003e [('a', 'b'), ('c', 'd')] list(group_each_2(a, longest=True)) # =\u003e [('a', 'b'), ('c', 'd'), ('e', None)] ","date":"2022-02-13","objectID":"/posts/python-tips-and-tricks/:2:1","series":null,"tags":["Python","Tips","Tricks","常见错误"],"title":"Python 实用技巧与常见错误集锦","uri":"/posts/python-tips-and-tricks/#group_size"},{"categories":["tech"],"content":" 2. 扁平版本的 map稍微接触过函数式应该都知道 flat_map，可 Python 标准库却没有提供。下面是我在 stackoverflow 上找到的实现，其实很简单 from itertools import chain def flat_map(f, items): return chain.from_iterable(map(f, items)) 它和 map 的差别在于是不是扁平(flat) 的（废话。。），举个例子 \u003e\u003e\u003e list(map(list, ['123', '456'])) [['1', '2', '3'], ['4', '5', '6']] \u003e\u003e\u003e list(flat_map(list, ['123', '456'])) ['1', '2', '3', '4', '5', '6'] ","date":"2022-02-13","objectID":"/posts/python-tips-and-tricks/:2:2","series":null,"tags":["Python","Tips","Tricks","常见错误"],"title":"Python 实用技巧与常见错误集锦","uri":"/posts/python-tips-and-tricks/#2-扁平版本的-map"},{"categories":["tech"],"content":" 3. 轮流迭代多个迭代器假设我有多个可迭代对象（迭代器、列表等），现在我需要每次从每个对象中取一个值，直到某个对象为空。如果用循环写会比较繁琐，但是用 itertools 可以这样写： from itertools import chain def iter_one_by_one(items): return chain.from_iterable(zip(*items)) a = [1,2,3] b = [4,5,6] c = [7,8,9,10] list(iter_one_by_one([a,b,c])) # =\u003e [1, 4, 7, 2, 5, 8, 3, 6, 9] ","date":"2022-02-13","objectID":"/posts/python-tips-and-tricks/:2:3","series":null,"tags":["Python","Tips","Tricks","常见错误"],"title":"Python 实用技巧与常见错误集锦","uri":"/posts/python-tips-and-tricks/#3-轮流迭代多个迭代器"},{"categories":["tech"],"content":" 4. 多 dict 的去重假设我们有一个 dict 的列表，里面可能有内容一模一样的 dict，我们需要对它做去重。 容易想到的方法就是使用 set，可是 set 中的元素必须是 hashable 的，而 dict 是 unhashable 的，因此不能直接放进 set 里。 \u003e\u003e\u003e a = [{'a': 1}, {'a': 1}, {'b': 2}] \u003e\u003e\u003e set(a) Traceback (most recent call last): File \"/usr/local/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2961, in run_code exec(code_obj, self.user_global_ns, self.user_ns) File \"\u003cipython-input-5-5b4c643a6feb\u003e\", line 1, in \u003cmodule\u003e set(a) TypeError: unhashable type: 'dict' 难道就必须手写递归了么？未必，我在 stackoverflow 看到这样一个小技巧 import json def unique_dicts(data_list: list): \"\"\"unique a list of dict dict 是 unhashable 的，不能放入 set 中，所以先转换成 str unique_dicts([{'a': 1}, {'a': 1}, {'b': 2}]) -\u003e [{'a': 1}, {'b': 2}] \"\"\" data_json_set = set(json.dumps(item) for item in data_list) return [json.loads(item) for item in data_json_set] ","date":"2022-02-13","objectID":"/posts/python-tips-and-tricks/:2:4","series":null,"tags":["Python","Tips","Tricks","常见错误"],"title":"Python 实用技巧与常见错误集锦","uri":"/posts/python-tips-and-tricks/#4-多-dict-的去重"},{"categories":["tech"],"content":" 5. str 的 startswith 和 endswith 的参数可以是元组 In[7]: a = \"bb.gif\" In[8]: b = 'a.jpg' In[9]: a.endswith(('.jpg', '.gif')) Out[9]: True In[10]: b.startswith(('bb', 'a')) Out[10]: True ","date":"2022-02-13","objectID":"/posts/python-tips-and-tricks/:2:5","series":null,"tags":["Python","Tips","Tricks","常见错误"],"title":"Python 实用技巧与常见错误集锦","uri":"/posts/python-tips-and-tricks/#5-str-的-startswith-和-endswith-的参数可以是元组"},{"categories":["tech"],"content":" 6. 判断两个对象的所有属性都相同python 和 java 一样，直接用 == 做判断，默认是比较的引用，相当于 is。对自定义的类，你需要重写 __eq__ 函数。 判断值相等的方法很简单，一行代码： class A: ... def __eq__(self, obj): return self.__dict__ == obj.__dict__ # 转成 __dict__ 再比较 ","date":"2022-02-13","objectID":"/posts/python-tips-and-tricks/:2:6","series":null,"tags":["Python","Tips","Tricks","常见错误"],"title":"Python 实用技巧与常见错误集锦","uri":"/posts/python-tips-and-tricks/#6-判断两个对象的所有属性都相同"},{"categories":["tech"],"content":" 7. 案例 7.1 html table 元素的处理在做爬虫工作时，有时会遇到这样的 table 元素： 对这种 html 元素，我一般会直接把它转换成 list，结果如下： table = [['label1', 'value1', 'label2', 'value2'], ['label3', 'value3'], ['label4', 'value4', 'label5', 'value5'], ... ] 为了方便索引，现在我需要把上面的数据转换成下面这个样子的 dict { 'label1': 'value1', 'label2': 'value2', 'label3': 'value3', 'label4': 'value4', 'label5': 'value5' } 如果是平常，大概需要写循环了。不过如果用刚刚说到的几个函数的话，会变得异常简单 # 1. 分组 groups = flat_map(group_each_2, table) # 1.1 flat_map 返回的是迭代器，list 后内容如下： # [('label1', 'value1'), # ('label2', 'value2'), # ('label3', 'value3'), # ('label4', 'value4'), # ('label5', 'value5')] # 2. 转换成 dict key_values = dict(groups) # 得到的 key_values 与上面需要的 dict 别无二致。 ","date":"2022-02-13","objectID":"/posts/python-tips-and-tricks/:2:7","series":null,"tags":["Python","Tips","Tricks","常见错误"],"title":"Python 实用技巧与常见错误集锦","uri":"/posts/python-tips-and-tricks/#7-案例"},{"categories":["tech"],"content":" 7. 案例 7.1 html table 元素的处理在做爬虫工作时，有时会遇到这样的 table 元素： 对这种 html 元素，我一般会直接把它转换成 list，结果如下： table = [['label1', 'value1', 'label2', 'value2'], ['label3', 'value3'], ['label4', 'value4', 'label5', 'value5'], ... ] 为了方便索引，现在我需要把上面的数据转换成下面这个样子的 dict { 'label1': 'value1', 'label2': 'value2', 'label3': 'value3', 'label4': 'value4', 'label5': 'value5' } 如果是平常，大概需要写循环了。不过如果用刚刚说到的几个函数的话，会变得异常简单 # 1. 分组 groups = flat_map(group_each_2, table) # 1.1 flat_map 返回的是迭代器，list 后内容如下： # [('label1', 'value1'), # ('label2', 'value2'), # ('label3', 'value3'), # ('label4', 'value4'), # ('label5', 'value5')] # 2. 转换成 dict key_values = dict(groups) # 得到的 key_values 与上面需要的 dict 别无二致。 ","date":"2022-02-13","objectID":"/posts/python-tips-and-tricks/:2:7","series":null,"tags":["Python","Tips","Tricks","常见错误"],"title":"Python 实用技巧与常见错误集锦","uri":"/posts/python-tips-and-tricks/#71-html-table-元素的处理"},{"categories":["tech"],"content":" 三、常见错误","date":"2022-02-13","objectID":"/posts/python-tips-and-tricks/:3:0","series":null,"tags":["Python","Tips","Tricks","常见错误"],"title":"Python 实用技巧与常见错误集锦","uri":"/posts/python-tips-and-tricks/#三常见错误"},{"categories":["tech"],"content":" 1. 浅复制导致错误利用好浅复制，可以非常简洁的实现前面提到的元素分组/group功能，但是如果不注意，也会导致非常隐晦的错误！ 比如在使用 * 作为重复运算符时，如果目标是一个嵌套的可变对象，就会产生令人费解的问题： \u003e\u003e\u003e a = [1,2,3] \u003e\u003e\u003e b = a * 3 \u003e\u003e\u003e b [1, 2, 3, 1, 2, 3, 1, 2, 3] \u003e\u003e\u003e b = [a] * 3 # nested \u003e\u003e\u003e b [[1, 2, 3], [1, 2, 3], [1, 2, 3]] \u003e\u003e\u003e b[1][1] = 4 \u003e\u003e\u003e b [[1, 4, 3], [1, 4, 3], [1, 4, 3]] 因为 * 并不是深拷贝，它只是简单地复制了 [a] 这个列表，里面的 [1,2,3] 都是同一个对象，所以改了一个，所有的都会改变。 解决方法是不要使用 * 号，改用[a.copy() for i in range(3)] 执行深拷贝。如果不需要修改，请直接使用不可变对象。 ","date":"2022-02-13","objectID":"/posts/python-tips-and-tricks/:3:1","series":null,"tags":["Python","Tips","Tricks","常见错误"],"title":"Python 实用技巧与常见错误集锦","uri":"/posts/python-tips-and-tricks/#1-浅复制导致错误"},{"categories":["tech"],"content":" 2. 变量作用域 Python 中只有模块，类以及函数才会引入新的作用域，其它的代码块是不会引入新的作用域的。（而在 C/Java 中，任何一个 {} 块就构成一个局部作用域。另外 Julia 中 for/while/try-catch 都是局部作用域，但 if-else 又不是局部作用域。总之这些小差别要注意。） 局部变量可以与外部变量同名，并且在其作用域中，局部变量会覆盖掉外部变量。 不知是出于实现简单或是性能，还是其他的原因，好像所有的语言都是这样的。其实我更希望变量的作用域覆盖会报错。 如果有函数与其他函数或变量（甚至某些保留字）同名，后定义的会覆盖掉先定义的。（这是因为 Python 中函数也是对象。而在 C/Java 中这是会报错的） 此外，还有一个小问题，先看一个例子： \u003e\u003e\u003e i = 4 \u003e\u003e\u003e def f(): # 单纯的从函数作用域访问外部作用域是没问题的 ... print(i) ... \u003e\u003e\u003e f() 4 再看一个问题举例： \u003e\u003e\u003e i = 3 \u003e\u003e\u003e def f(): ... print(i) # 这里应该是访问外部作用域 ... i = 5 # 可这里又定义了一个同名局部变量 i ... \u003e\u003e\u003e f() # 于是就出错了 Traceback (most recent call last): File \"\u003cstdin\u003e\", line 1, in \u003cmodule\u003e File \"\u003cstdin\u003e\", line 2, in f UnboundLocalError: local variable 'i' referenced before assignment 如果在内部作用域先访问外部作用域，再定义一个同名的局部变量，解释器就懵逼了。 如果你其实想做的是改变全局变量 i 的值，就应该在开头声明 global i. 而如果 外部变量 i 不是存在于全局作用域，而是在某个闭合作用域内的话，就该用 nonlocal i ","date":"2022-02-13","objectID":"/posts/python-tips-and-tricks/:3:2","series":null,"tags":["Python","Tips","Tricks","常见错误"],"title":"Python 实用技巧与常见错误集锦","uri":"/posts/python-tips-and-tricks/#2-变量作用域"},{"categories":["tech"],"content":" 四、自定义装饰器装饰器有两种：用函数定义的装饰器，还有用类定义的装饰器。函数装饰器最常用。 装饰器可用于装饰函数，修改函数/类的某些行为，或者将函数注册到别的地方。 ","date":"2022-02-13","objectID":"/posts/python-tips-and-tricks/:4:0","series":null,"tags":["Python","Tips","Tricks","常见错误"],"title":"Python 实用技巧与常见错误集锦","uri":"/posts/python-tips-and-tricks/#四自定义装饰器"},{"categories":["tech"],"content":" 1. 函数定义装饰器 @decc def gg(xx): ... # 等同于 def gg(xx) gg = decc(gg) 带参的装饰器 @decorator(A, B) def F(arg): ... F(99) # 等同于 def F(arg): ... F = decorator(A, B)(F) # Rebind F to result of decorator's return value F(99) # Essentially calls decorator(A, B)(F)(99) 上面演示的是用函数定义的装饰器，也是最常用的装饰器。 装饰器接收的参数可以是各种各样的，下面是一个带参的装饰器： @on_command(\"info\") def get_info(): return \"这就是你需要的 info\" def on_command(name: str): # 调用此函数获得装饰器，这样就实现了带参装饰器 def deco(func: Callable) -\u003e Callable: # 这个才是真正的装饰器 # 将命令处理器注册到命令列表内 return func # 直接返回原函数，这样的话，多个装饰器就不会相互影响了。 return deco # 上面的等同于： get_info = on_command(\"info\")(get_info) # on_command(\"info\") 返回真正的装饰器 如果你的 on_command 有通用的部分，还可以将通用的部分抽离出来复用： def _deco_maker(event_type: str) -\u003e Callable: # 调用这个，获取 on_xxx 的 deco_deco， def deco_deco(self) -\u003e Callable: # 这个对应 on_xxx def deco(func: Callable) -\u003e Callable: # 这个才是真正的装饰器 # do something return func # 返回原函数 return deco return deco_deco 我们知道 Python 的类实际上是可以很方便的修改的，因此函数装饰器也能用于装饰类，修改类的某些行为。 def log_getattribute(cls): # Get the original implementation orig_getattribute = cls.__getattribute__ # Make a new definition def new_getattribute(self, name): print('getting:', name) return orig_getattribute(self, name) # Attach to the class and return cls.__getattribute__ = new_getattribute # 修改了被装饰类 cls 的 __getattribute__ return cls # Example use @log_getattribute class A: def __init__(self,x): self.x = x def spam(self): pass ","date":"2022-02-13","objectID":"/posts/python-tips-and-tricks/:4:1","series":null,"tags":["Python","Tips","Tricks","常见错误"],"title":"Python 实用技巧与常见错误集锦","uri":"/posts/python-tips-and-tricks/#1-函数定义装饰器"},{"categories":["tech"],"content":" 1. 函数定义装饰器 @decc def gg(xx): ... # 等同于 def gg(xx) gg = decc(gg) 带参的装饰器 @decorator(A, B) def F(arg): ... F(99) # 等同于 def F(arg): ... F = decorator(A, B)(F) # Rebind F to result of decorator's return value F(99) # Essentially calls decorator(A, B)(F)(99) 上面演示的是用函数定义的装饰器，也是最常用的装饰器。 装饰器接收的参数可以是各种各样的，下面是一个带参的装饰器： @on_command(\"info\") def get_info(): return \"这就是你需要的 info\" def on_command(name: str): # 调用此函数获得装饰器，这样就实现了带参装饰器 def deco(func: Callable) -\u003e Callable: # 这个才是真正的装饰器 # 将命令处理器注册到命令列表内 return func # 直接返回原函数，这样的话，多个装饰器就不会相互影响了。 return deco # 上面的等同于： get_info = on_command(\"info\")(get_info) # on_command(\"info\") 返回真正的装饰器 如果你的 on_command 有通用的部分，还可以将通用的部分抽离出来复用： def _deco_maker(event_type: str) -\u003e Callable: # 调用这个，获取 on_xxx 的 deco_deco， def deco_deco(self) -\u003e Callable: # 这个对应 on_xxx def deco(func: Callable) -\u003e Callable: # 这个才是真正的装饰器 # do something return func # 返回原函数 return deco return deco_deco 我们知道 Python 的类实际上是可以很方便的修改的，因此函数装饰器也能用于装饰类，修改类的某些行为。 def log_getattribute(cls): # Get the original implementation orig_getattribute = cls.__getattribute__ # Make a new definition def new_getattribute(self, name): print('getting:', name) return orig_getattribute(self, name) # Attach to the class and return cls.__getattribute__ = new_getattribute # 修改了被装饰类 cls 的 __getattribute__ return cls # Example use @log_getattribute class A: def __init__(self,x): self.x = x def spam(self): pass ","date":"2022-02-13","objectID":"/posts/python-tips-and-tricks/:4:1","series":null,"tags":["Python","Tips","Tricks","常见错误"],"title":"Python 实用技巧与常见错误集锦","uri":"/posts/python-tips-and-tricks/#带参的装饰器"},{"categories":["tech"],"content":" 2. 类定义装饰器类定义装饰器和函数定义装饰器的使用方式完全一致。它也可以用于装饰函数或者类。 那么为啥还需要类定义装饰器呢？它的优势在于类是可以继承的，这样的话，就能用继承的方式定义装饰器，将通用部分定义成超类。 类定义装饰器的定义方法如下： # PythonDecorators/entry_exit_class.py class entry_exit(object): def __init__(self, f): self.f = f def __call__(self): #关键在于这个函数，它使此类的对象变成 Callable print(\"Entering\", self.f.__name__) self.f() print(\"Exited\", self.f.__name__) @entry_exit def func1(): print(\"inside func1()\") # 上面的装饰器相当于 func1 = entry_exit(func1) # 从这里看的话，装饰器的行为完全一致 # 接下来调用该函数（实际上是调用了 entry_exit 对象的 call 函数） func1() 输出结果如下： Entering func1 inside func1() Exited func1 ","date":"2022-02-13","objectID":"/posts/python-tips-and-tricks/:4:2","series":null,"tags":["Python","Tips","Tricks","常见错误"],"title":"Python 实用技巧与常见错误集锦","uri":"/posts/python-tips-and-tricks/#2-类定义装饰器"},{"categories":["tech"],"content":" 五、OOP 调用超类方法： 直接通过超类名.__init__(self,xx)调用 通过super(__class__, self).__init__()调用。 （Python3 可直接用 super().__init__() 但是要搞清楚，super() 方法返回的是一个代理类。另外被代理的类也不一定是其超类。如果不清楚这些差别，最好还是显式用方法一最好。） 抽象超类：@abstractmethod @staticmethod @classmethod 与 Java 的 static 方法对比 python的类方法、静态方法，与java的静态方法： java 中 constants、utils 这样的静态类，对应的是python的一个模块（文件），类属性对应模块的全局属性，静态方法对应模块的函数 对于 java 中需要访问类属性的静态方法，如果它不属于第一类，应该用 @classmethod 实现它。classmethod最大的特点就是一定有一个 cls 传入。这种方法的主要用途是实现工厂函数。 对于不需要访问任何类属性，也不属于第一类的方法，应该用 @staticmathod 实现。这种方法其实完全不需要放到类里面，它就是一个独立的函数。（仍然放里面，是为了把功能类似的函数组织到一起而已。） __slots__: 属性导出，不在该列表内的属性，若存在则为只读。不存在的话，就不存在。。 6. __getattr__: 拦截对不存在的属性的访问，可用于实现动态分配属性。 __getattribute__: 和上面相同，但是它拦截对所有属性的访问，包括对已存在的属性的访问。 @property: 提供对属性访问的安全检查 descriptor: get set delete 控制对类的访问。（上面的 getattr 等是控制对类的属性的访问） 类构造器 __new__：在 __init__ 之前运行，它接收一个 cls 参数，然后使用它构造并返回类实例 self。 类方法的 cls 即是当前类，是 type 的实例，cls.xxx 和 \u003c类名\u003e.xxx 调用结果是一致的。而 self 由 __new__ 构造，是 cls 的实例。 ","date":"2022-02-13","objectID":"/posts/python-tips-and-tricks/:5:0","series":null,"tags":["Python","Tips","Tricks","常见错误"],"title":"Python 实用技巧与常见错误集锦","uri":"/posts/python-tips-and-tricks/#五oop"},{"categories":["tech"],"content":" 元类 metaclasses元类，也就是用于创建class 的 class，算是很高级的话题了（If you wonder whether you need metaclasses, you don’t ） 元类的工作流程： 拦截类的创建 修改类 返回修改之后的类 详细直接看 http://blog.jobbole.com/21351/ 吧。 ","date":"2022-02-13","objectID":"/posts/python-tips-and-tricks/:5:1","series":null,"tags":["Python","Tips","Tricks","常见错误"],"title":"Python 实用技巧与常见错误集锦","uri":"/posts/python-tips-and-tricks/#元类-metaclasses"},{"categories":["tech"],"content":" 六、查看 Python 源码对一般的标准库的模块，要查看其具体的 Python 实现是很简单的：直接通过 __file__ 属性就能看到 .py 文件的位置。 但是 Python 很多功能是 C 写的，对于这类函数/类，__file__ 就没啥用了。 如果是需要查看 builtins 模块 的具体实现，直接查看 Python/bltinmodule.c 就行。 其他 C 模块的源码，待补充具体的查看方法。 ","date":"2022-02-13","objectID":"/posts/python-tips-and-tricks/:6:0","series":null,"tags":["Python","Tips","Tricks","常见错误"],"title":"Python 实用技巧与常见错误集锦","uri":"/posts/python-tips-and-tricks/#六查看-python-源码"},{"categories":["tech"],"content":" 七、参考文档 Python中一些不为人知的基础技巧总结 Python3 官方文档 ","date":"2022-02-13","objectID":"/posts/python-tips-and-tricks/:7:0","series":null,"tags":["Python","Tips","Tricks","常见错误"],"title":"Python 实用技巧与常见错误集锦","uri":"/posts/python-tips-and-tricks/#七参考文档"},{"categories":null,"content":" 更新时间: 2022-08-19T23:56UTC+08:00 ","date":"2022-02-07","objectID":"/statistics/:0:0","series":null,"tags":null,"title":"本站统计数据","uri":"/statistics/#"},{"categories":null,"content":" 一、全站统计 总字数 总访客数 UV 总访问量 PV 总阅读时长 人均阅读时长 268862 32680 80132 28 days, 4h 04m 52s 01m 14s ","date":"2022-02-07","objectID":"/statistics/:1:0","series":null,"tags":null,"title":"本站统计数据","uri":"/statistics/#一全站统计"},{"categories":null,"content":" 二、90 天阅读排行 序号 标题 人均阅读时长 访客数 访问量 1 通过 systemctl 设置自定义 Service 03m 32s 1 0 2 写给开发人员的实用密码学（七）—— 非对称密钥加密算法 RSA/ECC 02m 06s 533 754 3 写给开发人员的实用密码学（五）—— 密钥交换 DHKE 与完美前向保密 PFS 02m 04s 281 352 4 Kubernetes 微服务最佳实践 02m 01s 282 445 5 写给开发人员的实用密码学（三）—— MAC 与密钥派生函数 KDF 01m 57s 344 434 6 secrets 管理工具 Vault 的介绍、安装及使用 01m 51s 354 514 7 写给开发人员的实用密码学（二）—— 哈希函数 01m 44s 463 601 8 2021 年年终总结 01m 44s 92 129 9 写给开发人员的实用密码学（八）—— 数字证书与 TLS 协议 01m 38s 556 803 10 iptables 及 docker 容器网络分析 01m 36s 236 380 11 写给开发人员的实用密码学（六）—— 对称密钥加密算法 01m 29s 522 641 12 写给开发人员的实用密码学（四）—— 安全随机数生成器 CSPRNG 01m 28s 295 339 13 云原生流水线 Argo Workflows 的安装、使用以及个人体验 01m 26s 467 650 14 Linux 中的虚拟网络接口 01m 25s 225 285 15 NAT 网关、NAT 穿越以及虚拟网络 01m 20s 327 446 16 「转」且看有思想的年轻人 01m 09s 61 71 17 写给开发人员的实用密码学（一）—— 概览 01m 07s 508 802 18 Linux网络学习笔记（二）：域名解析(DNS)——以 CoreDNS 为例 01m 06s 158 184 19 分布式数据库的一致性问题与共识算法 51s 87 110 20 Summary of My 2021 51s 5 5 21 QEMU-KVM 虚拟化环境的搭建与使用 43s 679 807 22 WebSocket、HTTP/2 与 gRPC 43s 262 279 23 FinOps for Kubernetes - 如何拆分 Kubernetes 成本 36s 323 434 24 Pulumi 使用体验 - 基础设施代码化 36s 205 220 25 Python 实用技巧与常见错误集锦 36s 29 32 26 「转」仙马赛记——我又 PB 了 36s 24 24 27 2020 年年终总结 34s 47 49 28 Linux 网络工具中的瑞士军刀 - socat \u0026 netcat 33s 398 439 29 JWT 签名算法 HS256、RS256 及 ES256 及密钥生成 33s 393 456 30 openSUSE 使用指南 33s 144 154 31 2019 年年终总结 30s 31 33 32 Linux/Windows/MacOSX 系统常用命令集锦 28s 58 66 33 Death Is But a Dream 26s 237 339 34 使用 tcpdump 和 Wireshark 进行远程实时抓包分析 25s 336 362 35 使用 Istio 进行 JWT 身份验证（充当 API 网关） 23s 180 217 36 欧几里得算法求最大公约数(GCD)的数学原理 22s 101 104 37 变革与创新 21s 139 192 38 Manjaro 使用指南 18s 136 165 39 Kubernetes 常见错误、原因及处理方法 17s 195 209 ","date":"2022-02-07","objectID":"/statistics/:2:0","series":null,"tags":null,"title":"本站统计数据","uri":"/statistics/#二90-天阅读排行"},{"categories":null,"content":" 三、说明 此页面受 极客兔兔 - 博客折腾记(七) - Gitalk Plus 的启发而创建，其核心诉求是「帮助访客发现本站的优质文章」~ 此页面的数据由 Github Action 自动从 Google Analytics 获取，更新间隔为 6 小时，目前由博主看心情更新。 ","date":"2022-02-07","objectID":"/statistics/:3:0","series":null,"tags":null,"title":"本站统计数据","uri":"/statistics/#三说明"},{"categories":["tech"],"content":" 本文由个人笔记 ryan4yin/knowledge 整理而来，不保证正确 ","date":"2022-01-25","objectID":"/posts/kubernetes-deployment-using-kubeadm/:0:0","series":null,"tags":["Kubernetes","云原生"],"title":"部署一个 Kubernetes 集群","uri":"/posts/kubernetes-deployment-using-kubeadm/#"},{"categories":["tech"],"content":" 本地 Kubernetes 集群安装工具 云上的 Kubernetes 集群，基本上各云厂商都支持一键部署。这里主要关注本地部署，或者叫做裸机(baremetal)部署 本文介绍的方法适合开发测试使用，安全性、稳定性、长期可用性等方案都可能还有问题。 kubernetes 是一个组件化的系统，安装过程有很大的灵活性，很多组件都有多种实现，这些实现各有特点，让初学者眼花缭乱。 而且要把这些组件一个个安装配置好并且能协同工作，也是很不容易的。 因此社区出现了各种各样的安装方案，下面介绍下几种支持裸机（Baremetal）部署的工具： kubeadm: 社区的集群安装工具，目前已经很成熟了。 使用难度：简单 k3s: 轻量级 kubernetes，资源需求小，部署非常简单，适合开发测试用或者边缘环境 支持 airgap 离线部署 使用难度：超级简单 alibaba/sealer: 支持将整个 kubernetes 打包成一个镜像进行交付，而且部署也非常简单。 使用难度：超级简单 这个项目目前还在发展中，不过貌似已经有很多 toB 的公司在使用它进行 k8s 应用的交付了。 kubespray: 适合自建生产级别的集群，是一个大而全的 kubernetes 安装方案，自动安装容器运行时、k8s、网络插件等组件，而且各组件都有很多方案可选，但是感觉有点复杂。 使用难度：中等 支持 airgap 离线部署，但是以前我试用过是有坑，现在不知道咋样了 底层使用了 kubeadm 部署集群 sealos: 也很方便，一行命令部署 其他社区部署方案 自己写脚本，使用各组件的二进制文件进行部署。 笔者为了学习 Kubernetes，下面采用官方的 kubeadm 进行部署，容器运行时使用 containerd，网络插件则使用目前最潮的基于 eBPF 的 Cilium. kubernetes 官方介绍了两种高可用集群的拓扑结构：「堆叠 Etcd 拓扑（Stacked Etcd Topology）」和「外部 Etcd 拓扑（External Etcd Topology）」。 「堆叠 Etcd 拓扑」是指 Etcd 跟 Kubernetes Master 的其他组件部署在同一节点上，而「外部 Etcd 拓扑（External Etcd Topology）」则是指 Etcd 单独部署，与 Kubernetes Master 分开。 简单起见，本文使用「堆叠 Etcd 拓扑」结构，创建一个 3 master 的高可用集群。 参考： Kubernetes Docs - Installing kubeadm Kubernetes Docs - Creating Highly Available clusters with kubeadm ","date":"2022-01-25","objectID":"/posts/kubernetes-deployment-using-kubeadm/:1:0","series":null,"tags":["Kubernetes","云原生"],"title":"部署一个 Kubernetes 集群","uri":"/posts/kubernetes-deployment-using-kubeadm/#本地-kubernetes-集群安装工具"},{"categories":["tech"],"content":" 0. 网络环境的准备本文行文未考虑国内网络环境，但是 Kubernetes 用到的很多镜像都在 gcr.io 上，在国内访问会有困难。 如果对可靠性要求高，最好是自建私有镜像仓库，把镜像推送到私有仓库。可以通过如下命令列出所有 kubeadm 需要用到的镜像地址： ❯ kubeadm config images list --kubernetes-version v1.22.1 k8s.gcr.io/kube-apiserver:v1.22.1 k8s.gcr.io/kube-controller-manager:v1.22.1 k8s.gcr.io/kube-scheduler:v1.22.1 k8s.gcr.io/kube-proxy:v1.22.1 k8s.gcr.io/pause:3.5 k8s.gcr.io/etcd:3.5.0-0 k8s.gcr.io/coredns/coredns:v1.8.4 这里提供三个解决办法： 在家庭路由器上整个科学代理，实现全局科学上网。（我就是这么干的） 使用 liangyuanpeng 大佬在评论区提供的 gcr 国内镜像地址，这需要进行如下替换： k8s.gcr.io—\u003e lank8s.cn 自己维护一个国内镜像仓库（或私有镜像仓库如 harbor），使用 skopeo 等工具或脚本将上述镜像列表拷贝到你的私有仓库 ","date":"2022-01-25","objectID":"/posts/kubernetes-deployment-using-kubeadm/:2:0","series":null,"tags":["Kubernetes","云原生"],"title":"部署一个 Kubernetes 集群","uri":"/posts/kubernetes-deployment-using-kubeadm/#0-网络环境的准备"},{"categories":["tech"],"content":" 1. 节点的环境准备首先准备三台 Linux 虚拟机，系统按需选择，然后调整这三台机器的设置： 节点配置： master：不低于 2c/3g，硬盘 20G 主节点性能也受集群 Pods 个数的影响，上述配置应该可以支撑到每个 Worker 节点跑 100 个 Pod. worker：看需求，建议不低于 2c/4g，硬盘不小于 20G，资源充分的话建议 40G 以上。 处于同一网络内并可互通（通常是同一局域网） 各主机的 hostname 和 mac/ip 地址以及 /sys/class/dmi/id/product_uuid，都必须唯一 这里新手最容易遇到的问题，是 hostname 冲突 必须关闭 swap 交换内存，kubelet 才能正常工作 方便起见，我直接使用 ryan4yin/pulumi-libvirt 自动创建了五个 opensuse leap 15.3 虚拟机，并设置好了 ip/hostname. ","date":"2022-01-25","objectID":"/posts/kubernetes-deployment-using-kubeadm/:3:0","series":null,"tags":["Kubernetes","云原生"],"title":"部署一个 Kubernetes 集群","uri":"/posts/kubernetes-deployment-using-kubeadm/#1-节点的环境准备"},{"categories":["tech"],"content":" 1.1 iptables 设置目前 kubernetes 的容器网络，默认使用的是 bridge 模式，这种模式下，需要使 iptables 能够接管 bridge 上的流量。 配置如下： sudo modprobe br_netfilter cat \u003c\u003cEOF | sudo tee /etc/modules-load.d/k8s.conf br_netfilter EOF cat \u003c\u003cEOF | sudo tee /etc/sysctl.d/k8s.conf net.bridge.bridge-nf-call-ip6tables = 1 net.bridge.bridge-nf-call-iptables = 1 EOF sudo sysctl --system ","date":"2022-01-25","objectID":"/posts/kubernetes-deployment-using-kubeadm/:3:1","series":null,"tags":["Kubernetes","云原生"],"title":"部署一个 Kubernetes 集群","uri":"/posts/kubernetes-deployment-using-kubeadm/#11-iptables-设置"},{"categories":["tech"],"content":" 1.2 开放节点端口 局域网环境的话，建议直接关闭防火墙。这样所有端口都可用，方便快捷。 通常我们的云上集群，也是关闭防火墙的，只是会通过云服务提供的「安全组」来限制客户端 ip Control-plane 节点，也就是 master，需要开放如下端口： Protocol Direction Port Range Purpose Used By TCP Inbound 6443* Kubernetes API server All TCP Inbound 2379-2380 etcd server client API kube-apiserver, etcd TCP Inbound 10250 kubelet API Self, Control plane TCP Inbound 10251 kube-scheduler Self TCP Inbound 10252 kube-controller-manager Self Worker 节点需要开发如下端口： Protocol Direction Port Range Purpose Used By TCP Inbound 10250 kubelet API Self, Control plane TCP Inbound 30000-32767 NodePort Services† All 另外通常我们本地测试的时候，可能更想直接在 80 443 8080 等端口上使用 NodePort， 就需要修改 kube-apiserver 的 --service-node-port-range 参数来自定义 NodePort 的端口范围，相应的 Worker 节点也得开放这些端口。 ","date":"2022-01-25","objectID":"/posts/kubernetes-deployment-using-kubeadm/:3:2","series":null,"tags":["Kubernetes","云原生"],"title":"部署一个 Kubernetes 集群","uri":"/posts/kubernetes-deployment-using-kubeadm/#12-开放节点端口"},{"categories":["tech"],"content":" 2. 安装 containerd首先是环境配置： cat \u003c\u003cEOF | sudo tee /etc/modules-load.d/containerd.conf overlay br_netfilter nf_conntrack EOF sudo modprobe overlay sudo modprobe br_netfilter sudo modprobe nf_conntrack # Setup required sysctl params, these persist across reboots. cat \u003c\u003cEOF | sudo tee /etc/sysctl.d/99-kubernetes-cri.conf net.bridge.bridge-nf-call-iptables = 1 net.ipv4.ip_forward = 1 net.bridge.bridge-nf-call-ip6tables = 1 EOF # Apply sysctl params without reboot sudo sysctl --system 安装 containerd+nerdctl: wget https://github.com/containerd/nerdctl/releases/download/v0.11.1/nerdctl-full-0.11.1-linux-amd64.tar.gz tar -axvf nerdctl-full-0.11.1-linux-amd64.tar.gz # 这里简单起见，rootless 相关的东西也一起装进去了，测试嘛就无所谓了... mv bin/* /usr/local/bin/ mv lib/systemd/system/containerd.service /usr/lib/systemd/system/ systemctl enable containerd systemctl start containerd nerdctl 是一个 containerd 的命令行工具，但是它的容器、镜像与 Kubernetes 的容器、镜像是完全隔离的，不能互通！ 目前只能通过 crictl 来查看、拉取 Kubernetes 的容器、镜像，下一节会介绍 crictl 的安装。 ","date":"2022-01-25","objectID":"/posts/kubernetes-deployment-using-kubeadm/:4:0","series":null,"tags":["Kubernetes","云原生"],"title":"部署一个 Kubernetes 集群","uri":"/posts/kubernetes-deployment-using-kubeadm/#2-安装-containerd"},{"categories":["tech"],"content":" 3. 安装 kubelet/kubeadm/kubectl # 一些全局都需要用的变量 CNI_VERSION=\"v0.8.2\" CRICTL_VERSION=\"v1.17.0\" # kubernetes 的版本号 # RELEASE=\"$(curl -sSL https://dl.k8s.io/release/stable.txt)\" RELEASE=\"1.22.1\" # kubelet 配置文件的版本号 RELEASE_VERSION=\"v0.4.0\" # 架构 ARCH=\"amd64\" #　安装目录 DOWNLOAD_DIR=/usr/local/bin # CNI 插件 sudo mkdir -p /opt/cni/bin curl -L \"https://github.com/containernetworking/plugins/releases/download/${CNI_VERSION}/cni-plugins-linux-${ARCH}-${CNI_VERSION}.tgz\" | sudo tar -C /opt/cni/bin -xz # crictl 相关工具 curl -L \"https://github.com/kubernetes-sigs/cri-tools/releases/download/${CRICTL_VERSION}/crictl-${CRICTL_VERSION}-linux-${ARCH}.tar.gz\" | sudo tar -C $DOWNLOAD_DIR -xz # kubelet/kubeadm/kubectl cd $DOWNLOAD_DIR sudo curl -L --remote-name-all https://storage.googleapis.com/kubernetes-release/release/${RELEASE}/bin/linux/${ARCH}/{kubeadm,kubelet,kubectl} sudo chmod +x {kubeadm,kubelet,kubectl} # kubelet/kubeadm 配置 curl -sSL \"https://raw.githubusercontent.com/kubernetes/release/${RELEASE_VERSION}/cmd/kubepkg/templates/latest/deb/kubelet/lib/systemd/system/kubelet.service\" | sed \"s:/usr/bin:${DOWNLOAD_DIR}:g\" | sudo tee /etc/systemd/system/kubelet.service sudo mkdir -p /etc/systemd/system/kubelet.service.d curl -sSL \"https://raw.githubusercontent.com/kubernetes/release/${RELEASE_VERSION}/cmd/kubepkg/templates/latest/deb/kubeadm/10-kubeadm.conf\" | sed \"s:/usr/bin:${DOWNLOAD_DIR}:g\" | sudo tee /etc/systemd/system/kubelet.service.d/10-kubeadm.conf systemctl enable --now kubelet # 验证 kubelet 启动起来了，但是目前还没有初始化配置，过一阵就会重启一次 systemctl status kubelet 试用 crictl: export CONTAINER_RUNTIME_ENDPOINT='unix:///var/run/containerd/containerd.sock' # 列出所有 pods，现在应该啥也没 crictl pods # 列出所有镜像 crictl images ","date":"2022-01-25","objectID":"/posts/kubernetes-deployment-using-kubeadm/:5:0","series":null,"tags":["Kubernetes","云原生"],"title":"部署一个 Kubernetes 集群","uri":"/posts/kubernetes-deployment-using-kubeadm/#3-安装-kubeletkubeadmkubectl"},{"categories":["tech"],"content":" 4. 为 master 的 kube-apiserver 创建负载均衡实现高可用根据 kubeadm 官方文档 Kubeadm Docs - High Availability Considerations 介绍，要实现 kube-apiserver 的高可用，目前最知名的负载均衡方式是 keepalived+haproxy，另外也可以考虑使用 kube-vip 等更简单的工具。 简单起见，我们直接用 kube-vip 吧，参考了 kube-vip 的官方文档：Kube-vip as a Static Pod with Kubelet. P.S. 我也见过有的安装工具会直接抛弃 keepalived，直接在每个节点上跑一个 nginx 做负载均衡，配置里写死了所有 master 的地址… 首先使用如下命令生成 kube-vip 的配置文件，以 ARP 为例（生产环境建议换成 BGP）： cat \u003c\u003cEOF | sudo tee add-kube-vip.sh # 你的虚拟机网卡，opensuse/centos 等都是 eth0，但是 ubuntu 可能是 ens3 export INTERFACE=eth0 # 用于实现高可用的 vip，需要和前面的网络接口在同一网段内，否则就无法路由了。 export VIP=192.168.122.200 # 生成 static-pod 的配置文件 mkdir -p /etc/kubernetes/manifests nerdctl run --rm --network=host --entrypoint=/kube-vip ghcr.io/kube-vip/kube-vip:v0.3.8 \\ manifest pod \\ --interface $INTERFACE \\ --vip $VIP \\ --controlplane \\ --services \\ --arp \\ --leaderElection | tee /etc/kubernetes/manifests/kube-vip.yaml EOF bash add-kube-vip.sh 三个 master 节点都需要跑下上面的命令（worker 不需要），创建好 kube-vip 的 static-pod 配置文件。 在完成 kubeadm 初始化后，kubelet 会自动把它们拉起为 static pod. ","date":"2022-01-25","objectID":"/posts/kubernetes-deployment-using-kubeadm/:6:0","series":null,"tags":["Kubernetes","云原生"],"title":"部署一个 Kubernetes 集群","uri":"/posts/kubernetes-deployment-using-kubeadm/#4-为-master-的-kube-apiserver-创建负载均衡实现高可用"},{"categories":["tech"],"content":" 5. 使用 kubeadm 创建集群其实需要运行的就是这条命令： # 极简配置： cat \u003c\u003cEOF | sudo tee kubeadm-config.yaml apiVersion: kubeadm.k8s.io/v1beta3 kind: InitConfiguration nodeRegistration: criSocket: \"/var/run/containerd/containerd.sock\" imagePullPolicy: IfNotPresent --- kind: ClusterConfiguration apiVersion: kubeadm.k8s.io/v1beta3 kubernetesVersion: v1.22.1 clusterName: kubernetes certificatesDir: /etc/kubernetes/pki imageRepository: k8s.gcr.io controlPlaneEndpoint: \"192.168.122.200:6443\" # 填 apiserver 的 vip 地址，或者整个域名也行，但是就得加 /etc/hosts 或者内网 DNS 解析 networking: serviceSubnet: \"10.96.0.0/16\" podSubnet: \"10.244.0.0/16\" etcd: local: dataDir: /var/lib/etcd --- apiVersion: kubelet.config.k8s.io/v1beta1 kind: KubeletConfiguration cgroupDriver: systemd # 让 kubelet 从 certificates.k8s.io 申请由集群 CA Root 签名的 tls 证书，而非直接使用自签名证书 # 如果不启用这个， 安装 metrics-server 时就会遇到证书报错，后面会详细介绍。 serverTLSBootstrap: true EOF # 查看 kubeadm 默认的完整配置，供参考 kubeadm config print init-defaults \u003e init.default.yaml # 执行集群的初始化，这会直接将当前节点创建为 master # 成功运行的前提：前面该装的东西都装好了，而且 kubelet 已经在后台运行了 # `--upload-certs` 会将生成的集群证书上传到 kubeadm 服务器，在两小时内加入集群的 master 节点会自动拉证书，主要是方便集群创建。 kubeadm init --config kubeadm-config.yaml --upload-certs kubeadm 应该会报错，提示你有些依赖不存在，下面先安装好依赖项。 sudo zypper in -y socat ebtables conntrack-tools 再重新运行前面的 kubeadm 命令，应该就能正常执行了，它做的操作有： 拉取控制面的容器镜像 生成 ca 根证书 使用根证书为 etcd/apiserver 等一票工具生成 tls 证书 为控制面的各个组件生成 kubeconfig 配置 生成 static pod 配置，kubelet 会根据这些配置自动拉起 kube-proxy 以及其他所有的 k8s master 组件 运行完会给出三部分命令： 将 kubeconfig 放到 $HOME/.kube/config 下，kubectl 需要使用该配置文件连接 kube-apiserver control-plane 节点加入集群的命令: 这里由于我们提前添加了 kube-vip 的 static-pod 配置，这里的 preflight-check 会报错，需要添加此参数忽略该报错 - --ignore-preflight-errors=DirAvailable--etc-kubernetes-manifests kubeadm join 192.168.122.200:6443 --token \u003ctoken\u003e \\ --discovery-token-ca-cert-hash sha256:\u003chash\u003e \\ --control-plane --certificate-key \u003ckey\u003e \\ --ignore-preflight-errors=DirAvailable--etc-kubernetes-manifests worker 节点加入集群的命令: kubeadm join 192.168.122.200:6443 --token \u003ctoken\u003e \\ --discovery-token-ca-cert-hash sha256:\u003chash\u003e 跑完第一部分 kubeconfig 的处理命令后，就可以使用 kubectl 查看集群状况了： k8s-master-0:~/kubeadm # kubectl get no NAME STATUS ROLES AGE VERSION k8s-master-0 NotReady control-plane,master 79s v1.22.1 k8s-master-0:~/kubeadm # kubectl get po --all-namespaces NAMESPACE NAME READY STATUS RESTARTS AGE kube-system coredns-78fcd69978-6tlnw 0/1 Pending 0 83s kube-system coredns-78fcd69978-hxtvs 0/1 Pending 0 83s kube-system etcd-k8s-master-0 1/1 Running 6 90s kube-system kube-apiserver-k8s-master-0 1/1 Running 4 90s kube-system kube-controller-manager-k8s-master-0 1/1 Running 4 90s kube-system kube-proxy-6w2bx 1/1 Running 0 83s kube-system kube-scheduler-k8s-master-0 1/1 Running 7 97s 现在在其他节点运行前面打印出的加入集群的命令，就可以搭建好一个高可用的集群了。 所有节点都加入集群后，通过 kubectl 查看，应该是三个控制面 master，两个 worker： k8s-master-0:~/kubeadm # kubectl get node NAME STATUS ROLES AGE VERSION k8s-master-0 NotReady control-plane,master 26m v1.22.1 k8s-master-1 NotReady control-plane,master 7m2s v1.22.1 k8s-master-2 NotReady control-plane,master 2m10s v1.22.1 k8s-worker-0 NotReady \u003cnone\u003e 97s v1.22.1 k8s-worker-1 NotReady \u003cnone\u003e 86s v1.22.1 现在它们都还处于 NotReady 状态，需要等到我们把网络插件安装好，才会 Ready. 现在再看下集群的证书签发状态： ❯ kubectl get csr --sort-by='{.spec.username}' NAME AGE SIGNERNAME REQUESTOR REQUESTEDDURATION CONDITION csr-95hll 6m58s kubernetes.io/kube-apiserver-client-kubelet system:bootstrap:q8ivnz \u003cnone\u003e Approved,Issued csr-tklnr 7m5s kubernetes.io/kube-apiserver-client-kubelet system:bootstrap:q8ivnz \u003cnone\u003e Approved,Issued csr-w92jv 9m15s kubernetes.io/kube-apiserver-client-kubelet system:bootstrap:q8ivnz \u003cnone\u003e Approved,Issued csr-rv7sj 8m11s kubernetes.io/kube-apiserver-client-kubelet system:bootstrap:q8ivnz \u003cnone\u003e Approved,Issued csr-nxkgx 10m kubernetes.io/kube-apiserver-client-kubelet system:node:k8s-master-0 \u003cnone\u003e Approved,Issued csr-cd22c 10m kubernetes.io/kubelet-serving system:node:k8s-master-0 \u003cnone\u003e Pending csr-wjrnr 9m53s kubernetes.io/kubelet-serving system:node:k8s-master-0 \u003cnone\u003e Pending csr-sjq42 9m8s kubernetes.io/kubelet-serv","date":"2022-01-25","objectID":"/posts/kubernetes-deployment-using-kubeadm/:7:0","series":null,"tags":["Kubernetes","云原生"],"title":"部署一个 Kubernetes 集群","uri":"/posts/kubernetes-deployment-using-kubeadm/#5-使用-kubeadm-创建集群"},{"categories":["tech"],"content":" 5.1 常见问题 5.1.1 重置集群配置创建集群的过程中出现任何问题，都可以通过在所有节点上运行 kubeadm reset 来还原配置，然后重新走 kubeadm 的集群创建流程。 但是要注意几点： kubeadm reset 会清除包含 kube-vip 配置在内的所有 static-pod 配置文件，所以 master 节点需要重新跑下前面给的 kube-vip 命令，生成下 kube-vip 配置。 kubeadm reset 不会重置网络接口的配置，master 节点需要手动清理下 kube-vip 添加的 vip: ip addr del 192.168.122.200/32 dev eth0. 如果你在安装了网络插件之后希望重装集群，顺序如下： 通过 kubectl delete -f xxx.yaml/helm uninstall 删除所有除网络之外的其他应用配置 删除网络插件 先重启一遍所有节点，或者手动重置所有节点的网络配置 建议重启，因为我不知道该怎么手动重置… 试了 systemctl restart network 并不会清理所有虚拟网络接口。 如此操作后，再重新执行集群安装，应该就没啥毛病了。 ","date":"2022-01-25","objectID":"/posts/kubernetes-deployment-using-kubeadm/:7:1","series":null,"tags":["Kubernetes","云原生"],"title":"部署一个 Kubernetes 集群","uri":"/posts/kubernetes-deployment-using-kubeadm/#51-常见问题"},{"categories":["tech"],"content":" 5.1 常见问题 5.1.1 重置集群配置创建集群的过程中出现任何问题，都可以通过在所有节点上运行 kubeadm reset 来还原配置，然后重新走 kubeadm 的集群创建流程。 但是要注意几点： kubeadm reset 会清除包含 kube-vip 配置在内的所有 static-pod 配置文件，所以 master 节点需要重新跑下前面给的 kube-vip 命令，生成下 kube-vip 配置。 kubeadm reset 不会重置网络接口的配置，master 节点需要手动清理下 kube-vip 添加的 vip: ip addr del 192.168.122.200/32 dev eth0. 如果你在安装了网络插件之后希望重装集群，顺序如下： 通过 kubectl delete -f xxx.yaml/helm uninstall 删除所有除网络之外的其他应用配置 删除网络插件 先重启一遍所有节点，或者手动重置所有节点的网络配置 建议重启，因为我不知道该怎么手动重置… 试了 systemctl restart network 并不会清理所有虚拟网络接口。 如此操作后，再重新执行集群安装，应该就没啥毛病了。 ","date":"2022-01-25","objectID":"/posts/kubernetes-deployment-using-kubeadm/:7:1","series":null,"tags":["Kubernetes","云原生"],"title":"部署一个 Kubernetes 集群","uri":"/posts/kubernetes-deployment-using-kubeadm/#511-重置集群配置"},{"categories":["tech"],"content":" 6. 验证集群的高可用性虽然网络插件还没装导致集群所有节点都还没 ready，但是我们已经可以通过 kubectl 命令来简单验证集群的高可用性了。 首先，我们将前面放置在 k8s-master-0 的认证文件 $HOME/.kube/config 以及 kunbectl 安装在另一台机器上，比如我直接放我的宿主机。 然后在宿主机上跑 kubectl get node 命令验证集群的高可用性： 三个主节点都正常运行时，kubectl 命令也正常 pause 或者 stop 其中一个 master，kubectl 命令仍然能正常运行 再 pause 第二个 master，kubectl 命令应该就会卡住，并且超时，无法使用了 resume 恢复停掉的两个 master 之一，会发现 kubectl 命令又能正常运行了 到这里 kubeadm 的工作就完成了，接下来再安装网络插件，集群就可用了。 ","date":"2022-01-25","objectID":"/posts/kubernetes-deployment-using-kubeadm/:8:0","series":null,"tags":["Kubernetes","云原生"],"title":"部署一个 Kubernetes 集群","uri":"/posts/kubernetes-deployment-using-kubeadm/#6-验证集群的高可用性"},{"categories":["tech"],"content":" 7. 安装网络插件社区有很多种网络插件可选，比较知名且性能也不错的，应该是 Calico 和 Cilium，其中 Cilium 主打基于 eBPF 的高性能与高可观测性。 下面分别介绍这两个插件的安装方法。（注意只能安装其中一个网络插件，不能重复安装。） 需要提前在本机安装好 helm，我这里使用宿主机，因此只需要在宿主机安装: # 一行命令安装，也可以自己手动下载安装包，都行 curl https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3 | bash # 或者 opensuse 直接用包管理器安装 sudo zypper in helm ","date":"2022-01-25","objectID":"/posts/kubernetes-deployment-using-kubeadm/:9:0","series":null,"tags":["Kubernetes","云原生"],"title":"部署一个 Kubernetes 集群","uri":"/posts/kubernetes-deployment-using-kubeadm/#7-安装网络插件"},{"categories":["tech"],"content":" 7.1 安装 Cilium 官方文档：https://docs.cilium.io/en/v1.10/gettingstarted/k8s-install-kubeadm/ cilium 通过 eBPF 提供了高性能与高可观测的 k8s 集群网络， 另外 cilium 还提供了比 kube-proxy 更高效的实现，可以完全替代 kube-proxy. 这里我们还是先使用 kube-proxy 模式，先熟悉下 cilium 的使用： helm repo add cilium https://helm.cilium.io/ helm search repo cilium/cilium -l | head helm install cilium cilium/cilium --version 1.10.4 --namespace kube-system 可以通过 kubectl get pod -A 查看 cilium 的安装进度，当所有 pod 都 ready 后，集群就 ready 了~ cilium 也提供了专用的客户端： curl -L --remote-name-all https://github.com/cilium/cilium-cli/releases/latest/download/cilium-linux-amd64.tar.gz{,.sha256sum} sha256sum --check cilium-linux-amd64.tar.gz.sha256sum sudo tar xzvfC cilium-linux-amd64.tar.gz /usr/local/bin rm cilium-linux-amd64.tar.gz{,.sha256sum} 然后使用 cilium 客户端检查网络插件的状态： $ cilium status --wait /¯¯\\ /¯¯\\__/¯¯\\ Cilium: OK \\__/¯¯\\__/ Operator: OK /¯¯\\__/¯¯\\ Hubble: disabled \\__/¯¯\\__/ ClusterMesh: disabled \\__/ DaemonSet cilium Desired: 5, Ready: 5/5, Available: 5/5 Deployment cilium-operator Desired: 2, Ready: 2/2, Available: 2/2 Containers: cilium Running: 5 cilium-operator Running: 2 Cluster Pods: 2/2 managed by Cilium Image versions cilium quay.io/cilium/cilium:v1.10.4@sha256:7d354052ccf2a7445101d78cebd14444c7c40129ce7889f2f04b89374dbf8a1d: 5 cilium-operator quay.io/cilium/operator-generic:v1.10.4@sha256:c49a14e34634ff1a494c84b718641f27267fb3a0291ce3d74352b44f8a8d2f93: 2 cilium 还提供了命令，自动创建 pod 进行集群网络的连接性测试： ❯ cilium connectivity test ℹ️ Monitor aggregation detected, will skip some flow validation steps ✨ [kubernetes] Creating namespace for connectivity check... ✨ [kubernetes] Deploying echo-same-node service... ✨ [kubernetes] Deploying same-node deployment... ✨ [kubernetes] Deploying client deployment... ✨ [kubernetes] Deploying client2 deployment... ✨ [kubernetes] Deploying echo-other-node service... ✨ [kubernetes] Deploying other-node deployment... ... ℹ️ Expose Relay locally with: cilium hubble enable cilium status --wait cilium hubble port-forward\u0026 🏃 Running tests... ... --------------------------------------------------------------------------------------------------------------------- ✅ All 11 tests (134 actions) successful, 0 tests skipped, 0 scenarios skipped. 通过 kubectl get po -A 能观察到，这个测试命令会自动创建一个 cilium-test 名字空间，并在启动创建若干 pod 进行详细的测试。 整个测试流程大概会持续 5 分多钟，测试完成后，相关 Pod 不会自动删除，使用如下命令手动删除： kubectl delete namespace cilium-test ","date":"2022-01-25","objectID":"/posts/kubernetes-deployment-using-kubeadm/:9:1","series":null,"tags":["Kubernetes","云原生"],"title":"部署一个 Kubernetes 集群","uri":"/posts/kubernetes-deployment-using-kubeadm/#71-安装-cilium"},{"categories":["tech"],"content":" 7.2 安装 Calico 官方文档：https://docs.projectcalico.org/getting-started/kubernetes/self-managed-onprem/onpremises 也就两三行命令。安装确实特别简单，懒得介绍了，看官方文档吧。 但是实际上 calico 的细节还蛮多的，建议通读下它的官方文档，了解下 calico 的架构。 ","date":"2022-01-25","objectID":"/posts/kubernetes-deployment-using-kubeadm/:9:2","series":null,"tags":["Kubernetes","云原生"],"title":"部署一个 Kubernetes 集群","uri":"/posts/kubernetes-deployment-using-kubeadm/#72-安装-calico"},{"categories":["tech"],"content":" 8. 查看集群状态官方的 dashboard 个人感觉不太好用，建议直接在本地装个 k9s 用，特别爽。 sudo zypper in k9s 然后就可以愉快地玩耍了。 ","date":"2022-01-25","objectID":"/posts/kubernetes-deployment-using-kubeadm/:10:0","series":null,"tags":["Kubernetes","云原生"],"title":"部署一个 Kubernetes 集群","uri":"/posts/kubernetes-deployment-using-kubeadm/#8-查看集群状态"},{"categories":["tech"],"content":" 9. 安装 metrics-server 这一步可能遇到的问题：Enabling signed kubelet serving certificates 如果需要使用 HPA 以及简单的集群监控，那么 metrics-server 是必须安装的，现在我们安装一下它。 首先，跑 kubectl 的监控命令应该会报错： ❯ kubectl top node error: Metrics API not available k9s 里面应该也看不到任何监控指标。 现在通过 helm 安装它： helm repo add metrics-server https://kubernetes-sigs.github.io/metrics-server/ helm search repo metrics-server/metrics-server -l | head helm upgrade --install metrics-server metrics-server/metrics-server --version 3.5.0 --namespace kube-system metrics-server 默认只会部署一个实例，如果希望高可用，请参考官方配置：metrics-server - high-availability manifests 等 metrics-server 启动好后，就可以使用 kubectl top 命令啦： ❯ kubectl top node NAME CPU(cores) CPU% MEMORY(bytes) MEMORY% k8s-master-0 327m 16% 1465Mi 50% k8s-master-1 263m 13% 1279Mi 44% k8s-master-2 289m 14% 1282Mi 44% k8s-worker-0 62m 3% 518Mi 13% k8s-worker-1 115m 2% 659Mi 8% ❯ kubectl top pod No resources found in default namespace. ❯ kubectl top pod -A NAMESPACE NAME CPU(cores) MEMORY(bytes) kube-system cilium-45nw4 9m 135Mi kube-system cilium-5x7jf 6m 154Mi kube-system cilium-84sr2 7m 160Mi kube-system cilium-operator-78f45675-dp4b6 2m 30Mi kube-system cilium-operator-78f45675-fpm5g 1m 30Mi kube-system cilium-tkhl4 6m 141Mi kube-system cilium-zxbvm 5m 138Mi kube-system coredns-78fcd69978-dpxxk 3m 16Mi kube-system coredns-78fcd69978-ptd9p 1m 18Mi kube-system etcd-k8s-master-0 61m 88Mi kube-system etcd-k8s-master-1 50m 85Mi kube-system etcd-k8s-master-2 55m 83Mi kube-system kube-apiserver-k8s-master-0 98m 462Mi kube-system kube-apiserver-k8s-master-1 85m 468Mi kube-system kube-apiserver-k8s-master-2 85m 423Mi kube-system kube-controller-manager-k8s-master-0 22m 57Mi kube-system kube-controller-manager-k8s-master-1 2m 23Mi kube-system kube-controller-manager-k8s-master-2 2m 23Mi kube-system kube-proxy-j2s76 1m 24Mi kube-system kube-proxy-k6d6z 1m 18Mi kube-system kube-proxy-k85rx 1m 23Mi kube-system kube-proxy-pknsc 1m 20Mi kube-system kube-proxy-xsq4m 1m 15Mi kube-system kube-scheduler-k8s-master-0 3m 25Mi kube-system kube-scheduler-k8s-master-1 4m 21Mi kube-system kube-scheduler-k8s-master-2 5m 21Mi kube-system kube-vip-k8s-master-0 4m 17Mi kube-system kube-vip-k8s-master-1 2m 16Mi kube-system kube-vip-k8s-master-2 2m 17Mi kube-system metrics-server-559f85484-5b6xf 7m 27Mi ","date":"2022-01-25","objectID":"/posts/kubernetes-deployment-using-kubeadm/:11:0","series":null,"tags":["Kubernetes","云原生"],"title":"部署一个 Kubernetes 集群","uri":"/posts/kubernetes-deployment-using-kubeadm/#9-安装-metrics-server"},{"categories":["tech"],"content":" 10. 为 etcd 添加定期备份能力请移步 etcd 的备份与恢复 ","date":"2022-01-25","objectID":"/posts/kubernetes-deployment-using-kubeadm/:12:0","series":null,"tags":["Kubernetes","云原生"],"title":"部署一个 Kubernetes 集群","uri":"/posts/kubernetes-deployment-using-kubeadm/#10-为-etcd-添加定期备份能力"},{"categories":["tech"],"content":" 11. 安装 Volume Provisioner在我们学习使用 Prometheus/MinIO/Tekton 等有状态应用时，它们默认情况下会通过 PVC 声明需要的数据卷。 为了支持这个能力，我们需要在集群中部署一个 Volume Provisioner. 对于云上环境，直接接入云服务商提供的 Volume Provisioner 就 OK 了，方便省事而且足够可靠。 而对于 bare-metal 环境，比较有名的应该是 rook-ceph，但是这个玩意部署复杂，维护难度又高，不适合用来测试学习。 对于开发、测试环境，或者个人集群，建议使用： local 数据卷，适合数据可丢失，且不要求分布式的场景，如开发测试环境 https://github.com/kubernetes-sigs/sig-storage-local-static-provisioner https://github.com/rancher/local-path-provisioner NFS 数据卷，适合数据可丢失，对性能要求不高，并且要求分布式的场景。比如开发测试环境、或者线上没啥压力的应用 https://github.com/kubernetes-sigs/nfs-subdir-external-provisioner https://github.com/kubernetes-csi/csi-driver-nfs NFS 数据的可靠性依赖于外部 NFS 服务器，企业通常使用群晖等 NAS 来做 NFS 服务器 如果外部 NFS 服务器出问题，应用就会崩。 直接使用云上的对象存储，适合希望数据不丢失、对性能要求不高的场景。 直接使用 https://github.com/rclone/rclone mount 模式来保存数据，或者直接同步文件夹数据到云端（可能会有一定数据丢失）。 ","date":"2022-01-25","objectID":"/posts/kubernetes-deployment-using-kubeadm/:13:0","series":null,"tags":["Kubernetes","云原生"],"title":"部署一个 Kubernetes 集群","uri":"/posts/kubernetes-deployment-using-kubeadm/#11-安装-volume-provisioner"},{"categories":["tech"],"content":" 本文由个人笔记 ryan4yin/knowledge 整理而来 本文主要介绍我个人在使用 Kubernetes 的过程中，总结出的一套「Kubernetes 配置」，是我个人的「最佳实践」。 其中大部分内容都经历过线上环境的考验，但是也有少部分还只在我脑子里模拟过，请谨慎参考。 阅读前的几个注意事项： 这份文档比较长，囊括了很多内容，建议当成参考手册使用，先参照目录简单读一读，有需要再细读相关内容。 这份文档需要一定的 Kubernetes 基础才能理解，而且如果没有过实践经验的话，看上去可能会比较枯燥。 而有过实践经验的大佬，可能会跟我有不同的见解，欢迎各路大佬评论~ 我会视情况不定期更新这份文档。 ","date":"2022-01-25","objectID":"/posts/kubernetes-best-practices/:0:0","series":null,"tags":["Kubernetes","最佳实践","云原生"],"title":"Kubernetes 微服务最佳实践","uri":"/posts/kubernetes-best-practices/#"},{"categories":["tech"],"content":" 零、示例首先，这里给出一些本文遵守的前提，这些前提只是契合我遇到的场景，可灵活变通： 这里只讨论无状态服务，有状态服务不在讨论范围内 我们不使用 Deployment 的滚动更新能力，而是为每个服务的每个版本，都创建不同的 Deployment + HPA + PodDisruptionBudget，这是为了方便做金丝雀/灰度发布 我们的服务可能会使用 IngressController / Service Mesh 来进行服务的负载均衡、流量切分 下面先给出一个 Deployment + HPA + PodDisruptionBudget 的 demo，后面再拆开详细说下： apiVersion: apps/v1 kind: Deployment metadata: name: my-app-v3 namespace: prod # 建议按业务逻辑划分名字空间，prod 仅为示例 labels: app: my-app spec: replicas: 3 strategy: type: RollingUpdate # 因为服务的每个版本都使用各自的 Deployment，服务更新时其实是用不上这里的滚动更新策略的 # 这个配置应该只在 SRE 手动修改 Deployment 配置时才会生效（通常不应该发生这种事） rollingUpdate: maxSurge: 10% # 滚动更新时，每次最多更新 10% 的 Pods maxUnavailable: 0 # 滚动更新时，不允许出现不可用的 Pods，也就是说始终要维持 3 个可用副本 selector: matchLabels: app: my-app version: v3 template: metadata: labels: app: my-app version: v3 spec: affinity: # 注意，podAffinity/podAntiAffinity 可能不是最佳方案，这部分配置待更新 # topologySpreadConstraints 可能是更好的选择 podAffinity: preferredDuringSchedulingIgnoredDuringExecution: # 非强制性条件 - weight: 100 # weight 用于为节点评分，会优先选择评分最高的节点（只有一条规则的情况下，这个值没啥意义） podAffinityTerm: labelSelector: matchExpressions: - key: app operator: In values: - my-app - key: version operator: In values: - v3 # pod 尽量使用同一种节点类型，也就是尽量保证节点的性能一致 topologyKey: node.kubernetes.io/instance-type podAntiAffinity: preferredDuringSchedulingIgnoredDuringExecution: # 非强制性条件 - weight: 100 # weight 用于为节点评分，会优先选择评分最高的节点（只有一条规则的情况下，这个值没啥意义） podAffinityTerm: labelSelector: matchExpressions: - key: app operator: In values: - my-app - key: version operator: In values: - v3 # 将 pod 尽量打散在多个可用区 topologyKey: topology.kubernetes.io/zone requiredDuringSchedulingIgnoredDuringExecution: # 强制性要求（这个建议按需添加） # 注意这个没有 weights，必须满足列表中的所有条件 - labelSelector: matchExpressions: - key: app operator: In values: - my-app - key: version operator: In values: - v3 # Pod 必须运行在不同的节点上 topologyKey: kubernetes.io/hostname securityContext: # runAsUser: 1000 # 设定用户 # runAsGroup: 1000 # 设定用户组 runAsNonRoot: true # Pod 必须以非 root 用户运行 seccompProfile: # security compute mode type: RuntimeDefault nodeSelector: nodegroup: common # 使用专用节点组，如果希望使用多个节点组，可改用节点亲和性 volumes: - name: tmp-dir emptyDir: {} containers: - name: my-app-v3 image: my-app:v3 # 建议使用私有镜像仓库，规避 docker.io 的镜像拉取限制 imagePullPolicy: IfNotPresent volumeMounts: - mountPath: /tmp name: tmp-dir lifecycle: preStop: # 在容器被 kill 之前执行 exec: command: - /bin/sh - -c - \"while [ $(netstat -plunt | grep tcp | wc -l | xargs) -ne 0 ]; do sleep 1; done\" resources: # 资源请求与限制 # 对于核心服务，建议设置 requests = limits，避免资源竞争 requests: # HPA 会使用 requests 计算资源利用率 # 建议将 requests 设为服务正常状态下的 CPU 使用率，HPA 的目前指标设为 80% # 所有容器的 requests 总量不建议为 2c/4G 4c/8G 等常见值，因为节点通常也是这个配置，这会导致 Pod 只能调度到更大的节点上，适当调小 requests 等扩充可用的节点类型，从而扩充节点池。 cpu: 1000m memory: 1Gi limits: # limits - requests 为允许超卖的资源量，建议为 requests 的 1 到 2 倍，酌情配置。 cpu: 1000m memory: 1Gi securityContext: # 将容器层设为只读，防止容器文件被篡改 ## 如果需要写入临时文件，建议额外挂载 emptyDir 来提供可读写的数据卷 readOnlyRootFilesystem: true # 禁止 Pod 做任何权限提升 allowPrivilegeEscalation: false capabilities: # drop ALL 的权限比较严格，可按需修改 drop: - ALL startupProbe: # 要求 kubernetes 1.18+ httpGet: path: /actuator/health # 直接使用健康检查接口即可 port: 8080 periodSeconds: 5 timeoutSeconds: 1 failureThreshold: 20 # 最多提供给服务 5s * 20 的启动时间 successThreshold: 1 livenessProbe: httpGet: path: /actuator/health # spring 的通用健康检查路径 port: 8080 periodSeconds: 5 timeoutSeconds: 1 failureThreshold: 5 successThreshold: 1 # Readiness probes are very important for a RollingUpdate to work properly, readinessProbe: httpGet: path: /actuator/health # 简单起见可直接使用 livenessProbe 相同的接口，当然也可额外定义 port: 8080 periodSeconds: 5 timeoutSeconds: 1 failureThreshold: 5 successThreshold: 1 --- apiVersion: autoscaling/v2beta2 kind: HorizontalPodAutoscaler metadata: labels: app: my-app name: my-app-v3 namespace: prod spec: scaleTargetRef: apiVersion: apps/v1 kind: Deployment name: my-app-v3 maxReplicas: 50 minReplicas: 3 metrics: - type: Resource resource: name: cpu target: type: Utilization averageUtilization: 70 --- apiVersion: policy/v1 kind: PodDisruptionBudget metadata: name: my-app-v3 namespace: p","date":"2022-01-25","objectID":"/posts/kubernetes-best-practices/:1:0","series":null,"tags":["Kubernetes","最佳实践","云原生"],"title":"Kubernetes 微服务最佳实践","uri":"/posts/kubernetes-best-practices/#零示例"},{"categories":["tech"],"content":" 一、优雅停止（Gracful Shutdown）与 502/504 报错如果 Pod 正在处理大量请求（比如 1000 QPS+）时，因为节点故障或「竞价节点」被回收等原因被重新调度， 你可能会观察到在容器被 terminate 的一段时间内出现少量 502/504。 为了搞清楚这个问题，需要先理解清楚 terminate 一个 Pod 的流程： Pod 的状态被设为 Terminating，（几乎）同时该 Pod 被从所有关联的 Service Endpoints 中移除 preStop 钩子被执行 它的执行阶段很好理解：在容器被 stop 之前执行 它可以是一个命令，或者一个对 Pod 中容器的 http 调用 如果在收到 SIGTERM 信号时，无法优雅退出，要支持优雅退出比较麻烦的话，用 preStop 实现优雅退出是一个非常好的方式 preStop 的定义位置：https://github.com/kubernetes/api/blob/master/core/v1/types.go#L2515 preStop 执行完毕后，SIGTERM 信号被发送给 Pod 中的所有容器 继续等待，直到容器停止，或者超时 spec.terminationGracePeriodSeconds，这个值默认为 30s 需要注意的是，这个优雅退出的等待计时是与 preStop 同步开始的！而且它也不会等待 preStop 结束！ 如果超过了 spec.terminationGracePeriodSeconds 容器仍然没有停止，k8s 将会发送 SIGKILL 信号给容器 进程全部终止后，整个 Pod 完全被清理掉 注意：1 跟 2 两个工作是异步发生的，所以在未设置 preStop 时，可能会出现「Pod 还在 Service Endpoints 中，但是 SIGTERM 已经被发送给 Pod 导致容器都挂掉」的情况，我们需要考虑到这种状况的发生。 了解了上面的流程后，我们就能分析出两种错误码出现的原因： 502：应用程序在收到 SIGTERM 信号后直接终止了运行，导致部分还没有被处理完的请求直接中断，代理层返回 502 表示这种情况 504：Service Endpoints 移除不够及时，在 Pod 已经被终止后，仍然有个别请求被路由到了该 Pod，得不到响应导致 504 通常的解决方案是，在 Pod 的 preStop 步骤加一个 15s 的等待时间。 其原理是：在 Pod 处理 terminating 状态的时候，就会被从 Service Endpoints 中移除，也就不会再有新的请求过来了。 在 preStop 等待 15s，基本就能保证所有的请求都在容器死掉之前被处理完成（一般来说，绝大部分请求的处理时间都在 300ms 以内吧）。 一个简单的示例如下，它使 Pod 被 Terminate 时，总是在 stop 前先等待 15s，再发送 SIGTERM 信号给容器： containers: - name: my-app # 添加下面这部分 lifecycle: preStop: exec: command: - /bin/sleep - \"15\" 更好的解决办法，是直接等待所有 tcp 连接都关闭（需要镜像中有 netstat）： containers: - name: my-app # 添加下面这部分 lifecycle: preStop: exec: command: - /bin/sh - -c - \"while [ $(netstat -plunt | grep tcp | wc -l | xargs) -ne 0 ]; do sleep 1; done\" ","date":"2022-01-25","objectID":"/posts/kubernetes-best-practices/:2:0","series":null,"tags":["Kubernetes","最佳实践","云原生"],"title":"Kubernetes 微服务最佳实践","uri":"/posts/kubernetes-best-practices/#一优雅停止gracful-shutdown与-502504-报错"},{"categories":["tech"],"content":" 如果我的服务还使用了 Sidecar 代理网络请求，该怎么处理？以服务网格 Istio 为例，在 Envoy 代理了 Pod 流量的情况下，502/504 的问题会变得更复杂一点——还需要考虑 Sidecar 与主容器的关闭顺序： 如果在 Envoy 已关闭后，有新的请求再进来，将会导致 504（没人响应这个请求了） 所以 Envoy 最好在 Terminating 至少 3s 后才能关，确保 Istio 网格配置已完全更新 如果在 Envoy 还没停止时，主容器先关闭，然后又有新的请求再进来，Envoy 将因为无法连接到 upstream 导致 503 所以主容器也最好在 Terminating 至少 3s 后，才能关闭。 如果主容器处理还未处理完遗留请求时，Envoy 或者主容器的其中一个停止了，会因为 tcp 连接直接断开连接导致 502 因此 Envoy 必须在主容器处理完遗留请求后（即没有 tcp 连接时），才能关闭 所以总结下：Envoy 及主容器的 preStop 都至少得设成 3s，并且在「没有 tcp 连接」时，才能关闭，避免出现 502/503/504. 主容器的修改方法在前文中已经写过了，下面介绍下 Envoy 的修改方法。 和主容器一样，Envoy 也能直接加 preStop，修改 istio-sidecar-injector 这个 configmap，在 sidecar 里添加 preStop sleep 命令: containers: - name: istio-proxy # 添加下面这部分 lifecycle: preStop: exec: command: - /bin/sh - -c - \"while [ $(netstat -plunt | grep tcp | grep -v envoy | wc -l | xargs) -ne 0 ]; do sleep 1; done\" ","date":"2022-01-25","objectID":"/posts/kubernetes-best-practices/:2:1","series":null,"tags":["Kubernetes","最佳实践","云原生"],"title":"Kubernetes 微服务最佳实践","uri":"/posts/kubernetes-best-practices/#k8s-istio-pod-prestop"},{"categories":["tech"],"content":" 参考 Kubernetes best practices: terminating with grace Graceful shutdown in Kubernetes is not always trivial ","date":"2022-01-25","objectID":"/posts/kubernetes-best-practices/:2:2","series":null,"tags":["Kubernetes","最佳实践","云原生"],"title":"Kubernetes 微服务最佳实践","uri":"/posts/kubernetes-best-practices/#参考"},{"categories":["tech"],"content":" 二、服务的伸缩配置 - HPAKubernetes 官方主要支持基于 Pod CPU 的伸缩，这是应用最为广泛的伸缩指标，需要部署 metrics-server 才可使用。 先回顾下前面给出的，基于 Pod CPU 使用率进行伸缩的示例： apiVersion: autoscaling/v2beta2 # k8s 1.23+ 此 API 已经 GA kind: HorizontalPodAutoscaler metadata: labels: app: my-app name: my-app-v3 namespace: prod spec: scaleTargetRef: apiVersion: apps/v1 kind: Deployment name: my-app-v3 maxReplicas: 50 minReplicas: 3 metrics: - type: Resource resource: name: cpu target: type: Utilization averageUtilization: 70 ","date":"2022-01-25","objectID":"/posts/kubernetes-best-practices/:3:0","series":null,"tags":["Kubernetes","最佳实践","云原生"],"title":"Kubernetes 微服务最佳实践","uri":"/posts/kubernetes-best-practices/#k8s-hpa"},{"categories":["tech"],"content":" 1. 当前指标值的计算方式提前总结：每个 Pod 的指标是其中所有容器指标之和，如果计算百分比，就再除以 Pod 的 requests. HPA 默认使用 Pod 的当前指标进行计算，以 CPU 使用率为例，其计算公式为： 「Pod 的 CPU 使用率」= 100% * 「所有 Container 的 CPU 用量之和」/「所有 Container 的 CPU requests 之和」 注意分母是总的 requests 量，而不是 limits. 1.1 存在的问题与解决方法在 Pod 只有一个容器时这没啥问题，但是当 Pod 注入了 envoy 等 sidecar 时，这就会有问题了。 因为 Istio 的 Sidecar requests 默认为 100m 也就是 0.1 核。 在未 tuning 的情况下，服务负载一高，sidecar 的实际用量很容易就能涨到 0.2-0.4 核。 把这两个值代入前面的公式，会发现 对于 QPS 较高的服务，添加 Sidecar 后，「Pod 的 CPU 利用率」可能会高于「应用容器的 CPU 利用率」，造成不必要的扩容。 主容器的 requests 与 limits 差距越小，这样的扩容造成的资源浪费就越大。 而且还有个问题是，不同应用的 Pod，数据流特征、应用负载特征等都有区别（请求/响应的数据量、处理时长等），这会造成 sidecar 与主容器的 cpu 利用率不一，加大了优化 HPA 机制的困难度。 解决方法： 方法一：HPA 改用绝对指标进行扩缩容，即 Pod 的总 CPU 用量。这使 HPA 不受任何容器 requests 设置的影响。 但是因为不同服务负载的区别，需要根据实际负载为每个服务调整 HPA 的期望指标。 方法二：HPA 仍然使用 Pod 利用率进行扩缩容，但是针对每个服务的 CPU 使用情况，为每个服务的 sidecar 设置不同的 requests/limits，降低 sidecar 对扩缩容的影响。 方法三：使用 KEDA 等第三方组件，获取到应用容器的 CPU 利用率（排除掉 Sidecar），使用它进行扩缩容 方法四：使用 k8s 1.20 提供的 alpha 特性：Container Resourse Metrics. 这种方式可以将 Pod 的不同容器的指标区分看待，算是最佳的处理方法了，但是该特性仍未进入 beta 阶段，慎用。 ","date":"2022-01-25","objectID":"/posts/kubernetes-best-practices/:3:1","series":null,"tags":["Kubernetes","最佳实践","云原生"],"title":"Kubernetes 微服务最佳实践","uri":"/posts/kubernetes-best-practices/#1-当前指标值的计算方式"},{"categories":["tech"],"content":" 1. 当前指标值的计算方式提前总结：每个 Pod 的指标是其中所有容器指标之和，如果计算百分比，就再除以 Pod 的 requests. HPA 默认使用 Pod 的当前指标进行计算，以 CPU 使用率为例，其计算公式为： 「Pod 的 CPU 使用率」= 100% * 「所有 Container 的 CPU 用量之和」/「所有 Container 的 CPU requests 之和」 注意分母是总的 requests 量，而不是 limits. 1.1 存在的问题与解决方法在 Pod 只有一个容器时这没啥问题，但是当 Pod 注入了 envoy 等 sidecar 时，这就会有问题了。 因为 Istio 的 Sidecar requests 默认为 100m 也就是 0.1 核。 在未 tuning 的情况下，服务负载一高，sidecar 的实际用量很容易就能涨到 0.2-0.4 核。 把这两个值代入前面的公式，会发现 对于 QPS 较高的服务，添加 Sidecar 后，「Pod 的 CPU 利用率」可能会高于「应用容器的 CPU 利用率」，造成不必要的扩容。 主容器的 requests 与 limits 差距越小，这样的扩容造成的资源浪费就越大。 而且还有个问题是，不同应用的 Pod，数据流特征、应用负载特征等都有区别（请求/响应的数据量、处理时长等），这会造成 sidecar 与主容器的 cpu 利用率不一，加大了优化 HPA 机制的困难度。 解决方法： 方法一：HPA 改用绝对指标进行扩缩容，即 Pod 的总 CPU 用量。这使 HPA 不受任何容器 requests 设置的影响。 但是因为不同服务负载的区别，需要根据实际负载为每个服务调整 HPA 的期望指标。 方法二：HPA 仍然使用 Pod 利用率进行扩缩容，但是针对每个服务的 CPU 使用情况，为每个服务的 sidecar 设置不同的 requests/limits，降低 sidecar 对扩缩容的影响。 方法三：使用 KEDA 等第三方组件，获取到应用容器的 CPU 利用率（排除掉 Sidecar），使用它进行扩缩容 方法四：使用 k8s 1.20 提供的 alpha 特性：Container Resourse Metrics. 这种方式可以将 Pod 的不同容器的指标区分看待，算是最佳的处理方法了，但是该特性仍未进入 beta 阶段，慎用。 ","date":"2022-01-25","objectID":"/posts/kubernetes-best-practices/:3:1","series":null,"tags":["Kubernetes","最佳实践","云原生"],"title":"Kubernetes 微服务最佳实践","uri":"/posts/kubernetes-best-practices/#11-存在的问题与解决方法"},{"categories":["tech"],"content":" 2. HPA 的扩缩容算法HPA 什么时候会扩容，这一点是很好理解的。但是 HPA 的缩容策略，会有些迷惑，下面简单分析下。 HPA 的「目标指标」可以使用两种形式：绝对度量指标和资源利用率。 绝对度量指标：比如 CPU，就是指 CPU 的使用量 资源利用率（资源使用量/资源请求 * 100%）：在 Pod 设置了资源请求时，可以使用资源利用率进行 Pod 伸缩 HPA 的「当前指标」是一段时间内所有 Pods 的平均值，不是峰值。 HPA 的扩缩容算法为： 期望副本数 = ceil[当前副本数 * ( 当前指标 / 目标指标 )] 从上面的参数可以看到： 只要「当前指标」超过了目标指标，就一定会发生扩容。 当前指标 / 目标指标要小到一定的程度，才会触发缩容。 比如双副本的情况下，上述比值要小于等于 1/2，才会缩容到单副本。 三副本的情况下，上述比值的临界点是 2/3。 五副本时临界值是 4/5，100副本时临界值是 99/100，依此类推。 如果 当前指标 / 目标指标 从 1 降到 0.5，副本的数量将会减半。（虽然说副本数越多，发生这么大变化的可能性就越小。） 当前副本数 / 目标指标的值越大，「当前指标」的波动对「期望副本数」的影响就越大。 为了防止扩缩容过于敏感，HPA 有几个相关参数： Hardcoded 参数 HPA Loop 延时：默认 15 秒，每 15 秒钟进行一次 HPA 扫描。 缩容冷却时间：默认 5 分钟。 对于 K8s 1.18+，HPA 通过 spec.behavior 提供了多种控制扩缩容行为的参数，后面会具体介绍。 ","date":"2022-01-25","objectID":"/posts/kubernetes-best-practices/:3:2","series":null,"tags":["Kubernetes","最佳实践","云原生"],"title":"Kubernetes 微服务最佳实践","uri":"/posts/kubernetes-best-practices/#2-hpa-的扩缩容算法"},{"categories":["tech"],"content":" 3. HPA 的期望值设成多少合适这个需要针对每个服务的具体情况，具体分析。 以最常用的按 CPU 值伸缩为例， 核心服务 requests/limits 值: 建议设成相等的，保证服务质量等级为 Guaranteed 需要注意 CPU 跟 Memory 的 limits 限制策略是不同的，CPU 是真正地限制了上限，而 Memory 是用超了就干掉容器（OOMKilled） k8s 一直使用 cgroups v1 (cpu_shares/memory.limit_in_bytes)来限制 cpu/memory，但是对于 Guaranteed 的 Pods 而言，内存并不能完全预留，资源竞争总是有可能发生的。1.22 有 alpha 特性改用 cgroups v2，可以关注下。 HPA: 一般来说，期望值设为 60% 到 70% 可能是比较合适的，最小副本数建议设为 2 - 5. （仅供参考） PodDisruptionBudget: 建议按服务的健壮性与 HPA 期望值，来设置 PDB，后面会详细介绍，这里就先略过了 非核心服务 requests/limits 值: 建议 requests 设为 limits 的 0.6 - 0.9 倍（仅供参考），对应的服务质量等级为 Burstable 也就是超卖了资源，这样做主要的考量点是，很多非核心服务负载都很低，根本跑不到 limits 这么高，降低 requests 可以提高集群资源利用率，也不会损害服务稳定性。 HPA: 因为 requests 降低了，而 HPA 是以 requests 为 100% 计算使用率的，我们可以提高 HPA 的期望值（如果使用百分比为期望值的话），比如 80% ~ 90%，最小副本数建议设为 1 - 3. （仅供参考） PodDisruptionBudget: 非核心服务嘛，保证最少副本数为 1 就行了。 ","date":"2022-01-25","objectID":"/posts/kubernetes-best-practices/:3:3","series":null,"tags":["Kubernetes","最佳实践","云原生"],"title":"Kubernetes 微服务最佳实践","uri":"/posts/kubernetes-best-practices/#3-hpa-的期望值设成多少合适"},{"categories":["tech"],"content":" 4. HPA 的常见问题 4.1. Pod 扩容 - 预热陷阱 预热：Java/C# 这类运行在虚拟机上的语言，第一次使用到某些功能时，往往需要初始化一些资源，例如「JIT 即时编译」。 如果代码里还应用了动态类加载之类的功能，就很可能导致微服务某些 API 第一次被调用时，响应特别慢（要动态编译 class）。 因此 Pod 在提供服务前，需要提前「预热（slow_start）」一次这些接口，将需要用到的资源提前初始化好。 在负载很高的情况下，HPA 会自动扩容。 但是如果扩容的 Pod 需要预热，就可能会遇到「预热陷阱」。 在有大量用户访问的时候，不论使用何种负载均衡策略，只要请求被转发到新建的 Pod 上，这个请求就会「卡住」。 如果请求速度太快，Pod 启动的瞬间「卡住」的请求就越多，这将会导致新建 Pod 因为压力过大而垮掉。 然后 Pod 一重启就被压垮，进入 CrashLoopBackoff 循环。 如果是在使用多线程做负载测试时，效果更明显：50 个线程在不间断地请求， 别的 Pod 响应时间是「毫秒级」，而新建的 Pod 的首次响应是「秒级」。几乎是一瞬间，50 个线程就会全部陷在新建的 Pod 这里。 而新建的 Pod 在启动的瞬间可能特别脆弱，瞬间的 50 个并发请求就可以将它压垮。 然后 Pod 一重启就被压垮，进入 CrashLoopBackoff 循环。 解决方法： 可以在「应用层面」解决： 在启动探针 API 的后端控制器里面，依次调用所有需要预热的接口或者其他方式，提前初始化好所有资源。 启动探针的控制器中，可以通过 localhost 回环地址调用它自身的接口。 使用「AOT 预编译」技术：预热，通常都是因为「JIT 即时编译」导致的问题，在需要用到时它才编译。而 AOT 是预先编译，在使用前完成编译，因此 AOT 能解决预热的问题。 也可以在「基础设施层面」解决： 像 AWS ALB TargetGroup 以及其他云服务商的 ALB 服务，通常都可以设置 slow_start 时长，即对新加入的实例，使用一定时间慢慢地把流量切过去，最终达到预期的负载均衡状态。这个可以解决服务预热问题。 Envoy 也已经支持 slow_start 模式，支持在一个设置好的时间窗口内，把流量慢慢负载到新加入的实例上，达成预热效果。 4.2. HPA 扩缩容过于敏感，导致 Pod 数量震荡通常来讲，K8s 上绝大部分负载都应该选择使用 CPU 进行扩缩容。因为 CPU 通常能很好的反映服务的负载情况 但是有些服务会存在其他影响 CPU 使用率的因素，导致使用 CPU 扩缩容变得不那么可靠，比如： 有些 Java 服务堆内存设得很大，GC pause 也设得比较长，因此内存 GC 会造成 CPU 间歇性飙升，CPU 监控会有大量的尖峰。 有些服务有定时任务，定时任务一运行 CPU 就涨，但是这跟服务的 QPS 是无关的 有些服务可能一运行 CPU 就会立即处于一个高位状态，它可能希望使用别的业务侧指标来进行扩容，而不是 CPU. 因为上述问题存在，使用 CPU 扩缩容，就可能会造成服务频繁的扩容然后缩容，或者无限扩容。 而有些服务（如我们的「推荐服务」），对「扩容」和「缩容」都是比较敏感的，每次扩缩都会造成服务可用率抖动。 对这类服务而言，HPA 有这几种调整策略： 选择使用 QPS 等相对比较平滑，没有 GC 这类干扰的指标来进行扩缩容，这需要借助 KEDA 等社区组件。 对 kubernetes 1.18+，可以直接使用 HPA 的 behavior.scaleDown 和 behavior.scaleUp 两个参数，控制每次扩缩容的最多 pod 数量或者比例。 示例如下： --- apiVersion: autoscaling/v2beta2 kind: HorizontalPodAutoscaler metadata: name: podinfo namespace: default spec: scaleTargetRef: apiVersion: apps/v1 kind: Deployment name: podinfo minReplicas: 3 maxReplicas: 50 metrics: - type: Resource resource: name: cpu target: type: Utilization averageUtilization: 50 # 期望的 CPU 平均值 behavior: scaleUp: stabilizationWindowSeconds: 0 # 默认为 0，只使用当前值进行扩缩容 policies: - periodSeconds: 180 # 每 3 分钟最多扩容 5% 的 Pods type: Percent value: 5 - periodSeconds: 60 # 每分钟最多扩容 1 个 Pod，扩的慢一点主要是为了一个个地预热，避免一次扩容太多未预热的 Pods 导致服务可用率剧烈抖动 type: Pods value: 1 selectPolicy: Min # 选择最小的策略 # 以下的一切配置，都是为了更平滑地缩容 scaleDown: stabilizationWindowSeconds: 600 # 使用过去 10 mins 的最大 cpu 值进行缩容计算，避免过快缩容 policies: - type: Percent # 每 3 mins 最多缩容 `ceil[当前副本数 * 5%]` 个 pod（20 个 pod 以内，一次只缩容 1 个 pod） value: 5 periodSeconds: 180 - type: Pods # 每 1 mins 最多缩容 1 个 pod value: 1 periodSeconds: 60 selectPolicy: Min # 上面的 policies 列表，只生效其中最小的值作为缩容限制（保证平滑缩容） 而对于扩容不够平滑这个问题，可以考虑提供类似 AWS ALB TargetGroup slow_start 的功能，在扩容时缓慢将流量切到新 Pod 上，以实现预热服务（JVM 预热以及本地缓存预热），这样就能达到比较好的平滑扩容效果。 ","date":"2022-01-25","objectID":"/posts/kubernetes-best-practices/:3:4","series":null,"tags":["Kubernetes","最佳实践","云原生"],"title":"Kubernetes 微服务最佳实践","uri":"/posts/kubernetes-best-practices/#4-hpa-的常见问题"},{"categories":["tech"],"content":" 4. HPA 的常见问题 4.1. Pod 扩容 - 预热陷阱 预热：Java/C# 这类运行在虚拟机上的语言，第一次使用到某些功能时，往往需要初始化一些资源，例如「JIT 即时编译」。 如果代码里还应用了动态类加载之类的功能，就很可能导致微服务某些 API 第一次被调用时，响应特别慢（要动态编译 class）。 因此 Pod 在提供服务前，需要提前「预热（slow_start）」一次这些接口，将需要用到的资源提前初始化好。 在负载很高的情况下，HPA 会自动扩容。 但是如果扩容的 Pod 需要预热，就可能会遇到「预热陷阱」。 在有大量用户访问的时候，不论使用何种负载均衡策略，只要请求被转发到新建的 Pod 上，这个请求就会「卡住」。 如果请求速度太快，Pod 启动的瞬间「卡住」的请求就越多，这将会导致新建 Pod 因为压力过大而垮掉。 然后 Pod 一重启就被压垮，进入 CrashLoopBackoff 循环。 如果是在使用多线程做负载测试时，效果更明显：50 个线程在不间断地请求， 别的 Pod 响应时间是「毫秒级」，而新建的 Pod 的首次响应是「秒级」。几乎是一瞬间，50 个线程就会全部陷在新建的 Pod 这里。 而新建的 Pod 在启动的瞬间可能特别脆弱，瞬间的 50 个并发请求就可以将它压垮。 然后 Pod 一重启就被压垮，进入 CrashLoopBackoff 循环。 解决方法： 可以在「应用层面」解决： 在启动探针 API 的后端控制器里面，依次调用所有需要预热的接口或者其他方式，提前初始化好所有资源。 启动探针的控制器中，可以通过 localhost 回环地址调用它自身的接口。 使用「AOT 预编译」技术：预热，通常都是因为「JIT 即时编译」导致的问题，在需要用到时它才编译。而 AOT 是预先编译，在使用前完成编译，因此 AOT 能解决预热的问题。 也可以在「基础设施层面」解决： 像 AWS ALB TargetGroup 以及其他云服务商的 ALB 服务，通常都可以设置 slow_start 时长，即对新加入的实例，使用一定时间慢慢地把流量切过去，最终达到预期的负载均衡状态。这个可以解决服务预热问题。 Envoy 也已经支持 slow_start 模式，支持在一个设置好的时间窗口内，把流量慢慢负载到新加入的实例上，达成预热效果。 4.2. HPA 扩缩容过于敏感，导致 Pod 数量震荡通常来讲，K8s 上绝大部分负载都应该选择使用 CPU 进行扩缩容。因为 CPU 通常能很好的反映服务的负载情况 但是有些服务会存在其他影响 CPU 使用率的因素，导致使用 CPU 扩缩容变得不那么可靠，比如： 有些 Java 服务堆内存设得很大，GC pause 也设得比较长，因此内存 GC 会造成 CPU 间歇性飙升，CPU 监控会有大量的尖峰。 有些服务有定时任务，定时任务一运行 CPU 就涨，但是这跟服务的 QPS 是无关的 有些服务可能一运行 CPU 就会立即处于一个高位状态，它可能希望使用别的业务侧指标来进行扩容，而不是 CPU. 因为上述问题存在，使用 CPU 扩缩容，就可能会造成服务频繁的扩容然后缩容，或者无限扩容。 而有些服务（如我们的「推荐服务」），对「扩容」和「缩容」都是比较敏感的，每次扩缩都会造成服务可用率抖动。 对这类服务而言，HPA 有这几种调整策略： 选择使用 QPS 等相对比较平滑，没有 GC 这类干扰的指标来进行扩缩容，这需要借助 KEDA 等社区组件。 对 kubernetes 1.18+，可以直接使用 HPA 的 behavior.scaleDown 和 behavior.scaleUp 两个参数，控制每次扩缩容的最多 pod 数量或者比例。 示例如下： --- apiVersion: autoscaling/v2beta2 kind: HorizontalPodAutoscaler metadata: name: podinfo namespace: default spec: scaleTargetRef: apiVersion: apps/v1 kind: Deployment name: podinfo minReplicas: 3 maxReplicas: 50 metrics: - type: Resource resource: name: cpu target: type: Utilization averageUtilization: 50 # 期望的 CPU 平均值 behavior: scaleUp: stabilizationWindowSeconds: 0 # 默认为 0，只使用当前值进行扩缩容 policies: - periodSeconds: 180 # 每 3 分钟最多扩容 5% 的 Pods type: Percent value: 5 - periodSeconds: 60 # 每分钟最多扩容 1 个 Pod，扩的慢一点主要是为了一个个地预热，避免一次扩容太多未预热的 Pods 导致服务可用率剧烈抖动 type: Pods value: 1 selectPolicy: Min # 选择最小的策略 # 以下的一切配置，都是为了更平滑地缩容 scaleDown: stabilizationWindowSeconds: 600 # 使用过去 10 mins 的最大 cpu 值进行缩容计算，避免过快缩容 policies: - type: Percent # 每 3 mins 最多缩容 `ceil[当前副本数 * 5%]` 个 pod（20 个 pod 以内，一次只缩容 1 个 pod） value: 5 periodSeconds: 180 - type: Pods # 每 1 mins 最多缩容 1 个 pod value: 1 periodSeconds: 60 selectPolicy: Min # 上面的 policies 列表，只生效其中最小的值作为缩容限制（保证平滑缩容） 而对于扩容不够平滑这个问题，可以考虑提供类似 AWS ALB TargetGroup slow_start 的功能，在扩容时缓慢将流量切到新 Pod 上，以实现预热服务（JVM 预热以及本地缓存预热），这样就能达到比较好的平滑扩容效果。 ","date":"2022-01-25","objectID":"/posts/kubernetes-best-practices/:3:4","series":null,"tags":["Kubernetes","最佳实践","云原生"],"title":"Kubernetes 微服务最佳实践","uri":"/posts/kubernetes-best-practices/#41-pod-扩容---预热陷阱"},{"categories":["tech"],"content":" 4. HPA 的常见问题 4.1. Pod 扩容 - 预热陷阱 预热：Java/C# 这类运行在虚拟机上的语言，第一次使用到某些功能时，往往需要初始化一些资源，例如「JIT 即时编译」。 如果代码里还应用了动态类加载之类的功能，就很可能导致微服务某些 API 第一次被调用时，响应特别慢（要动态编译 class）。 因此 Pod 在提供服务前，需要提前「预热（slow_start）」一次这些接口，将需要用到的资源提前初始化好。 在负载很高的情况下，HPA 会自动扩容。 但是如果扩容的 Pod 需要预热，就可能会遇到「预热陷阱」。 在有大量用户访问的时候，不论使用何种负载均衡策略，只要请求被转发到新建的 Pod 上，这个请求就会「卡住」。 如果请求速度太快，Pod 启动的瞬间「卡住」的请求就越多，这将会导致新建 Pod 因为压力过大而垮掉。 然后 Pod 一重启就被压垮，进入 CrashLoopBackoff 循环。 如果是在使用多线程做负载测试时，效果更明显：50 个线程在不间断地请求， 别的 Pod 响应时间是「毫秒级」，而新建的 Pod 的首次响应是「秒级」。几乎是一瞬间，50 个线程就会全部陷在新建的 Pod 这里。 而新建的 Pod 在启动的瞬间可能特别脆弱，瞬间的 50 个并发请求就可以将它压垮。 然后 Pod 一重启就被压垮，进入 CrashLoopBackoff 循环。 解决方法： 可以在「应用层面」解决： 在启动探针 API 的后端控制器里面，依次调用所有需要预热的接口或者其他方式，提前初始化好所有资源。 启动探针的控制器中，可以通过 localhost 回环地址调用它自身的接口。 使用「AOT 预编译」技术：预热，通常都是因为「JIT 即时编译」导致的问题，在需要用到时它才编译。而 AOT 是预先编译，在使用前完成编译，因此 AOT 能解决预热的问题。 也可以在「基础设施层面」解决： 像 AWS ALB TargetGroup 以及其他云服务商的 ALB 服务，通常都可以设置 slow_start 时长，即对新加入的实例，使用一定时间慢慢地把流量切过去，最终达到预期的负载均衡状态。这个可以解决服务预热问题。 Envoy 也已经支持 slow_start 模式，支持在一个设置好的时间窗口内，把流量慢慢负载到新加入的实例上，达成预热效果。 4.2. HPA 扩缩容过于敏感，导致 Pod 数量震荡通常来讲，K8s 上绝大部分负载都应该选择使用 CPU 进行扩缩容。因为 CPU 通常能很好的反映服务的负载情况 但是有些服务会存在其他影响 CPU 使用率的因素，导致使用 CPU 扩缩容变得不那么可靠，比如： 有些 Java 服务堆内存设得很大，GC pause 也设得比较长，因此内存 GC 会造成 CPU 间歇性飙升，CPU 监控会有大量的尖峰。 有些服务有定时任务，定时任务一运行 CPU 就涨，但是这跟服务的 QPS 是无关的 有些服务可能一运行 CPU 就会立即处于一个高位状态，它可能希望使用别的业务侧指标来进行扩容，而不是 CPU. 因为上述问题存在，使用 CPU 扩缩容，就可能会造成服务频繁的扩容然后缩容，或者无限扩容。 而有些服务（如我们的「推荐服务」），对「扩容」和「缩容」都是比较敏感的，每次扩缩都会造成服务可用率抖动。 对这类服务而言，HPA 有这几种调整策略： 选择使用 QPS 等相对比较平滑，没有 GC 这类干扰的指标来进行扩缩容，这需要借助 KEDA 等社区组件。 对 kubernetes 1.18+，可以直接使用 HPA 的 behavior.scaleDown 和 behavior.scaleUp 两个参数，控制每次扩缩容的最多 pod 数量或者比例。 示例如下： --- apiVersion: autoscaling/v2beta2 kind: HorizontalPodAutoscaler metadata: name: podinfo namespace: default spec: scaleTargetRef: apiVersion: apps/v1 kind: Deployment name: podinfo minReplicas: 3 maxReplicas: 50 metrics: - type: Resource resource: name: cpu target: type: Utilization averageUtilization: 50 # 期望的 CPU 平均值 behavior: scaleUp: stabilizationWindowSeconds: 0 # 默认为 0，只使用当前值进行扩缩容 policies: - periodSeconds: 180 # 每 3 分钟最多扩容 5% 的 Pods type: Percent value: 5 - periodSeconds: 60 # 每分钟最多扩容 1 个 Pod，扩的慢一点主要是为了一个个地预热，避免一次扩容太多未预热的 Pods 导致服务可用率剧烈抖动 type: Pods value: 1 selectPolicy: Min # 选择最小的策略 # 以下的一切配置，都是为了更平滑地缩容 scaleDown: stabilizationWindowSeconds: 600 # 使用过去 10 mins 的最大 cpu 值进行缩容计算，避免过快缩容 policies: - type: Percent # 每 3 mins 最多缩容 `ceil[当前副本数 * 5%]` 个 pod（20 个 pod 以内，一次只缩容 1 个 pod） value: 5 periodSeconds: 180 - type: Pods # 每 1 mins 最多缩容 1 个 pod value: 1 periodSeconds: 60 selectPolicy: Min # 上面的 policies 列表，只生效其中最小的值作为缩容限制（保证平滑缩容） 而对于扩容不够平滑这个问题，可以考虑提供类似 AWS ALB TargetGroup slow_start 的功能，在扩容时缓慢将流量切到新 Pod 上，以实现预热服务（JVM 预热以及本地缓存预热），这样就能达到比较好的平滑扩容效果。 ","date":"2022-01-25","objectID":"/posts/kubernetes-best-practices/:3:4","series":null,"tags":["Kubernetes","最佳实践","云原生"],"title":"Kubernetes 微服务最佳实践","uri":"/posts/kubernetes-best-practices/#42-hpa-扩缩容过于敏感导致-pod-数量震荡"},{"categories":["tech"],"content":" 5. HPA 注意事项注意 kubectl 1.23 以下的版本，默认使用 hpa.v1.autoscaling 来查询 HPA 配置，v2beta2 相关的参数会被编码到 metadata.annotations 中。 比如 behavior 就会被编码到 autoscaling.alpha.kubernetes.io/behavior 这个 key 所对应的值中。 因此如果使用了 v2beta2 的 HPA，一定要明确指定使用 v2beta2 版本的 HPA： kubectl get hpa.v2beta2.autoscaling 否则不小心动到 annotations 中编码的某些参数，可能会产生意料之外的效果，甚至直接把控制面搞崩… 比如这个 issue: Nil pointer dereference in KCM after v1 HPA patch request ","date":"2022-01-25","objectID":"/posts/kubernetes-best-practices/:3:5","series":null,"tags":["Kubernetes","最佳实践","云原生"],"title":"Kubernetes 微服务最佳实践","uri":"/posts/kubernetes-best-practices/#5-hpa-注意事项"},{"categories":["tech"],"content":" 6. 参考 Pod 水平自动伸缩 - Kubernetes Docs Horizontal Pod Autoscaler演练 - Kubernetes Docs ","date":"2022-01-25","objectID":"/posts/kubernetes-best-practices/:3:6","series":null,"tags":["Kubernetes","最佳实践","云原生"],"title":"Kubernetes 微服务最佳实践","uri":"/posts/kubernetes-best-practices/#6-参考"},{"categories":["tech"],"content":" 三、节点维护与Pod干扰预算在我们通过 kubectl drain 将某个节点上的容器驱逐走的时候， kubernetes 会依据 Pod 的「PodDistruptionBuget」来进行 Pod 的驱逐。 如果不设置任何明确的 PodDistruptionBuget，Pod 将会被直接杀死，然后在别的节点重新调度，这可能导致服务中断！ PDB 是一个单独的 CR 自定义资源，示例如下： apiVersion: policy/v1beta1 kind: PodDisruptionBudget metadata: name: podinfo-pdb spec: # 如果不满足 PDB，Pod 驱逐将会失败！ minAvailable: 1 # 最少也要维持一个 Pod 可用 # maxUnavailable: 1 # 最大不可用的 Pod 数，与 minAvailable 不能同时配置！二选一 selector: matchLabels: app: podinfo 如果在进行节点维护时(kubectl drain)，Pod 不满足 PDB，drain 将会失败，示例： \u003e kubectl drain node-205 --ignore-daemonsets --delete-local-data node/node-205 cordoned WARNING: ignoring DaemonSet-managed Pods: kube-system/calico-node-nfhj7, kube-system/kube-proxy-94dz5 evicting pod default/podinfo-7c84d8c94d-h9brq evicting pod default/podinfo-7c84d8c94d-gw6qf error when evicting pod \"podinfo-7c84d8c94d-h9brq\" (will retry after 5s): Cannot evict pod as it would violate the pod's disruption budget. evicting pod default/podinfo-7c84d8c94d-h9brq error when evicting pod \"podinfo-7c84d8c94d-h9brq\" (will retry after 5s): Cannot evict pod as it would violate the pod's disruption budget. evicting pod default/podinfo-7c84d8c94d-h9brq error when evicting pod \"podinfo-7c84d8c94d-h9brq\" (will retry after 5s): Cannot evict pod as it would violate the pod's disruption budget. evicting pod default/podinfo-7c84d8c94d-h9brq pod/podinfo-7c84d8c94d-gw6qf evicted pod/podinfo-7c84d8c94d-h9brq evicted node/node-205 evicted 上面的示例中，podinfo 一共有两个副本，都运行在 node-205 上面。我给它设置了干扰预算 PDB minAvailable: 1。 然后使用 kubectl drain 驱逐 Pod 时，其中一个 Pod 被立即驱逐走了，而另一个 Pod 大概在 15 秒内一直驱逐失败。 因为第一个 Pod 还没有在新的节点上启动完成，它不满足干扰预算 PDB minAvailable: 1 这个条件。 大约 15 秒后，最先被驱逐走的 Pod 在新节点上启动完成了，另一个 Pod 满足了 PDB 所以终于也被驱逐了。这才完成了一个节点的 drain 操作。 ClusterAutoscaler 等集群节点伸缩组件，在缩容节点时也会考虑 PodDisruptionBudget. 如果你的集群使用了 ClusterAutoscaler 等动态扩缩容节点的组件，强烈建议设置为所有服务设置 PodDisruptionBudget. 在 PDB 中使用百分比的注意事项在使用百分比时，计算出的实例数都会被向上取整，这会造成两个现象： 如果使用 minAvailable，实例数较少的情况下，可能会导致 ALLOWED DISRUPTIONS 为 0，所有实例都无法被驱逐了。 如果使用 maxUnavailable，因为是向上取整，ALLOWED DISRUPTIONS 的值一定不会低于 1，至少有 1 个实例可以被驱逐。 因此从「便于驱逐」的角度看，如果你的服务至少有 2-3 个实例，建议在 PDB 中使用百分比配置 maxUnavailable，而不是 minAvailable. 相对的从「确保服务稳定性」的角度看，我们则应该使用 minAvailable，确保至少有 1 个实例可用。 ","date":"2022-01-25","objectID":"/posts/kubernetes-best-practices/:4:0","series":null,"tags":["Kubernetes","最佳实践","云原生"],"title":"Kubernetes 微服务最佳实践","uri":"/posts/kubernetes-best-practices/#k8s-PodDistruptionBuget"},{"categories":["tech"],"content":" 三、节点维护与Pod干扰预算在我们通过 kubectl drain 将某个节点上的容器驱逐走的时候， kubernetes 会依据 Pod 的「PodDistruptionBuget」来进行 Pod 的驱逐。 如果不设置任何明确的 PodDistruptionBuget，Pod 将会被直接杀死，然后在别的节点重新调度，这可能导致服务中断！ PDB 是一个单独的 CR 自定义资源，示例如下： apiVersion: policy/v1beta1 kind: PodDisruptionBudget metadata: name: podinfo-pdb spec: # 如果不满足 PDB，Pod 驱逐将会失败！ minAvailable: 1 # 最少也要维持一个 Pod 可用 # maxUnavailable: 1 # 最大不可用的 Pod 数，与 minAvailable 不能同时配置！二选一 selector: matchLabels: app: podinfo 如果在进行节点维护时(kubectl drain)，Pod 不满足 PDB，drain 将会失败，示例： \u003e kubectl drain node-205 --ignore-daemonsets --delete-local-data node/node-205 cordoned WARNING: ignoring DaemonSet-managed Pods: kube-system/calico-node-nfhj7, kube-system/kube-proxy-94dz5 evicting pod default/podinfo-7c84d8c94d-h9brq evicting pod default/podinfo-7c84d8c94d-gw6qf error when evicting pod \"podinfo-7c84d8c94d-h9brq\" (will retry after 5s): Cannot evict pod as it would violate the pod's disruption budget. evicting pod default/podinfo-7c84d8c94d-h9brq error when evicting pod \"podinfo-7c84d8c94d-h9brq\" (will retry after 5s): Cannot evict pod as it would violate the pod's disruption budget. evicting pod default/podinfo-7c84d8c94d-h9brq error when evicting pod \"podinfo-7c84d8c94d-h9brq\" (will retry after 5s): Cannot evict pod as it would violate the pod's disruption budget. evicting pod default/podinfo-7c84d8c94d-h9brq pod/podinfo-7c84d8c94d-gw6qf evicted pod/podinfo-7c84d8c94d-h9brq evicted node/node-205 evicted 上面的示例中，podinfo 一共有两个副本，都运行在 node-205 上面。我给它设置了干扰预算 PDB minAvailable: 1。 然后使用 kubectl drain 驱逐 Pod 时，其中一个 Pod 被立即驱逐走了，而另一个 Pod 大概在 15 秒内一直驱逐失败。 因为第一个 Pod 还没有在新的节点上启动完成，它不满足干扰预算 PDB minAvailable: 1 这个条件。 大约 15 秒后，最先被驱逐走的 Pod 在新节点上启动完成了，另一个 Pod 满足了 PDB 所以终于也被驱逐了。这才完成了一个节点的 drain 操作。 ClusterAutoscaler 等集群节点伸缩组件，在缩容节点时也会考虑 PodDisruptionBudget. 如果你的集群使用了 ClusterAutoscaler 等动态扩缩容节点的组件，强烈建议设置为所有服务设置 PodDisruptionBudget. 在 PDB 中使用百分比的注意事项在使用百分比时，计算出的实例数都会被向上取整，这会造成两个现象： 如果使用 minAvailable，实例数较少的情况下，可能会导致 ALLOWED DISRUPTIONS 为 0，所有实例都无法被驱逐了。 如果使用 maxUnavailable，因为是向上取整，ALLOWED DISRUPTIONS 的值一定不会低于 1，至少有 1 个实例可以被驱逐。 因此从「便于驱逐」的角度看，如果你的服务至少有 2-3 个实例，建议在 PDB 中使用百分比配置 maxUnavailable，而不是 minAvailable. 相对的从「确保服务稳定性」的角度看，我们则应该使用 minAvailable，确保至少有 1 个实例可用。 ","date":"2022-01-25","objectID":"/posts/kubernetes-best-practices/:4:0","series":null,"tags":["Kubernetes","最佳实践","云原生"],"title":"Kubernetes 微服务最佳实践","uri":"/posts/kubernetes-best-practices/#在-pdb-中使用百分比的注意事项"},{"categories":["tech"],"content":" 最佳实践 Deployment + HPA + PodDisruptionBudget一般而言，一个服务的每个版本，都应该包含如下三个资源： Deployment: 管理服务自身的 Pods 嘛 HPA: 负责 Pods 的扩缩容，通常使用 CPU 指标进行扩缩容 PodDisruptionBudget(PDB): 建议按照 HPA 的目标值，来设置 PDB. 比如 HPA CPU 目标值为 60%，就可以考虑设置 PDB minAvailable=65%，保证至少有 65% 的 Pod 可用。这样理论上极限情况下 QPS 均摊到剩下 65% 的 Pods 上也不会造成雪崩（这里假设 QPS 和 CPU 是完全的线性关系） ","date":"2022-01-25","objectID":"/posts/kubernetes-best-practices/:4:1","series":null,"tags":["Kubernetes","最佳实践","云原生"],"title":"Kubernetes 微服务最佳实践","uri":"/posts/kubernetes-best-practices/#最佳实践-deployment--hpa--poddisruptionbudget"},{"categories":["tech"],"content":" 四、节点亲和性与节点组我们一个集群，通常会使用不同的标签为节点组进行分类，比如 kubernetes 自动生成的一些节点标签： kubernetes.io/os: 通常都用 linux kubernetes.io/arch: amd64, arm64 topology.kubernetes.io/region 和 topology.kubernetes.io/zone: 云服务的区域及可用区 我们使用得比较多的，是「节点亲和性」以及「Pod 反亲和性」，另外两个策略视情况使用。 ","date":"2022-01-25","objectID":"/posts/kubernetes-best-practices/:5:0","series":null,"tags":["Kubernetes","最佳实践","云原生"],"title":"Kubernetes 微服务最佳实践","uri":"/posts/kubernetes-best-practices/#k8s-affinity"},{"categories":["tech"],"content":" 1. 节点亲和性如果你使用的是 aws，那 aws 有一些自定义的节点标签： eks.amazonaws.com/nodegroup: aws eks 节点组的名称，同一个节点组使用同样的 aws ec2 实例模板 比如 arm64 节点组、amd64/x64 节点组 内存比例高的节点组如 m 系实例，计算性能高的节点组如 c 系列 竞价实例节点组：这个省钱啊，但是动态性很高，随时可能被回收 按量付费节点组：这类实例贵，但是稳定。 假设你希望优先选择竞价实例跑你的 Pod，如果竞价实例暂时跑满了，就选择按量付费实例。 那 nodeSelector 就满足不了你的需求了，你需要使用 nodeAffinity，示例如下: apiVersion: apps/v1 kind: Deployment metadata: name: xxx namespace: xxx spec: # ... template: # ... spec: affinity: nodeAffinity: # 优先选择 spot-group-c 的节点 preferredDuringSchedulingIgnoredDuringExecution: - preference: matchExpressions: - key: eks.amazonaws.com/nodegroup operator: In values: - spot-group-c weight: 80 # weight 用于为节点评分，会优先选择评分最高的节点 - preference: matchExpressions: # 优先选择 aws c6i 的机器 - key: node.kubernetes.io/instance-type operator: In values: - \"c6i.xlarge\" - \"c6i.2xlarge\" - \"c6i.4xlarge\" - \"c6i.8xlarge\" weight: 70 - preference: matchExpressions: # 其次选择 aws c5 的机器 - key: node.kubernetes.io/instance-type operator: In values: - \"c5.xlarge\" - \"c5.2xlarge\" - \"c5.4xlarge\" - \"c5.9xlarge\" weight: 60 # 如果没 spot-group-c 可用，也可选择 ondemand-group-c 的节点跑 requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: - key: eks.amazonaws.com/nodegroup operator: In values: - spot-group-c - ondemand-group-c containers: # ... ","date":"2022-01-25","objectID":"/posts/kubernetes-best-practices/:5:1","series":null,"tags":["Kubernetes","最佳实践","云原生"],"title":"Kubernetes 微服务最佳实践","uri":"/posts/kubernetes-best-practices/#1-节点亲和性"},{"categories":["tech"],"content":" 2. Pod 反亲和性 Pod 亲和性与反亲和性可能不是最佳的实现手段，这部分内容待更新 相关 Issue: https://github.com/kubernetes/kubernetes/issues/72479 相关替代方案：https://kubernetes.io/docs/concepts/workloads/pods/pod-topology-spread-constraints/ 通常建议为每个 Deployment 的 template 配置 Pod 反亲和性，把 Pods 打散在所有节点上： apiVersion: apps/v1 kind: Deployment metadata: name: xxx namespace: xxx spec: # ... template: # ... spec: replicas: 3 affinity: podAntiAffinity: preferredDuringSchedulingIgnoredDuringExecution: # 非强制性条件 - weight: 100 # weight 用于为节点评分，会优先选择评分最高的节点 podAffinityTerm: labelSelector: matchExpressions: - key: app operator: In values: - xxx - key: version operator: In values: - v12 # 将 pod 尽量打散在多个可用区 topologyKey: topology.kubernetes.io/zone requiredDuringSchedulingIgnoredDuringExecution: # 强制性要求 # 注意这个没有 weights，必须满足列表中的所有条件 - labelSelector: matchExpressions: - key: app operator: In values: - xxx - key: version operator: In values: - v12 # Pod 必须运行在不同的节点上 topologyKey: kubernetes.io/hostname ","date":"2022-01-25","objectID":"/posts/kubernetes-best-practices/:5:2","series":null,"tags":["Kubernetes","最佳实践","云原生"],"title":"Kubernetes 微服务最佳实践","uri":"/posts/kubernetes-best-practices/#2-pod-反亲和性"},{"categories":["tech"],"content":" 五、Pod 的就绪探针、存活探针与启动探针Pod 提供如下三种探针，均支持使用 Command、HTTP API、TCP Socket 这三种手段来进行服务可用性探测。 startupProbe 启动探针（Kubernetes v1.18 [beta]）: 此探针通过后，「就绪探针」与「存活探针」才会进行存活性与就绪检查 用于对慢启动容器进行存活性检测，避免它们在启动运行之前就被杀掉 startupProbe 显然比 livenessProbe 的 initialDelaySeconds 参数更灵活。 同时它也能延迟 readinessProbe 的生效时间，这主要是为了避免无意义的探测。容器都还没 startUp，显然是不可能就绪的。 程序将最多有 failureThreshold * periodSeconds 的时间用于启动，比如设置 failureThreshold=20、periodSeconds=5，程序启动时间最长就为 100s，如果超过 100s 仍然未通过「启动探测」，容器会被杀死。 readinessProbe 就绪探针: 就绪探针失败次数超过 failureThreshold 限制（默认三次），服务将被暂时从 Service 的 Endpoints 中踢出，直到服务再次满足 successThreshold. livenessProbe 存活探针: 检测服务是否存活，它可以捕捉到死锁等情况，及时杀死这种容器。 存活探针失败可能的原因： 服务发生死锁，对所有请求均无响应 服务线程全部卡在对外部 redis/mysql 等外部依赖的等待中，导致请求无响应 存活探针失败次数超过 failureThreshold 限制（默认三次），容器将被杀死，随后根据重启策略执行重启。 kubectl describe pod 会显示重启原因为 State.Last State.Reason = Error, Exit Code=137，同时 Events 中会有 Liveness probe failed: ... 这样的描述。 上述三类探测器的参数都是通用的，五个时间相关的参数列举如下： # 下面的值就是 k8s 的默认值 initialDelaySeconds: 0 # 默认没有 delay 时间 periodSeconds: 10 timeoutSeconds: 1 failureThreshold: 3 successThreshold: 1 示例： apiVersion: apps/v1 kind: Deployment metadata: name: my-app-v3 spec: # ... template: # ... spec: containers: - name: my-app-v3 image: xxx.com/app/my-app:v3 imagePullPolicy: IfNotPresent # ... 省略若干配置 startupProbe: httpGet: path: /actuator/health # 直接使用健康检查接口即可 port: 8080 periodSeconds: 5 timeoutSeconds: 1 failureThreshold: 20 # 最多提供给服务 5s * 20 的启动时间 successThreshold: 1 livenessProbe: httpGet: path: /actuator/health # spring 的通用健康检查路径 port: 8080 periodSeconds: 5 timeoutSeconds: 1 failureThreshold: 5 successThreshold: 1 # Readiness probes are very important for a RollingUpdate to work properly, readinessProbe: httpGet: path: /actuator/health # 简单起见可直接使用 livenessProbe 相同的接口，当然也可额外定义 port: 8080 periodSeconds: 5 timeoutSeconds: 1 failureThreshold: 5 successThreshold: 1 在 Kubernetes 1.18 之前，通用的手段是为「就绪探针」添加较长的 initialDelaySeconds 来实现类似「启动探针」的功能动，避免容器因为启动太慢，存活探针失败导致容器被重启。示例如下： apiVersion: apps/v1 kind: Deployment metadata: name: my-app-v3 spec: # ... template: # ... spec: containers: - name: my-app-v3 image: xxx.com/app/my-app:v3 imagePullPolicy: IfNotPresent # ... 省略若干配置 livenessProbe: httpGet: path: /actuator/health # spring 的通用健康检查路径 port: 8080 initialDelaySeconds: 120 # 前两分钟，都假设服务健康，避免 livenessProbe 失败导致服务重启 periodSeconds: 5 timeoutSeconds: 1 failureThreshold: 5 successThreshold: 1 # 容器一启动，Readiness probes 就会不断进行检测 readinessProbe: httpGet: path: /actuator/health port: 8080 initialDelaySeconds: 3 # readiness probe 不需要设太长时间，使 Pod 尽快加入到 Endpoints. periodSeconds: 5 timeoutSeconds: 1 failureThreshold: 5 successThreshold: 1 ","date":"2022-01-25","objectID":"/posts/kubernetes-best-practices/:6:0","series":null,"tags":["Kubernetes","最佳实践","云原生"],"title":"Kubernetes 微服务最佳实践","uri":"/posts/kubernetes-best-practices/#k8s-container-probe"},{"categories":["tech"],"content":" 六、Pod 安全这里只介绍 Pod 中安全相关的参数，其他诸如集群全局的安全策略，不在这里讨论。 ","date":"2022-01-25","objectID":"/posts/kubernetes-best-practices/:7:0","series":null,"tags":["Kubernetes","最佳实践","云原生"],"title":"Kubernetes 微服务最佳实践","uri":"/posts/kubernetes-best-practices/#k8s-pod-security"},{"categories":["tech"],"content":" 1. Pod SecurityContext通过设置 Pod 的 SecurityContext，可以为每个 Pod 设置特定的安全策略。 SecurityContext 有两种类型： spec.securityContext: 这是一个 PodSecurityContext 对象 顾名思义，它对 Pod 中的所有 contaienrs 都有效。 spec.containers[*].securityContext: 这是一个 SecurityContext 对象 container 私有的 SecurityContext 这两个 SecurityContext 的参数只有部分重叠，重叠的部分 spec.containers[*].securityContext 优先级更高。 我们比较常遇到的一些提升权限的安全策略： 特权容器：spec.containers[*].securityContext.privileged 添加（Capabilities）可选的系统级能力: spec.containers[*].securityContext.capabilities.add 只有 ntp 同步服务等少数容器，可以开启这项功能。请注意这非常危险。 Sysctls: 系统参数: spec.securityContext.sysctls 权限限制相关的安全策略有（强烈建议在所有 Pod 上按需配置如下安全策略！）： spec.volumes: 所有的数据卷都可以设定读写权限 spec.securityContext.runAsNonRoot: true Pod 必须以非 root 用户运行 spec.containers[*].securityContext.readOnlyRootFileSystem:true 将容器层设为只读，防止容器文件被篡改。 如果微服务需要读写文件，建议额外挂载 emptydir 类型的数据卷。 spec.containers[*].securityContext.allowPrivilegeEscalation: false 不允许 Pod 做任何权限提升！ spec.containers[*].securityContext.capabilities.drop: 移除（Capabilities）可选的系统级能力 还有其他诸如指定容器的运行用户(user)/用户组(group)等功能未列出，请自行查阅 Kubernetes 相关文档。 一个无状态的微服务 Pod 配置举例： apiVersion: v1 kind: Pod metadata: name: \u003cPod name\u003e spec: containers: - name: \u003ccontainer name\u003e image: \u003cimage\u003e imagePullPolicy: IfNotPresent # ......此处省略 500 字 securityContext: readOnlyRootFilesystem: true # 将容器层设为只读，防止容器文件被篡改。 allowPrivilegeEscalation: false # 禁止 Pod 做任何权限提升 capabilities: drop: # 禁止容器使用 raw 套接字，通常只有 hacker 才会用到 raw 套接字。 # raw_socket 可自定义网络层数据，避开 tcp/udp 协议栈，直接操作底层的 ip/icmp 数据包。可实现 ip 伪装、自定义协议等功能。 # 去掉 net_raw 会导致 tcpdump 无法使用，无法进行容器内抓包。需要抓包时可临时去除这项配置 - NET_RAW # 更好的选择：直接禁用所有 capabilities # - ALL securityContext: # runAsUser: 1000 # 设定用户 # runAsGroup: 1000 # 设定用户组 runAsNonRoot: true # Pod 必须以非 root 用户运行 seccompProfile: # security compute mode type: RuntimeDefault ","date":"2022-01-25","objectID":"/posts/kubernetes-best-practices/:7:1","series":null,"tags":["Kubernetes","最佳实践","云原生"],"title":"Kubernetes 微服务最佳实践","uri":"/posts/kubernetes-best-practices/#1-pod-securitycontexthttpskubernetesiodocstasksconfigure-pod-containersecurity-context"},{"categories":["tech"],"content":" 2. seccomp: security compute modeseccomp 和 seccomp-bpf 允许对系统调用进行过滤，可以防止用户的二进制文对主机操作系统件执行通常情况下并不需要的危险操作。它和 Falco 有些类似，不过 Seccomp 没有为容器提供特别的支持。 视频: Seccomp: What Can It Do For You? - Justin Cormack, Docker ","date":"2022-01-25","objectID":"/posts/kubernetes-best-practices/:7:2","series":null,"tags":["Kubernetes","最佳实践","云原生"],"title":"Kubernetes 微服务最佳实践","uri":"/posts/kubernetes-best-practices/#2-seccomp-security-compute-mode"},{"categories":["tech"],"content":" 六、隔离性 推荐按业务线或者业务团队进行名字空间划分，方便对每个业务线/业务团队分别进行资源限制 推荐使用 network policy 对服务实施强力的网络管控，避免长期发展过程中，业务服务之间出现混乱的跨业务线相互调用关系，也避免服务被黑后，往未知地址发送数据。 ","date":"2022-01-25","objectID":"/posts/kubernetes-best-practices/:8:0","series":null,"tags":["Kubernetes","最佳实践","云原生"],"title":"Kubernetes 微服务最佳实践","uri":"/posts/kubernetes-best-practices/#六隔离性"},{"categories":["tech"],"content":" 其他问题 不同节点类型的性能有差距，导致 QPS 均衡的情况下，CPU 负载不均衡 解决办法（未验证）： 尽量使用性能相同的实例类型：通过 podAffinity 及 nodeAffinity 添加节点类型的亲和性 ","date":"2022-01-25","objectID":"/posts/kubernetes-best-practices/:9:0","series":null,"tags":["Kubernetes","最佳实践","云原生"],"title":"Kubernetes 微服务最佳实践","uri":"/posts/kubernetes-best-practices/#其他问题"},{"categories":["tech"],"content":" 参考 istio 实践指南 - imroc.cc Kubernetes 实践指南 - imroc.cc ","date":"2022-01-25","objectID":"/posts/kubernetes-best-practices/:10:0","series":null,"tags":["Kubernetes","最佳实践","云原生"],"title":"Kubernetes 微服务最佳实践","uri":"/posts/kubernetes-best-practices/#参考-1"},{"categories":["life","tech"],"content":" 更新：2022/1/22 ","date":"2022-01-03","objectID":"/posts/2021-summary/:0:0","series":null,"tags":["总结"],"title":"2021 年年终总结","uri":"/posts/2021-summary/#"},{"categories":["life","tech"],"content":" 闲言碎语一晃一年又是过去了，这个新年，全球疫情再创新高，圣诞节后美国单日新增更是直接突破 50 万直逼 60 万大关❌ 100 万✅，国内也有西安管理不力导致民众忍饥挨饿。 新冠已经两年多了啊。 言归正传，我今年年初从 W 公司离职后，非常幸运地进了现在的公司——大宇无限，在融入大宇的过程中也是五味杂陈。 不过总体结果还是挺满意的，目前工作已经步入正轨，也发现了非常多的机会，大宇的基础设施领域仍然大有可为。 一些重要事情还是没怎么想通，不过毕竟风口上的猪都能飞，今年小小努力了一把，大部分时间仍然随波逐流，却也渐入佳境。 ","date":"2022-01-03","objectID":"/posts/2021-summary/:1:0","series":null,"tags":["总结"],"title":"2021 年年终总结","uri":"/posts/2021-summary/#闲言碎语"},{"categories":["life","tech"],"content":" 生活 1 月的时候从博客园迁移到这个独立博客，还认识了 @芝士，芝士帮我调整好了博客「友链」页面的样式，超级感谢~ 2 月的时候从 W 公司离职，然后怎么说呢，瞬间感觉海阔天空，心态 180 度转变，好得不得了，但是其实也很担心自己各方面的不足。总之心里有好多的想法，跟 @是格子啊、@芝士 以及前同事聊了好多，非常感谢这几位朋友跟同事帮我梳理思路，给我肯定。也是这个时间点，我被 @芝士 拉进了中文 twitter 的圈子。 过年响应号召没回家（其实是嫌核酸检测麻烦，家里也建议先别回），每天爬爬山看看风景，买了个吊床去公园午睡，练习口琴竹笛，就这样玩了一个月。 到了 3 月份的时候我开始找工作，面了几家公司后，非常幸运地进入了大宇无限，成为了一名 SRE 萌新。在大宇一年的感受，就放在后面的「工作」部分写了，这里先略过。 加入大宇后，全年都有定期的团建，跟 SRE 小伙伴公款吃喝，我 2021 年下馆子次数估计是上一年的七八倍 3 月底，看了电影——《寻龙传说》（2021 年看的唯一一部电影），片尾曲超好听。 4 月份，各种巧合下，意外发现初中同学住得离我 1km 不到，在他家吃了顿家乡菜，还有杨梅酒，味道非常棒！还有回甘强烈的城步青钱柳茶，让我念念不忘。 8 月份，堂弟来深圳暑期实习，跟两个堂弟一起穿越深圳东西冲海岸线，风景非常棒，不过路上也是又热又渴 10 月份 加入了大宇的冲浪小分队，第一次冲浪、海边烧烤 买了双轮滑鞋，学会了倒滑、压步转向，复习了大学时学过的若干基础技巧 12 月，买了台云米泉先净饮机后，有了随时随地的矿物质热水，就想起了 4 月份在初中同学家喝过的青钱柳，然后就喝茶上瘾了，一桌子的滇红、祁门红茶、安吉白茶、黄山毛峰、青钱柳、莓茶、梅子菁…目前感觉滇红跟祁门红茶最好喝，安吉白茶跟黄山毛峰都非常清香，青钱柳回甘最强烈，莓茶怎么说呢味道感觉不太好（也可能是泡的手法不对？） 我的云米净饮机 桌面上的各种茶叶 2022 年 1 月，第一次买动漫手办，妆点后感觉房间都增色不少~ 我的房间-挂画-手办 ","date":"2022-01-03","objectID":"/posts/2021-summary/:2:0","series":null,"tags":["总结"],"title":"2021 年年终总结","uri":"/posts/2021-summary/#生活"},{"categories":["life","tech"],"content":" 读书 年初辞职后游山玩水，心思稍微安定了些，看了大半本《走出荒野》。 6 月份社区组织打新冠疫苗时，在等候室看了本《青春驿站——深圳打工妹写真》，讲述八九十年代打工妹的生活。很真实，感情很细腻。 年末二爷爷去世，参加完葬礼后，心态有些变化，看完了大一时买下的《月宫 Moon Palace》，讲述主角的悲剧人生。 其余大部分业余时间，无聊，又不想学点东西，也不想运动，于是看了非常多的网络小说打发时间。 ","date":"2022-01-03","objectID":"/posts/2021-summary/:3:0","series":null,"tags":["总结"],"title":"2021 年年终总结","uri":"/posts/2021-summary/#读书"},{"categories":["life","tech"],"content":" 音乐年初辞职后，练了一段时间的竹笛跟蓝调口琴，但后来找到工作后就基本沉寂了。 总的来说还是原地踏步吧。 ","date":"2022-01-03","objectID":"/posts/2021-summary/:4:0","series":null,"tags":["总结"],"title":"2021 年年终总结","uri":"/posts/2021-summary/#音乐"},{"categories":["life","tech"],"content":" 工作 - 我在大宇无限的这一年3 月份刚进大宇的我充满好奇，但也小心谨慎，甚至有点不敢相信自己能进到一家这么棒的公司，感觉自己运气爆棚。 毕竟大宇无论是同事水平还是工作氛围，亦或是用户体量，相比我上家公司都是质的差别。 我在大宇的第一个工位 之后慢慢熟悉工作的内容与方法，leader 尽力把最匹配我兴趣的工作安排给我，帮我排疑解难，同时又给我极大的自主性，真的是棒极了。 然而自主性高带来的也是更高的工作难度，遇到困难时也曾手忙脚乱、迷茫、甚至自我怀疑，很担心是不是隔天就得跑路了… 但好在我终究还是能调节好心态，负起责任，一步步把工作完成。 中间有几次工作有延误时，leader 还陪我加班，事情干完后又带我去吃大餐犒劳自己，真的超级感谢他的帮助与支持。 换座位后的新工位，落地窗风景很棒 这样经历了几个项目的洗礼后，现在我终于能说自己是脚踏实地了，心态从「明天是不是得提桶跑路」转变成了「哇还有这个可以搞，那个 ROI 也很高，有好多有趣的事可以做啊」，我终于能说自己真正融入了大宇无限这家公司，成为了它的一员。 回看下了 2020 年的总结与展望，今年实际的进步，跟去年期望的差别很大。最初的目标大概只实现了 10%，但是接触到了许多意料之外的东西，总体还是满意的： 熟悉了新公司的文化与工作方式，这感觉是个很大的收获，我的工作方式有了很大的改善 接触并且熟悉了新公司的 AWS 线上环境 负责维护线上 Kubernetes 管理平台，第一次接触到的线上集群峰值 QPS 就有好几万。从一开始的小心翼翼，到现在也转变成了老手，这算是意义重大吧 使用 python 写了几个 Kubernetes 管理平台的服务，这也是我第一次写线上服务，很有些成就感 下半年在 AWS 成本的分析与管控上花了很多精力，也有了一些不错的成果，受益匪浅 学会了 Nginx 的简单使用，刚好够用于维护公司先有的 Nginx 代理配置 主导完成了「新建 K8s 集群，将服务迁移到新集群」。虽然并不是一件很难的事，但这应该算是我 2021 年最大的成就了。 升级过程中也是遇到了各种问题，第一次升级迁移时我准备了好久，慌的不行，结果升级时部分服务还是出了问题，当时脑子真的是个懵的，跟 leader 搞到半夜 1 点多后还是没解决，回退到了旧集群，升级失败。之后通过各种测试分析，确认到是某个服务扩缩容震荡导致可用率无法恢复，尝试通过 HPA 的 behavior 来控制扩缩容速率，又意外触发了 K8s HPA 的 bug 把集群控制面搞崩了… 再之后把问题都确认了，第二次尝试升级，又是有个别服务可用率抖动，调试了好几天。那几天神经一直紧绷，每天早上都是被服务可用率的告警吵醒的。跨年的那天晚上业务量上涨，我就在观察服务可用率的过程中跨年了。这样才终于完成了 K8s 集群的升级，期间各位同事也有参与帮忙分析排查各种问题，非常感谢他们，还有努力的我自己。 随便写了几个 go 的 demo，基本没啥进步 学了一个星期的 rust 语言，快速看完了 the book，用 rust 重写了个 video2chars 学习了 Linux 容器的底层原理：cgroups/namespace 技术，并且用 go/rust 实现了个 demo 学习了 Linux 的各种网络接口、Iptables 熟悉了 PromQL/Grafana，现在也能拷贝些 PromQL 查各种数据了 如果要给自己打分的话，那就是「良好」吧。因为并没有很强的进取心，所以出来的结果也并不能称之为「优秀」。 顺便公司的新办公区真的超赞，详情见我的 twitter： 新办公区真好呐～ 值此良辰美景，好想整个榻榻米坐垫，坐在角落的落地窗边工作🤣 那种使用公共设施工（mo）作（yu）的乐趣，以及平常工位见不到的景色交相辉映，是不太好表述的奇妙体验 pic.twitter.com/FASffzw8N3 — ryan4yin | 於清樂 (@ryan4yin) January 17, 2022 ","date":"2022-01-03","objectID":"/posts/2021-summary/:5:0","series":null,"tags":["总结"],"title":"2021 年年终总结","uri":"/posts/2021-summary/#工作---我在大宇无限的这一年"},{"categories":["life","tech"],"content":" 技术方面的感受 Istio 服务网格：体会到了它有点重，而且它的发展跟我们的需求不一定匹配 Sidecar 模式的成本比较高，在未调优的情况下，它会给服务带来 1/3 到 1/4 的成本提升，以及延迟上升 比如切量权重固定为 100（新版本将会放宽限制），不支持 pod 的 warm up（社区已经有 PR，持续观望吧） 而它重点发展的虚拟机支持我们却完全不需要 一直在思考是持续往 Istio 投入，还是换其他的方案 服务网格仍然在快速发展，未来的趋势应该是 eBPF + Envoy + WASM Cilium 推出的基于 eBPF 的 Service Mesh 是一个新趋势（它使用高级特性时会退化成 Per Node Proxy 模式），成本、延迟方面都有望吊打 Sidecar 模式的其他服务网格，是今年服务网格领域的大新闻。 我们曾尝试使用中心化网关来替代 Sidecar 以降低成本。但是跨区流量成本、HTTP/gRPC 多协议共存，这些都是挑战。而且这也并不是社区的最佳实践，现在我觉得维持 Sidecar 其实反而能提升资源利用率，我们的集群资源利用率目前很低。如果能把控好，这部分成本或许是可以接受的。 K8s 集群的日志方面，我们目前是使用自研的基于 gelf 协议的系统，但是问题挺多的 从提升系统的可维护性、易用性等角度来说，loki 是值得探索下的 K8s 集群管理方面，觉得集群的升级迭代，可以做得更自动化、更可靠。明年可以在多集群管理这个方向上多探索下。 Pod 服务质量：对非核心服务，可以适当调低 requests 的资源量，而不是完全预留(Guaranteed)，以提升资源利用率。 官方的 HPA 能力是不够用的，业务侧可能会需要基于 QPS/Queue 或者业务侧的其他参数来进行扩缩容 推广基于 KEDA 的扩缩容能力 关注 Container resource metrics 的进展 成本控制方面，体会到了 ARM 架构以及 Spot 竞价实例的好处 2022-02-17 更新：数据库等中间件可以切换到 ARM。EKS 服务目前都是 Spot 实例，它的 ARM 化 ROI 并不高。 跨区流量成本有很大的潜在优化空间 跨区流量成本是进出该可用区都会收费，而且不仅涉及 Kubernetes 集群内服务间的调用，还会涉及对 RDS/ES/ElastiCache/EC2 等其他资源的调用。 今年各云厂商故障频发，没有跨 region 的服务迁移就会很难受，需要持续关注下 karmada 这类多集群管理方案。 Google 账号系统宕机 Fastly CDN 故障 Facebook 故障 AWS 更是各种可用区故障，12/7 的故障导致 AWS 大部分服务都崩了。因此我们 SRE 今年经常是救各种大火小火… Rust/Go/WASM 蓬勃发展，未来可期。 AI 落地到各个领域，影响到了我们日常使用的语音导航、歌声合成、语音合成等多个领域，当然也包括与 SRE 工作相关的场景：AIOps ","date":"2022-01-03","objectID":"/posts/2021-summary/:6:0","series":null,"tags":["总结"],"title":"2021 年年终总结","uri":"/posts/2021-summary/#技术方面的感受"},{"categories":["life","tech"],"content":" 2022 年的展望","date":"2022-01-03","objectID":"/posts/2021-summary/:7:0","series":null,"tags":["总结"],"title":"2021 年年终总结","uri":"/posts/2021-summary/#2022-年的展望"},{"categories":["life","tech"],"content":" 技术侧今年的展望写得更聚焦一些，争取能实现 50%，就是很大的突破了。 重点仍然是网络技术与 Kubernetes 技术，Redis/Search/Database 等技术还得靠后排，或许明年吧哈哈。 熟练掌握 Go 语言，并分别用于至少两个项目中 打铁还需自身硬，编码能力是基础中的基础 Kubernetes 相关 以 kubebuilder 为代表的 k8s 开发、拓展技术 阅读 k8s 及相关生态的源码，了解其实现逻辑 网络技术 服务网格 Istio 代理工具 Envoy/APISIX 网络插件 Cilium + eBPF AWS K8s 成本与服务稳定性优化 通过拓扑感知的请求转发，节约跨可用区/跨域的流量成本 K8s 新特性：Topology Aware Hints Istio: Locality Load Balancing 推广 gRPC 协议 通过亲和性与反亲和性 + descheduler，实现合理调度 Pods 减少跨域流量、也提升服务容灾能力 提升本地开发效率： nocalhost 多集群的应用部署、容灾 karmada 探索新技术与可能性（优先级低） 基于 Kubernetes 的服务平台，未来的发展方向 kubevela buildpack 是否应该推进 gitops openkruise Serverless 平台的进展 Knative OpenFunction 机器学习、深度学习技术：想尝试下将 AI 应用在音乐、语音、SRE 等我感兴趣的领域，即使是调包也行啊，总之想出点成果… 可以预料到明年 SRE 团队有超多的机会，这其中我具体能负责哪些部分，又能做出怎样的成果，真的相当期待~ ","date":"2022-01-03","objectID":"/posts/2021-summary/:7:1","series":null,"tags":["总结"],"title":"2021 年年终总结","uri":"/posts/2021-summary/#技术侧"},{"categories":["life","tech"],"content":" 生活侧 运动： 把轮滑练好，学会点花样吧，每个月至少两次。 进行三次以上的次短途旅行，东西冲穿越可以再来一次。 音乐： 再一次学习乐理… midi 键盘买了一直吃灰，多多练习吧 买了个 Synthesizer V Stduio Pro + 「青溯 AI」，新的一年想学下调教，翻唱些自己喜欢的歌。 阅读：清单如下，一个月至少读完其中一本。 文学类： 《人间失格》：久仰大名的一本书，曾经有同学力荐，但是一直没看。 《生命最后的读书会》：或许曾经看过，但是一点印象都没了 《百年孤独》：高中的时候读过一遍，但是都忘差不多了 《霍乱时期的爱情》 《苏菲的世界》：据说是哲学启蒙读物，曾经看过，但是对内容完全没印象了。 《你一生的故事》：我也曾是个科幻迷 《沈从文的后半生》 《我与地坛》 《将饮茶》 《吾国与吾民 - 林语堂》 《房思琪的初恋乐园》 人文社科 《在生命的尽头拥抱你-临终关怀医生手记》：今年想更多地了解下「死亡」 《怎样征服美丽少女》：哈哈 《爱的艺术》 《社会心理学》 《被讨厌的勇气》 《人体简史》 《科学革命的结构》 《邓小平时代》 《论中国》 《刘擎西方现代思想讲义》 《时间的秩序》 《极简宇宙史》 《圆圈正义-作为自由前提的信念》 《人生脚本》 技术类 《复杂》 《SRE - Google 运维解密》 《凤凰项目：一个 IT 运维的传奇故事》 《人月神话》 《绩效使能：超越 OKR》 《奈飞文化手册》 《幕后产品-打造突破式思维》 《深入 Linux 内核架构》 《Linux/UNIX 系统编程手册》 《重构 - 改善既有代码的设计》 《网络是怎样连接的》：曾经学习过《计算机网络：自顶向下方法》，不过只学到网络层。就从这本书开始重新学习吧。 ","date":"2022-01-03","objectID":"/posts/2021-summary/:7:2","series":null,"tags":["总结"],"title":"2021 年年终总结","uri":"/posts/2021-summary/#生活侧"},{"categories":["life","tech"],"content":" 结语2021 年初朋友与我给自己的期许是「拆破玉笼飞彩凤，顿开金锁走蛟龙」，感觉确实应验了。 今年我希望不论是在生活上还是在工作上，都能「更上一层楼」~ 更多有趣的、有深度的 2021 年度总结：https://github.com/saveweb/review-2021 ","date":"2022-01-03","objectID":"/posts/2021-summary/:8:0","series":null,"tags":["总结"],"title":"2021 年年终总结","uri":"/posts/2021-summary/#结语"},{"categories":["life"],"content":"大雪，沙雪。 到晴岚桥等送葬队伍时，非常冷。 转头一望，发现送葬的几位师傅在渠渡庙门口就地取材生起了火堆取暖。这样寒冷的天气下，很有种惊喜的感觉。 送葬路上又是风又是雪，像是老天也在哀伤。辉辉说这还是他第一次在风雪天里送葬，我也有同感。 到了山上，雨伞上已经结了薄薄一层冰，老爸跟老妈衣服也冻上了冰晶，辉辉更是头发都冻上了。 风雪之中，二爷爷被葬在我家后山。 我们就这样送走了二爷爷。 这次的送葬，对我而言像是一个仪式——多年前的高考备战、以及后来的异乡求学，使我失去了一些一生只有一次的告别机会，我在尝试弥补这些曾经的遗憾。 事情都办完后，我到洞口赶高铁，结果不论是高铁还是火车都晚点，就连只隔一个站的 K809 都晚点 99 分钟。 漫长的等车时间里，我又看起了《月宫》这本小说。这是我刚上大学时买的书，因为看到说主角想把自己逼到极限，这引起了当时苦行僧般的我的共鸣，于是就想买来读一读，但我始终没有看完它，因为越读内心就越压抑。 不知道该如何描述这种心态的变化，我意识到我现在终于能够沉下心去读这本书了。 我边读边回忆多年前读过的故事情节。在记起是 Kitty 救了自我放逐中的 Fogg，并且重新获得希望之后，我发觉自己目前的状态可能有些问题。 业余时间沉迷在自我中心的网络小说中，其他时间只关注技术，人就渐渐变得跟人脱节。 终于上了高铁，在车上我同样用《月宫》打发时间。 在晚点两个半小时后，一点半，到达了深圳北，这时候我刚好看到书中 Kitty 对 Fogg 说：「已经太晚了，我不能再一次冒险。再见，请你好好对自己。」 心里突然就空落落的，我意识到这是一个彻头彻尾的悲剧，我居然想在悲剧中期许一个美好的转折，真的是有些妄想了。 下了车，站在站台上，眼泪就涌了出来。为书中的悲剧哭泣，也再一次意识到，那些记忆中满脸皱纹的身影，是真的永别了。 从 2015 年 11 月到 2021 年 12 月 27 日的凌晨，二爷爷下葬的翌日，我借着火车站路边昏黄的灯光，看完了保罗·奥斯特的《月宫》。 ","date":"2021-12-27","objectID":"/posts/moon-palace/:0:0","series":null,"tags":["生死"],"title":"月宫","uri":"/posts/moon-palace/#"},{"categories":["life"],"content":" 已过了立冬，却没想象中的那么冷。 忽闻堂弟打算去河南，而且后天就走。 一瞬间感觉生活有点梦幻，惶惶然又脱离了掌控。 又想到今年找到的新工作，梦幻般的待遇，不限量的三餐供应，窗明几净的落地窗工位，这一切都像是在做梦。 即使如此，我一边担心自己工作搞不定要提桶跑路，一边却又还不满足。 浮生若梦，为欢几何？ 嘿，又想要喝点酒了，梦里或许有好酒呢。 恍惚间，又回到了那年大二开学，我拖着个旧皮箱，在凌晨薄雾的校园里走着，耳边只有皮箱轮子的滚动声和几声鸟鸣。耳机里放着一首《遥远的歌》。 ","date":"2021-11-16","objectID":"/posts/life-is-just-like-a-dream/:0:0","series":null,"tags":[],"title":"浮生若梦，为欢几何？","uri":"/posts/life-is-just-like-a-dream/#"},{"categories":["音乐","life"],"content":" 「此岸弃草，彼岸繁花。」取自前永动机主唱「河津樱/白金」的个人简介 今天想推几首歌 emmmm 音乐插件出了点毛病，直接上链接了： https://music.163.com/#/playlist?id=901077788 ","date":"2021-08-28","objectID":"/posts/weeds-on-this-side-flowers-on-the-other/:0:0","series":null,"tags":["后摇"],"title":"此岸弃草，彼岸繁花","uri":"/posts/weeds-on-this-side-flowers-on-the-other/#"},{"categories":["tech"],"content":" 本文仅针对 ipv4 网络 本文先介绍 iptables 的基本概念及常用命令，然后分析 docker/podman 是如何利用 iptables 和 Linux 虚拟网络接口实现的单机容器网络。 ","date":"2021-08-15","objectID":"/posts/iptables-and-container-networks/:0:0","series":null,"tags":["Linux","网络","虚拟化","容器","iptables","conntrack"],"title":"iptables 及 docker 容器网络分析","uri":"/posts/iptables-and-container-networks/#"},{"categories":["tech"],"content":" 一、iptablesiptables 提供了包过滤、NAT 以及其他的包处理能力，iptables 应用最多的两个场景是 firewall 和 NAT iptables 及新的 nftables 都是基于 netfilter 开发的，是 netfilter 的子项目。 但是 eBPF 社区目前正在开发旨在取代 netfilter 的新项目 bpfilter，他们的目标之一是兼容 iptables/nftables 规则，让我们拭目以待吧。 ","date":"2021-08-15","objectID":"/posts/iptables-and-container-networks/:1:0","series":null,"tags":["Linux","网络","虚拟化","容器","iptables","conntrack"],"title":"iptables 及 docker 容器网络分析","uri":"/posts/iptables-and-container-networks/#一iptables"},{"categories":["tech"],"content":" 1. iptables 基础概念 - 四表五链 实际上还有张 SELinux 相关的 security 表（应该是较新的内核新增的，但是不清楚是哪个版本加的），但是我基本没接触过，就略过了。 这里只对 iptables 做简短介绍，详细的教程参见 iptables详解（1）：iptables概念 - 朱双印，这篇文章写得非常棒！把 iptables 讲清楚了。 默认情况下，iptables 提供了四张表（不考虑 security 的话）和五条链，数据在这四表五链中的处理流程如下图所示： 在这里的介绍中，可以先忽略掉图中 link layer 层的链路，它属于 ebtables 的范畴。另外 conntrack 也暂时忽略，在下一小节会详细介绍 conntrack 的功能。 netfilter 数据包处理流程，来自 wikipedia 对照上图，对于发送到某个用户层程序的数据而言，流量顺序如下： 首先进入 PREROUTING 链，依次经过这三个表： raw -\u003e mangle -\u003e nat 然后进入 INPUT 链，这个链上也有三个表，处理顺序是：mangle -\u003e nat -\u003e filter 过了 INPUT 链后，数据才会进入内核协议栈，最终到达用户层程序。 用户层程序发出的报文，则依次经过这几个表：OUTPUT -\u003e POSTROUTING 从图中也很容易看出，如果数据 dst ip 不是本机任一接口的 ip，那它通过的几个链依次是：PREROUTEING -\u003e FORWARD -\u003e POSTROUTING 五链的功能和名称完全一致，应该很容易理解。 除了默认的五条链外，用户也可以创建自定义的链，自定义的链需要被默认链引用才能生效，我们后面要介绍的 Docker 实际上就定义了好几条自定义链。 除了「链」外，iptables 还有「表」的概念，四个表的优先级顺序如下： raw: 对收到的数据包在连接跟踪前进行处理。一般用不到，可以忽略 一旦用户使用了 raw 表，raw 表处理完后，将跳过 nat 表和 ip_conntrack 处理，即不再做地址转换和数据包的链接跟踪处理了 mangle: 用于修改报文、给报文打标签，用得也较少。 nat: 主要用于做网络地址转换，SNAT 或者 DNAT filter: 主要用于过滤数据包 数据在按优先级经过四个表的处理时，一旦在某个表中匹配到一条规则 A,下一条处理规则就由规则 A 的 target 参数指定，后续的所有表都会被忽略。target 有如下几种类型： ACCEPT: 直接允许数据包通过 DROP: 直接丢弃数据包，对程序而言就是 100% 丢包 REJECT: 丢弃数据包，但是会给程序返回 RESET。这个对程序更友好，但是存在安全隐患，通常不使用。 MASQUERADE: （伪装）将 src ip 改写为网卡 ip，和 SNAT 的区别是它会自动读取网卡 ip。路由设备必备。 SNAT/DNAT: 顾名思义，做网络地址转换 REDIRECT: 在本机做端口映射 LOG: 在 /var/log/messages 文件中记录日志信息，然后将数据包传递给下一条规则，也就是说除了记录以外不对数据包做任何其他操作，仍然让下一条规则去匹配。 只有这个 target 特殊一些，匹配它的数据仍然可以匹配后续规则，不会直接跳过。 其他自定义链的名称：表示将数据包交给该链进行下一步处理。 RETURN: 如果是在子链（自定义链）遇到 RETURN，则返回父链的下一条规则继续进行条件的比较。如果是在默认链 RETURN 则直接使用默认的动作（ACCEPT/DROP） 其他类型，可以用到的时候再查 理解了上面这张图，以及四个表的用途，就很容易理解 iptables 的命令了。 ","date":"2021-08-15","objectID":"/posts/iptables-and-container-networks/:1:1","series":null,"tags":["Linux","网络","虚拟化","容器","iptables","conntrack"],"title":"iptables 及 docker 容器网络分析","uri":"/posts/iptables-and-container-networks/#1-iptables-基础概念---四表五链"},{"categories":["tech"],"content":" 2. 常用命令 注意: 下面提供的 iptables 命令做的修改是未持久化的，重启就会丢失！在下一节会简单介绍持久化配置的方法。 命令格式： iptables [-t table] {-A|-C|-D} chain [-m matchname [per-match-options]] -j targetname [per-target-options] 其中 table 默认为 filter 表，其中系统管理员实际使用最多的是 INPUT 链，用于设置防火墙。 以下简单介绍在 INPUT 链上添加、修改规则，来设置防火墙： # --add 允许 80 端口通过 iptables -A INPUT -p tcp --dport 80 -j ACCEPT # --list-rules 查看所有规则 iptables -S # --list-rules 查看 INPUT 表中的所有规则 iptables -S INPUT # 查看 iptables 中的所有规则（比 -L 更详细） # ---delete 通过编号删除规则 iptables -D 1 # 或者通过完整的规则参数来删除规则 iptables -D INPUT -p tcp --dport 80 -j ACCEPT # --replace 通过编号来替换规则内容 iptables -R INPUT 1 -s 192.168.0.1 -j DROP # --insert 在指定的位置插入规则，可类比链表的插入 iptables -I INPUT 1 -p tcp --dport 80 -j ACCEPT # 在匹配条件前面使用感叹号表示取反 # 如下规则表示接受所有来自 docker0，但是目标接口不是 docker0 的流量 iptables -A FORWARD -i docker0 ! -o docker0 -j ACCEPT # --policy 设置某个链的默认规则 # 很多系统管理员会习惯将连接公网的服务器，默认规则设为 DROP，提升安全性，避免错误地开放了端口。 # 但是也要注意，默认规则设为 DROP 前，一定要先把允许 ssh 端口的规则加上，否则就尴尬了。 iptables -P INPUT DROP # --flush 清空 INPUT 表上的所有规则 iptables -F INPUT 本文后续分析时，假设用户已经清楚 linux bridge、veth 等虚拟网络接口相关知识。 如果你还缺少这些前置知识，请先阅读文章 Linux 中的虚拟网络接口。 ","date":"2021-08-15","objectID":"/posts/iptables-and-container-networks/:1:2","series":null,"tags":["Linux","网络","虚拟化","容器","iptables","conntrack"],"title":"iptables 及 docker 容器网络分析","uri":"/posts/iptables-and-container-networks/#2-常用命令"},{"categories":["tech"],"content":" 3. conntrack 连接跟踪与 NAT在讲 conntrack 之间，我们再回顾下前面给出过的 netfilter 数据处理流程图： netfilter 数据包处理流程，来自 wikipedia 上一节中我们忽略了图中的 conntrack，它就是本节的主角——netfilter 的连接跟踪（connection tracking）模块。 netfilter/conntrack 是 iptables 实现 SNAT/DNAT/MASQUERADE 的前提条件，上面的流程图显示， conntrack 在 PREROUTEING 和 OUTPUT 链的 raw 表之后生效。 下面以 docker 默认的 bridge 网络为例详细介绍下 conntrack 的功能。 首先，这是我在「Linux 的虚拟网络接口」文中给出过的 docker0 网络架构图: +-----------------------------------------------+-----------------------------------+-----------------------------------+ | Host | Container A | Container B | | | | | | +---------------------------------------+ | +-------------------------+ | +-------------------------+ | | | Network Protocol Stack | | | Network Protocol Stack | | | Network Protocol Stack | | | +----+-------------+--------------------+ | +-----------+-------------+ | +------------+------------+ | | ^ ^ | ^ | ^ | |........|.............|........................|................|..................|.................|.................| | v v ↓ | v | v | | +----+----+ +-----+------+ | +-----+-------+ | +-----+-------+ | | | .31.101 | | 172.17.0.1 | +------+ | | 172.17.0.2 | | | 172.17.0.3 | | | +---------+ +-------------\u003c----\u003e+ veth | | +-------------+ | +-------------+ | | | eth0 | | docker0 | +--+---+ | | eth0(veth) | | | eth0(veth) | | | +----+----+ +-----+------+ ^ | +-----+-------+ | +-----+-------+ | | ^ ^ | | ^ | ^ | | | | +------------------------+ | | | | | v | | | | | | +--+---+ | | | | | | | veth | | | | | | | +--+---+ | | | | | | ^ | | | | | | +------------------------------------------------------------------------------+ | | | | | | | | | | | +-----------------------------------------------+-----------------------------------+-----------------------------------+ v Physical Network (192.168.31.0/24) docker 会在 iptables 中为 docker0 网桥添加如下规则： -t nat -A POSTROUTING -s 172.17.0.0/16 ! -o docker0 -j MASQUERADE -t filter -P DROP -t filter -A FORWARD -o docker0 -m conntrack --ctstate RELATED,ESTABLISHED -j ACCEPT 这几行规则使 docker 容器能正常访问外部网络。MASQUERADE 在请求出网时，会自动做 SNAT，将 src ip 替换成出口网卡的 ip. 这样数据包能正常出网，而且对端返回的数据包现在也能正常回到出口网卡。 现在问题就来了：出口网卡收到返回的数据包后，还能否将数据包转发到数据的初始来源端——某个 docker 容器？难道 docker 还额外添加了与 MASQUERADE 对应的 dst ip 反向转换规则？ 实际上这一步依赖的是本节的主角——iptables 提供的 conntrack 连接跟踪功能（在「参考」中有一篇文章详细介绍了此功能）。 连接跟踪对 NAT 的贡献是：在做 NAT 转换时，无需手动添加额外的规则来执行反向转换以实现数据的双向传输。netfilter/conntrack 系统会记录 NAT 的连接状态，NAT 地址的反向转换是根据这个状态自动完成的。 比如上图中的 Container A 通过 bridge 网络向 baidu.com 发起了 N 个连接，这时数据的处理流程如下： 首先 Container A 发出的数据包被 MASQUERADE 规则处理，将 src ip 替换成 eth0 的 ip，然后发送到物理网络 192.168.31.0/24。 conntrack 系统记录此连接被 NAT 处理前后的状态信息，并将其状态设置为 NEW，表示这是新发起的一个连接 对端 baidu.com 返回数据包后，会首先到达 eth0 网卡 conntrack 查表，发现返回数据包的连接已经记录在表中并且状态为 NEW，于是它将连接的状态修改为 ESTABLISHED，并且将 dst_ip 改为 172.17.0.2 然后发送出去 注意，这个和 tcp 的 ESTABLISHED 没任何关系 经过路由匹配，数据包会进入到 docker0，然后匹配上 iptables 规则：-t filter -A FORWARD -o docker0 -m conntrack --ctstate RELATED,ESTABLISHED -j ACCEPT，数据直接被放行 数据经过 veth 后，最终进入到 Container A 中，交由容器的内核协议栈处理。 数据被 Container A 的内核协议栈发送到「发起连接的应用程序」。 1. 支持哪些协议conntrack 连接跟踪模块目前只支持以下六种协议：TCP、UDP、ICMP、DCCP、SCTP、GRE 要注意的一点是，conntrack 跟踪的「连接」，跟「TCP 连接」不是一个层面的概念，可以看到 conntrack 也支持 UDP 这种无连接通讯协议。 2. 实际测试 conntrack现在我们来实际测试一下，看看是不是这么回事： # 使用 tcpdump 分别在出口网卡 wlp4s0 （相当于 eth0）和 dcoker0 网桥上抓包，后面会用来分析 ❯ sudo tcpdump -i wlp4s0 -n \u003e wlp4s0.dump # 窗口一，抓 wlp4s0 的包 ❯ sudo tcpdump -i docker0 -n \u003e docker0.dump # 窗口二，抓 docker0 的包 现在新建窗口三，启动一个容器，通过 curl 命令低速下载一个视频文件： ❯ docker run --rm --name curl -it curlimages/curl \"https://media.w3.org/2010/05/sintel/trailer.mp4\" -o /tmp/video.mp4 --limit-rate 100k 然后新建窗口四，在宿主机查看 conntrack 状态 ❯ sudo zypper in conntrack-tools # 这个记得先提前安装好 ❯ sudo conntrack -L | grep 172.17 # curl 通过 NAT 网络发起了一个 dns 查询请求，DNS 服务器是网关上的 192.168.31.1 udp 17 22 src=172.17.0.4 dst=192.168.31.1 sport=59423 dport=53 src=192.168.31.1 dst=192.168.31.228 sport=53 dport=59423 [ASSURED] mark=0 use=1 # curl 通过 NAT 网络向 media.w3.org 发起了 tcp 连接 tcp 6 298 ESTABLISHED src=172.17.0.4 dst=198.18.5.130 sport=54636 dport=443 src=198.18.5.130 dst=192.168.31.228 sport=443 dpor","date":"2021-08-15","objectID":"/posts/iptables-and-container-networks/:1:3","series":null,"tags":["Linux","网络","虚拟化","容器","iptables","conntrack"],"title":"iptables 及 docker 容器网络分析","uri":"/posts/iptables-and-container-networks/#3-conntrack-连接跟踪与-nat"},{"categories":["tech"],"content":" 3. conntrack 连接跟踪与 NAT在讲 conntrack 之间，我们再回顾下前面给出过的 netfilter 数据处理流程图： netfilter 数据包处理流程，来自 wikipedia 上一节中我们忽略了图中的 conntrack，它就是本节的主角——netfilter 的连接跟踪（connection tracking）模块。 netfilter/conntrack 是 iptables 实现 SNAT/DNAT/MASQUERADE 的前提条件，上面的流程图显示， conntrack 在 PREROUTEING 和 OUTPUT 链的 raw 表之后生效。 下面以 docker 默认的 bridge 网络为例详细介绍下 conntrack 的功能。 首先，这是我在「Linux 的虚拟网络接口」文中给出过的 docker0 网络架构图: +-----------------------------------------------+-----------------------------------+-----------------------------------+ | Host | Container A | Container B | | | | | | +---------------------------------------+ | +-------------------------+ | +-------------------------+ | | | Network Protocol Stack | | | Network Protocol Stack | | | Network Protocol Stack | | | +----+-------------+--------------------+ | +-----------+-------------+ | +------------+------------+ | | ^ ^ | ^ | ^ | |........|.............|........................|................|..................|.................|.................| | v v ↓ | v | v | | +----+----+ +-----+------+ | +-----+-------+ | +-----+-------+ | | | .31.101 | | 172.17.0.1 | +------+ | | 172.17.0.2 | | | 172.17.0.3 | | | +---------+ +-------------\u003c----\u003e+ veth | | +-------------+ | +-------------+ | | | eth0 | | docker0 | +--+---+ | | eth0(veth) | | | eth0(veth) | | | +----+----+ +-----+------+ ^ | +-----+-------+ | +-----+-------+ | | ^ ^ | | ^ | ^ | | | | +------------------------+ | | | | | v | | | | | | +--+---+ | | | | | | | veth | | | | | | | +--+---+ | | | | | | ^ | | | | | | +------------------------------------------------------------------------------+ | | | | | | | | | | | +-----------------------------------------------+-----------------------------------+-----------------------------------+ v Physical Network (192.168.31.0/24) docker 会在 iptables 中为 docker0 网桥添加如下规则： -t nat -A POSTROUTING -s 172.17.0.0/16 ! -o docker0 -j MASQUERADE -t filter -P DROP -t filter -A FORWARD -o docker0 -m conntrack --ctstate RELATED,ESTABLISHED -j ACCEPT 这几行规则使 docker 容器能正常访问外部网络。MASQUERADE 在请求出网时，会自动做 SNAT，将 src ip 替换成出口网卡的 ip. 这样数据包能正常出网，而且对端返回的数据包现在也能正常回到出口网卡。 现在问题就来了：出口网卡收到返回的数据包后，还能否将数据包转发到数据的初始来源端——某个 docker 容器？难道 docker 还额外添加了与 MASQUERADE 对应的 dst ip 反向转换规则？ 实际上这一步依赖的是本节的主角——iptables 提供的 conntrack 连接跟踪功能（在「参考」中有一篇文章详细介绍了此功能）。 连接跟踪对 NAT 的贡献是：在做 NAT 转换时，无需手动添加额外的规则来执行反向转换以实现数据的双向传输。netfilter/conntrack 系统会记录 NAT 的连接状态，NAT 地址的反向转换是根据这个状态自动完成的。 比如上图中的 Container A 通过 bridge 网络向 baidu.com 发起了 N 个连接，这时数据的处理流程如下： 首先 Container A 发出的数据包被 MASQUERADE 规则处理，将 src ip 替换成 eth0 的 ip，然后发送到物理网络 192.168.31.0/24。 conntrack 系统记录此连接被 NAT 处理前后的状态信息，并将其状态设置为 NEW，表示这是新发起的一个连接 对端 baidu.com 返回数据包后，会首先到达 eth0 网卡 conntrack 查表，发现返回数据包的连接已经记录在表中并且状态为 NEW，于是它将连接的状态修改为 ESTABLISHED，并且将 dst_ip 改为 172.17.0.2 然后发送出去 注意，这个和 tcp 的 ESTABLISHED 没任何关系 经过路由匹配，数据包会进入到 docker0，然后匹配上 iptables 规则：-t filter -A FORWARD -o docker0 -m conntrack --ctstate RELATED,ESTABLISHED -j ACCEPT，数据直接被放行 数据经过 veth 后，最终进入到 Container A 中，交由容器的内核协议栈处理。 数据被 Container A 的内核协议栈发送到「发起连接的应用程序」。 1. 支持哪些协议conntrack 连接跟踪模块目前只支持以下六种协议：TCP、UDP、ICMP、DCCP、SCTP、GRE 要注意的一点是，conntrack 跟踪的「连接」，跟「TCP 连接」不是一个层面的概念，可以看到 conntrack 也支持 UDP 这种无连接通讯协议。 2. 实际测试 conntrack现在我们来实际测试一下，看看是不是这么回事： # 使用 tcpdump 分别在出口网卡 wlp4s0 （相当于 eth0）和 dcoker0 网桥上抓包，后面会用来分析 ❯ sudo tcpdump -i wlp4s0 -n \u003e wlp4s0.dump # 窗口一，抓 wlp4s0 的包 ❯ sudo tcpdump -i docker0 -n \u003e docker0.dump # 窗口二，抓 docker0 的包 现在新建窗口三，启动一个容器，通过 curl 命令低速下载一个视频文件： ❯ docker run --rm --name curl -it curlimages/curl \"https://media.w3.org/2010/05/sintel/trailer.mp4\" -o /tmp/video.mp4 --limit-rate 100k 然后新建窗口四，在宿主机查看 conntrack 状态 ❯ sudo zypper in conntrack-tools # 这个记得先提前安装好 ❯ sudo conntrack -L | grep 172.17 # curl 通过 NAT 网络发起了一个 dns 查询请求，DNS 服务器是网关上的 192.168.31.1 udp 17 22 src=172.17.0.4 dst=192.168.31.1 sport=59423 dport=53 src=192.168.31.1 dst=192.168.31.228 sport=53 dport=59423 [ASSURED] mark=0 use=1 # curl 通过 NAT 网络向 media.w3.org 发起了 tcp 连接 tcp 6 298 ESTABLISHED src=172.17.0.4 dst=198.18.5.130 sport=54636 dport=443 src=198.18.5.130 dst=192.168.31.228 sport=443 dpor","date":"2021-08-15","objectID":"/posts/iptables-and-container-networks/:1:3","series":null,"tags":["Linux","网络","虚拟化","容器","iptables","conntrack"],"title":"iptables 及 docker 容器网络分析","uri":"/posts/iptables-and-container-networks/#1-支持哪些协议"},{"categories":["tech"],"content":" 3. conntrack 连接跟踪与 NAT在讲 conntrack 之间，我们再回顾下前面给出过的 netfilter 数据处理流程图： netfilter 数据包处理流程，来自 wikipedia 上一节中我们忽略了图中的 conntrack，它就是本节的主角——netfilter 的连接跟踪（connection tracking）模块。 netfilter/conntrack 是 iptables 实现 SNAT/DNAT/MASQUERADE 的前提条件，上面的流程图显示， conntrack 在 PREROUTEING 和 OUTPUT 链的 raw 表之后生效。 下面以 docker 默认的 bridge 网络为例详细介绍下 conntrack 的功能。 首先，这是我在「Linux 的虚拟网络接口」文中给出过的 docker0 网络架构图: +-----------------------------------------------+-----------------------------------+-----------------------------------+ | Host | Container A | Container B | | | | | | +---------------------------------------+ | +-------------------------+ | +-------------------------+ | | | Network Protocol Stack | | | Network Protocol Stack | | | Network Protocol Stack | | | +----+-------------+--------------------+ | +-----------+-------------+ | +------------+------------+ | | ^ ^ | ^ | ^ | |........|.............|........................|................|..................|.................|.................| | v v ↓ | v | v | | +----+----+ +-----+------+ | +-----+-------+ | +-----+-------+ | | | .31.101 | | 172.17.0.1 | +------+ | | 172.17.0.2 | | | 172.17.0.3 | | | +---------+ +-------------\u003c----\u003e+ veth | | +-------------+ | +-------------+ | | | eth0 | | docker0 | +--+---+ | | eth0(veth) | | | eth0(veth) | | | +----+----+ +-----+------+ ^ | +-----+-------+ | +-----+-------+ | | ^ ^ | | ^ | ^ | | | | +------------------------+ | | | | | v | | | | | | +--+---+ | | | | | | | veth | | | | | | | +--+---+ | | | | | | ^ | | | | | | +------------------------------------------------------------------------------+ | | | | | | | | | | | +-----------------------------------------------+-----------------------------------+-----------------------------------+ v Physical Network (192.168.31.0/24) docker 会在 iptables 中为 docker0 网桥添加如下规则： -t nat -A POSTROUTING -s 172.17.0.0/16 ! -o docker0 -j MASQUERADE -t filter -P DROP -t filter -A FORWARD -o docker0 -m conntrack --ctstate RELATED,ESTABLISHED -j ACCEPT 这几行规则使 docker 容器能正常访问外部网络。MASQUERADE 在请求出网时，会自动做 SNAT，将 src ip 替换成出口网卡的 ip. 这样数据包能正常出网，而且对端返回的数据包现在也能正常回到出口网卡。 现在问题就来了：出口网卡收到返回的数据包后，还能否将数据包转发到数据的初始来源端——某个 docker 容器？难道 docker 还额外添加了与 MASQUERADE 对应的 dst ip 反向转换规则？ 实际上这一步依赖的是本节的主角——iptables 提供的 conntrack 连接跟踪功能（在「参考」中有一篇文章详细介绍了此功能）。 连接跟踪对 NAT 的贡献是：在做 NAT 转换时，无需手动添加额外的规则来执行反向转换以实现数据的双向传输。netfilter/conntrack 系统会记录 NAT 的连接状态，NAT 地址的反向转换是根据这个状态自动完成的。 比如上图中的 Container A 通过 bridge 网络向 baidu.com 发起了 N 个连接，这时数据的处理流程如下： 首先 Container A 发出的数据包被 MASQUERADE 规则处理，将 src ip 替换成 eth0 的 ip，然后发送到物理网络 192.168.31.0/24。 conntrack 系统记录此连接被 NAT 处理前后的状态信息，并将其状态设置为 NEW，表示这是新发起的一个连接 对端 baidu.com 返回数据包后，会首先到达 eth0 网卡 conntrack 查表，发现返回数据包的连接已经记录在表中并且状态为 NEW，于是它将连接的状态修改为 ESTABLISHED，并且将 dst_ip 改为 172.17.0.2 然后发送出去 注意，这个和 tcp 的 ESTABLISHED 没任何关系 经过路由匹配，数据包会进入到 docker0，然后匹配上 iptables 规则：-t filter -A FORWARD -o docker0 -m conntrack --ctstate RELATED,ESTABLISHED -j ACCEPT，数据直接被放行 数据经过 veth 后，最终进入到 Container A 中，交由容器的内核协议栈处理。 数据被 Container A 的内核协议栈发送到「发起连接的应用程序」。 1. 支持哪些协议conntrack 连接跟踪模块目前只支持以下六种协议：TCP、UDP、ICMP、DCCP、SCTP、GRE 要注意的一点是，conntrack 跟踪的「连接」，跟「TCP 连接」不是一个层面的概念，可以看到 conntrack 也支持 UDP 这种无连接通讯协议。 2. 实际测试 conntrack现在我们来实际测试一下，看看是不是这么回事： # 使用 tcpdump 分别在出口网卡 wlp4s0 （相当于 eth0）和 dcoker0 网桥上抓包，后面会用来分析 ❯ sudo tcpdump -i wlp4s0 -n \u003e wlp4s0.dump # 窗口一，抓 wlp4s0 的包 ❯ sudo tcpdump -i docker0 -n \u003e docker0.dump # 窗口二，抓 docker0 的包 现在新建窗口三，启动一个容器，通过 curl 命令低速下载一个视频文件： ❯ docker run --rm --name curl -it curlimages/curl \"https://media.w3.org/2010/05/sintel/trailer.mp4\" -o /tmp/video.mp4 --limit-rate 100k 然后新建窗口四，在宿主机查看 conntrack 状态 ❯ sudo zypper in conntrack-tools # 这个记得先提前安装好 ❯ sudo conntrack -L | grep 172.17 # curl 通过 NAT 网络发起了一个 dns 查询请求，DNS 服务器是网关上的 192.168.31.1 udp 17 22 src=172.17.0.4 dst=192.168.31.1 sport=59423 dport=53 src=192.168.31.1 dst=192.168.31.228 sport=53 dport=59423 [ASSURED] mark=0 use=1 # curl 通过 NAT 网络向 media.w3.org 发起了 tcp 连接 tcp 6 298 ESTABLISHED src=172.17.0.4 dst=198.18.5.130 sport=54636 dport=443 src=198.18.5.130 dst=192.168.31.228 sport=443 dpor","date":"2021-08-15","objectID":"/posts/iptables-and-container-networks/:1:3","series":null,"tags":["Linux","网络","虚拟化","容器","iptables","conntrack"],"title":"iptables 及 docker 容器网络分析","uri":"/posts/iptables-and-container-networks/#2-实际测试-conntrack"},{"categories":["tech"],"content":" 3. conntrack 连接跟踪与 NAT在讲 conntrack 之间，我们再回顾下前面给出过的 netfilter 数据处理流程图： netfilter 数据包处理流程，来自 wikipedia 上一节中我们忽略了图中的 conntrack，它就是本节的主角——netfilter 的连接跟踪（connection tracking）模块。 netfilter/conntrack 是 iptables 实现 SNAT/DNAT/MASQUERADE 的前提条件，上面的流程图显示， conntrack 在 PREROUTEING 和 OUTPUT 链的 raw 表之后生效。 下面以 docker 默认的 bridge 网络为例详细介绍下 conntrack 的功能。 首先，这是我在「Linux 的虚拟网络接口」文中给出过的 docker0 网络架构图: +-----------------------------------------------+-----------------------------------+-----------------------------------+ | Host | Container A | Container B | | | | | | +---------------------------------------+ | +-------------------------+ | +-------------------------+ | | | Network Protocol Stack | | | Network Protocol Stack | | | Network Protocol Stack | | | +----+-------------+--------------------+ | +-----------+-------------+ | +------------+------------+ | | ^ ^ | ^ | ^ | |........|.............|........................|................|..................|.................|.................| | v v ↓ | v | v | | +----+----+ +-----+------+ | +-----+-------+ | +-----+-------+ | | | .31.101 | | 172.17.0.1 | +------+ | | 172.17.0.2 | | | 172.17.0.3 | | | +---------+ +-------------\u003c----\u003e+ veth | | +-------------+ | +-------------+ | | | eth0 | | docker0 | +--+---+ | | eth0(veth) | | | eth0(veth) | | | +----+----+ +-----+------+ ^ | +-----+-------+ | +-----+-------+ | | ^ ^ | | ^ | ^ | | | | +------------------------+ | | | | | v | | | | | | +--+---+ | | | | | | | veth | | | | | | | +--+---+ | | | | | | ^ | | | | | | +------------------------------------------------------------------------------+ | | | | | | | | | | | +-----------------------------------------------+-----------------------------------+-----------------------------------+ v Physical Network (192.168.31.0/24) docker 会在 iptables 中为 docker0 网桥添加如下规则： -t nat -A POSTROUTING -s 172.17.0.0/16 ! -o docker0 -j MASQUERADE -t filter -P DROP -t filter -A FORWARD -o docker0 -m conntrack --ctstate RELATED,ESTABLISHED -j ACCEPT 这几行规则使 docker 容器能正常访问外部网络。MASQUERADE 在请求出网时，会自动做 SNAT，将 src ip 替换成出口网卡的 ip. 这样数据包能正常出网，而且对端返回的数据包现在也能正常回到出口网卡。 现在问题就来了：出口网卡收到返回的数据包后，还能否将数据包转发到数据的初始来源端——某个 docker 容器？难道 docker 还额外添加了与 MASQUERADE 对应的 dst ip 反向转换规则？ 实际上这一步依赖的是本节的主角——iptables 提供的 conntrack 连接跟踪功能（在「参考」中有一篇文章详细介绍了此功能）。 连接跟踪对 NAT 的贡献是：在做 NAT 转换时，无需手动添加额外的规则来执行反向转换以实现数据的双向传输。netfilter/conntrack 系统会记录 NAT 的连接状态，NAT 地址的反向转换是根据这个状态自动完成的。 比如上图中的 Container A 通过 bridge 网络向 baidu.com 发起了 N 个连接，这时数据的处理流程如下： 首先 Container A 发出的数据包被 MASQUERADE 规则处理，将 src ip 替换成 eth0 的 ip，然后发送到物理网络 192.168.31.0/24。 conntrack 系统记录此连接被 NAT 处理前后的状态信息，并将其状态设置为 NEW，表示这是新发起的一个连接 对端 baidu.com 返回数据包后，会首先到达 eth0 网卡 conntrack 查表，发现返回数据包的连接已经记录在表中并且状态为 NEW，于是它将连接的状态修改为 ESTABLISHED，并且将 dst_ip 改为 172.17.0.2 然后发送出去 注意，这个和 tcp 的 ESTABLISHED 没任何关系 经过路由匹配，数据包会进入到 docker0，然后匹配上 iptables 规则：-t filter -A FORWARD -o docker0 -m conntrack --ctstate RELATED,ESTABLISHED -j ACCEPT，数据直接被放行 数据经过 veth 后，最终进入到 Container A 中，交由容器的内核协议栈处理。 数据被 Container A 的内核协议栈发送到「发起连接的应用程序」。 1. 支持哪些协议conntrack 连接跟踪模块目前只支持以下六种协议：TCP、UDP、ICMP、DCCP、SCTP、GRE 要注意的一点是，conntrack 跟踪的「连接」，跟「TCP 连接」不是一个层面的概念，可以看到 conntrack 也支持 UDP 这种无连接通讯协议。 2. 实际测试 conntrack现在我们来实际测试一下，看看是不是这么回事： # 使用 tcpdump 分别在出口网卡 wlp4s0 （相当于 eth0）和 dcoker0 网桥上抓包，后面会用来分析 ❯ sudo tcpdump -i wlp4s0 -n \u003e wlp4s0.dump # 窗口一，抓 wlp4s0 的包 ❯ sudo tcpdump -i docker0 -n \u003e docker0.dump # 窗口二，抓 docker0 的包 现在新建窗口三，启动一个容器，通过 curl 命令低速下载一个视频文件： ❯ docker run --rm --name curl -it curlimages/curl \"https://media.w3.org/2010/05/sintel/trailer.mp4\" -o /tmp/video.mp4 --limit-rate 100k 然后新建窗口四，在宿主机查看 conntrack 状态 ❯ sudo zypper in conntrack-tools # 这个记得先提前安装好 ❯ sudo conntrack -L | grep 172.17 # curl 通过 NAT 网络发起了一个 dns 查询请求，DNS 服务器是网关上的 192.168.31.1 udp 17 22 src=172.17.0.4 dst=192.168.31.1 sport=59423 dport=53 src=192.168.31.1 dst=192.168.31.228 sport=53 dport=59423 [ASSURED] mark=0 use=1 # curl 通过 NAT 网络向 media.w3.org 发起了 tcp 连接 tcp 6 298 ESTABLISHED src=172.17.0.4 dst=198.18.5.130 sport=54636 dport=443 src=198.18.5.130 dst=192.168.31.228 sport=443 dpor","date":"2021-08-15","objectID":"/posts/iptables-and-container-networks/:1:3","series":null,"tags":["Linux","网络","虚拟化","容器","iptables","conntrack"],"title":"iptables 及 docker 容器网络分析","uri":"/posts/iptables-and-container-networks/#3-nat-如何分配端口"},{"categories":["tech"],"content":" 4. 如何持久化 iptables 配置首先需要注意的是，centos7/opensuse 15 都已经切换到了 firewalld 作为防火墙配置软件， 而 ubuntu18.04 lts 也换成了 ufw 来配置防火墙。 包括 docker 应该也是在启动的时候动态添加 iptables 配置。 对于上述新系统，还是建议直接使用 firewalld/ufw 配置防火墙吧，或者网上搜下关闭 ufw/firewalld、启用 iptables 持久化的解决方案。 本文主要目的在于理解 docker 容器网络的原理，以及为后面理解 kubernetes 网络插件 calico/flannel 打好基础，因此就不多介绍持久化了。 ","date":"2021-08-15","objectID":"/posts/iptables-and-container-networks/:1:4","series":null,"tags":["Linux","网络","虚拟化","容器","iptables","conntrack"],"title":"iptables 及 docker 容器网络分析","uri":"/posts/iptables-and-container-networks/#4-如何持久化-iptables-配置"},{"categories":["tech"],"content":" 二、容器网络实现原理 - iptables + bridge + vethDocker/Podman 默认使用的都是 bridge 网络，它们的底层实现完全类似。下面以 docker 为例进行分析（Podman 的分析流程也基本一样）。 首先，使用 docker run 运行几个容器，检查下网络状况： # 运行一个 debian 容器和一个 nginx ❯ docker run -dit --name debian --rm debian:buster sleep 1000000 ❯ docker run -dit --name nginx --rm nginx:1.19-alpine #　查看网络接口，有两个 veth 接口（而且都没设 ip 地址），分别连接到两个容器的 eth0（dcoker0 网络架构图前面给过了，可以往前面翻翻对照下） ❯ ip addr ls ... 5: docker0: \u003cBROADCAST,MULTICAST,UP,LOWER_UP\u003e mtu 1500 qdisc noqueue state UP group default link/ether 02:42:42:c7:12:ba brd ff:ff:ff:ff:ff:ff inet 172.17.0.1/16 brd 172.17.255.255 scope global docker0 valid_lft forever preferred_lft forever inet6 fe80::42:42ff:fec7:12ba/64 scope link valid_lft forever preferred_lft forever 100: veth16b37ea@if99: \u003cBROADCAST,MULTICAST,UP,LOWER_UP\u003e mtu 1500 qdisc noqueue master docker0 state UP group default link/ether 42:af:34:ae:74:ae brd ff:ff:ff:ff:ff:ff link-netnsid 0 inet6 fe80::40af:34ff:feae:74ae/64 scope link valid_lft forever preferred_lft forever 102: veth4b4dada@if101: \u003cBROADCAST,MULTICAST,UP,LOWER_UP\u003e mtu 1500 qdisc noqueue master docker0 state UP group default link/ether 9e:f1:58:1a:cf:ae brd ff:ff:ff:ff:ff:ff link-netnsid 1 inet6 fe80::9cf1:58ff:fe1a:cfae/64 scope link valid_lft forever preferred_lft forever # 两个 veth 接口都连接到了 docker0 上面，说明两个容器都使用了 docker 默认的 bridge 网络 ❯ sudo brctl show bridge name bridge id STP enabled interfaces docker0 8000.024242c712ba no veth16b37ea veth4b4dada # 查看路由规则 ❯ ip route ls default via 192.168.31.1 dev wlp4s0 proto dhcp metric 600 #下列路由规则将 `172.17.0.0/16` 网段的所有流量转发到 docker0 172.17.0.0/16 dev docker0 proto kernel scope link src 172.17.0.1 linkdown 192.168.31.0/24 dev wlp4s0 proto kernel scope link src 192.168.31.228 metric 600 # 查看　iptables 规则 # NAT 表 ❯ sudo iptables -t nat -S -P PREROUTING ACCEPT -P INPUT ACCEPT -P OUTPUT ACCEPT -P POSTROUTING ACCEPT # 在 nat 表中新建一条自定义链 DOCKER -N DOCKER # 所有目的地址在本机的，都先交给 DOCKER 链处理一波 -A PREROUTING -m addrtype --dst-type LOCAL -j DOCKER -A OUTPUT ! -d 127.0.0.0/8 -m addrtype --dst-type LOCAL -j DOCKER # （容器访问外部网络）所有出口不为 docker0 的流量，都做下 SNAT，把 src ip 换成出口接口的 ip 地址 -A POSTROUTING -s 172.17.0.0/16 ! -o docker0 -j MASQUERADE # DOCKER 链目前没任何内容，单纯直接返回父链进行进一步匹配 -A DOCKER -i docker0 -j RETURN # filter 表 ❯ sudo iptables -t filter -S -P INPUT ACCEPT -P FORWARD DROP -P OUTPUT ACCEPT # 在 filter 表中新建四条自定义链 -N DOCKER -N DOCKER-ISOLATION-STAGE-1 -N DOCKER-ISOLATION-STAGE-2 -N DOCKER-USER # 所有流量都必须先经过如下两个自定义链的处理，没问题才能继续往下走 -A FORWARD -j DOCKER-ISOLATION-STAGE-1 -A FORWARD -j DOCKER-USER # （容器访问外部网络）出去的流量走了 MASQUERADE，回来的流量会被 conntrack 识别并转发回来，这里允许返回的数据包通过。 # 这里直接 ACCEPT 被 conntrack 识别到的流量 -A FORWARD -o docker0 -m conntrack --ctstate RELATED,ESTABLISHED -j ACCEPT # 将所有访问 docker0 的流量都转给自定义链 DOCKER 处理 -A FORWARD -o docker0 -j DOCKER # 允许所有来自 docker0 的流量通过，不论下一跳是否是 docker0 -A FORWARD -i docker0 ! -o docker0 -j ACCEPT -A FORWARD -i docker0 -o docker0 -j ACCEPT # 下面三个链目前啥规则也没有，就是简单的 RETURN，直接返回父链进行进一步匹配 -A DOCKER-ISOLATION-STAGE-1 -j RETURN -A DOCKER-ISOLATION-STAGE-2 -j RETURN -A DOCKER-USER -j RETURN 接下来使用如下 docker-compose 配置启动一个 caddy　容器，添加自定义 network 和端口映射，待会就能验证 docker 是如何实现这两种网络的了。 docker-compose.yml 内容： version: \"3.3\" services: caddy: image: \"caddy:2.2.1-alpine\" container_name: \"caddy\" restart: always command: caddy file-server --browse --root /data/static ports: - \"8081:80\" volumes: - \"/home/ryan/Downloads:/data/static\" networks: - caddy-1 networks: caddy-1: 现在先用上面的配置启动 caddy 容器，然后再查看网络状况： # 启动 caddy ❯ docker-compose up -d # 查下 caddy 容器的 ip \u003e docker inspect caddy | grep IPAddress ... \"IPAddress\": \"172.18.0.2\", # 查看网络接口，可以看到多了一个网桥，它就是上一行命令创建的 caddy-1 网络 # 还多了一个 veth，它连接到了 caddy 容器的 eth0(veth) 接口 ❯ ip addr ls ... 5: docker0: \u003cBROADCAST,MULTICAST,UP,LOWER_UP\u003e mtu 1500 qdisc noqueue state UP group default link/ether 02:42:42:c7:12:ba brd ff:ff:ff:ff:ff:ff inet 172.17.0.1/16 brd 172.17.255.255 scope global docker0 valid_lft forever preferred_lft forever inet6 fe80::42:42ff:fec7:12ba/64 scope link valid_lft forever preferred_lft forever 100: veth16b3","date":"2021-08-15","objectID":"/posts/iptables-and-container-networks/:2:0","series":null,"tags":["Linux","网络","虚拟化","容器","iptables","conntrack"],"title":"iptables 及 docker 容器网络分析","uri":"/posts/iptables-and-container-networks/#二容器网络实现原理---iptables--bridge--veth"},{"categories":["tech"],"content":" 三、Docker/Podman 的 macvlan/ipvlan 模式 注意：macvlan 和 wifi 好像不兼容，测试时不要使用无线网络的接口！ 我在前面介绍 Linux 虚拟网络接口的文章中，有介绍过 macvlan 和 ipvlan 两种新的虚拟接口。 目前 Podman/Docker 都支持使用 macvlan 来构建容器网络，这种模式下创建的容器直连外部网络，容器可以拥有独立的外部 IP，不需要端口映射，也不需要借助 iptables. 这和虚拟机的 Bridge 模式就很类似，主要适用于希望容器拥有独立外部 IP 的情况。 下面详细分析下 Docker 的 macvlan 网络（Podman 应该也完全类似）。 # 首先创建一个 macvlan 网络 # subnet/gateway 的参数需要和物理网络一致 # 通过 -o parent 设定父接口，我本机的以太网口名称为 eno1 $ docker network create -d macvlan \\ --subnet=192.168.31.0/24 \\ --gateway=192.168.31.1 \\ -o parent=eno1 \\ macnet0 # 现在使用 macvlan 启动一个容器试试 # 建议和我一样，通过 --ip 手动配置静态 ip 地址，当然不配也可以，DHCP 会自动分配 IP $ docker run --network macnet0 --ip=192.168.31.233 --rm -it buildpack-deps:buster-curl /bin/bash # 在容器中查看网络接口状况，能看到 eth0 是一个 macvlan 接口 root@4319488cb5e7:/# ip -d addr ls 1: lo: \u003cLOOPBACK,UP,LOWER_UP\u003e mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 promiscuity 0 minmtu 0 maxmtu 0 numtxqueues 1 numrxqueues 1 gso_max_size 65536 gso_max_segs 65535 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever 8: eth0@if2: \u003cBROADCAST,MULTICAST,UP,LOWER_UP\u003e mtu 1500 qdisc noqueue state UP group default link/ether 02:42:c0:a8:1f:e9 brd ff:ff:ff:ff:ff:ff link-netnsid 0 promiscuity 0 minmtu 68 maxmtu 9194 macvlan mode bridge numtxqueues 1 numrxqueues 1 gso_max_size 64000 gso_max_segs 64 inet 192.168.31.233/24 brd 192.168.31.255 scope global eth0 valid_lft forever preferred_lft forever # 路由表，默认 gateway 被自动配置进来了 root@4319488cb5e7:/# ip route ls default via 192.168.31.1 dev eth0 192.168.31.0/24 dev eth0 proto kernel scope link src 192.168.31.233 # 可以正常访问 baidu root@4319488cb5e7:/# curl baidu.com \u003chtml\u003e \u003cmeta http-equiv=\"refresh\" content=\"0;url=http://www.baidu.com/\"\u003e \u003c/html\u003e Docker 支持的另一种网络模式是 ipvlan（ipvlan 和 macvlan 的区别我在前一篇文章中已经介绍过，不再赘言），创建命令和 macvlan 几乎一样： # 首先创建一个 macvlan 网络 # subnet/gateway 的参数需要和物理网络一致 # 通过 -o parent 设定父接口，我本机的以太网口名称为 eno1 # ipvlan_mode 默认为 l2，表示工作在数据链路层。 $ docker network create -d ipvlan \\ --subnet=192.168.31.0/24 \\ --gateway=192.168.31.1 \\ -o parent=eno1 \\ -o ipvlan_mode=l2 \\ ipvnet0 # 现在使用 macvlan 启动一个容器试试 # 建议和我一样，通过 --ip 手动配置静态 ip 地址，当然不配也可以，DHCP 会自动分配 IP $ docker run --network ipvnet0 --ip=192.168.31.234 --rm -it buildpack-deps:buster-curl /bin/bash # 在容器中查看网络接口状况，能看到 eth0 是一个 ipvlan 接口 root@d0764ebbbf42:/# ip -d addr ls 1: lo: \u003cLOOPBACK,UP,LOWER_UP\u003e mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 promiscuity 0 minmtu 0 maxmtu 0 numtxqueues 1 numrxqueues 1 gso_max_size 65536 gso_max_segs 65535 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever 12: eth0@if2: \u003cBROADCAST,MULTICAST,UP,LOWER_UP\u003e mtu 1500 qdisc noqueue state UNKNOWN group default link/ether 38:f3:ab:a3:e6:71 brd ff:ff:ff:ff:ff:ff link-netnsid 0 promiscuity 0 minmtu 68 maxmtu 65535 ipvlan mode l2 bridge numtxqueues 1 numrxqueues 1 gso_max_size 64000 gso_max_segs 64 inet 192.168.31.234/24 brd 192.168.31.255 scope global eth0 valid_lft forever preferred_lft forever # 路由表，默认 gateway 被自动配置进来了 root@d0764ebbbf42:/# ip route ls default via 192.168.31.1 dev eth0 192.168.31.0/24 dev eth0 proto kernel scope link src 192.168.31.234 # 可以正常访问 baidu root@d0764ebbbf42:/# curl baidu.com \u003chtml\u003e \u003cmeta http-equiv=\"refresh\" content=\"0;url=http://www.baidu.com/\"\u003e \u003c/html\u003e ","date":"2021-08-15","objectID":"/posts/iptables-and-container-networks/:3:0","series":null,"tags":["Linux","网络","虚拟化","容器","iptables","conntrack"],"title":"iptables 及 docker 容器网络分析","uri":"/posts/iptables-and-container-networks/#三dockerpodman-的-macvlanipvlan-模式"},{"categories":["tech"],"content":" 四、Rootless 容器的网络实现如果容器运行时也在 Rootless 模式下运行，那它就没有权限在宿主机添加 bridge/veth 等虚拟网络接口，这种情况下，我们前面描述的容器网络就无法设置了。 那么 podman/containerd(nerdctl) 目前是如何在 Rootless 模式下构建容器网络的呢？ 查看文档，发现它们都用到了 rootlesskit 相关的东西，而 rootlesskit 提供了 rootless 网络的几个实现，文档参见 rootlesskit/docs/network.md 其中目前推荐使用，而且 podman/containerd(nerdctl) 都默认使用的方案，是 rootless-containers/slirp4netns 以 containerd(nerdctl) 为例，按官方文档安装好后，随便启动几个容器，然后在宿主机查 iptables/ip addr ls，会发现啥也没有。 这显然是因为 rootless 模式下 containerd 改不了宿主机的 iptables 配置和虚拟网络接口。但是可以查看到宿主机 slirp4netns 在后台运行： ❯ ps aux | grep tap ryan 11644 0.0 0.0 5288 3312 ? S 00:01 0:02 slirp4netns --mtu 65520 -r 3 --disable-host-loopback --enable-sandbox --enable-seccomp 11625 tap0 但是我看半天文档，只看到怎么使用 rootlesskit/slirp4netns 创建新的名字空间，没看到有介绍如何进入一个已存在的 slirp4netns 名字空间… 使用 nsenter -a -t 11644 也一直报错，任何程序都是 no such binary… 以后有空再重新研究一波… 总之能确定的是，它通过在虚拟的名字空间中创建了一个 tap 虚拟接口来实现容器网络，性能相比前面介绍的网络多少是要差一点的。 ","date":"2021-08-15","objectID":"/posts/iptables-and-container-networks/:4:0","series":null,"tags":["Linux","网络","虚拟化","容器","iptables","conntrack"],"title":"iptables 及 docker 容器网络分析","uri":"/posts/iptables-and-container-networks/#四rootless-容器的网络实现"},{"categories":["tech"],"content":" 五、nftables前面介绍了 iptables 以及其在 docker 和防火墙上的应用。但是实际上目前各大 Linux 发行版都已经不建议使用 iptables 了，甚至把 iptables 重命名为了 iptables-leagacy. 目前 opensuse/debian/opensuse 都已经预装了并且推荐使用 nftables，而且 firewalld 已经默认使用 nftables 作为它的后端了。 我在 opensuse tumbleweed 上实测，firewalld 添加的是 nftables 配置，而 docker 仍然在用旧的 iptables，也就是说我现在的机器上有两套 netfilter 工具并存： # 查看 iptables 数据 \u003e iptables -S -P INPUT ACCEPT -P FORWARD DROP -P OUTPUT ACCEPT -N DOCKER -N DOCKER-ISOLATION-STAGE-1 -N DOCKER-ISOLATION-STAGE-2 -N DOCKER-USER -A FORWARD -j DOCKER-ISOLATION-STAGE-1 -A FORWARD -o br-e3fbbb7a1b3a -m conntrack --ctstate RELATED,ESTABLISHED -j ACCEPT -A FORWARD -o br-e3fbbb7a1b3a -j DOCKER ... # 确认下是否使用了 nftables 的兼容层，结果提示请我使用 iptables-legacy \u003e iptables-nft -S # Warning: iptables-legacy tables present, use iptables-legacy to see them -P INPUT ACCEPT -P FORWARD ACCEPT -P OUTPUT ACCEPT # 查看 nftables 规则，能看到三张 firewalld 生成的 table \u003e nft list ruleset table inet firewalld { ... } table ip firewalld { ... } table ip6 firewalld { ... } 但是现在 kubernetes/docker 都还是用的 iptables，nftables 我学了用处不大，以后有空再补充。 ","date":"2021-08-15","objectID":"/posts/iptables-and-container-networks/:5:0","series":null,"tags":["Linux","网络","虚拟化","容器","iptables","conntrack"],"title":"iptables 及 docker 容器网络分析","uri":"/posts/iptables-and-container-networks/#五nftables"},{"categories":["tech"],"content":" 参考 iptables详解（1）：iptables概念 连接跟踪（conntrack）：原理、应用及 Linux 内核实现 网络地址转换（NAT）之报文跟踪 容器安全拾遗 - Rootless Container初探 netfilter - wikipedia ","date":"2021-08-15","objectID":"/posts/iptables-and-container-networks/:6:0","series":null,"tags":["Linux","网络","虚拟化","容器","iptables","conntrack"],"title":"iptables 及 docker 容器网络分析","uri":"/posts/iptables-and-container-networks/#参考"},{"categories":["tech"],"content":" 本文用到的字符画工具：vscode-asciiflow2 注意: 本文中使用 ip 命令创建或修改的任何网络配置，都是未持久化的，主机重启即消失。 Linux 具有强大的虚拟网络能力，这也是 openstack 网络、docker 容器网络以及 kubernetes 网络等虚拟网络的基础。 这里介绍 Linux 常用的虚拟网络接口类型：TUN/TAP、bridge、veth、ipvlan/macvlan、vlan 以及 vxlan/geneve. ","date":"2021-08-14","objectID":"/posts/linux-virtual-network-interfaces/:0:0","series":null,"tags":["Linux","网络","虚拟化","容器"],"title":"Linux 中的虚拟网络接口","uri":"/posts/linux-virtual-network-interfaces/#"},{"categories":["tech"],"content":" 一、tun/tap 虚拟网络接口tun/tap 是操作系统内核中的虚拟网络设备，他们为用户层程序提供数据的接收与传输。 普通的物理网络接口如 eth0，它的两端分别是内核协议栈和外面的物理网络。 而对于 TUN/TAP 虚拟接口如 tun0，它的一端一定是连接的用户层程序，另一端则视配置方式的不同而变化，可以直连内核协议栈，也可以是某个 bridge（后面会介绍）。 Linux 通过内核模块 TUN 提供 tun/tap 功能，该模块提供了一个设备接口 /dev/net/tun 供用户层程序读写，用户层程序通过 /dev/net/tun 读写主机内核协议栈的数据。 \u003e modinfo tun filename: /lib/modules/5.13.6-1-default/kernel/drivers/net/tun.ko.xz alias: devname:net/tun alias: char-major-10-200 license: GPL author: (C) 1999-2004 Max Krasnyansky \u003cmaxk@qualcomm.com\u003e description: Universal TUN/TAP device driver ... \u003e ls /dev/net/tun /dev/net/tun 一个 TUN 设备的示例图如下： +----------------------------------------------------------------------+ | | | +--------------------+ +--------------------+ | | | User Application A | | User Application B +\u003c-----+ | | +------------+-------+ +-------+------------+ | | | | 1 | 5 | | |...............+......................+...................|...........| | ↓ ↓ | | | +----------+ +----------+ | | | | socket A | | socket B | | | | +-------+--+ +--+-------+ | | | | 2 | 6 | | |.................+.................+......................|...........| | ↓ ↓ | | | +------------------------+ +--------+-------+ | | | Network Protocol Stack | | /dev/net/tun | | | +--+-------------------+-+ +--------+-------+ | | | 7 | 3 ^ | |................+...................+.....................|...........| | ↓ ↓ | | | +----------------+ +----------------+ 4 | | | | eth0 | | tun0 | | | | +-------+--------+ +-----+----------+ | | | 10.32.0.11 | | 192.168.3.11 | | | | 8 +---------------------+ | | | | +----------------+-----------------------------------------------------+ ↓ Physical Network 因为 TUN/TAP 设备的一端是内核协议栈，显然流入 tun0 的数据包是先经过本地的路由规则匹配的。 路由匹配成功，数据包被发送到 tun0 后，tun0 发现另一端是通过 /dev/net/tun 连接到应用程序 B，就会将数据丢给应用程序 B。 应用程序对数据包进行处理后，可能会构造新的数据包，通过物理网卡发送出去。比如常见的 VPN 程序就是把原来的数据包封装/加密一遍，再发送给 VPN 服务器。 ","date":"2021-08-14","objectID":"/posts/linux-virtual-network-interfaces/:1:0","series":null,"tags":["Linux","网络","虚拟化","容器"],"title":"Linux 中的虚拟网络接口","uri":"/posts/linux-virtual-network-interfaces/#一tuntap-虚拟网络接口"},{"categories":["tech"],"content":" C 语言编程测试 TUN 设备为了使用 tun/tap 设备，用户层程序需要通过系统调用打开 /dev/net/tun 获得一个读写该设备的文件描述符(FD)，并且调用 ioctl() 向内核注册一个 TUN 或 TAP 类型的虚拟网卡(实例化一个 tun/tap 设备)，其名称可能是 tun0/tap0 等。 此后，用户程序可以通过该 TUN/TAP 虚拟网卡与主机内核协议栈（或者其他网络设备）交互。当用户层程序关闭后，其注册的 TUN/TAP 虚拟网卡以及自动生成的路由表相关条目都会被内核释放。 可以把用户层程序看做是网络上另一台主机，他们通过 tun/tap 虚拟网卡相连。 一个简单的 C 程序示例如下，它每次收到数据后，都只单纯地打印一下收到的字节数： #include \u003clinux/if.h\u003e #include \u003clinux/if_tun.h\u003e #include \u003csys/ioctl.h\u003e #include \u003cfcntl.h\u003e #include \u003cstring.h\u003e #include \u003cunistd.h\u003e #include\u003cstdlib.h\u003e #include\u003cstdio.h\u003e int tun_alloc(int flags) { struct ifreq ifr; int fd, err; char *clonedev = \"/dev/net/tun\"; // 打开 tun 文件，获得 fd if ((fd = open(clonedev, O_RDWR)) \u003c 0) { return fd; } memset(\u0026ifr, 0, sizeof(ifr)); ifr.ifr_flags = flags; // 向内核注册一个 TUN 网卡，并与前面拿到的 fd 关联起来 // 程序关闭时，注册的 tun 网卡及自动生成的相关路由策略，会被自动释放 if ((err = ioctl(fd, TUNSETIFF, (void *) \u0026ifr)) \u003c 0) { close(fd); return err; } printf(\"Open tun/tap device: %s for reading...\\n\", ifr.ifr_name); return fd; } int main() { int tun_fd, nread; char buffer[1500]; /* Flags: IFF_TUN - TUN device (no Ethernet headers) * IFF_TAP - TAP device * IFF_NO_PI - Do not provide packet information */ tun_fd = tun_alloc(IFF_TUN | IFF_NO_PI); if (tun_fd \u003c 0) { perror(\"Allocating interface\"); exit(1); } while (1) { nread = read(tun_fd, buffer, sizeof(buffer)); if (nread \u003c 0) { perror(\"Reading from interface\"); close(tun_fd); exit(1); } printf(\"Read %d bytes from tun/tap device\\n\", nread); } return 0; } 接下来开启三个终端窗口来测试上述程序，分别运行上面的 tun 程序、tcpdump 和 iproute2 指令。 首先通过编译运行上述 c 程序，程序会阻塞住，等待数据到达： # 编译，请忽略部分 warning \u003e gcc mytun.c -o mytun # 创建并监听 tun 设备需要 root 权限 \u003e sudo mytun Open tun/tap device: tun0 for reading... 现在使用 iproute2 查看下链路层设备： # 能发现最后面有列出名为 tun0 的接口，但是状态为 down ❯ ip addr ls ...... 3: wlp4s0: \u003cBROADCAST,MULTICAST,UP,LOWER_UP\u003e mtu 1500 qdisc noqueue state UP group default qlen 1000 link/ether c0:3c:59:36:a4:16 brd ff:ff:ff:ff:ff:ff inet 192.168.31.228/24 brd 192.168.31.255 scope global dynamic noprefixroute wlp4s0 valid_lft 41010sec preferred_lft 41010sec inet6 fe80::4ab0:130f:423b:5d37/64 scope link noprefixroute valid_lft forever preferred_lft forever 7: tun0: \u003cPOINTOPOINT,MULTICAST,NOARP\u003e mtu 1500 qdisc noop state DOWN group default qlen 500 link/none # 为 tun0 设置 ip 地址，注意不要和其他接口在同一网段，会导致路由冲突 \u003e sudo ip addr add 172.21.22.23/24 dev tun0 # 启动 tun0 这个接口，这一步会自动向路由表中添加将 172.21.22.23/24 路由到 tun0 的策略 \u003e sudo ip link set tun0 up #确认上一步添加的路由策略是否存在 ❯ ip route ls default via 192.168.31.1 dev wlp4s0 proto dhcp metric 600 172.17.0.0/16 dev docker0 proto kernel scope link src 172.17.0.1 linkdown 172.21.22.0/24 dev tun0 proto kernel scope link src 172.21.22.23 192.168.31.0/24 dev wlp4s0 proto kernel scope link src 192.168.31.228 metric 600 # 此时再查看接口，发现 tun0 状态为 unknown \u003e ip addr ls ...... 8: tun0: \u003cPOINTOPOINT,MULTICAST,NOARP,UP,LOWER_UP\u003e mtu 1500 qdisc pfifo_fast state UNKNOWN group default qlen 500 link/none inet 172.21.22.23/24 scope global tun0 valid_lft forever preferred_lft forever inet6 fe80::3d52:49b5:1cf3:38fd/64 scope link stable-privacy valid_lft forever preferred_lft forever # 使用 tcpdump 尝试抓下 tun0 的数据，会阻塞在这里，等待数据到达 \u003e tcpdump -i tun0 现在再启动第三个窗口发点数据给 tun0，持续观察前面 tcpdump 和 mytun 的日志: # 直接 ping tun0 的地址，貌似有问题，数据没进 mytun 程序，而且还有响应 ❯ ping -c 4 172.21.22.23 PING 172.21.22.23 (172.21.22.23) 56(84) bytes of data. 64 bytes from 172.21.22.23: icmp_seq=1 ttl=64 time=0.167 ms 64 bytes from 172.21.22.23: icmp_seq=2 ttl=64 time=0.180 ms 64 bytes from 172.21.22.23: icmp_seq=3 ttl=64 time=0.126 ms 64 bytes from 172.21.22.23: icmp_seq=4 ttl=64 time=0.141 ms --- 172.21.22.23 ping statistics --- 4 packets transmitted, 4 received, 0% packet loss, time 3060ms rtt min/avg/max/mdev = 0.126/0.153/0.180/0.021 ms # 但是 ping 该网段下的其他地址，流量就会被转发给 mytun 程序，因为 mytun 啥数据也没回，自然丢包率 100% # tcpdump 和 mytun 都会打印出相关日志 ❯ ping -c 4 172.21.22.26 PING 172.21.22.26 (172.21.22.26) 56(84) bytes of data. --- 172.21.22.26 ping statistics --- 4 packets transmitted, 0 received, 100% packet loss, time 3055ms 下面给出 mytun 的输出： Read 84 bytes from tun/tap device Read 84 bytes","date":"2021-08-14","objectID":"/posts/linux-virtual-network-interfaces/:1:1","series":null,"tags":["Linux","网络","虚拟化","容器"],"title":"Linux 中的虚拟网络接口","uri":"/posts/linux-virtual-network-interfaces/#c-语言编程测试-tun-设备"},{"categories":["tech"],"content":" TUN 与 TAP 的区别TUN 和 TAP 的区别在于工作的网络层次不同，用户程序通过 TUN 设备只能读写网络层的 IP 数据包，而 TAP 设备则支持读写链路层的数据包（通常是以太网数据包，带有 Ethernet headers）。 TUN 与 TAP 的关系，就类似于 socket 和 raw socket. TUN/TAP 应用最多的场景是 VPN 代理，比如: clash: 一个支持各种规则的隧道，也支持 TUN 模式 tun2socks: 一个全局透明代理，和 VPN 的工作模式一样，它通过创建虚拟网卡+修改路由表，在第三层网络层代理系统流量。 ","date":"2021-08-14","objectID":"/posts/linux-virtual-network-interfaces/:1:2","series":null,"tags":["Linux","网络","虚拟化","容器"],"title":"Linux 中的虚拟网络接口","uri":"/posts/linux-virtual-network-interfaces/#tun-与-tap-的区别"},{"categories":["tech"],"content":" 二、vethveth 接口总是成对出现，一对 veth 接口就类似一根网线，从一端进来的数据会从另一端出去。 同时 veth 又是一个虚拟网络接口，因此它和 TUN/TAP 或者其他物理网络接口一样，也都能配置 mac/ip 地址（但是并不是一定得配 mac/ip 地址）。 其主要作用就是连接不同的网络，比如在容器网络中，用于将容器的 namespace 与 root namespace 的网桥 br0 相连。 容器网络中，容器侧的 veth 自身设置了 ip/mac 地址并被重命名为 eth0，作为容器的网络接口使用，而主机侧的 veth 则直接连接在 docker0/br0 上面。 使用 veth 实现容器网络，需要结合下一小节介绍的 bridge，在下一小节将给出容器网络结构图。 ","date":"2021-08-14","objectID":"/posts/linux-virtual-network-interfaces/:2:0","series":null,"tags":["Linux","网络","虚拟化","容器"],"title":"Linux 中的虚拟网络接口","uri":"/posts/linux-virtual-network-interfaces/#二veth"},{"categories":["tech"],"content":" 三、bridgeLinux Bridge 是工作在链路层的网络交换机，由 Linux 内核模块 brige 提供，它负责在所有连接到它的接口之间转发链路层数据包。 添加到 Bridge 上的设备被设置为只接受二层数据帧并且转发所有收到的数据包到 Bridge 中。 在 Bridge 中会进行一个类似物理交换机的查MAC端口映射表、转发、更新MAC端口映射表这样的处理逻辑，从而数据包可以被转发到另一个接口/丢弃/广播/发往上层协议栈，由此 Bridge 实现了数据转发的功能。 如果使用 tcpdump 在 Bridge 接口上抓包，可以抓到网桥上所有接口进出的包，因为这些数据包都要通过网桥进行转发。 与物理交换机不同的是，Bridge 本身可以设置 IP 地址，可以认为当使用 brctl addbr br0 新建一个 br0 网桥时，系统自动创建了一个同名的隐藏 br0 网络接口。br0 一旦设置 IP 地址，就意味着这个隐藏的 br0 接口可以作为路由接口设备，参与 IP 层的路由选择(可以使用 route -n 查看最后一列 Iface)。因此只有当 br0 设置 IP 地址时，Bridge 才有可能将数据包发往上层协议栈。 但被添加到 Bridge 上的网卡是不能配置 IP 地址的，他们工作在数据链路层，对路由系统不可见。 它常被用于在虚拟机、主机上不同的 namepsaces 之间转发数据。 ","date":"2021-08-14","objectID":"/posts/linux-virtual-network-interfaces/:3:0","series":null,"tags":["Linux","网络","虚拟化","容器"],"title":"Linux 中的虚拟网络接口","uri":"/posts/linux-virtual-network-interfaces/#三bridge"},{"categories":["tech"],"content":" 虚拟机场景（桥接模式）以 qemu-kvm 为例，在虚拟机的桥接模式下，qemu-kvm 会为每个虚拟机创建一个 tun/tap 虚拟网卡并连接到 br0 网桥。 虚拟机内部的网络接口 eth0 是 qemu-kvm 软件模拟的，实际上虚拟机内网络数据的收发都会被 qemu-kvm 转换成对 /dev/net/tun 的读写。 以发送数据为例，整个流程如下： 虚拟机发出去的数据包先到达 qemu-kvm 程序 数据被用户层程序 qemu-kvm 写入到 /dev/net/tun，到达 tap 设备 tap 设备把数据传送到 br0 网桥 br0 把数据交给 eth0 发送出去 整个流程跑完，数据包都不需要经过宿主机的协议栈，效率高。 +------------------------------------------------+-----------------------------------+-----------------------------------+ | Host | VirtualMachine1 | VirtualMachine2 | | | | | | +--------------------------------------+ | +-------------------------+ | +-------------------------+ | | | Network Protocol Stack | | | Network Protocol Stack | | | Network Protocol Stack | | | +--------------------------------------+ | +-------------------------+ | +-------------------------+ | | ↑ | ↑ | ↑ | |.......................|........................|................|..................|.................|.................| | ↓ | ↓ | ↓ | | +--------+ | +-------+ | +-------+ | | | .3.101 | | | .3.102| | | .3.103| | | +------+ +--------+ +-------+ | +-------+ | +-------+ | | | eth0 |\u003c---\u003e| br0 |\u003c---\u003e|tun/tap| | | eth0 | | | eth0 | | | +------+ +--------+ +-------+ | +-------+ | +-------+ | | ↑ ↑ ↑ +--------+ ↑ | ↑ | | | | +------|qemu-kvm|-----------+ | | | | | ↓ +--------+ | | | | | +-------+ | | | | | | |tun/tap| | | | | | | +-------+ | | | | | | ↑ | +--------+ | | | | | +-------------------------------------|qemu-kvm|-------------|-----------------+ | | | | +--------+ | | | | | | | +---------|--------------------------------------+-----------------------------------+-----------------------------------+ ↓ Physical Network (192.168.3.0/24) ","date":"2021-08-14","objectID":"/posts/linux-virtual-network-interfaces/:3:1","series":null,"tags":["Linux","网络","虚拟化","容器"],"title":"Linux 中的虚拟网络接口","uri":"/posts/linux-virtual-network-interfaces/#虚拟机场景桥接模式"},{"categories":["tech"],"content":" 跨 namespace 通信场景（容器网络，NAT 模式） docker/podman 提供的 bridge 网络模式，就是使用 veth+bridge+iptalbes 实现的。我会在下一篇文章详细介绍「容器网络」。 由于容器运行在自己单独的 network namespace 里面，所以和虚拟机一样，它们也都有自己单独的协议栈。 容器网络的结构和虚拟机差不多，但是它改用了 NAT 网络，并把 tun/tap 换成了 veth，导致 docker0 过来的数据，要先经过宿主机协议栈，然后才进入 veth 接口。 多了一层 NAT，以及多走了一层宿主机协议栈，都会导致性能下降。 示意图如下： +-----------------------------------------------+-----------------------------------+-----------------------------------+ | Host | Container 1 | Container 2 | | | | | | +---------------------------------------+ | +-------------------------+ | +-------------------------+ | | | Network Protocol Stack | | | Network Protocol Stack | | | Network Protocol Stack | | | +----+-------------+--------------------+ | +-----------+-------------+ | +------------+------------+ | | ^ ^ | ^ | ^ | |........|.............|........................|................|..................|.................|.................| | v v ↓ | v | v | | +----+----+ +-----+------+ | +-----+-------+ | +-----+-------+ | | | .31.101 | | 172.17.0.1 | +------+ | | 172.17.0.2 | | | 172.17.0.3 | | | +---------+ +-------------\u003c----\u003e+ veth | | +-------------+ | +-------------+ | | | eth0 | | docker0 | +--+---+ | | eth0(veth) | | | eth0(veth) | | | +----+----+ +-----+------+ ^ | +-----+-------+ | +-----+-------+ | | ^ ^ | | ^ | ^ | | | | +------------------------+ | | | | | v | | | | | | +--+---+ | | | | | | | veth | | | | | | | +--+---+ | | | | | | ^ | | | | | | +------------------------------------------------------------------------------+ | | | | | | | | | | | +-----------------------------------------------+-----------------------------------+-----------------------------------+ v Physical Network (192.168.31.0/24) 每创建一个新容器，都会在容器的 namespace 里新建一个 veth 接口并命令为 eth0，同时在主 namespace 创建一个 veth，将容器的 eth0 与 docker0 连接。 可以在容器中通过 iproute2 查看到， eth0 的接口类型为 veth： ❯ docker run -it --rm debian:buster bash root@5facbe4ddc1e:/# ip --details addr ls 1: lo: \u003cLOOPBACK,UP,LOWER_UP\u003e mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 promiscuity 0 minmtu 0 maxmtu 0 numtxqueues 1 numrxqueues 1 gso_max_size 65536 gso_max_segs 65535 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever 20: eth0@if21: \u003cBROADCAST,MULTICAST,UP,LOWER_UP\u003e mtu 1500 qdisc noqueue state UP group default link/ether 02:42:ac:11:00:02 brd ff:ff:ff:ff:ff:ff link-netnsid 0 promiscuity 0 minmtu 68 maxmtu 65535 veth numtxqueues 1 numrxqueues 1 gso_max_size 65536 gso_max_segs 65535 inet 172.17.0.2/16 brd 172.17.255.255 scope global eth0 valid_lft forever preferred_lft forever 同时在宿主机中能看到对应的 veth 设备是绑定到了 docker0 网桥的： ❯ sudo brctl show bridge name bridge id STP enabled interfaces docker0 8000.0242fce99ef5 no vethea4171a ","date":"2021-08-14","objectID":"/posts/linux-virtual-network-interfaces/:3:2","series":null,"tags":["Linux","网络","虚拟化","容器"],"title":"Linux 中的虚拟网络接口","uri":"/posts/linux-virtual-network-interfaces/#跨-namespace-通信场景容器网络nat-模式"},{"categories":["tech"],"content":" 四、macvlan 目前 docker/podman 都支持创建基于 macvlan 的 Linux 容器网络。 注意 macvlan 和 WiFi 存在兼容问题，如果使用笔记本测试，可能会遇到麻烦。 参考文档：linux 网络虚拟化： macvlan macvlan 是比较新的 Linux 特性，需要内核版本 \u003e= 3.9，它被用于在主机的网络接口（父接口）上配置多个虚拟子接口，这些子接口都拥有各自独立的 mac 地址，也可以配上 ip 地址进行通讯。 macvlan 下的虚拟机或者容器网络和主机在同一个网段中，共享同一个广播域。macvlan 和 bridge 比较相似，但因为它省去了 bridge 的存在，所以配置和调试起来比较简单，而且效率也相对高。除此之外，macvlan 自身也完美支持 VLAN。 如果希望容器或者虚拟机放在主机相同的网络中，享受已经存在网络栈的各种优势，可以考虑 macvlan。 我会在下一篇文章对 docker 的 macvlan/ipvlan 做个分析，这里先略过了… ","date":"2021-08-14","objectID":"/posts/linux-virtual-network-interfaces/:4:0","series":null,"tags":["Linux","网络","虚拟化","容器"],"title":"Linux 中的虚拟网络接口","uri":"/posts/linux-virtual-network-interfaces/#四macvlan"},{"categories":["tech"],"content":" 五、ipvlan linux 网络虚拟化： ipvlan cilium 1.9 已经提供了基于 ipvlan 的网络（beta 特性），用于替换传统的 veth+bridge 容器网络。详见 IPVLAN based Networking (beta) - Cilium 1.9 Docs ipvlan 和 macvlan 的功能很类似，也是用于在主机的网络接口（父接口）上配置出多个虚拟的子接口。但不同的是，ipvlan 的各子接口没有独立的 mac 地址，它们和主机的父接口共享 mac 地址。 因为 mac 地址共享，所以如果使用 DHCP，就要注意不能使用 mac 地址做 DHCP，需要额外配置唯一的 clientID. 如果你遇到以下的情况，请考虑使用 ipvlan： 父接口对 mac 地址数目有限制，或者在 mac 地址过多的情况下会造成严重的性能损失 工作在 802.11(wireless)无线网络中（macvlan 无法和无线网络共同工作） 希望搭建比较复杂的网络拓扑（不是简单的二层网络和 VLAN），比如要和 BGP 网络一起工作 基于 ipvlan/macvlan 的容器网络，比 veth+bridge+iptables 的性能要更高。 我会在下一篇文章对 docker 的 macvlan/ipvlan 做个分析，这里先略过了… ","date":"2021-08-14","objectID":"/posts/linux-virtual-network-interfaces/:5:0","series":null,"tags":["Linux","网络","虚拟化","容器"],"title":"Linux 中的虚拟网络接口","uri":"/posts/linux-virtual-network-interfaces/#五ipvlan"},{"categories":["tech"],"content":" 六、vlanvlan 即虚拟局域网，是一个链路层的广播域隔离技术，可以用于切分局域网，解决广播泛滥和安全性问题。被隔离的广播域之间需要上升到第三层才能完成通讯。 常用的企业路由器如 ER-X 基本都可以设置 vlan，Linux 也直接支持了 vlan. 以太网数据包有一个专门的字段提供给 vlan 使用，vlan 数据包会在该位置记录它的 VLAN ID，交换机通过该 ID 来区分不同的 VLAN，只将该以太网报文广播到该 ID 对应的 VLAN 中。 ","date":"2021-08-14","objectID":"/posts/linux-virtual-network-interfaces/:6:0","series":null,"tags":["Linux","网络","虚拟化","容器"],"title":"Linux 中的虚拟网络接口","uri":"/posts/linux-virtual-network-interfaces/#六vlan"},{"categories":["tech"],"content":" 七、vxlan/geneve rfc8926 - Geneve: Generic Network Virtualization Encapsulation rfc7348 - Virtual eXtensible Local Area Network (VXLAN) linux 上实现 vxlan 网络 在介绍 vxlan 前，先说明下两个名词的含义： underlay 网络：即物理网络 overlay 网络：指在现有的物理网络之上构建的虚拟网络。其实就是一种隧道技术，将原生态的二层数据帧报文进行封装后通过隧道进行传输。 vxlan 与 geneve 都是 overlay 网络协议，它俩都是使用 UDP 包来封装链路层的以太网帧。 vxlan 在 2014 年标准化，而 geneve 在 2020 年底才通过草案阶段，目前尚未形成最终标准。但是目前 linux/cilium 都已经支持了 geneve. geneve 相对 vxlan 最大的变化，是它更灵活——它的 header 长度是可变的。 目前所有 overlay 的跨主机容器网络方案，几乎都是基于 vxlan 实现的（例外：cilium 也支持 geneve）。 我们在学习单机的容器网络时，不需要接触到 vxlan，但是在学习跨主机容器网络方案如 flannel/calico/cilium 时，那 vxlan(overlay) 及 BGP(underlay) 就不可避免地要接触了。 先介绍下 vxlan 的数据包结构： VXLAN 栈帧结构 在创建 vxlan 的 vtep 虚拟设备时，我们需要手动设置图中的如下属性： VXLAN 目标端口：即接收方 vtep 使用的端口，这里 IANA 定义的端口是 4789，但是只有 calico 的 vxlan 模式默认使用该端口 calico，而 cilium/flannel 的默认端口都是 Linux 默认的 8472. VNID: 每个 VXLAN 网络接口都会被分配一个独立的 VNID 一个点对点的 vxlan 网络架构图如下: VXLAN 点对点网络架构 可以看到每台虚拟机 VM 都会被分配一个唯一的 VNID，然后两台物理机之间通过 VTEP 虚拟网络设备建立了 VXLAN 隧道，所有 VXLAN 网络中的虚拟机，都通过 VTEP 来互相通信。 有了上面这些知识，我们就可以通过如下命令在两台 Linux 机器间建立一个点对点的 VXLAN 隧道： # 在主机 A 上创建 VTEP 设备 vxlan0 # 与另一个 vtep 接口 B（192.168.8.101）建立隧道 # 将 vxlan0 自身的 IP 地址设为 192.168.8.100 # 使用的 VXLAN 目标端口为 4789(IANA 标准) ip link add vxlan0 type vxlan \\ id 42 \\ dstport 4789 \\ remote 192.168.8.101 \\ local 192.168.8.100 \\ dev enp0s8 # 为我们的 VXLAN 网络设置虚拟网段，vxlan0 就是默认网关 ip addr add 10.20.1.2/24 dev vxlan0 # 启用我们的 vxlan0 设备，这会自动生成路由规则 ip link set vxlan0 up # 现在在主机 B 上运行如下命令，同样创建一个 VTEP 设备 vxlan0，remote 和 local 的 ip 与前面用的命令刚好相反。 # 注意 VNID 和 dstport 必须和前面完全一致 ip link add vxlan0 type vxlan \\ id 42 \\ dstport 4789 \\ remote 192.168.8.100 \\ local 192.168.8.101 \\ dev enp0s8 # 为我们的 VXLAN 网络设置虚拟网段，vxlan0 就是默认网关 ip addr add 10.20.1.3/24 dev vxlan0 ip link set vxlan0 up # 到这里，两台机器就完成连接，可以通信了。可以在主机 B 上 ping 10.20.1.2 试试，应该能收到主机 A 的回应。 ping 10.20.1.2 点对点的 vxlan 隧道实际用处不大，如果集群中的每个节点都互相建 vxlan 隧道，代价太高了。 一种更好的方式，是使用 「组播模式」的 vxlan 隧道，这种模式下一个 vtep 可以一次与组内的所有 vtep 建立隧道。 示例命令如下（这里略过了如何设置组播地址 239.1.1.1 的信息）： ip link add vxlan0 type vxlan \\ id 42 \\ dstport 4789 \\ group 239.1.1.1 \\ dev enp0s8 ip addr add 10.20.1.2/24 dev vxlan0 ip link set vxlan0 up 可以看到，只需要简单地把 local_ip/remote_ip 替换成一个组播地址就行。组播功能会将收到的数据包发送给组里的所有 vtep 接口，但是只有 VNID 能对上的 vtep 会处理该报文，其他 vtep 会直接丢弃数据。 接下来，为了能让所有的虚拟机/容器，都通过 vtep 通信，我们再添加一个 bridge 网络，充当 vtep 与容器间的交换机。架构如下： VXLAN 多播网络架构 使用 ip 命令创建网桥、网络名字空间、veth pairs 组成上图中的容器网络： # 创建 br0 并将 vxlan0 绑定上去 ip link add br0 type bridge ip link set vxlan0 master br0 ip link set vxlan0 up ip link set br0 up # 模拟将容器加入到网桥中的操作 ip netns add container1 ## 创建 veth pair，并把一端加到网桥上 ip link add veth0 type veth peer name veth1 ip link set dev veth0 master br0 ip link set dev veth0 up ## 配置容器内部的网络和 IP ip link set dev veth1 netns container1 ip netns exec container1 ip link set lo up ip netns exec container1 ip link set veth1 name eth0 ip netns exec container1 ip addr add 10.20.1.11/24 dev eth0 ip netns exec container1 ip link set eth0 up 然后在另一台机器上做同样的操作，并创建新容器，两个容器就能通过 vxlan 通信啦~ ","date":"2021-08-14","objectID":"/posts/linux-virtual-network-interfaces/:7:0","series":null,"tags":["Linux","网络","虚拟化","容器"],"title":"Linux 中的虚拟网络接口","uri":"/posts/linux-virtual-network-interfaces/#vxlan-geneve"},{"categories":["tech"],"content":" 比组播更高效的 vxlan 实现组播最大的问题在于，因为它不知道数据的目的地，所以每个 vtep 都发了一份。如果每次发数据时，如果能够精确到对应的 vtep，就能节约大量资源。 另一个问题是 ARP 查询也会被组播，要知道 vxlan 本身就是个 overlay 网络，ARP 的成本也很高。 上述问题都可以通过一个中心化的注册中心（如 etcd）来解决，所有容器、网络的注册与变更，都写入到这个注册中心，然后由程序自动维护 vtep 之间的隧道、fdb 表及 ARP 表. ","date":"2021-08-14","objectID":"/posts/linux-virtual-network-interfaces/:7:1","series":null,"tags":["Linux","网络","虚拟化","容器"],"title":"Linux 中的虚拟网络接口","uri":"/posts/linux-virtual-network-interfaces/#比组播更高效的-vxlan-实现"},{"categories":["tech"],"content":" 八、虚拟网络接口的速率Loopback 和本章讲到的其他虚拟网络接口一样，都是一种软件模拟的网络设备。 他们的速率是不是也像物理链路一样，存在链路层（比如以太网）协议的带宽限制呢？ 比如目前很多老旧的网络设备，都是只支持到百兆以太网，这就决定了它的带宽上限。 即使是较新的设备，目前基本也都只支持到千兆，也就是 1GbE 以太网标准，那本文提到的虚拟网络接口单纯在本机内部通信，是否也存在这样的制约呢？是否也只能跑到 1GbE? 另外物理网络还存在链路层协议协商机制，将一个千兆接口与一个百兆接口连接，它们会自动协商使用百兆以太网标准进行通讯。虚拟网络接口是否也存在这样的机制呢？ 先使用 ethtool 检查看看： # docker 容器的 veth 接口速率 \u003e ethtool vethe899841 | grep Speed Speed: 10000Mb/s # 网桥看起来没有固定的速率 \u003e ethtool docker0 | grep Speed Speed: Unknown! # tun0 设备的默认速率貌似是 10Mb/s ? \u003e ethtool tun0 | grep Speed Speed: 10Mb/s # 此外 ethtool 无法检查 lo 以及 wifi 的速率，先略过不提 从上面的输出能看到，虚拟接口的 Speed 属性都有点离谱，veth 接口显示 10Gb/s，tun0 更是离谱的 10Mb/s. 那么事实真的如此么？话不多说，先实测一波。 ","date":"2021-08-14","objectID":"/posts/linux-virtual-network-interfaces/:8:0","series":null,"tags":["Linux","网络","虚拟化","容器"],"title":"Linux 中的虚拟网络接口","uri":"/posts/linux-virtual-network-interfaces/#八虚拟网络接口的速率"},{"categories":["tech"],"content":" 网络性能实测接下来实际测试一下，受先给出测试机的配置： ❯ cat /etc/os-release NAME=\"openSUSE Tumbleweed\" # VERSION=\"20210810\" ... ❯ uname -a Linux legion-book 5.13.8-1-default #1 SMP Thu Aug 5 08:56:22 UTC 2021 (967c6a8) x86_64 x86_64 x86_64 GNU/Linux ❯ lscpu Architecture: x86_64 CPU(s): 16 Model name: AMD Ryzen 7 5800H with Radeon Graphics ... # 内存，单位 MB ❯ free -m total used free shared buff/cache available Mem: 27929 4482 17324 249 6122 22797 Swap: 2048 0 2048 好了，现在使用 iperf3 测试： # 启动服务端 iperf3 -s ------------- # 新窗口启动客户端，通过 loopback 接口访问 iperf3-server，大概 49Gb/s ❯ iperf3 -c 127.0.0.1 Connecting to host 127.0.0.1, port 5201 [ 5] local 127.0.0.1 port 48656 connected to 127.0.0.1 port 5201 [ ID] Interval Transfer Bitrate Retr Cwnd [ 5] 0.00-1.00 sec 4.46 GBytes 38.3 Gbits/sec 0 1.62 MBytes [ 5] 1.00-2.00 sec 4.61 GBytes 39.6 Gbits/sec 0 1.62 MBytes [ 5] 2.00-3.00 sec 5.69 GBytes 48.9 Gbits/sec 0 1.62 MBytes [ 5] 3.00-4.00 sec 6.11 GBytes 52.5 Gbits/sec 0 1.62 MBytes [ 5] 4.00-5.00 sec 6.04 GBytes 51.9 Gbits/sec 0 1.62 MBytes [ 5] 5.00-6.00 sec 6.05 GBytes 52.0 Gbits/sec 0 1.62 MBytes [ 5] 6.00-7.00 sec 6.01 GBytes 51.6 Gbits/sec 0 1.62 MBytes [ 5] 7.00-8.00 sec 6.05 GBytes 52.0 Gbits/sec 0 1.62 MBytes [ 5] 8.00-9.00 sec 6.34 GBytes 54.5 Gbits/sec 0 1.62 MBytes [ 5] 9.00-10.00 sec 5.91 GBytes 50.8 Gbits/sec 0 1.62 MBytes - - - - - - - - - - - - - - - - - - - - - - - - - [ ID] Interval Transfer Bitrate Retr [ 5] 0.00-10.00 sec 57.3 GBytes 49.2 Gbits/sec 0 sender [ 5] 0.00-10.00 sec 57.3 GBytes 49.2 Gbits/sec receiver # 客户端通过 wlp4s0 wifi 网卡(192.168.31.228)访问 iperf3-server，实际还是走的本机，但是速度要比 loopback 快一点，可能是默认设置的问题 ❯ iperf3 -c 192.168.31.228 Connecting to host 192.168.31.228, port 5201 [ 5] local 192.168.31.228 port 43430 connected to 192.168.31.228 port 5201 [ ID] Interval Transfer Bitrate Retr Cwnd [ 5] 0.00-1.00 sec 5.12 GBytes 43.9 Gbits/sec 0 1.25 MBytes [ 5] 1.00-2.00 sec 5.29 GBytes 45.5 Gbits/sec 0 1.25 MBytes [ 5] 2.00-3.00 sec 5.92 GBytes 50.9 Gbits/sec 0 1.25 MBytes [ 5] 3.00-4.00 sec 6.00 GBytes 51.5 Gbits/sec 0 1.25 MBytes [ 5] 4.00-5.00 sec 5.98 GBytes 51.4 Gbits/sec 0 1.25 MBytes [ 5] 5.00-6.00 sec 6.05 GBytes 52.0 Gbits/sec 0 1.25 MBytes [ 5] 6.00-7.00 sec 6.16 GBytes 52.9 Gbits/sec 0 1.25 MBytes [ 5] 7.00-8.00 sec 6.08 GBytes 52.2 Gbits/sec 0 1.25 MBytes [ 5] 8.00-9.00 sec 6.00 GBytes 51.6 Gbits/sec 0 1.25 MBytes [ 5] 9.00-10.00 sec 6.01 GBytes 51.6 Gbits/sec 0 1.25 MBytes - - - - - - - - - - - - - - - - - - - - - - - - - [ ID] Interval Transfer Bitrate Retr [ 5] 0.00-10.00 sec 58.6 GBytes 50.3 Gbits/sec 0 sender [ 5] 0.00-10.00 sec 58.6 GBytes 50.3 Gbits/sec receiver # 从容器中访问宿主机的 iperf3-server，速度几乎没区别 ❯ docker run -it --rm --name=iperf3-server networkstatic/iperf3 -c 192.168.31.228 Connecting to host 192.168.31.228, port 5201 [ 5] local 172.17.0.2 port 43436 connected to 192.168.31.228 port 5201 [ ID] Interval Transfer Bitrate Retr Cwnd [ 5] 0.00-1.00 sec 4.49 GBytes 38.5 Gbits/sec 0 403 KBytes [ 5] 1.00-2.00 sec 5.31 GBytes 45.6 Gbits/sec 0 544 KBytes [ 5] 2.00-3.00 sec 6.14 GBytes 52.8 Gbits/sec 0 544 KBytes [ 5] 3.00-4.00 sec 5.85 GBytes 50.3 Gbits/sec 0 544 KBytes [ 5] 4.00-5.00 sec 6.14 GBytes 52.7 Gbits/sec 0 544 KBytes [ 5] 5.00-6.00 sec 5.99 GBytes 51.5 Gbits/sec 0 544 KBytes [ 5] 6.00-7.00 sec 5.86 GBytes 50.4 Gbits/sec 0 544 KBytes [ 5] 7.00-8.00 sec 6.05 GBytes 52.0 Gbits/sec 0 544 KBytes [ 5] 8.00-9.00 sec 5.99 GBytes 51.5 Gbits/sec 0 544 KBytes [ 5] 9.00-10.00 sec 6.12 GBytes 52.5 Gbits/sec 0 544 KBytes - - - - - - - - - - - - - - - - - - - - - - - - - [ ID] Interval Transfer Bitrate Retr [ 5] 0.00-10.00 sec 58.0 GBytes 49.8 Gbits/sec 0 sender [ 5] 0.00-10.00 sec 58.0 GBytes 49.8 Gbits/sec receiver 把 iperf3-server 跑在容器里再测一遍： # 在容器中启动 iperf3-server，并映射到宿主机端口 6201 \u003e docker run -it --rm --name=iperf3-server -p 6201:5201 networkstatic/iperf3 -s \u003e docker inspect --format \"{{ .NetworkSettings.IPAddress }}\" iperf3-server 172.17.0.2 ----------------------------- # 测试容器之间互访的速度，ip 为 iperf3-server 的容器 ip，速度要慢一些。 # 毕竟过了 veth -\u003e veth -\u003e docker0","date":"2021-08-14","objectID":"/posts/linux-virtual-network-interfaces/:8:1","series":null,"tags":["Linux","网络","虚拟化","容器"],"title":"Linux 中的虚拟网络接口","uri":"/posts/linux-virtual-network-interfaces/#网络性能实测"},{"categories":["tech"],"content":" 参考 Linux虚拟网络设备之tun/tap Linux虚拟网络设备之veth 云计算底层技术-虚拟网络设备(Bridge,VLAN) 云计算底层技术-虚拟网络设备(tun/tap,veth) Universal TUN/TAP device driver - Kernel Docs Tun/Tap interface tutorial Linux Loopback performance with TCP_NODELAY enabled ","date":"2021-08-14","objectID":"/posts/linux-virtual-network-interfaces/:9:0","series":null,"tags":["Linux","网络","虚拟化","容器"],"title":"Linux 中的虚拟网络接口","uri":"/posts/linux-virtual-network-interfaces/#参考"},{"categories":["tech"],"content":" 文中的命令均在 macOS Big Sur 和 Opensuse Tumbleweed 上测试通过 ","date":"2021-04-11","objectID":"/posts/socat-netcat/:0:0","series":null,"tags":["网络","Linux","网络调试"],"title":"Linux 网络工具中的瑞士军刀 - socat \u0026 netcat","uri":"/posts/socat-netcat/#"},{"categories":["tech"],"content":" socat \u0026 netcatnetcat(network cat) 是一个历史悠久的网络工具包，被称作 TCP/IP 的瑞士军刀，各大 Linux 发行版都有默认安装 openbsd 版本的 netcat，它的命令行名称为 nc. 而 socat(socket cat)，官方文档描述它是 \"netcat++\" (extended design, new implementation)，项目比较活跃，kubernetes-client(kubectl) 底层就是使用的它做各种流量转发。 在不方便安装 socat 的环境中，我们可以使用系统自带的 netcat. 而在其他环境，可以考虑优先使用 socat. ","date":"2021-04-11","objectID":"/posts/socat-netcat/:1:0","series":null,"tags":["网络","Linux","网络调试"],"title":"Linux 网络工具中的瑞士军刀 - socat \u0026 netcat","uri":"/posts/socat-netcat/#socat--netcat"},{"categories":["tech"],"content":" 一、简介socat 的基本命令格式： socat [参数] 地址1 地址2 给 socat 提供两个地址，socat 干的活就是把两个地址的流对接起来。左边地址的输出传给右边，同时又把右边地址的输出传给左边，也就是一个双向的数据管道。 听起来好像没啥特别的，但是实际上计算机网络干的活也就是数据传输而已，却影响了整个世界，不可小觑它的功能。 socat 支持非常多的地址类型：-/stdio，TCP, TCP-LISTEN, UDP, UDP-LISTEN, OPEN, EXEC, SOCKS, PROXY 等等，可用于端口监听、链接，文件和进程读写，代理桥接等等。 socat 的功能就是这么简单，命令行参数也很简洁，唯一需要花点精力学习的就是它各种地址的定义和搭配写法。 而 netcat 定义貌似没这么严谨，可以简单的理解为网络版的 cat 命令 2333 ","date":"2021-04-11","objectID":"/posts/socat-netcat/:2:0","series":null,"tags":["网络","Linux","网络调试"],"title":"Linux 网络工具中的瑞士军刀 - socat \u0026 netcat","uri":"/posts/socat-netcat/#一简介"},{"categories":["tech"],"content":" 二、安装方法各发行版都自带 netcat，包名通常为 nc-openbsd，因此这里只介绍 socat 的安装方法： # Debian/Ubuntu sudo apt install socat # CentOS/RedHat sudo yum install socat # macOS brew install socat 其他发行版基本也都可以使用包管理器安装 socat ","date":"2021-04-11","objectID":"/posts/socat-netcat/:3:0","series":null,"tags":["网络","Linux","网络调试"],"title":"Linux 网络工具中的瑞士军刀 - socat \u0026 netcat","uri":"/posts/socat-netcat/#二安装方法"},{"categories":["tech"],"content":" 三、常用命令","date":"2021-04-11","objectID":"/posts/socat-netcat/:4:0","series":null,"tags":["网络","Linux","网络调试"],"title":"Linux 网络工具中的瑞士军刀 - socat \u0026 netcat","uri":"/posts/socat-netcat/#三常用命令"},{"categories":["tech"],"content":" 1. 网络调试 1.1 检测远程端口的可连接性（确认防火墙没问题） 以前你可能学过如何用 telnet 来做这项测试，不过现在很多发行版基本都不自带 telnet 了，还需要额外安装。 telnet 差不多已经快寿终正寝了，还是建议使用更专业的 socat/netcat 使用 socat/netcat 检测远程端口的可连接性： # -d[ddd] 增加日志详细程度，-dd Prints fatal, error, warning, and notice messages. socat -dd - TCP:192.168.1.252:3306 # -v 显示详细信息 # -z 不发送数据，效果为立即关闭连接，快速得出结果 nc -vz 192.168.1.2 8080 # -vv 显示更详细的内容 # -w2 超时时间设为 2 秒 # 使用 nc 做简单的端口扫描 nc -vv -w2 -z 192.168.1.2 20-500 1.2 测试本机端口是否能正常被外部访问（检测防火墙、路由）在本机监听一个 TCP 端口，接收到的内容传到 stdout，同时将 stdin 的输入传给客户端： # 服务端启动命令，socat/nc 二选一 socat TCP-LISTEN:7000 - # -l --listening nc -l 7000 # 客户端连接命令，socat/nc 二选一 socat TCP:192.168.31.123:7000 - nc 192.168.11.123 7000 UDP 协议的测试也非常类似，使用 netcat 的示例如下： # 服务端，只监听 ipv4 nc -u -l 8080 # 客户端 nc -u 192.168.31.123 8080 # 客户端本机测试，注意 localhost 会被优先解析为 ipv6! 这会导致服务端(ipv4)的 nc 接收不到数据！ nc -u localhost 8080 使用 socat 的 UDP 测试示例如下： socat UDP-LISTEN:7000 - socat UDP:192.168.31.123:7000 - 1.3 调试 TLS 协议 参考 socat 官方文档：Securing Traffic Between two Socat Instances Using SSL 测试证书及私钥的生成参见 写给开发人员的实用密码学（八）—— 数字证书与 TLS 协议 模拟一个 mTLS 服务器，监听 4433 端口，接收到的数据同样输出到 stdout： # socat 需要使用同时包含证书和私钥的 pem 文件，生成方法如下 cat server.key server.crt \u003e server.pem cat client.key client.crt \u003e client.pem # 服务端启动命令 socat openssl-listen:4433,reuseaddr,cert=server.pem,cafile=client.crt - # 客户端连接命令 socat - openssl-connect:192.168.31.123:4433,cert=client.pem,cafile=server.crt # 或者使用 curl 连接(我们知道 ca.crt 和 server.crt 都能被用做 cacert/cafile) curl -v --cacert ca.crt --cert client.crt --key client.key --tls-max 1.2 https://192.168.31.123:4433 上面的命令使用了 mTLS 双向认证的协议，可通过设定 verify=0 来关掉客户端认证，示例如下： # socat 需要使用同时包含证书和私钥的 pem 文件，生成方法如下 cat server.key server.crt \u003e server.pem # 服务端启动命令 socat openssl-listen:4433,reuseaddr,cert=server.pem,verify=0 - # 客户端连接命令，如果 ip/域名不受证书保护，就也需要添加 verify=0 socat - openssl-connect:192.168.31.123:4433,cafile=server.crt # 或者使用 curl 连接，证书无效请添加 -k 跳过证书验证 curl -v --cacert server.crt https://192.168.31.123:4433 ","date":"2021-04-11","objectID":"/posts/socat-netcat/:4:1","series":null,"tags":["网络","Linux","网络调试"],"title":"Linux 网络工具中的瑞士军刀 - socat \u0026 netcat","uri":"/posts/socat-netcat/#1-网络调试"},{"categories":["tech"],"content":" 1. 网络调试 1.1 检测远程端口的可连接性（确认防火墙没问题） 以前你可能学过如何用 telnet 来做这项测试，不过现在很多发行版基本都不自带 telnet 了，还需要额外安装。 telnet 差不多已经快寿终正寝了，还是建议使用更专业的 socat/netcat 使用 socat/netcat 检测远程端口的可连接性： # -d[ddd] 增加日志详细程度，-dd Prints fatal, error, warning, and notice messages. socat -dd - TCP:192.168.1.252:3306 # -v 显示详细信息 # -z 不发送数据，效果为立即关闭连接，快速得出结果 nc -vz 192.168.1.2 8080 # -vv 显示更详细的内容 # -w2 超时时间设为 2 秒 # 使用 nc 做简单的端口扫描 nc -vv -w2 -z 192.168.1.2 20-500 1.2 测试本机端口是否能正常被外部访问（检测防火墙、路由）在本机监听一个 TCP 端口，接收到的内容传到 stdout，同时将 stdin 的输入传给客户端： # 服务端启动命令，socat/nc 二选一 socat TCP-LISTEN:7000 - # -l --listening nc -l 7000 # 客户端连接命令，socat/nc 二选一 socat TCP:192.168.31.123:7000 - nc 192.168.11.123 7000 UDP 协议的测试也非常类似，使用 netcat 的示例如下： # 服务端，只监听 ipv4 nc -u -l 8080 # 客户端 nc -u 192.168.31.123 8080 # 客户端本机测试，注意 localhost 会被优先解析为 ipv6! 这会导致服务端(ipv4)的 nc 接收不到数据！ nc -u localhost 8080 使用 socat 的 UDP 测试示例如下： socat UDP-LISTEN:7000 - socat UDP:192.168.31.123:7000 - 1.3 调试 TLS 协议 参考 socat 官方文档：Securing Traffic Between two Socat Instances Using SSL 测试证书及私钥的生成参见 写给开发人员的实用密码学（八）—— 数字证书与 TLS 协议 模拟一个 mTLS 服务器，监听 4433 端口，接收到的数据同样输出到 stdout： # socat 需要使用同时包含证书和私钥的 pem 文件，生成方法如下 cat server.key server.crt \u003e server.pem cat client.key client.crt \u003e client.pem # 服务端启动命令 socat openssl-listen:4433,reuseaddr,cert=server.pem,cafile=client.crt - # 客户端连接命令 socat - openssl-connect:192.168.31.123:4433,cert=client.pem,cafile=server.crt # 或者使用 curl 连接(我们知道 ca.crt 和 server.crt 都能被用做 cacert/cafile) curl -v --cacert ca.crt --cert client.crt --key client.key --tls-max 1.2 https://192.168.31.123:4433 上面的命令使用了 mTLS 双向认证的协议，可通过设定 verify=0 来关掉客户端认证，示例如下： # socat 需要使用同时包含证书和私钥的 pem 文件，生成方法如下 cat server.key server.crt \u003e server.pem # 服务端启动命令 socat openssl-listen:4433,reuseaddr,cert=server.pem,verify=0 - # 客户端连接命令，如果 ip/域名不受证书保护，就也需要添加 verify=0 socat - openssl-connect:192.168.31.123:4433,cafile=server.crt # 或者使用 curl 连接，证书无效请添加 -k 跳过证书验证 curl -v --cacert server.crt https://192.168.31.123:4433 ","date":"2021-04-11","objectID":"/posts/socat-netcat/:4:1","series":null,"tags":["网络","Linux","网络调试"],"title":"Linux 网络工具中的瑞士军刀 - socat \u0026 netcat","uri":"/posts/socat-netcat/#11-检测远程端口的可连接性确认防火墙没问题"},{"categories":["tech"],"content":" 1. 网络调试 1.1 检测远程端口的可连接性（确认防火墙没问题） 以前你可能学过如何用 telnet 来做这项测试，不过现在很多发行版基本都不自带 telnet 了，还需要额外安装。 telnet 差不多已经快寿终正寝了，还是建议使用更专业的 socat/netcat 使用 socat/netcat 检测远程端口的可连接性： # -d[ddd] 增加日志详细程度，-dd Prints fatal, error, warning, and notice messages. socat -dd - TCP:192.168.1.252:3306 # -v 显示详细信息 # -z 不发送数据，效果为立即关闭连接，快速得出结果 nc -vz 192.168.1.2 8080 # -vv 显示更详细的内容 # -w2 超时时间设为 2 秒 # 使用 nc 做简单的端口扫描 nc -vv -w2 -z 192.168.1.2 20-500 1.2 测试本机端口是否能正常被外部访问（检测防火墙、路由）在本机监听一个 TCP 端口，接收到的内容传到 stdout，同时将 stdin 的输入传给客户端： # 服务端启动命令，socat/nc 二选一 socat TCP-LISTEN:7000 - # -l --listening nc -l 7000 # 客户端连接命令，socat/nc 二选一 socat TCP:192.168.31.123:7000 - nc 192.168.11.123 7000 UDP 协议的测试也非常类似，使用 netcat 的示例如下： # 服务端，只监听 ipv4 nc -u -l 8080 # 客户端 nc -u 192.168.31.123 8080 # 客户端本机测试，注意 localhost 会被优先解析为 ipv6! 这会导致服务端(ipv4)的 nc 接收不到数据！ nc -u localhost 8080 使用 socat 的 UDP 测试示例如下： socat UDP-LISTEN:7000 - socat UDP:192.168.31.123:7000 - 1.3 调试 TLS 协议 参考 socat 官方文档：Securing Traffic Between two Socat Instances Using SSL 测试证书及私钥的生成参见 写给开发人员的实用密码学（八）—— 数字证书与 TLS 协议 模拟一个 mTLS 服务器，监听 4433 端口，接收到的数据同样输出到 stdout： # socat 需要使用同时包含证书和私钥的 pem 文件，生成方法如下 cat server.key server.crt \u003e server.pem cat client.key client.crt \u003e client.pem # 服务端启动命令 socat openssl-listen:4433,reuseaddr,cert=server.pem,cafile=client.crt - # 客户端连接命令 socat - openssl-connect:192.168.31.123:4433,cert=client.pem,cafile=server.crt # 或者使用 curl 连接(我们知道 ca.crt 和 server.crt 都能被用做 cacert/cafile) curl -v --cacert ca.crt --cert client.crt --key client.key --tls-max 1.2 https://192.168.31.123:4433 上面的命令使用了 mTLS 双向认证的协议，可通过设定 verify=0 来关掉客户端认证，示例如下： # socat 需要使用同时包含证书和私钥的 pem 文件，生成方法如下 cat server.key server.crt \u003e server.pem # 服务端启动命令 socat openssl-listen:4433,reuseaddr,cert=server.pem,verify=0 - # 客户端连接命令，如果 ip/域名不受证书保护，就也需要添加 verify=0 socat - openssl-connect:192.168.31.123:4433,cafile=server.crt # 或者使用 curl 连接，证书无效请添加 -k 跳过证书验证 curl -v --cacert server.crt https://192.168.31.123:4433 ","date":"2021-04-11","objectID":"/posts/socat-netcat/:4:1","series":null,"tags":["网络","Linux","网络调试"],"title":"Linux 网络工具中的瑞士军刀 - socat \u0026 netcat","uri":"/posts/socat-netcat/#12-测试本机端口是否能正常被外部访问检测防火墙路由"},{"categories":["tech"],"content":" 1. 网络调试 1.1 检测远程端口的可连接性（确认防火墙没问题） 以前你可能学过如何用 telnet 来做这项测试，不过现在很多发行版基本都不自带 telnet 了，还需要额外安装。 telnet 差不多已经快寿终正寝了，还是建议使用更专业的 socat/netcat 使用 socat/netcat 检测远程端口的可连接性： # -d[ddd] 增加日志详细程度，-dd Prints fatal, error, warning, and notice messages. socat -dd - TCP:192.168.1.252:3306 # -v 显示详细信息 # -z 不发送数据，效果为立即关闭连接，快速得出结果 nc -vz 192.168.1.2 8080 # -vv 显示更详细的内容 # -w2 超时时间设为 2 秒 # 使用 nc 做简单的端口扫描 nc -vv -w2 -z 192.168.1.2 20-500 1.2 测试本机端口是否能正常被外部访问（检测防火墙、路由）在本机监听一个 TCP 端口，接收到的内容传到 stdout，同时将 stdin 的输入传给客户端： # 服务端启动命令，socat/nc 二选一 socat TCP-LISTEN:7000 - # -l --listening nc -l 7000 # 客户端连接命令，socat/nc 二选一 socat TCP:192.168.31.123:7000 - nc 192.168.11.123 7000 UDP 协议的测试也非常类似，使用 netcat 的示例如下： # 服务端，只监听 ipv4 nc -u -l 8080 # 客户端 nc -u 192.168.31.123 8080 # 客户端本机测试，注意 localhost 会被优先解析为 ipv6! 这会导致服务端(ipv4)的 nc 接收不到数据！ nc -u localhost 8080 使用 socat 的 UDP 测试示例如下： socat UDP-LISTEN:7000 - socat UDP:192.168.31.123:7000 - 1.3 调试 TLS 协议 参考 socat 官方文档：Securing Traffic Between two Socat Instances Using SSL 测试证书及私钥的生成参见 写给开发人员的实用密码学（八）—— 数字证书与 TLS 协议 模拟一个 mTLS 服务器，监听 4433 端口，接收到的数据同样输出到 stdout： # socat 需要使用同时包含证书和私钥的 pem 文件，生成方法如下 cat server.key server.crt \u003e server.pem cat client.key client.crt \u003e client.pem # 服务端启动命令 socat openssl-listen:4433,reuseaddr,cert=server.pem,cafile=client.crt - # 客户端连接命令 socat - openssl-connect:192.168.31.123:4433,cert=client.pem,cafile=server.crt # 或者使用 curl 连接(我们知道 ca.crt 和 server.crt 都能被用做 cacert/cafile) curl -v --cacert ca.crt --cert client.crt --key client.key --tls-max 1.2 https://192.168.31.123:4433 上面的命令使用了 mTLS 双向认证的协议，可通过设定 verify=0 来关掉客户端认证，示例如下： # socat 需要使用同时包含证书和私钥的 pem 文件，生成方法如下 cat server.key server.crt \u003e server.pem # 服务端启动命令 socat openssl-listen:4433,reuseaddr,cert=server.pem,verify=0 - # 客户端连接命令，如果 ip/域名不受证书保护，就也需要添加 verify=0 socat - openssl-connect:192.168.31.123:4433,cafile=server.crt # 或者使用 curl 连接，证书无效请添加 -k 跳过证书验证 curl -v --cacert server.crt https://192.168.31.123:4433 ","date":"2021-04-11","objectID":"/posts/socat-netcat/:4:1","series":null,"tags":["网络","Linux","网络调试"],"title":"Linux 网络工具中的瑞士军刀 - socat \u0026 netcat","uri":"/posts/socat-netcat/#13-调试-tls-协议"},{"categories":["tech"],"content":" 2. 数据传输通常传输文件时，我都习惯使用 scp/ssh/rsync，但是 socat 其实也可以传输文件。 以将 demo.tar.gz 从主机 A 发送到主机 B 为例， 首先在数据发送方 A 执行如下命令： # -u 表示数据只从左边的地址单向传输给右边（socat 默认是一个双向管道） # -U 和 -u 相反，数据只从右边单向传输给左边 socat -u open:demo.tar.gz tcp-listen:2000,reuseaddr 然后在数据接收方 B 执行如下命令，就能把文件接收到： socat -u tcp:192.168.1.252:2000 open:demo.tar.gz,create # 如果觉得太繁琐，也可以直接通过 stdout 重定向 socat -u tcp:192.168.1.252:2000 - \u003e demo.tar.gz 使用 netcat 也可以实现数据传输： # 先在接收方启动服务端 nc -l -p 8080 \u003e demo.tar.gz # 再在发送方启动客户端发送数据 nc 192.168.1.2 8080 \u003c demo.tar.gz ","date":"2021-04-11","objectID":"/posts/socat-netcat/:5:0","series":null,"tags":["网络","Linux","网络调试"],"title":"Linux 网络工具中的瑞士军刀 - socat \u0026 netcat","uri":"/posts/socat-netcat/#2-数据传输"},{"categories":["tech"],"content":" 3. 担当临时的 web 服务器使用 fork reuseaddr SYSTEM 三个命令，再用 systemd/supervisor 管理一下，就可以用几行命令实现一个简单的后台服务器。 下面的命令将监听 8080 端口，并将数据流和 web.py 的 stdio 连接起来，可以直接使用浏览器访问 http://\u003cip\u003e:8080 来查看效果。 socat TCP-LISTEN:8080,reuseaddr,fork SYSTEM:\"python3 web.py\" 假设 web.py 的内容为： print(\"hello world\") 那 curl localhost:8080 就应该会输出 hello world ","date":"2021-04-11","objectID":"/posts/socat-netcat/:6:0","series":null,"tags":["网络","Linux","网络调试"],"title":"Linux 网络工具中的瑞士军刀 - socat \u0026 netcat","uri":"/posts/socat-netcat/#3-担当临时的-web-服务器"},{"categories":["tech"],"content":" 4. 端口转发监听 8080 端口，建立该端口与 baidu.com:80 之间的双向管道: socat TCP-LISTEN:8080,fork,reuseaddr TCP:baidu.com:80 拿 curl 命令测试一下，应该能正常访问到百度： # 注意指定 Host curl -v -H 'Host: baidu.com' localhost:8080 ","date":"2021-04-11","objectID":"/posts/socat-netcat/:7:0","series":null,"tags":["网络","Linux","网络调试"],"title":"Linux 网络工具中的瑞士军刀 - socat \u0026 netcat","uri":"/posts/socat-netcat/#4-端口转发"},{"categories":["tech"],"content":" 参考 新版瑞士军刀：socat - 韦易笑 - 知乎 用好你的瑞士军刀/netcat - 韦易笑 - 知乎 socat - Multipurpose relay ","date":"2021-04-11","objectID":"/posts/socat-netcat/:8:0","series":null,"tags":["网络","Linux","网络调试"],"title":"Linux 网络工具中的瑞士军刀 - socat \u0026 netcat","uri":"/posts/socat-netcat/#参考"},{"categories":["life"],"content":" 本文可能充斥着学生型思维，请谨慎阅读… ","date":"2021-02-13","objectID":"/posts/no-more-dreams/:0:0","series":null,"tags":[],"title":"脚踏实地，仰望星空","uri":"/posts/no-more-dreams/#"},{"categories":["life"],"content":" 年轻真好最近看了些前辈们的博客，很多是在计算机行业工作几十年的前辈，还有许嵩的文章。 我更深刻地认识到了一件事：我当下的很多文章，都能看得出我在很认真的思考、总结，但是总是有很明显的稚嫩的感觉在里面——我自认为这是「学生型思维」。 我总是喜欢讲「且行且寻」、「自己的眼界还太狭窄了，我对世界还很缺乏了解」、「根本看不清好坏，就无法独立做出决策」诸如此类。 我把这样的文章写出来，前辈们给我留言「博主只是沉淀的时间还远远不够。憋着急，年轻就是最大的资本。」、「只想说年轻真好，使劲折腾才知道要什么东西」。 嗯，我理解到了，因为我「年轻」，所以写出这样的文章没问题，可以使劲去折腾、去探索、去思考。 ","date":"2021-02-13","objectID":"/posts/no-more-dreams/:1:0","series":null,"tags":[],"title":"脚踏实地，仰望星空","uri":"/posts/no-more-dreams/#年轻真好"},{"categories":["life"],"content":" 三十而立 子曰：吾，十有五，而志于学，三十而立，四十而不惑，五十而知天命，六十而耳顺，七十而从心所欲，不逾矩。 孔子说：“我十五岁立志学习，三十岁在人生道路上站稳脚跟，四十岁心中不再迷惘，五十岁知道上天给我安排的命运，六十岁听到别人说话就能分辨是非真假，七十岁能随心所欲地说话做事，又不会超越规矩。” 「四十而不惑」对我而言可能还太远，但「三十而立」却是已经能预见到了的，没几年了。 三十而立，人到了三十岁，就应该知道自己如何立身处世，尘世滚滚中能守住自己的一点本真不失。 三十岁，已不是一个年轻的年纪了。 如果我到了三十岁，还去写些「自己的眼界还太狭窄了，我对世界还很缺乏了解」、「我根本看不清好坏，很多时候无法独立做出决策」，那就贻笑大方了。 所以即使说「年轻就是最大的资本」，也不是能随意挥霍的。 人生这条道路上我们踽踽独行，道阻且长，眼光要放长远一点、多看一点，不要把自己限制住了，更不应该原地踏步。 ","date":"2021-02-13","objectID":"/posts/no-more-dreams/:2:0","series":null,"tags":[],"title":"脚踏实地，仰望星空","uri":"/posts/no-more-dreams/#三十而立"},{"categories":["life"],"content":" 许嵩——我没有梦想这两天看多了前辈们的博客，就想找点非虚构的书藉看看，补充点阅历。 昨天向朋友们讨书看，@rea.ink 就给我推荐许嵩的《海上灵光》。意外地发现了许嵩的新浪博客。 博客的内容都很老了，最新的一篇是 2013 年。但是这并不妨碍其中见解的价值 海上灵光——许嵩 以前媒体问我接下来有什么计划或梦想时我总是很愣的回答，我没有梦想。 真的，一个年过半百的人还把梦想这种字眼挂在嘴上是很乏味的。 睁大眼看看眼前的生活，周遭的一切吧。 脚踏实地认真过好每一天的生活吧。 至于心底的信念——是决计不必拿出来高谈阔论的。 出离心——许嵩 这几个月，走过了不少地方。 每到一处，采访我的媒体通常会有这么一问：你的音乐理想是什么？ 而当答案是“我从来没有理想”时，我迎接那些错愕的眼神。 年轻的时候，拥有一些世俗的念想（比如声名远播？）、一些物质上的期待（比如大房子好车子？）、一些精神上的憧憬（比如寻得佳偶？）、一些相对崇高的目标（比如造福子孙？！），似乎的确能让一些人更有动力的过活每一天。 但如果，岁月在你脸上已然留下不少年轮——你坐船的动机仍然只是到达一座岛，别人把岛上的一切美妙和宝藏说给你听就可以让你划船划的更带劲儿——那我能对你说些什么呢？ ","date":"2021-02-13","objectID":"/posts/no-more-dreams/:3:0","series":null,"tags":[],"title":"脚踏实地，仰望星空","uri":"/posts/no-more-dreams/#许嵩我没有梦想"},{"categories":["life"],"content":" 池建强——你老了这两天读到了一篇池建强写的《你老了》，作者是极客时间创始人，真的是年过半百的技术人了。 你老了 - 池建强的随想录 40 以后，不惑是不可能的，恐慌是与日俱增的。四十不惑，说得不是你想明白了，而是你想不明白的，可能就想不明白了，生日变成另一种仪式，它严肃的告诉你，同学，不要有任何幻想了，接受这个现实，你已经不再年轻了。再卖萌也改变不了这个事实。 人们总会长大，成熟，衰老，一如万事万物。今何在说，人从一出生开始，就踏上了自己的西游路，一路向西，到了尽头，就是虚无，人就没了。所有人都不可避免要奔向那个归宿，你没办法选择，没办法回头。 你老了 - 池建强的随想录 你跳不出这个世界，是因为你不知道这个世界有多大，一旦你知道了，你就超出了它。 年龄也是如此。 ","date":"2021-02-13","objectID":"/posts/no-more-dreams/:4:0","series":null,"tags":[],"title":"脚踏实地，仰望星空","uri":"/posts/no-more-dreams/#池建强你老了"},{"categories":["life"],"content":" 梦想不要多的，想看世界也不是靠说的既然说了要多走走看看，那就多看多想。 就像许嵩写的那样，不必去高谈阔论什么理想与信念，实际行动才是最有力的证明。 Keep eyes on the stars, and feet on the ground. ","date":"2021-02-13","objectID":"/posts/no-more-dreams/:5:0","series":null,"tags":[],"title":"脚踏实地，仰望星空","uri":"/posts/no-more-dreams/#梦想不要多的想看世界也不是靠说的"},{"categories":["life"],"content":" 2022-02-09 更新：2022 年再回看这篇文章，明显感觉到我的进步很大很大，不论是工作文化与环境、薪资、吃喝玩乐、还是接触到的线上环境规模都有了质的变化，详情见 2021 年总结。 目前对自己的认知更清晰了，期待 2022 年我能「更上一层楼」哈哈~ 2021-09-04 更新：在新公司认识到了自己技术、方法论、思维模式等多方面的不足。 这篇文章的部分内容让我觉得有点羞耻…不过就这样吧，毕竟这确实是我当时的所思所想… ","date":"2021-02-06","objectID":"/posts/end-of-the-first-round/:0:0","series":null,"tags":["总结","心得"],"title":"我在创业公司做技术一年多的一点体会","uri":"/posts/end-of-the-first-round/#"},{"categories":["life"],"content":" 人有悲欢离合，月有阴晴圆缺今年年底的时候，自己心思摇摆不定，这影响到了我的工作，顺势就向公司提出了离职。 这两三天和老板、技术经理，还包括公司比较强的同事们，都做了一番沟通。 这一是公司希望我能够认同公司的路线和理念，跟着一条道往前走。二呢我也很想知道，老板、技术经理、还有技术骨干们，为什么能这么坚定不疑？为什么这么拼？ 结果是我和公司都发现，我们不是一路人，观念存在冲突。公司的技术骨干们都是创业思维，他们或者乐在其中，或者愿意为了老板描述的未来忍一时痛苦。他们都愿意为了产品付出更多。 但是我发现对公司，我不愿意付出太多。在这里，我一直就是个普通上班族的想法，高点工资，多点个人时间，做着自己喜欢的事情。 于是我火速离职。当天办完交接，签完离职协议，拿着离职协议和离职证明，光速撤退。 这是我毕业后的第一份工作，2019 年 6 月底入职，在公司呆了一年多，学到了很多东西，绝不仅仅只是技术。因此我觉得自己有必要做一个技术以外的总结。 任何一家公司都有好有坏，但是按照惯例，这篇文章会避而不谈公司不好的东西。公司的名称呢，这里就用 W 来代替吧。 因为有前辈在博客园评论里为 W 公司感到可惜，在开始正题之前，还是先说下离职原因。 其实说来也简单，基本都能猜到：工资超低、画饼充饥、鼓励无意义加班、技术能力到了瓶颈，以及技术能力提升使我信心膨胀。 ","date":"2021-02-06","objectID":"/posts/end-of-the-first-round/:1:0","series":null,"tags":["总结","心得"],"title":"我在创业公司做技术一年多的一点体会","uri":"/posts/end-of-the-first-round/#人有悲欢离合月有阴晴圆缺"},{"categories":["life"],"content":" 自我认识我刚进 W 公司时，是一个刚毕业的小白，只是兴趣使然喜欢技术。因为专业不同，周围也接触不到多少搞技术的，就比较「独」。 在 W 公司我获得了和一群有上进心的人们互相协作的机会，大家在一步一步往前走的感觉，让我在职期间一直非常快乐。 我们的技术经理也给了 DevOps 团队足够的自由，甚至是鼓励我们去探索、尝试新技术。这是我这一年多 DevOps 这个方向进步这么快的最大原因。 然后在和技术经理、同事们坦诚沟通的时候，我也了解到了自己的能力，不仅仅在技术。我对公司的价值，也绝不仅仅在技术。 这是我以前从来没有想过的，我喜欢技术，而且找工作发现职位要求也都是写的技术，我真的就一直以为技术就是一切。 这里我尤其要感谢技术经理，是他帮我把自己对公司的价值和不足分析得如此透彻。 下面是我结合经理和同事对我的评价，对我个人能力做个评估（三人行必有我师，仅供参考）： 理解能力、洞察能力：在公司，我这方面的能力是拔尖的。和人交流，我经常能很快地把握住核心。 表达能力：我的表达能力也是公司里拔尖的。同事跟我讲，听我描述一条鱼，他能清晰地看到鱼的骨头。 其实我日常写博客时，经常觉得自己表达地不够好，很多人的文章就比我写得更好。不同的角度看到的东西真的区别很大，感谢我的同事们。 探索能力：我日常喜欢逛 github，翻 CNCF Landscape，我的兴趣驱动着自己去探索各种新技术，思考它们的优劣。 但是我的大部分同事们都不是这样的，很多同事只读中文文档和博客，英文也必须依赖不怎么靠谱的翻译。 另外他们工期紧业务多，也没我们 DevOps 这边这么多的时间去探索试用新技术。 因此，我的探索能力要强于大多数同事。 全局思考能力: 放眼全局、思考未来，在众多选择中能够并且敢于做出决策。我目前还很缺乏这样的能力。 说到底我目前还是个普通人的思路，没有把自己放在决策者的位置上去思考。 其次呢，我的知识面还太窄，导致我根本看不清好坏，很多时候就无法独立做出决策。我需要扩大自己的知识面。 技术能力：我的技术能力在公司里能评到 80 分吧。我技术不算好，基础薄弱，但是在我们一个小创业公司内部比较，能到 80 分。 管理能力：DevOps 就两三人，因为我具有上面这些能力，矮个子里拔高的，理所当然地我成了领头的。但是性格使然，我管理能力是公司最差的… 创业思维：公司是创业公司，技术骨干们都是创业思维。但是说实话我从来没想过要去创业，不愿意投入太多。这也是我离职的原因。 因此，技术经理认为，我可能更适合当讲师哈哈。 在公司也确实给同事们讲过几次课，能够看到同事们高兴地鼓掌，告诉我「讲得可以」，我就很开心。这种心情就和有人在我博客里评论「感谢，很有帮助」是一样的。 ","date":"2021-02-06","objectID":"/posts/end-of-the-first-round/:2:0","series":null,"tags":["总结","心得"],"title":"我在创业公司做技术一年多的一点体会","uri":"/posts/end-of-the-first-round/#自我认识"},{"categories":["life"],"content":" 我的收获首先技术就不用说了，从我这一年多的博客文章就能看到，我的技术进步相当大。 还有就是提升了对自我的认识，这一点前面也已经阐述过了。 那其他方面我收获了啥呢？大概有下面这些： 我发现，技术经理几十年的技术经验和生活经历，能让他不了解的技术领域中，也能快速找出真正有价值的东西。——经验和阅历给了他强大的洞察能力。 技术产品中最有价值的东西，也最难看透的东西，并不是技术本身，而是理念、抽象。比如 DevOps、基础设施即代码、云计算、开源。 这很难，但是能领先所有人，最先发现这些宏观概念的价值，并押注的人，就能获得巨大的先手优势。 但是「世人大都愚昧」，或者说「太过聪明」，导致这类创业团队可能和社会格格不入。不论成功失败，这类永远是少数人。 理性的沟通是好的，但是有时候情绪化的沟通反而更有效果。 我们技术经理是一个超级理性的人，但是我和他沟通，他的想法并不能很好的传达给我。反而情绪化的老板跟我沟通，我更能感同身受。 我认识了形形色色的人，公司的同事、领导，很多都有值得我学习的地方。有些感悟 富二代不在意钱，没普通人这么斤斤计较，只在意公司氛围，以及自己能做什么。反而更愿意付出更多，能够乐在其中，也更容易成功。创业公司大概很喜欢这类人。 世界上大部分人都是普通人。大众认同的观点，不一定就是正确的观点。大众观点的变化也能体现出社会的变迁。 比如当年大跃进全民的狂热，和现在公司倡导 996，社会舆论则积极反抗。公司和民众站在了对立面。 除了上面这些虚的，还有更实在的： 快乐：经理为人相当好，同事之间合作大都也很愉快，人事小姐姐超级专业，无微不至地照顾我们。在职期间我收获了相当多的快乐。 自信心：我进公司之前，作为一个跨专业的小白，非常没有底气，而且在学校的时候整个人非常颓废。但是在 W 公司，我学到了技术，工作乐在其中，还收获了同事和领导的肯定。我建立起了自信心。 Money: 虽然不多，但是我好歹也是个有些闲钱了的人hhh ","date":"2021-02-06","objectID":"/posts/end-of-the-first-round/:3:0","series":null,"tags":["总结","心得"],"title":"我在创业公司做技术一年多的一点体会","uri":"/posts/end-of-the-first-round/#我的收获"},{"categories":["life"],"content":" 未来毕业后，第一份工作就这样结束了。有点仓促，因为很出乎意料，但细想下来也是情理之中。 下面就是过年了，过年呢，就照着既有的计划来吧，继续提升下技术能力。至少对目前的我而言，技术还是我的能力基础和找工作的最大依仗，其他能力目前还是在围绕技术成长。 年后准备找下一份工作。这一次，我希望能多走一走，看一看，不着急做决定。 我觉得自己的眼界还太狭窄了，我对世界还很缺乏了解。以至于很多东西，我根本无法作出评判。 既然现在跳出了一座我的「围城」，自然要去多看看，外面的世界是个啥样子。 或许也没什么区别？那也得看过才能下结论啊（笑 ","date":"2021-02-06","objectID":"/posts/end-of-the-first-round/:4:0","series":null,"tags":["总结","心得"],"title":"我在创业公司做技术一年多的一点体会","uri":"/posts/end-of-the-first-round/#未来"},{"categories":["life"],"content":" 文末文章的最后，祝大家、也祝我自己在 2021 年里—— 拆破玉笼飞彩凤，顿开金锁走蛟龙。 ","date":"2021-02-06","objectID":"/posts/end-of-the-first-round/:5:0","series":null,"tags":["总结","心得"],"title":"我在创业公司做技术一年多的一点体会","uri":"/posts/end-of-the-first-round/#文末"},{"categories":null,"content":"记录下我的学习轨迹（结果写着写着有点像是技术笔记跟日记的混合了 hhhh） ","date":"2021-02-01","objectID":"/history/:0:0","series":null,"tags":null,"title":"曾经的我","uri":"/history/#"},{"categories":null,"content":" 2022-08-19 Nginx 网关的 TLS 加密：打算换一个思路，用 APISIX 搞个网关再测一波。下周搞搞看吧 晚上读完了《在峡江的转弯处 - 陈行甲人生笔记》 这本书写得极好，作者分享的洞见非常契合我，感觉于我是很有益的。 同时文字中的感情相当丰富，有人在豆瓣上评论说为此感觉到不适，可我觉得这才是真性情，如此丰富的感觉其实正说明了作者的感性，以及对生活的态度！因为看到这样亮眼的光芒而难以直视的人，或许只是因为待在黑暗中太久了。 在这里，我也用书中一句话来回答一下 2017 年 2 月 6 日写下「山岭就像时间一样看不到边，翻过了一座又是一座，这又是一种更大的痛苦」的我，告诉他我现在的想法是「脚下虽有万水千山，但行者必至。」至于我是不是在说空话，事实胜于雄辩，就让时间来证明吧。 陈行甲人生笔记 ","date":"2021-02-01","objectID":"/history/:0:1","series":null,"tags":null,"title":"曾经的我","uri":"/history/#2022-08-19"},{"categories":null,"content":" 2022-08-18 尝试上线 Nginx Gateway 的 TLS 加密功能，使用了 Google Public CA 提供的三个月有效期 TLS 证书 AWS NLB 上观察到大量连接被 Client Reset，tcpdump 抓包后通过 wireshark 分析，确认是大量的 TLS 握手在 Change Cipher Spec 这一步后，被客户端强制 RST。正在到处查 TLS 握手的资料… 奇怪的是，TLS 加密功能放量到 0.5% 左右后，各项 QPS、可用率、延迟等指标都没啥明显的变化，客户端上报的业务指标也正常，就好像 RESET 对业务没任何影响一样… 本地压测一切正常，无法复现这个 RESET 问题 还有一个问题是，发现 cert-manager 给出的证书文件包含了域名证书、中间证书、根证书，导致 TLS 握手阶段发一个证书给客户端直接用了 4000 bytes… 有点费流量… ","date":"2021-02-01","objectID":"/history/:0:2","series":null,"tags":null,"title":"曾经的我","uri":"/history/#2022-08-18"},{"categories":null,"content":" 2022-08-17 阅读《在峡江的转弯处 - 陈行甲人生笔记》 - 进度 153/278 第五记「密歇根湖上有一千种飞鸟」，其中有些言行我敬佩不已，有些瞬间让我唏嘘感叹，有些经历我感同身受。 备考清华以及在清华两年学习生涯的描写让我惊觉自己已经工作快三年了，正好仍然是一人吃饱全家不饿的状态，得益于良好的公司文化跟领导带团队的风格，我工作闲暇时间好像也不少，也一直想在业余时间进行一些针对性的学习，提升自己的专业能力跟英文能力。看在大佬们在这个年纪都这么努力的份上，我也应该再加一把劲吧！ ","date":"2021-02-01","objectID":"/history/:0:3","series":null,"tags":null,"title":"曾经的我","uri":"/history/#2022-08-17"},{"categories":null,"content":" 2022-08-16 阅读《在峡江的转弯处 - 陈行甲人生笔记》 - 进度 36/278 读完了第一记「我和我的母亲」，很久没有这样被感动过了。 2022 年已经过去三分之二，回过头看看这份记录，发现自己确实还是学了不少东西的。值得肯定，周末犒劳下自己~ ","date":"2021-02-01","objectID":"/history/:0:4","series":null,"tags":null,"title":"曾经的我","uri":"/history/#2022-08-16"},{"categories":null,"content":" 2022-08-11 - 2022-08-14 拿 VISA 信用卡开了个 Azure 云的试用账户，研究了一波。 给 https://thiscute.world 加了个 Azure 的 Front Door 作为 vercel 的前置 CDN，发现效果出奇的好！现在站点访问速度跟国内服务器基本没差了，即使缓存不命中，回源速度也特别得快！ 不过价格也比较感人，也只有试用阶段才舍得用。 据说 Azure CDN (Microsoft Standard) 在国内虽然比 Front Door 差一点，但是速度也要强过 CloudFalre/CloudFront，试用期之后可以试试。 算了下 Azure CDN 一个月可能也就 10 刀出头，数据即使丢在 Azure Blob Storage 对象存储里，以我不到 1G 的总数据量一个月才不到 1 刀，完全可以接受。 堪称免备案站点加速方案中的战斗机！ 选 Azure 本来只是因为工作天天接触 AWS/GCP，想试用下全球排名第二的 Azure 是个啥感觉，结果意外发现它的国际 CDN 在国内这么快。 当然 Azure 的坑也多，我遇到的有 资源的删除操作存在各种延迟。比如列表还显示该资源，点进去又提示 not available，提示删除失败，但是点进页面资源又已经没了… Azure CDN 的坑 不支持通过 CNAME 绑定根域名，这一点官方没有任何文档说明，但是根据这个博客，实际上可以通过添加值为 cdnverify 的 CNAME 记录到 cdnverify.\u003cendpoint-name\u003e.azureedge.net，就可以解决这个报错…但是即使这样解决了报错信息，仍然存在一个问题——Azure CDN 现在不再给根域名提供 TLS 证书服务，也就是说 HTTPS 没戏了… HTTPS 证书的申请与部署、配置的修改速度特别的慢。 但是 Azure CDN 的上述这些毛病 Azure Front Door 都没有！Azure Front Door 唯一的缺点就是太贵（这或许是我自己的缺点…） 目录是用的 Active Directory，原生的多租户设计，但是感觉真的好难用啊，跟 AWS/Alicloud 的设计区别很大。 所有资源都是 uuid 这一点，感觉不太友好。 删掉了一个旧 CDN endpoint 后，又建了一个跟之前名字一样的 endpoint，结果创建成功了，但是页面到处报错 s[’@odata.type’] is not a function 2022-08-16 更新：这个 bug 持续四天了，而且我测试站点的 http 端口目前仍然报错… 我怀疑是 rules engine 配置错了，但是这个页面也挂了，现在没办法修改，简直离谱。 不得不说 Azure CDN 是我用过的稳定性最差的 CDN，要不是国内速度确实快，我就直接弃坑了… CDN 如果把源站改为 custom origin，会有五六分钟的时间疯狂报错 404，之后又莫名其妙地恢复… 收费：Azure 的大部分资源价格跟 AWS 相差无几，都是「平民止步」的定价策略。 而且 AWS/Azure/GCP 的出网流量、跨可用区流量都是额外计费的，不像国内云厂商，云服务器跟网络带宽可以绑在一起买。 InfoQ 翻译了一篇文章 为了追求速度，我们测试了全球所有的 CDN，测试了全球的 CDN 速度，画出了一张全球速度最快的 CDN 厂商分布图。其中显示 Azure 的确是中国区域最快的 CDN（仅比较了国际 CDN 服务商，不包含国内）。 试用了通过 Github Action azcopy 将站点上传到 Azure Blob Storage，发现上传太慢了，居然跑了 4mins+，权衡之下还是决定先使用 Vercel 作为 CDN 源站，免费而且部署比 Azure Blob Storage 快多了。 ","date":"2021-02-01","objectID":"/history/:0:5","series":null,"tags":null,"title":"曾经的我","uri":"/history/#2022-08-11---2022-08-14"},{"categories":null,"content":" 2022-08-06 - 2022-08-07 整理与补充今年 5 月份做的学习笔记《分布式数据库的一致性问题与共识算法》，并发表到博客中 极客时间《OpenResty 从入门到实战》 目前市面的网关产品中，性能、可定制性、稳定性三者兼得的仍然只有 openresty 基于 OpenResty 的 APISIX 有丰富的插件，支持插件热加载与配置动态更新，还有实验性的多语言插件支持，长远看或许也是一个很好的选择。 新兴的 envoy 等主推 wasm 插件的网关，性能仍然不如 openresty。另外 envoy 虽然也支持 lua，但是它的 lua 环境没有任何预置的 library，远不如 openresty 这样开箱即用 我目前判断，新兴的 envoy/traefik 等网关的优势在于配置语法简单、支持动态配置。但是如果需要写一些复杂的流量处理逻辑，openresty 仍然是最佳选择。 openresty 仍然是最流行的 CDN/软 WAF/边缘网关，在绝大多数公司的网关/CDN 中，都有 openresty 的身影。 ","date":"2021-02-01","objectID":"/history/:0:6","series":null,"tags":null,"title":"曾经的我","uri":"/history/#2022-08-06---2022-08-07"},{"categories":null,"content":" 2022-07-28 研究了一波 Nginx 的配置调优 关键点：reuseport aio http2 tls1.3 相关文章： https://www.nginx.com/blog/performance-tuning-tips-tricks/ https://www.nginx.com/blog/tuning-nginx/ https://docs.nginx.com/nginx/admin-guide/web-server/serving-static-content/ 【优化】nginx启用reuseport digitalocean/nginxconfig.io kubernetes/ingress-nginx nginx-admins-handbook ","date":"2021-02-01","objectID":"/history/:0:7","series":null,"tags":null,"title":"曾经的我","uri":"/history/#2022-07-28"},{"categories":null,"content":" 2022-07-26 Misaka 大佬在 twitter 上回复说，ambassador 可能最后会发现都不如 Istio 一把梭 仔细想了下还真挺有道理…毕竟都是 Envoy，而研究一圈发现 ambassador 好像也没比 Istio 多很多功能。目前就看到个不太用得上的 OpenAPI 支持？ ","date":"2021-02-01","objectID":"/history/:0:8","series":null,"tags":null,"title":"曾经的我","uri":"/history/#2022-07-26"},{"categories":null,"content":" 2022-07-25 今天研究了下 CNCF 中的 API 网关 目前发现 ambassador 的文档是比较不错的，我关心的点（Istio 集成、HTTP/3、Gateway API）都有写到，值得研究一波。 目前就找到这么两个 Istio 网关的潜在替代品：ambassador 跟之前研究过的 APISIX 其他的网关项目有的是功能上不太契合、有的是性能差了点、还有的是不够成熟度或者活跃度不够。 ","date":"2021-02-01","objectID":"/history/:0:9","series":null,"tags":null,"title":"曾经的我","uri":"/history/#2022-07-25"},{"categories":null,"content":" 2022-07-23 - 2022-07-24 Nginx Gateway 进展 在 AWS NLB 上添加 TLS 终止，遇到 AWS NLB 的 TLS 流量费用高、而且 Nginx 无法通过 X-Forwarded-Proto 判断客户端协议的问题 解决方法：使用 cert-manager 在 Nginx 中进行 TLS 终止，AWS NLB 改为纯 TCP 需要注意在 Nginx 上配置使用 OCSP stapling 等 TLS 性能优化手段，并淘汰掉旧的 TLS 协议与 ciphers. 研究了一波 cert-manager 通过 ACME 申请权威证书，并绑定到 Istio IngressGateeway 或者其他网关上 ","date":"2021-02-01","objectID":"/history/:0:10","series":null,"tags":null,"title":"曾经的我","uri":"/history/#2022-07-23---2022-07-24"},{"categories":null,"content":" 2022-06-29 - 2022-07-22 实施网关优化方案，使用 Go 语言写了一个 Nginx Gateway 控制器 目标： 将目前运行在虚拟机上的 Nginx 搬到 K8s 中运行，通过 Istio Sidecar 接入服务网格，并取代掉当前的 Istio IngressGateway 网关 使用 AWS NLB 作为 Nginx 的前置负载均衡器 功能：包含 Nginx 配置同步与 Reload、AccessLog 日志文件的收集与上传 使用 kubernetes/client-go 监控 configmap/pod 的变动 watch 接口不会处理网络问题，失败会直接断开连接，实践中不建议直接使用它！ 更建议使用 informer，这是一个带缓存的接口，底层是使用 watch 接口 + 队列实现，而且会自动处理网络问题（自动重连），也提供接口强制更新本地缓存 体验： 是第一次用 Go 语言写项目，体验还不错，编译期检查跟语法提示比 Python 强多了 遇到的问题与解决方法 Nginx 无法解析 K8s 内部域名 解决方法：在 http 配置块中添加 resolver kube-dns.kube-system.svc.cluster.local valid=10s; 即可，另外所有 k8s 域名都得使用 FQDN 形式，因为 Nginx 不会使用搜索域配置！ 客户端 Host 透传：改用 X-Forwarded-Host，而原 Host Header 仅供 Istio/Nginx 用于流量管理。同时在流量走到 Istio SIDECAR_OUTBOUND 时，再通过 Envoy 参数 host_rewrite_header: X-Forwarded-Host 将 Host rewrite 回来。 安全组问题：为了获取客户端 IP 需要在 NLB 上启用客户端 IP 透传，但是这样会导致流量被内网安全组拒绝！ 解决方法：在 Nginx 所在的 EC2 上添加安全组，允许公网 IP 访问其 http/https 端口即可 使用 aws-load-balancer-controller 绑定 IP 模式的 NLB，发现 pod 被重新调度会导致请求超时！ 相关 issue: pod termination might cause dropped connections 解决方法：在 pod 上设置 350s 的 preStop 以及对应的 terminationGracePeriodSeconds，确保所有请求都能被正常处理！ Nginx 注入 Istio Sidecar 后，响应头里带了些 x-envoy- 开头的不必要 headers 解决方法：参见 Istio 去除响应 Headers Istio Sidecar 性能很差，Nginx 与 Sidecar 的 CPU 比值接近 1:2.3 解决方法：为 pod 添加 annotation traffic.sidecar.istio.io/includeInboundPorts: \"\"，即可禁用掉 Istio Sidecar 的 inbound 流量拦截。 AWS NLB 跨可用区负载均衡会收跨区流量费 解决方法：关闭跨区负载均衡功能，不同可用区的 Nginx 使用不同的 Deployment+HPA+PDB，就是都独立进行扩缩容。 2022/7/20，Leader 告诉我，我上半年的表现出乎他的意料，综合表现上看，得到的绩效评价是 S，第一次拿 S，还是挺开心的。 我的优点： 善于观察与思考，真正做到了目标驱动，积极挖掘各种可能性。 善于将优秀前沿技术落地并取得价值，能够不盲从、玩的转、有落地。 ","date":"2021-02-01","objectID":"/history/:0:11","series":null,"tags":null,"title":"曾经的我","uri":"/history/#2022-06-29---2022-07-22"},{"categories":null,"content":" 2022-06-22 The ANSI C Programming Language - 83/236 快速过一遍语法 ","date":"2021-02-01","objectID":"/history/:0:12","series":null,"tags":null,"title":"曾经的我","uri":"/history/#2022-06-22"},{"categories":null,"content":" 2022-06-16 ~ 2022-06-17 研究云上网关及 Kubernetes 集群的网络架构优化方案 从之前的「多云+多集群网络方案」，先简化为各集群互相独立的网络方案，之后再往多集群、多云等方向去迭代（迂回策略）。 将遗留的 Nginx 网关直接移到 K8s 集群内，并注入 Sidecar 以接入 Istio 服务网格，由 Sidecar 帮助 Nginx 实现集群内的服务发现、流量切分、多集群支持等能力。好处是遗留的一堆 Nginx 配置基本不需要什么改动，改造难度低，收益明显（动态扩缩容的 Nginx、更短的网关链路、更简洁的配置）。后续还可以逐渐切换到新的 API 网关提升可维护性。 Nginx 可以通过云服务商提供的 L4 负载均衡（如 AWS NLB）暴露到公网，也可以自建 keepalived 高可用方案（自己整个 Pod 动态注册控制器，或者搞套静态的节点组都行），不过要注意跨区流量，网关最好是每个可用区单独部署一套，互相隔离。 相当于暂时使用 Nginx+Sidecar 彻底取代掉 Istio 的 IngressGateway。讲道理 Istio 的 IngressGateway 功能还是有点弱 等这个方案实施后，我们可能会考虑使用 APISIX/Traefik 或其他基于 Envoy 的网关来取代掉这个容器化后的 Nginx。目的是提供更简单的网关配置方法，当前写 Nginx 配置的方式还是不太友好。 ","date":"2021-02-01","objectID":"/history/:0:13","series":null,"tags":null,"title":"曾经的我","uri":"/history/#2022-06-16--2022-06-17"},{"categories":null,"content":" 2022-06-13 Linux/Unix 系统编程手册（上册） - 进度 21/572 感觉最近学东西有点随心所欲，东一榔头西一棒槌，感觉自己还在找方向吧。不过 Linux 跟 Kubernetes 开发这两件事应该能坚持下来。 ","date":"2021-02-01","objectID":"/history/:0:14","series":null,"tags":null,"title":"曾经的我","uri":"/history/#2022-06-13"},{"categories":null,"content":" 2022-06-11 《语言学的邀请》- 进度 68/288 感觉解答了一些我以前的对人类的一些疑惑 《Intimate Relationship》 - 14/449 读了文化对亲密关系的影响 跟堂弟大谈如何怀揣理想，平实生活，一点点地进步。 谈我们这一代，我想不必悲观也不必绝望，我们的未来由我们自己创造。 有学术大佬们走在学术前沿，有技术高手们工作在工程一线，我们也完全有能力去做一些有价值的事情，赚更多的钱，也帮助更多的人。 ","date":"2021-02-01","objectID":"/history/:0:15","series":null,"tags":null,"title":"曾经的我","uri":"/history/#2022-06-11"},{"categories":null,"content":" 2022-06-10 读完了《在生命的尽头拥抱你-临终关怀医生手记》 读这本书时，我也在持续回忆我的爷爷奶奶。很难去说明我从这本书中读懂了啥，本质上我只是想从这本书中找到一些慰藉，顺便了解下「死亡」，大概确实部分达成了目标。 ","date":"2021-02-01","objectID":"/history/:0:16","series":null,"tags":null,"title":"曾经的我","uri":"/history/#2022-06-10"},{"categories":null,"content":" 2022-06-09 买了一千多块钱的书，最近陆续到货了，现在还差一本《我的青春恋爱物语果然有问题——原画集》 多买了一本罗翔老师的《圆圈正义》，打算送给堂弟 阅读了《Intimate Relationships》的第一小节 了解了人类社会性动物的本质，这可以用进化论解释——越社会性的个体存活率越高，基因也越容易传续。 亲密关系的建立是很容易的，「你是我的唯一」更多的是一种浪漫的说法，只是「因为刚好遇到你」而已。 一旦建立了亲密关系，我们就会抗拒这份亲密关系的解离。当亲密关系遭遇危机时，我们会茶不思饭不想。 在 Youtube 上搜了下 Intimate Relationships，找了几个相关的 TED Talks 看了看。 还找到 UCLA 一个比较老的课程：Intimate Relationships: Undergraduate Lectures at UCLA，可以跟书一起看看。 ","date":"2021-02-01","objectID":"/history/:0:17","series":null,"tags":null,"title":"曾经的我","uri":"/history/#2022-06-09"},{"categories":null,"content":" 2022-06-08 折腾一晚上博客的 Hugo 跟 DoIt 主题 发现本地生成出的站点，mermaid 跟 music 两个插件的问题莫名其妙修复了，怀疑跟今天跑了一波 brew upgrade 有关 但是云上 github action 跟 vercel 都还有问题，同样的命令同样的 hugo 版本，本地生成的静态文件 mermaid 跟 aplay 正常加载，云上生成的就有问题，也是醉了… ","date":"2021-02-01","objectID":"/history/:0:18","series":null,"tags":null,"title":"曾经的我","uri":"/history/#2022-06-08"},{"categories":null,"content":" 2022-06-05 ~ 2022-06-06 观看 KubeCon + CloudNativeCon 2022 中我比较感兴趣的部分 主要关注与当前工作相关的点：多云管理、多集群（karmada）管理与应用部署、跨集群网络（Istio）、API 网关 有一些收获，但是都是比较浅的，只能提供个别方向的一些思路，主要还是得靠自己探索。 研究了一波 dapr，理念很先进，但是发现很多功能都还处于 alpha 阶段，不太适合向业务侧推广，继续观望吧。 ","date":"2021-02-01","objectID":"/history/:0:19","series":null,"tags":null,"title":"曾经的我","uri":"/history/#2022-06-05--2022-06-06"},{"categories":null,"content":" 2022-05-30 ~ 2022-06-02 研究跨云应用部署方案，如 karmada/kubevela 以 karmada 为代表的多集群 Deployment/Service 管理，需要一个控制面集群+多个子集群 配置只往控制面集群部署，karmada 负责按配置在子集群中创建或更新对应的资源 研究多云+多集群网络方案 以 Istio 为代表的多集群服务网格，部署模型之一也是控制面集群+多个子集群 配置只往控制面集群部署，istio 会将配置下发到数据面的 sidecar 与 gateway，完成相应的网络配置 其他的如 karmada 等也集成了一些集群间的网络打通方案，但是感觉都还不太成熟 cilium 的 service mesh 也是一个潜在的多云 k8s 网络方案，但是还处于 beta 状态，有待观望 研究云上 L4/L7 层网关的开源/商业方案 如 L4 的 dpvs/katran 与 L7 的 APISIX/Traefik/Contour，以及 AWS Gateway LoadBalancer 暂时认为云上 L4 还是直接使用云服务商的方案最合适，没必要自己搭 L7 为了支持多集群切量，同时尽量缩短链路，目前感觉使用 Istio 最合适 研究各跨云网络方案（L7 负载均衡（ADC）、SD-WAN、WireGuard、服务网格等）： 一是多云之间相互隔离，但是长远看不太现实 二是多云使用不冲突的 CIDRs 作为它们的 VPC 网段，然后使用 VPN 把多云网络直接串起来 三是直接在多云上搭建一套 overlay 网络，完全屏蔽掉不同云之间的网络差异 仅针对 k8s 的方案主要是 kilo，基于 wireguard 直接通过公网实现 overlay 网络，但是感觉时延很可能难以接受，还是得用 VPN 才行。 整个云通用的方案目前只有部分供应商在做，而且不开源，有 vendor lock-in 的可能，而且不清楚封装出的具体效果如何 ","date":"2021-02-01","objectID":"/history/:0:20","series":null,"tags":null,"title":"曾经的我","uri":"/history/#2022-05-30--2022-06-02"},{"categories":null,"content":" 2022-05-29 动手学深度学习 - Pytorch 版 - 14.3% 学习第二章：预备知识 微积分：复习了单变量函数的微分（导数） =\u003e 多变量函数的偏微分，单变量函数的斜率 =\u003e 多变量函数的梯度（梯度，即函数 $f(x)$ 关于输入向量 $x$ 的所有偏微分组成的一个向量） 深度学习模型的训练，即搜索出使损失函数的值最小的模型参数。而梯度下降是应用最广泛的一种损失函数优化方法。 梯度下降，即始终朝着损失函数的梯度值下降的方向进行模型参数的搜索 深度学习中的多元（变量）函数通常是复合的，而链式法则 $\\frac{dy}{dx} = \\frac{dy}{du} \\frac{du}{dx}$ 使我们能够微分复合函数。 自动微分：为了计算梯度我们必须要对函数进行求导，而手工求复杂函数的导数不仅容易出错，而且函数的更新也过于繁琐。深度学习框架通过提供自动微分能力解决这个问题。 实际上，深度学习框架会构建一个计算图（computational graph）用于跟踪所有数值是由哪些操作生成的，有了这个计算图后，我们还可以通过数值反向去更新每个参数的偏导数，这被称为反向传播（backpropagate）。 自动微分的另一个好处是，即使输入函数是一个由代码定义的黑箱，根本不清楚它的具体表达式，仍然可以通过反向传播自动计算出它的微分。 概率论： 采样 随机变量的分析方法：联合分布、条件分布、Bayes 定理、边缘化、独立性假设 概率分布的关键特征度量方式：期望、平方差/标准差 学习第三章：线性神经网络 线性回归模型是一个简单的单层神经网络，只有输入与输出两层 学习了「从零开始实现线性回归」的一小部分 ","date":"2021-02-01","objectID":"/history/:0:21","series":null,"tags":null,"title":"曾经的我","uri":"/history/#2022-05-29"},{"categories":null,"content":" 2022-05-28 动手学深度学习 - Pytorch 版 - 10.6% 学习第二章：预备知识 通过搜索 cheat sheet + 《Python for Data Analysis》学了下 Numpy/Pandas/Matplotlib 的使用方法 复习线性代数 ","date":"2021-02-01","objectID":"/history/:0:22","series":null,"tags":null,"title":"曾经的我","uri":"/history/#2022-05-28"},{"categories":null,"content":" 2022-05-24 动手学深度学习 - Pytorch 版 - 7.8% 完成第一章前言，了解了深度学习是机器学习的一个分支，机器学习的用途、分类，深度学习的简单原理及优势，近十年此领域的爆炸式发展 监督学习、无监督学习、强化学习 音视频数据生成领域的重要方法：GAN ","date":"2021-02-01","objectID":"/history/:0:23","series":null,"tags":null,"title":"曾经的我","uri":"/history/#2022-05-24"},{"categories":null,"content":" 2022-05-24 分布式系统与区块链 极客时间《分布式协议与算法实战》 - 40% AI 被 ACE 深度学习歌声合成激励到了，花了近两个小时简单学了点吴恩达的机器学习课程、微软的 ML for beginners，李沐的《动手深度学习》 明确了目标是「快速学习，暂时只是为了玩一玩」，确定我应该通过《动手深度学习 - Pytorch》入门。 ","date":"2021-02-01","objectID":"/history/:0:24","series":null,"tags":null,"title":"曾经的我","uri":"/history/#2022-05-24-1"},{"categories":null,"content":" 2022-05-24 极客时间《分布式协议与算法实战》 - 36% ","date":"2021-02-01","objectID":"/history/:0:25","series":null,"tags":null,"title":"曾经的我","uri":"/history/#2022-05-24-2"},{"categories":null,"content":" 2022-05-22 ~ 2022-05-23 学习分布式系统的一致性问题与共识算法 并记录笔记 极客时间《分布式协议与算法实战》 - 22% ","date":"2021-02-01","objectID":"/history/:0:26","series":null,"tags":null,"title":"曾经的我","uri":"/history/#2022-05-22---2022-05-23"},{"categories":null,"content":" 2022-05-20 学习极客时间的《深入剖析 Kuberntes》 - 100% 学完后第一次做测验，拿了 50 分，陷入自我怀疑 emmmm 容器运行时 Kubelet 控制循环 SyncLoop 绝对不会阻塞，任何长时间任务都会创建新的 goroutine 来异步执行 CRI 的接口非常简单宽松，给予了底层容器运行时足够大的自定义空间 云原生的发展方向 Kubernetes 的强大之处：声明式 API 和以此为基础的控制器模式、将「政治」与「技术」拆分开的社区运作模式 Kubernetes 生态与传统 PaaS 的区别：Kubernetes 提供了基础设施层能力（编排、调度、资源管理等），使得其上的 PaaS 可以专注于应用服务和发布流程管理这两个最核心的功能，开始向更轻、更薄、更以应用为中心的方向进行演进。从而 Serverless 开始蓬勃发展 Serverless 的本质：高可扩展性、工作流驱动、按用量计费 「云原生」是一个使用户能低心智负担的、敏捷的，以可扩展、可复制的方式，最大化利用“云”的能力、发挥“云”的价值的一条最佳路径。 学习分布式系统的一致性问题与共识算法 并记录笔记 一致性问题的核心是「ACID 理论中的事务一致性」，与「CAP 理论中的数据一致性」 数据一致性又分为强一致性与弱一致性，而弱一致性的最低限度就是最终一致性：数据最终会一致（再低就永远不会一致了） 最终一致性太模糊，具体实现上往往会最加上一些限定，得到许多一致性模型：读自己写一致性/写后读一致性、单调读一致性、前缀一致性、线性一致性、因果一致性 ","date":"2021-02-01","objectID":"/history/:0:27","series":null,"tags":null,"title":"曾经的我","uri":"/history/#2022-05-20"},{"categories":null,"content":" 2022-05-19 学习极客时间的《深入剖析 Kuberntes》 - 87% 简单学习了 CRD + Controller 的编写，包含 Informer 机制等。不过内容太老了，还是之后看 Programming Kubernetes 再详细学吧。 K8s API 资源的组织方式为 api/\u003capiGroup\u003e/\u003cGroupVersion\u003e/\u003cResource\u003e，yaml 中的 apiVersion 为 \u003capiGroup\u003e/\u003cGroupVersion\u003e，而 Kind 的值就是 \u003cResouce\u003e Pod/Node/configmap 等几个核心资源的 \u003capiGroup\u003e 为空，因此可以直接省略掉 其他核心资源都是以功能分类的，都有 \u003capiGroup\u003e 属性 RBAC 是以 Role 为授权的基本单位，Role 的规则会指定用户对不同 apiGroups/Resources/resourceNames 可以执行哪些动作 verbs apiGroups/Resources 属性跟前面介绍的 API 资源的组织方式是完全对应的，但是 Resources 需要使用复数形式，如 pods/configmaps/nodes 如果是核心资源如 Pod/Node，则 apiGroups 应该设为空字符串 apiGroups: [\"\"] RoleBinding/ClusterRoleBinding 有两个部分：subjects 被作用者，以及 roleRef，用于声明这两者之间的绑定关系 subjects 被作用者可以是集群内的 ServiceAccount，也可以是外部定义的对象如 User User 在集群中是一个不存在的对象，它的认证需要一台外部系统 RBAC 中还存在 Groups 用户组的概念 比如任意名字空间中所有 serviceaccount 的用户组，名称为 system:serviceaccounts:\u003cNamespace名字\u003e 每个 serviceAccount 的全名为 system:serviceaccount:\u003cNamespace名字\u003e:\u003cServiceAccount名字\u003e 我们可以在 subjects 中填写一个用户组，为整个用户组内所有的 ServiceAccount 授权 Kubernetes 中默认已经内置了多个 clusterrole，可通过 kubectl get clusterroles 查看 开发测试时，我们可能会经常用的一个 clusterrole 就是 cluster-admin，这个 role 拥有整个集群的最高权限，相当于 root，非开发测试环境一定要谨慎使用它。 view/edit 这两个 clusterrole 分别拥有整个集群的查看/编辑权限 Kubernetes 存储 存储的两个绑定阶段： 第一阶段（AttachDetachController，运行在 kube-controller-manager 中），K8s 将 nodeName 传递给存储插件，插件将数据卷 attach 到该节点上 第二阶段（VolumeManagerReconciler，运行在 kubelet 中），K8s 将 dir 传递给存储插件，插件将数据卷挂载到该目录下（如果是新数据卷还会提前格式化该卷）。 云上 K8s 存储的一个缺陷：无法跨可用区调度。如果你通过 affinity 强制把一个 p8s 调度到别的可用区，因为它的数据卷不在目标可用区，这会导致它无法被调度，卡在 Pending 状态。 学习了已被废弃的 FlexVolume 的实现方式，以及它的替代者 CSI 以 csi-digitalocean 为例，学习了一个 CSI 插件的实现原理 Kubernetes 调度 根据容器的 requests/limits 参数，k8s 将 Pod 分为三种类型：BestEffort Burstable Guaranteed 在因为资源不足而触发驱逐 Evection 时，会按 BestEffort =\u003e Burstable =\u003e Guaranteed 的顺序进行驱逐 当 Pod 中所有容器的 requests/limits 都相等的时候，Pod 的 QoS 等级为 Guaranteed 如果这时容器的 cpu requests 为整数值，K8s 会自动为容器进行绑核操作，这可以大幅提升容器性能，常用在在线应用场景下 疑问：如果 istio sidecar requests/limits 不相等，但是应用容器是设的相等的，这种情况下是否会执行绑核操作呢？ Pod 的优先级与抢占机制 首先创建不同优先级的 PriorityClass，然后为 Pod 指定 priorityClassName 调度失败的 Pod 会被放到 unschedulableQ 中，这会触发调度器为这些调度失败的 Pod 寻找牺牲者的逻辑 基于优先级与抢占机制，创建一些优先级为 -1 的占位 Pod，可以实现为整个集群预留一部分资源。这种方法被称为「Pod 空泡」资源预留法。 Device Plugin: 负责管理集群中的所有硬件加速设备如 GPU/FPGA 等 Device Plugin 只能基于「数量」进行调度，无法进行更复杂的异构调度策略，比如「运行在算力最强的节点上」 日志与监控：对我来讲，没什么新东西 容器运行时 gVisor - 在用户态重新实现了一遍 Linux ABI 接口、网络协议栈，启动速度跟资源占用小。但是工程量大，维护难度大，对于系统调用密集的应用，性能会急剧下降。 kata containers: 据说是性能比较差，运行了一个真正的 Linux 内核与 QEMU 虚拟设备实现强隔离 aws firecrackers: 跟 kata containers 的思路一致，但是使用 rust 实现了自己的 vmm，性能更高 ","date":"2021-02-01","objectID":"/history/:0:28","series":null,"tags":null,"title":"曾经的我","uri":"/history/#2022-05-19"},{"categories":null,"content":" 2022-05-15 了解到 2021 年是区块链投资大涨的一年，总投资涨了 7 倍多到了 252 亿美元，NFT 更离谱直接从 2020 年的 37m 涨到 4802m 美元，感觉确实非常有前景 数据来源 State Of Blockchain 2021 Report - CB Insights Research 分两次从币安转了 0.01 ETH + 0.05 ETH 到 Ethereum，币安收了固定手续费 0.0016 ETH * 2 购买 ENS 域名 thiscute.eth 10 年并设为我的主域名，花了约 0.027 ETH，算上 gas 费一共花了 0.0321 ETH 也就是 67 刀 给自己再次整了一个 mirror.xyz 账号，有了 ENS 就是爽。 但是发现我现有的几个域名如 thiscute.world，其实可以直接通过 DNSSEC 导入到 ENS，感觉血亏 0.027 ETH… 阅读郭宇最近写的《Web3 DApp 最佳编程实践指南》 晚上去测核酸的路上还参与了他开的一个 twitter space 聊 web3 开发，发言的很多大佬，很多干货。 也明确了，目前区块链还处于战国时代，百家争鸣 再次确认今年学习路线，精简与调整之前年度的计划（之前的计划太多了搞不定，而且当时没排区块链） 先学好分布式原理与算法这块 然后是 Kubernetes 编程，同时结合极客兔兔的几个教程深入学习 Go 深入学下 Go 语言底层 搞一搞区块链 学习 C 语言 通过 TLPR 学习 Linux 系统 通过 CS144 系统学习计算机网络 ","date":"2021-02-01","objectID":"/history/:0:29","series":null,"tags":null,"title":"曾经的我","uri":"/history/#2022-05-15"},{"categories":null,"content":" 2022-05-14 阅读 Web 3.0：穿越十年的讨论 - 知乎 系列内容，了解 Web 3.0 阅读 dcbuild3r/blockchain-development-guide，了解如何进行区块链开发 我把这个 guide 完整过了一遍（后面关于自我提升、社会影响力啥的仅走马观花看了看），真的好长的一篇文章啊。 很多干货，现在我对搞区块链开发要学的东西，认知更清晰了。 ","date":"2021-02-01","objectID":"/history/:0:30","series":null,"tags":null,"title":"曾经的我","uri":"/history/#2022-05-14"},{"categories":null,"content":" 2022-05-12 迭代博客内容《关于 NAT 网关、NAT 穿越以及虚拟网络》- 90% 真的低估了 NAT 网关与 NAT 穿越技术的知识量，又折腾了一个晚上，文章还没完成… 5/9 的时候我就觉得文章已经完成了 90%，结果今天又折腾了一晚上迭代了大量内容，现在感觉文章的进度还不到 90%…越学发现自己懂得越少 ","date":"2021-02-01","objectID":"/history/:0:31","series":null,"tags":null,"title":"曾经的我","uri":"/history/#2022-05-12"},{"categories":null,"content":" 2022-05-11 学习极客时间的《深入剖析 Kuberntes》 - 53% 学习了 NetoworkPolicy、kube-proxy 的实现原理，其实都是用 iptables 实现的，原理挺简单的。 不过 kube-proxy 很早就支持了 ipvs 模式，它在大规模场景下比 iptables 性能更好一些。但是 AWS EKS 目前官方仍然并不支持 ipvs 模式，打开可能会有坑。 极客时间《分布式协议与算法实战》 - 4% 过了一遍常见共识算法的名字：两阶段提交、Try-Confirm-Cancel、Paxos、ZAB、Raft、Gossip、PBFT、PoW、PoS、dPoS 过了一遍上述共识算法的特性：是否支持拜占庭容错、支持哪种程度的一致性、性能、高可用性 了解了一些区块链相关公司的方向，区块链开发岗位的要求 还研究了一波性能测试工具：grafana/k6 ","date":"2021-02-01","objectID":"/history/:0:32","series":null,"tags":null,"title":"曾经的我","uri":"/history/#2022-05-11"},{"categories":null,"content":" 2022-05-09 学习极客时间的《深入剖析 Kuberntes》 - 48% 复习了 Linux 虚拟网络接口以及容器网络原理、学习了 CNI 网络插件的原理 学习了两个 underlay 网络实现：flannel 的 host-gw 模式实现原理、calico 基于 BGP 的实现原理 calico 在跨 vlan 时需要使用 IPIP，学习了相关原理 完成博客《关于 NAT 网关、NAT 类型提升、NAT 穿透以及虚拟网络》- 90% 简单研究了 Go 的 STUN/TURN/ICE 库，以及 coturn server 简单学习了零知识证明的应用，zk-SNARKs，区块链混币服务，以及拜占庭将军问题 ","date":"2021-02-01","objectID":"/history/:0:33","series":null,"tags":null,"title":"曾经的我","uri":"/history/#2022-05-09"},{"categories":null,"content":" 2022-05-08 完成博客《关于 NAT 网关、NAT 类型提升、NAT 穿透以及虚拟网络》 已发布，但是还有些细节需要填充，另外还需要补些示意图 ","date":"2021-02-01","objectID":"/history/:0:34","series":null,"tags":null,"title":"曾经的我","uri":"/history/#2022-05-08"},{"categories":null,"content":" 2022-05-06 学习极客时间专栏《深入浅出 Kubernetes》 - 37% 主要学了下 Pod 的结构、名字空间共享等细节信息，这部分我以前只了解个大概 集群安装、Deployment、StatefulSet、Service 这几个部分我都已经比较熟了，走马观花看了看。 粗略过了下目录，其中对我而言最有价值的，应该就是容器网络、调度器、容器运行时 ","date":"2021-02-01","objectID":"/history/:0:35","series":null,"tags":null,"title":"曾经的我","uri":"/history/#2022-05-06"},{"categories":null,"content":" 2022-05-05 学习极客时间专栏《深入浅出 Kubernetes》 - 20% Kubernetes 与其他败北的编排工具比，最大的优势在于它的设计思想： 从更宏观的角度，以统一的方式来定义任务之间的各种关系（最底层是 Pod 与 PV，之上是各种控制器、亲和反亲和、拓扑扩散、自定义控制器，网络侧有 service，底层网络插件等等），并为将来支持更多种类的关系留有余地（开放、强大的自定义能力催生出了丰富的生态） 基于状态的声明式配置，由控制器负责自动达成期望的状态 研究 FinOps 与 kubecost，总结工作上的经验，完成一篇 Kubernetes 成本分析的文章 - 100% ","date":"2021-02-01","objectID":"/history/:0:36","series":null,"tags":null,"title":"曾经的我","uri":"/history/#2022-05-05"},{"categories":null,"content":" 2022-05-02 学习Go语言动手写Web框架 - 进度 20% ","date":"2021-02-01","objectID":"/history/:0:37","series":null,"tags":null,"title":"曾经的我","uri":"/history/#2022-05-02"},{"categories":null,"content":" 2022-05-01 研究 FinOps 与 kubecost，完成一篇 Kubernetes 成本分析的文章 - 50% ","date":"2021-02-01","objectID":"/history/:0:38","series":null,"tags":null,"title":"曾经的我","uri":"/history/#2022-05-01"},{"categories":null,"content":" 2022-04-26 - 2022-04-28 学习极客时间专栏《深入浅出 Kubernetes》，复习容器技术（Namespace/Cgroups/rootfs） Docker 最核心的创新： 将 rootfs 打包到镜像中，使镜像的运行环境一致（仅与宿主机共享内核） 使用 Dockerfile 描述镜像的打包流程，使构建出的镜像可预期、可重新生成 2022-04-28 调薪结果出来了，突然觉得身心都有点累了，有点惆怅。总之还是继续努力吧，技术才是我的核心竞争力，少管他什么妖风邪雨。 ","date":"2021-02-01","objectID":"/history/:0:39","series":null,"tags":null,"title":"曾经的我","uri":"/history/#2022-04-26---2022-04-28"},{"categories":null,"content":" 2022-04-25 完成了 19 年创建的 go 项目：https://github.com/ryan4yin/video2ascii 失眠，半夜随便翻了翻，把《Go 程序设计语言（英文版）》走马观花过了剩下的一部分，算是完成了一周目 阅读 Programming Kubernetes - Developing Cloud Native Applications - 进度 7% 主要是通过案例讲解 CRD Operator Controller 等 Kubernetes 编程技术 Go 程序设计语言（英文版） 2022-08-19 补图 ","date":"2021-02-01","objectID":"/history/:0:40","series":null,"tags":null,"title":"曾经的我","uri":"/history/#2022-04-25"},{"categories":null,"content":" 2022-04-24 阅读《Go 程序设计语言（英文版）》 - 进度 90% 目前还剩两章未读：反射 reflection 与底层编程 unsafe/uintptr ","date":"2021-02-01","objectID":"/history/:0:41","series":null,"tags":null,"title":"曾经的我","uri":"/history/#2022-04-24"},{"categories":null,"content":" 2022-04-22 - 2022-04-23 阅读《Go 程序设计语言（英文版）》 - 进度 72% 主要完成了 goroutines/channels 以及「并发与变量共享 sync」两个章节 ","date":"2021-02-01","objectID":"/history/:0:42","series":null,"tags":null,"title":"曾经的我","uri":"/history/#2022-04-22---2022-04-23"},{"categories":null,"content":" 2022-04-21 多抓鱼买的一批新书到手了，大致读了下几本书的前几页。 目前比较感兴趣的有：《复杂》、《陈行甲人生笔记》、《原则 - 应对变化中的世界秩序》、《这才是心理学》 打算首先读《复杂》 使用 kubernetes/autoscaler 实现集群弹性扩缩容 发现社区的这个工具（简称 CA），确实没 aws 出品的 karpenter 好用。 CA 自身的实现很简单，主要是依靠 AWS ASG 实现扩缩容。 而 EKS 的 NodeGroup 说实话做得太垃圾了，底层 ASG 的很多功能它都不支持，一旦创建很多参数（VPC 参数、实例类型、等等）就无法通过 EKS NodeGroup 变更了。如果越过 EKS NodeGroup 直接修改底层的 ASG 配置，它还会提示「Degraded」说配置不一致，真的是无力吐槽。 ","date":"2021-02-01","objectID":"/history/:0:43","series":null,"tags":null,"title":"曾经的我","uri":"/history/#2022-04-21"},{"categories":null,"content":" 2022-04-20 《在生命的尽头拥抱你-临终关怀医生手记》 - 进度 73% 使用 aws/karpenter 实现集群弹性扩缩容 已上线 prod 环境，目前给 EMR on EKS 集群专用。 更新 /now 页面以及 knowledge 的内容 ","date":"2021-02-01","objectID":"/history/:0:44","series":null,"tags":null,"title":"曾经的我","uri":"/history/#2022-04-20"},{"categories":null,"content":" 2022-04-18 研究使用 aws/karpenter 实现集群弹性扩缩容 阅读《Go 程序设计语言（英文版）》 - 进度 53% 第 7 章「接口」读了一半，大概 22 pages，预计明天能完成 《Operating Systems - Three Easy Pieces》 读到 Introduction 一章，行文真的很有趣，看 projects 也有深度，决定了要把这本书看完，把习题做好。 OSTEP 后面的部分会涉及 vx6 源码，这要求比较深的 C 语言知识以及 x86 汇编知识，不过这些可以在学到的时候，再做补充。 在需要用到的时候，学习 CSAPP 的 x86 汇编部分会是一个比较好的补充。 ","date":"2021-02-01","objectID":"/history/:0:45","series":null,"tags":null,"title":"曾经的我","uri":"/history/#2022-04-18"},{"categories":null,"content":" 2022-04-17 阅读《Go 程序设计语言（英文版）》 - 进度 7/13 《在生命的尽头拥抱你-临终关怀医生手记》 - 进度 61% 重新整理书单，放到 /now 页面中 学习 NAT 原理知识 ","date":"2021-02-01","objectID":"/history/:0:46","series":null,"tags":null,"title":"曾经的我","uri":"/history/#2022-04-17"},{"categories":null,"content":" 2022-04-15 ~ 2022-04-16 阅读《Go 程序设计语言（英文版）》 - 进度 5/13 ","date":"2021-02-01","objectID":"/history/:0:47","series":null,"tags":null,"title":"曾经的我","uri":"/history/#2022-04-15--2022-04-16"},{"categories":null,"content":" 2022-04-14 阅读《Go 程序设计语言（英文版）》 - 进度 4/13 ","date":"2021-02-01","objectID":"/history/:0:48","series":null,"tags":null,"title":"曾经的我","uri":"/history/#2022-04-14"},{"categories":null,"content":" 2022-04-13 阅读《Go 程序设计语言（英文版）》 - 进度 3/13 ","date":"2021-02-01","objectID":"/history/:0:49","series":null,"tags":null,"title":"曾经的我","uri":"/history/#2022-04-13"},{"categories":null,"content":" 2022-04-10 学习 3D 引擎的使用，简单试用了 unity3d 与 unreal engine 5. 确定学习方向：先学学 UE5 蓝图入个门，然后试试把 MMD 模型导入到 UE5 做做动画，中间也会简单接触下 Blender. 感受：UE5 挺不错的，尤其是它还提供 VR 编辑模式，手上的 Quest 2 又能派上用场了 输出文档：3D 图形相关 阅读《Go 程序设计语言（英文版）》 - 进度 2/13 第一章「导览」大概过了下 Go 的关键特性：完善的工具链，丰富的标准库，goroutine, channel 第二章主要讲程序结构，包含变量、类型声明、指针、结构体、作用域、包与文件结构等等 ","date":"2021-02-01","objectID":"/history/:0:50","series":null,"tags":null,"title":"曾经的我","uri":"/history/#2022-04-10"},{"categories":null,"content":" 2021-04-08 - 2021-04-09 学习区块链技术 Web3.0 Mastering Ethereum - 以太坊入门 进度：100% 跳过了智能合约代码相关的内容，因为代码比较老了，新版本的 solidity 有了许多新变化。 Youtube - Solidity, Blockchain, and Smart Contract Course – Beginner to Expert Python Tutorial 这个视频及相关的 Github 仓库，包含一些区块链可视化以及相关的介绍，更适合学习完理论后，实战合约编写 区块链技术指南: 《Docker - 从入门到实践》作者的新书，内容同样简洁易懂，侧重介绍原理及知识面，非常棒。 ","date":"2021-02-01","objectID":"/history/:0:51","series":null,"tags":null,"title":"曾经的我","uri":"/history/#2021-04-08---2021-04-09"},{"categories":null,"content":" 2021-03-26 - 2021-04-01 学习区块链技术 Web3.0 Mastering Ethereum - 以太坊入门 进度：7/14 这书适合用于学习理论，solidity 开发相关的内容可以跳过，即 7/8 两章 Youtube - Solidity, Blockchain, and Smart Contract Course – Beginner to Expert Python Tutorial 这个视频及相关的 Github 仓库，包含一些区块链可视化以及相关的介绍，更适合学习完理论后，实战合约编写 ","date":"2021-02-01","objectID":"/history/:0:52","series":null,"tags":null,"title":"曾经的我","uri":"/history/#2021-03-26---2021-04-01"},{"categories":null,"content":" 2021-03-23 - 2021-3-25 阅读《在生命的尽头拥抱你-临终关怀医生手记》 在 Manager 的帮助下申请职级晋升（初级 =\u003e 中级 SRE） 再一次认识到我自己写的文字有多么随意… Manager 帮我提炼补充后，文字变得言简意赅，精确客观，瞬间高大上档次了。 ","date":"2021-02-01","objectID":"/history/:0:53","series":null,"tags":null,"title":"曾经的我","uri":"/history/#2021-03-23---2021-3-25"},{"categories":null,"content":" 2021-03-22 注册模之屋，简单学了下 MMD 跟 Blender ","date":"2021-02-01","objectID":"/history/:0:54","series":null,"tags":null,"title":"曾经的我","uri":"/history/#2021-03-22"},{"categories":null,"content":" 2021-03-15 - 2021-03-19 学习 Envoy，完成 Envoy 笔记 ","date":"2021-02-01","objectID":"/history/:0:55","series":null,"tags":null,"title":"曾经的我","uri":"/history/#2021-03-15---2021-03-19"},{"categories":null,"content":" 2021-03-11 - 2021-03-14 《写给开发人员的实用密码学》 完成第七篇「非对称加密算法」的 ECC 部分，并为 RSA 部分补充了部分 Python 代码 将去年写的文章《TLS 协议、TLS 证书、TLS 证书的配置方法、TLS 加密的破解手段》改写并补充内容，改名为《写给开发人员的实用密码学（八）—— 数字证书与 TLS 协议》 为第五篇「密钥交换」补充了 DHKE/ECDH 的代码示例，另外还补充了 DHE/ECDHE 一节 此系列文章的其他小修改与润色 业务大佬在 gRPC 的基础上再添加了 gzip 压缩，TX 流量再次下降 80%+ 侧面说明以前业务侧对 HTTP 的用法是多么豪放 emmmm 周末上 gzip 压缩功能，业务大佬太肝了啊… ","date":"2021-02-01","objectID":"/history/:0:56","series":null,"tags":null,"title":"曾经的我","uri":"/history/#2021-03-11---2021-03-14"},{"categories":null,"content":" 2022-03-09 发布《写给开发人员的实用密码学》系列第七篇：非对称加密算法，但是暂时只完成了 RSA 部分 ","date":"2021-02-01","objectID":"/history/:0:57","series":null,"tags":null,"title":"曾经的我","uri":"/history/#2022-03-09"},{"categories":null,"content":" 2022-03-07 - 2022-03-08 跟推荐系统大佬一起将服务从 HTTP 切换到 gRPC，效果立竿见影，服务流量下降 50% ~ 60%，延迟下降 30% ~ 50% 提升了服务性能，降低了 AWS 跨区流量成本 ","date":"2021-02-01","objectID":"/history/:0:58","series":null,"tags":null,"title":"曾经的我","uri":"/history/#2022-03-07---2022-03-08"},{"categories":null,"content":" 2022-03-05 - 2022-03-06 发布《写给开发人员的实用密码学》系列的第六篇：对称加密算法 ","date":"2021-02-01","objectID":"/history/:0:59","series":null,"tags":null,"title":"曾经的我","uri":"/history/#2022-03-05---2022-03-06"},{"categories":null,"content":" 2022-03-01 深圳疫情形式严峻，开始居家办公 整理润色后，发布《写给开发人员的实用密码学》前五篇的内容 ","date":"2021-02-01","objectID":"/history/:0:60","series":null,"tags":null,"title":"曾经的我","uri":"/history/#2022-03-01"},{"categories":null,"content":" 2022-02-19 - 2022-02-25 阅读 Practical Cryptography for Developers，同时完成我的密码学笔记 起因是想学下区块链技术，结果发现课程一开始就讲加密哈希函数的基本性质，就决定先搞一波密码学。 完成了《写给开发人员的实用密码学》前五篇的草稿。 研究 istio 的 gRPC 支持与监控指标 ","date":"2021-02-01","objectID":"/history/:0:61","series":null,"tags":null,"title":"曾经的我","uri":"/history/#2022-02-19---2022-02-25"},{"categories":null,"content":" 2022-02-17 发现我们的 EKS 集群主使用的是 AWS Spot 实例，这类实例的 c6i/c6g 性能与价格差距并不高，做 ARM 化的 ROI 貌似并不高 发现对 aws 的 RDS/EC2-Volume/Redis 等资源进行全面评估，删掉闲置资源、缩小实例/集群规格，可以轻易节省大量成本（说明以前申请资源时风格比较豪放 2333） 继续迭代个人博客 ","date":"2021-02-01","objectID":"/history/:0:62","series":null,"tags":null,"title":"曾经的我","uri":"/history/#2022-02-17"},{"categories":null,"content":" 2022-02-07 - 2022-02-16 迭代我的独立博客 https://thiscute.world 添加「阅读排行」页，定期从 Google Analytics 同步数据 从博客园迁移部分有价值的文章到独立博客 ","date":"2021-02-01","objectID":"/history/:0:63","series":null,"tags":null,"title":"曾经的我","uri":"/history/#2022-02-07---2022-02-16"},{"categories":null,"content":" 2022-01-08 - 2022-01-16 购入 Synthesizer V + 青溯 AI 声库，简单调了几首歌试试，效果非常棒。 也调研了下歌声合成领域目前的进展，试用了免费的移动端软件 ACE 虚拟歌姬，渲染效果真的媲美 CNY 999 的 SynthV AI 套装，不得不感叹 AI 的效果真的强啊。 ","date":"2021-02-01","objectID":"/history/:0:64","series":null,"tags":null,"title":"曾经的我","uri":"/history/#2022-01-08---2022-01-16"},{"categories":null,"content":" 2022-01-01 了解 APISIX/Nginx/Envoy 中的各种负载均衡算法，及其适用场景、局限性。 ","date":"2021-02-01","objectID":"/history/:0:65","series":null,"tags":null,"title":"曾经的我","uri":"/history/#2022-01-01"},{"categories":null,"content":" 2021-12-12 练习二个半小时轮滑，学会了压步转弯技术 无聊，但是又啥都不想干，耽于网络小说… 感觉有点现充了，感觉需要找个更明确的、能给人动力的目标 做个三年的职业规划以及生活规划？ ","date":"2021-02-01","objectID":"/history/:0:66","series":null,"tags":null,"title":"曾经的我","uri":"/history/#2021-12-12"},{"categories":null,"content":" 2021-11-21 轮滑：复习前双鱼、前剪、前蛇，尝试侧压步、倒滑 ","date":"2021-02-01","objectID":"/history/:0:67","series":null,"tags":null,"title":"曾经的我","uri":"/history/#2021-11-21"},{"categories":null,"content":" 2021-11-08 - 2021-11-12 将上次 EKS 升级过程中，有问题的服务迁移到 1.21 的 EKS 集群，直接切线上流量测试。 复现了问题，通过 JFR + pods 数量监控，确认到是服务链路上的个别服务频繁扩缩容导致的，这些服务比较重，对扩缩容比较敏感。 测试在 HPA 中添加 behavior 降低缩容速率，同时添加上 PodDisruptionBudget 以避免节点回收导致大量 Pod 被回收，经测试问题基本解决。 遭遇 AWS EKS 托管的控制平面故障，controller-manager 挂了一整天。现象非常奇怪，又是第一次遇到，导致长时间未排查到问题。 确认问题来自 HPA behavior 的 Bug 储存于 etcd 中的 object 仅会有一个版本，透过 apiserver 读取时会转换成请求的 autoscaling API 版本。 autoscaling/v2beta2 scaleUp 及 scaleDown 对象不能为 null，并在其 Kubernetse 代码可以查看到相应的检查机制。 当使用 autoscaling/v1 时，v2beta2 版本中的相关对象字段将作为 annotation 保留，apiserver 不会检查 ScaleUp/ScaleDown 的 annotation是否为 non-null，而导致 kube-controller-manager panic 问题。 我们可以使用 v1 或 v2beta2 创建一个 HPA 对象，然后使用 v1 或 v2beta2 读取、更新或删除该对象。 etcd 中存储的对象只有一个版本，每当您使用 v1 或 v2beta2 获取 HPA 对象时，apiserver 从 etcd 读取它，然后将其转换为您请求的版本。 在使用 kubectl 时，客户端将默认使用 v1(kubectl get hpa)，因此我们必须明确请求 v2beta2 才能使用这些功能(kubectl get hpa.v2beta2.autoscaling) 如果在更新 v1 版本的 HPA 时（kubectl 默认用 v1），手动修改了 v2beta2 功能相关的 annotation 将 scaleUp/scaleDown 设为 null，会导致 controller-manager 挂掉. ","date":"2021-02-01","objectID":"/history/:0:68","series":null,"tags":null,"title":"曾经的我","uri":"/history/#2021-11-08---2021-11-12"},{"categories":null,"content":" 2021-10-23 跟公司冲浪小分队，第一次玩冲浪，最佳成绩是在板上站了大概 6s… ","date":"2021-02-01","objectID":"/history/:0:69","series":null,"tags":null,"title":"曾经的我","uri":"/history/#2021-10-23"},{"categories":null,"content":" 2021-10/11 - 2021-10-19 将 EKS 集群从 1.17 升级到 1.21（新建集群切量的方式），但是遇到部分服务迁移后可用率抖动。 未定位到原因，升级失败，回滚了流量。 ","date":"2021-02-01","objectID":"/history/:0:70","series":null,"tags":null,"title":"曾经的我","uri":"/history/#2021-1011---2021-10-19"},{"categories":null,"content":" 2021-09-13 - 2021-09-17 学习极客时间《10X程序员工作法》 以终推始 识别关键问题 ownership ","date":"2021-02-01","objectID":"/history/:0:71","series":null,"tags":null,"title":"曾经的我","uri":"/history/#2021-09-13---2021-09-17"},{"categories":null,"content":" 2021-09-02 - 2021-09-11 EKS 集群升级 了解 EKS 集群的原地升级的细节 输出 EKS 集群原地升级的测试方案，以及生产环境的 EKS 集群升级方案 学习使用 kubeadm+containerd 部署 k8s 测试集群 涉及到的组件：Kuberntes 控制面、网络插件 Cilium、kube-proxy、coredns、containerd ","date":"2021-02-01","objectID":"/history/:0:72","series":null,"tags":null,"title":"曾经的我","uri":"/history/#2021-09-02---2021-09-11"},{"categories":null,"content":" 2021-08-31 - 2021-09-01 思考我在工作中遇到的一些非技术问题，寻找解法 效率：如何在没人 push 的情况下（没有外部压力），维持住高效率的工作状态（早早干完活下班它不香么？）。 建立有效的「自检」与「纠错」机制 自检： 列出目前已知的「异常」和「健康」两类工作状态，每日做一个对比。 每日都列一下详细的工作计划，精确到小时（预留 1 小时 buffer 应对临时需求）。 沟通：遇到问题（各种意义上的问题）时，及时沟通清楚再继续推进，是一件 ROI 非常高的事。否则几乎肯定会在后面的某个节点，被这个问题坑一把。 目前的关键目标是啥？存在哪些关键问题（实现关键目标最大的阻碍）？我最近做的主要工作，是不是在为关键目标服务？ 如何把安排到手上的事情做好？ 思考这件事情真正的目标的什么？ 比如任务是排查下某服务状态码有无问题，真正的目的应该是想知道服务有没有异常 达成真正的目标，需要做哪些事？ 不仅仅状态码需要排查，还有服务负载、内存、延迟的分位值，或许都可以看看。 跟需求方沟通，询问是否真正需要做的，是前面分析得出的事情。 这些问题都是有解法的，关键是思路的转换。 ","date":"2021-02-01","objectID":"/history/:0:73","series":null,"tags":null,"title":"曾经的我","uri":"/history/#2021-08-31---2021-09-01"},{"categories":null,"content":" 2021-08-28 =\u003e 2021-08-29 容器底层原理： linux namespace 与 cgroups linux 虚拟网络接口 macvlan/ipvlan、vlan、vxlan ","date":"2021-02-01","objectID":"/history/:0:74","series":null,"tags":null,"title":"曾经的我","uri":"/history/#2021-08-28--2021-08-29"},{"categories":null,"content":" 2021-08-19 =\u003e 2021-08-23 阅读 rust 语言的官方文档：the book 边读文档边做 rustlings 的小习题 目前完成了除 macros 之外的所有题 遇到的最难的题：conversions/{try_from_into, from_str} 使用 rust 重写了一版 video2chars ","date":"2021-02-01","objectID":"/history/:0:75","series":null,"tags":null,"title":"曾经的我","uri":"/history/#2021-08-19--2021-08-23"},{"categories":null,"content":" 2021-08-12 =\u003e 2021-08-16 Linux 的虚拟网络接口 Linux 的 netfilter 网络处理框架，以及其子项目 iptables/conntrack ","date":"2021-02-01","objectID":"/history/:0:76","series":null,"tags":null,"title":"曾经的我","uri":"/history/#2021-08-12--2021-08-16"},{"categories":null,"content":" 2021-03-11 =\u003e 2021-08-09 学习 nginx - openresty - apisix 工作中，在自己负责的领域，建立起 ownership 学习新公司的工作模式：OKR 工作法 学习新公司的思维模式（识别关键问题） 如何从公司的角度去思考问题，找到我们目前最应该做的事情 从以下角度去评价一件事情的重要性 这件事情对我们目前的目标有多大帮助？ 需要投入多少资源和人力？ 在推进过程中，有哪些阶段性成果或者 check point？ ","date":"2021-02-01","objectID":"/history/:0:77","series":null,"tags":null,"title":"曾经的我","uri":"/history/#2021-03-11--2021-08-09"},{"categories":null,"content":" 在这个信息爆炸的时代，更需要能够放慢脚步，沉下心，系统性的学习。 过去的我：学习轨迹记录 ","date":"2021-02-01","objectID":"/now/:0:0","series":null,"tags":null,"title":"此时此刻的我","uri":"/now/#"},{"categories":null,"content":" 一、我正在研究这些 按优先级排序 The ANSI C Programming Language - 83/236 快速过一遍语法 Go语言动手写Web框架 - 进度 20% 极客时间《OpenResty 从入门到实战》 ","date":"2021-02-01","objectID":"/now/:1:0","series":null,"tags":null,"title":"此时此刻的我","uri":"/now/#一我正在研究这些"},{"categories":null,"content":" 二、我今年还想搞搞这些今年的学习进展： Go 语言 Web 编程 已完成 阅读了《Go 程序设计语言》，学会了 Go 语言基础 使用 Go+Gin 完成了一个 Nginx 网关控制器项目并上线，实现了一些简单的接口 待完成 Go 语言高性能编程 7天用Go从零实现分布式缓存GeeCache 7天用Go从零实现ORM框架GeeORM 7天用Go从零实现RPC框架GeeRPC balancer: 源码阅读，如何使用 go 实现常见 balancer 算法 Kubernetes 原理、Kubernetes APIServer/Operator 编程 已完成 学习了极客时间《深入理解 Kubernetes》专栏 完成了一个 Nginx 网关项目，熟悉并应用了 client-go 的 informer 待完成 Programming Kubernetes - Developing Cloud Native Applications Linux 性能调优与 Linux 网络技术 待完成 C 语言基础复习 The ANSI C Programming Language - 用来快速复习下 C 的语法 通过其他资料补充学习 make gdb ld objdump objcopy 等命令 Linux/Unix 系统编程手册（上册） - 进度 21/572 学习 Linux 的顶级书藉，据说内容组织比 APUE 对新手更友好些。 《深入理解 Linux 网络 - 张彦飞》 - 14/320 极客时间《网络排查案例课》 极客时间 《Linux 性能优化实战》 区块链与分布式系统 已完成 《Mastering Ethereum》 - 100% 待完成 极客时间《分布式协议与算法实战》 - 40% 机器学习与深度学习 待完成 动手学深度学习 - Pytorch 版 - 14.3% ","date":"2021-02-01","objectID":"/now/:2:0","series":null,"tags":null,"title":"此时此刻的我","uri":"/now/#二我今年还想搞搞这些"},{"categories":null,"content":" 三、今年的阅读进展 电子版都可以在 z-library 上很方便地下载到，实体书的话可以在多抓鱼等二手书平台碰碰运气。 已读： 《人间失格》 《月宫》 《Practical Cryptography for Developers》 《Mastering Ethereum》 《Go 程序设计语言（英文版）》 《深入浅出 Kubernetes - 张磊》 《在生命的尽头拥抱你-临终关怀医生手记》 《在峡江的转弯处 - 陈行甲人生笔记》 正在读： 《The ANSI C Programming Language》：上大学时看过中文版。为了学操作系统，有必要再看一遍，这次就读原著英文版吧。 阅读 Programming Kubernetes - Developing Cloud Native Applications - 进度 7% 想读，但是没啥计划： 《Intimate Relationships》 - 进度 14/449 《语言学的邀请》- 进度 68/288 对语言学有点兴趣，同时听说这本书对表达（沟通、写作）也大有帮助~ 《云原生服务网格 Istio：原理、实践、架构与源码解析》 比较老的书了，不过用来学下 Istio 的底层架构跟源码，感觉还是有价值的。 《手把手教你读财报》 《原则 - 应对变化中的世界秩序》 《生命最后的读书会》 《凤凰项目：一个 IT 运维的传奇故事》 《复杂 - 梅拉尼 米歇尔》 《性能之巅（第二版）：企业与云可观测性》 《BPF Performance Tools（英文版）》 《语言学的邀请》：对语言学有点兴趣，同时听说这本书对表达（沟通、写作）也大有帮助~ 《Intimate Relationships》 《Social Psychology, 13e, David Myers》 《Principles Of Economics, 9e, N. Gregory Mankiw》 ","date":"2021-02-01","objectID":"/now/:3:0","series":null,"tags":null,"title":"此时此刻的我","uri":"/now/#三今年的阅读进展"},{"categories":null,"content":" 四、我的知识清单","date":"2021-02-01","objectID":"/now/:4:0","series":null,"tags":null,"title":"此时此刻的我","uri":"/now/#四我的知识清单"},{"categories":null,"content":" 1. 高优先级 写几个小项目（使用 rust/go） 实现一个文本编辑器 https://viewsourcecode.org/snaptoken/kilo/ 实现一个简单的 Linux 容器 https://blog.lizzie.io/linux-containers-in-500-loc.html 网络代理（不到 2000 行的 TUN 库） https://github.com/songgao/water Go 语言进阶 《Go 学习笔记（第六版下卷）》 基于 go 1.10，详细分析 go 的实现机制：内存分配、垃圾回收、并发调度等等 Linux 系统 极客时间 《Linux 内核技术实战课》 flash-linux0.11-talk 极客时间《容器实战高手课》 极客时间《eBPF 核心技术与实战》 C 语言进阶 极客时间《深入 C 语言和程序运行原理》 Openresty 技术栈：（暂时感觉兴趣不大） 阅读《Lua 程序设计》 阅读《自己动手实现 Lua》 阅读 APISIX 源码 + Openresty 深入学习 Nginx 及 epoll 容器底层原理 容器镜像的文件系统：overlayfs 镜像的构建流程：研究 buildkit/buildah 的实现 Security Training for Engineers - PagerDuty: 花几个小时，快速学习开发人员需要了解的安全知识 生活： 娱乐+运动： 轮滑：倒滑后压步 游泳：学会蛙泳并且提升速度 ","date":"2021-02-01","objectID":"/now/:4:1","series":null,"tags":null,"title":"此时此刻的我","uri":"/now/#1-高优先级"},{"categories":null,"content":" 2. 中优先级 附一份屌炸天的 CS 自学指南：https://github.com/pkuflyingpig/cs-self-learning/ 学习英语，目标是能流利地读写交流。 主要是可以扩宽工作的选择面，外企很多职位会要求英文读写流利。 分布式协议与区块链 《区块链核心算法解析》 《Design Data-Intensive Applications》 《Blockchain in Action》 《Distributed Systems, 3rd Edition, 2017》 《Distributed Algorithms, 2nd Edition, 2018》 Rust 语言 极客时间《Rust 编程第一课》 [进阶]操作系统（大概是以 OSTEP 为核心，学习时缺啥补啥吧）： 核心课程：Operating Systems - Three Easy Pieces: 建议结合 6.S081 课程一起学习 OSTEP 学习指南：https://github.com/ryan4yin/computer-science/blob/master/coursepages/coresystems/ostep/OSTEP.md 学到 xv6 时可结合这份资料啃源码：xv6-annotated Advanced Programming in the UNIX Environment, 3rd Edition: 同样是 Linux/Unix 系统的神书。学 OSTEP 遇到瓶颈时或可阅读。 Systems Performance: Enterprise and the Cloud, 2nd Edition (2020): 进阶读物，搞系统性能优化的 计算机网络 《Computer Networking - A Top-Down Approach, 7e》：这本书我以前学过一次，但是主要只学了应用层到传输层的内容。 可以结合 CS 144: Introduction to Computer Networking 课程一起学习，不过我 C++ 全忘了，或许可以考虑用 rust/go 实现下协议栈？ TCP/IP 协议栈的实现：如果用 rust 的话，可以参考 google/gvisor [进阶]数据库、数据结构与算法（暂时感觉兴趣不大） kv 数据库 kv 数据库的简单实现: https://github.com/tidb-incubator/tinykv redis 原理 关系数据库 mysql/postgresql 底层原理 实现简单的关系数据库: https://github.com/tidb-incubator/tinysql 搜索技术 这就是搜索引擎 https://github.com/huichen/wukong 极客时间《检索技术 25 讲》 ","date":"2021-02-01","objectID":"/now/:4:2","series":null,"tags":null,"title":"此时此刻的我","uri":"/now/#2-中优先级"},{"categories":null,"content":" 五、我的备选书单如下是我目前想读的书单，如果决定读，就把对应的书移到「计划读」中。 文学类： 《百年孤独》：高中的时候读过一遍，但是都忘差不多了 《霍乱时期的爱情》 《苏菲的世界》：据说是哲学启蒙读物，曾经看过，但是对内容完全没印象了。 《你一生的故事》：我也曾是个科幻迷 《沈从文的后半生》 《房思琪的初恋乐园》 《月光落在左手上》 《了不起的盖茨比》 《The Windup Girl》：高中时读过中文版，刷新我三观，现在想再读一遍英文原版。 人文社科 《被讨厌的勇气》 《这才是心理学 - 看穿伪科学的批判性思维 第 11 版》 《江城》 《探路之役 - 1978-1922 年的中国经济改革》 《筚路维艰 - 中国社会主义路径的五次选择》 《刘擎西方现代思想讲义》 《圆圈正义-作为自由前提的信念》 《科学革命的结构》 《人体简史》 《邓小平时代》 《论中国》 《时间的秩序》 《极简宇宙史》 《人生脚本》 《投资中最简单的事》 《债务危机 - 我的应对原则》 技术类 《凤凰项目——一个 IT 运维的传奇故事》 《人月神话》 《绩效使能：超越 OKR》 《奈飞文化手册》 《幕后产品-打造突破式思维》 《分析与思考 - 黄奇帆的复旦经济课》：这本书会需要一定的经济学基础知识，打算在入门经济学后再看 《重构 - 改善既有代码的设计》 The Rust Programming Language: 2021 年 8 月读过，2022 可以再搞一搞，主要用来写写网络、操作系统。 SQL进阶教程 分布式系统：Designing Data-Intensive Applications - 可结合 MIT 6.824 课程视频学习 数据库系统：建议直接学习课程 CMU 15-445 《WebAssembly 核心原理》 用 Go 语言讲编程语言理论 《自己动手实现 Lua》 《自己动手实现 Java 虚拟机》 编程语言理论（如何设计一个编程语言） 《Crafting Interpreters》：亚马逊销量第一的编译器设计书籍，好评如潮。 之前挑战《编程语言实现模式》，很遗憾失败了，这次我决定拿此书再战。 Essentials of Programming Languages, 3rd Edition The Little Schemer - 4th Edition Kubernetes 与容器 Hacking Kubernetes: Threat-Driven Analysis and Defense: Kubernetes 安全，威胁模型以及如何防护。 Container Security: Fundamental Technology Concepts that Protect Containerized Applications: 容器安全，这书在亚马逊上评价很好。 ","date":"2021-02-01","objectID":"/now/:5:0","series":null,"tags":null,"title":"此时此刻的我","uri":"/now/#五我的备选书单"},{"categories":null,"content":" 其他 ideas 我看到 github 上 gopala-kr/10-weeks 这个项目，作者进行了一项挑战——每周学习一个新技术栈，目标是在一周内理解该技术栈各项热词的含义并列出大纲，使用该技术构建一个简单的程序，并写一篇博客。我觉得我也可以试试，不过可以把难度降低一些——利用业余时间，每两个月学习一门新技术，并达成与 gopala-kr 类似的目标。 其他感兴趣的 前端：Preact+Css 3D 建模与渲染：Blender、Unreal Engine 5、C++、taichi 音乐：乐理、Synthesizer V、Reaper、midi 键盘 其他：利用深度学习进行歌声合成、图片分辨率修复（超分辨率）、以及其他好玩的玩法 ","date":"2021-02-01","objectID":"/now/:6:0","series":null,"tags":null,"title":"此时此刻的我","uri":"/now/#其他-ideas"},{"categories":["tech"],"content":" 注意：这篇文章并不是一篇入门教程，学习 Argo Workflows 请移步官方文档 Argo Documentation Argo Workflows 是一个云原生工作流引擎，专注于编排并行任务。它的特点如下： 使用 Kubernetes 自定义资源(CR)定义工作流，其中工作流中的每个步骤都是一个容器。 将多步骤工作流建模为一系列任务，或者使用有向无环图（DAG）描述任务之间的依赖关系。 可以在短时间内轻松运行用于机器学习或数据处理的计算密集型作业。 Argo Workflows 可以看作 Tekton 的加强版，因此显然也可以通过 Argo Workflows 运行 CI/CD 流水线(Pipielines)。 阿里云是 Argo Workflows 的深度使用者和贡献者，另外 Kubeflow 底层的工作流引擎也是 Argo Workflows. ","date":"2021-01-27","objectID":"/posts/expirence-of-argo-workflow/:0:0","series":null,"tags":["云原生","CI","持续集成","流水线"],"title":"云原生流水线 Argo Workflows 的安装、使用以及个人体验","uri":"/posts/expirence-of-argo-workflow/#"},{"categories":["tech"],"content":" 一、Argo Workflows 对比 Jenkins我们在切换到 Argo Workflows 之前，使用的 CI/CD 工具是 Jenkins，下面对 Argo Workflows 和 Jenkins 做一个比较详细的对比， 以了解 Argo Workflows 的优缺点。 ","date":"2021-01-27","objectID":"/posts/expirence-of-argo-workflow/:1:0","series":null,"tags":["云原生","CI","持续集成","流水线"],"title":"云原生流水线 Argo Workflows 的安装、使用以及个人体验","uri":"/posts/expirence-of-argo-workflow/#一argo-workflows-对比-jenkins"},{"categories":["tech"],"content":" 1. Workflow 的定义Workflow 使用 kubernetes CR 进行定义，因此显然是一份 yaml 配置。 一个 Workflow，就是一个运行在 Kubernetes 上的流水线，对应 Jenkins 的一次 Build. 而 WorkflowTemplate 则是一个可重用的 Workflow 模板，对应 Jenkins 的一个 Job. WorkflowTemplate 的 yaml 定义和 Workflow 完全一致，只有 Kind 不同！ WorkflowTemplate 可以被其他 Workflow 引用并触发，也可以手动传参以生成一个 Workflow 工作流。 ","date":"2021-01-27","objectID":"/posts/expirence-of-argo-workflow/:1:1","series":null,"tags":["云原生","CI","持续集成","流水线"],"title":"云原生流水线 Argo Workflows 的安装、使用以及个人体验","uri":"/posts/expirence-of-argo-workflow/#1-workflow-的定义"},{"categories":["tech"],"content":" 2. Workflow 的编排Argo Workflows 相比其他流水线项目(Jenkins/Tekton/Drone/Gitlab-CI)而言，最大的特点，就是它强大的流水线编排能力。 其他流水线项目，对流水线之间的关联性考虑得很少，基本都假设流水线都是互相独立的。 而 Argo Workflows 则假设「任务」之间是有依赖关系的，针对这个依赖关系，它提供了两种协调编排「任务」的方法：Steps 和 DAG 再借助 templateRef 或者 Workflow of Workflows，就能实现 Workflows 的编排了。 我们之所以选择 Argo Workflows 而不是 Tekton，主要就是因为 Argo 的流水线编排能力比 Tekton 强大得多。（也许是因为我们的后端中台结构比较特殊，导致我们的 CI 流水线需要具备复杂的编排能力） 一个复杂工作流的示例如下： https://github.com/argoproj/argo/issues/1088#issuecomment-445884543 ","date":"2021-01-27","objectID":"/posts/expirence-of-argo-workflow/:1:2","series":null,"tags":["云原生","CI","持续集成","流水线"],"title":"云原生流水线 Argo Workflows 的安装、使用以及个人体验","uri":"/posts/expirence-of-argo-workflow/#2-workflow-的编排"},{"categories":["tech"],"content":" 3. Workflow 的声明式配置Argo 使用 Kubernetes 自定义资源(CR)来定义 Workflow，熟悉 Kubernetes Yaml 的同学上手应该都很快。 下面对 Workflow 定义文件和 Jenkinsfile 做个对比： argo 完全使用 yaml 来定义流水线，学习成本比 Jenkinsfile 的 groovy 低。对熟悉 Kubernetes 的同学尤其如此。 将 jenkinsfile 用 argo 重写后，代码量出现了明显的膨胀。一个 20 行的 Jenkinsfile，用 Argo 重写可能就变成了 60 行。 配置出现了膨胀是个问题，但是考虑到它的可读性还算不错， 而且 Argo 的 Workflow 编排功能，能替代掉我们目前维护的部分 Python 构建代码，以及一些其他优点，配置膨胀这个问题也就可以接受了。 ","date":"2021-01-27","objectID":"/posts/expirence-of-argo-workflow/:1:3","series":null,"tags":["云原生","CI","持续集成","流水线"],"title":"云原生流水线 Argo Workflows 的安装、使用以及个人体验","uri":"/posts/expirence-of-argo-workflow/#3-workflow-的声明式配置"},{"categories":["tech"],"content":" 4. Web UIArgo Workflows 的 Web UI 感觉还很原始。确实该支持的功能都有，但是它貌似不是面向「用户」的，功能比较底层。 它不像 Jenkins 一样，有很友好的使用界面(虽然说 Jenkins 的 UI 也很显老…) 另外它所有的 Workflow 都是相互独立的，没办法直观地找到一个 WorkflowTemplate 的所有构建记录，只能通过 label/namespace 进行分类，通过任务名称进行搜索。 而 Jenkins 可以很方便地看到同一个 Job 的所有构建历史。 ","date":"2021-01-27","objectID":"/posts/expirence-of-argo-workflow/:1:4","series":null,"tags":["云原生","CI","持续集成","流水线"],"title":"云原生流水线 Argo Workflows 的安装、使用以及个人体验","uri":"/posts/expirence-of-argo-workflow/#4-web-ui"},{"categories":["tech"],"content":" 5. Workflow 的分类 为何需要对 Workflow 做细致的分类常见的微服务项目，往往会拆分成众多 Git 仓库（微服务）进行开发，众多的 Git 仓库会使我们创建众多的 CI/CD 流水线。 如果没有任何的分类，这一大堆的流水线如何管理，就成了一个难题。 最显见的需求：前端和后端的流水线最好能区分一下，往下细分，前端的 Web 端和客户端最好也能区分，后端的业务层和中台最好也区分开来。 另外我们还希望将运维、自动化测试相关的任务也集成到这个系统中来（目前我们就是使用 Jenkins 完成运维、自动化测试任务的）， 如果没有任何分类，这一大堆流水线将混乱无比。 Argo Workflows 的分类能力当 Workflow 越来越多的时候，如果不做分类，一堆 WorkflowTemplate 堆在一起就会显得特别混乱。（没错，我觉得 Drone 就有这个问题…） Argo 是完全基于 Kubernetes 的，因此目前它也只能通过 namespace/labels 进行分类。 这样的分类结构和 Jenkins 的视图-文件夹体系大相径庭，目前感觉不是很好用（也可能纯粹是 Web UI 的锅）。 ","date":"2021-01-27","objectID":"/posts/expirence-of-argo-workflow/:1:5","series":null,"tags":["云原生","CI","持续集成","流水线"],"title":"云原生流水线 Argo Workflows 的安装、使用以及个人体验","uri":"/posts/expirence-of-argo-workflow/#5-workflow-的分类"},{"categories":["tech"],"content":" 5. Workflow 的分类 为何需要对 Workflow 做细致的分类常见的微服务项目，往往会拆分成众多 Git 仓库（微服务）进行开发，众多的 Git 仓库会使我们创建众多的 CI/CD 流水线。 如果没有任何的分类，这一大堆的流水线如何管理，就成了一个难题。 最显见的需求：前端和后端的流水线最好能区分一下，往下细分，前端的 Web 端和客户端最好也能区分，后端的业务层和中台最好也区分开来。 另外我们还希望将运维、自动化测试相关的任务也集成到这个系统中来（目前我们就是使用 Jenkins 完成运维、自动化测试任务的）， 如果没有任何分类，这一大堆流水线将混乱无比。 Argo Workflows 的分类能力当 Workflow 越来越多的时候，如果不做分类，一堆 WorkflowTemplate 堆在一起就会显得特别混乱。（没错，我觉得 Drone 就有这个问题…） Argo 是完全基于 Kubernetes 的，因此目前它也只能通过 namespace/labels 进行分类。 这样的分类结构和 Jenkins 的视图-文件夹体系大相径庭，目前感觉不是很好用（也可能纯粹是 Web UI 的锅）。 ","date":"2021-01-27","objectID":"/posts/expirence-of-argo-workflow/:1:5","series":null,"tags":["云原生","CI","持续集成","流水线"],"title":"云原生流水线 Argo Workflows 的安装、使用以及个人体验","uri":"/posts/expirence-of-argo-workflow/#为何需要对-workflow-做细致的分类"},{"categories":["tech"],"content":" 5. Workflow 的分类 为何需要对 Workflow 做细致的分类常见的微服务项目，往往会拆分成众多 Git 仓库（微服务）进行开发，众多的 Git 仓库会使我们创建众多的 CI/CD 流水线。 如果没有任何的分类，这一大堆的流水线如何管理，就成了一个难题。 最显见的需求：前端和后端的流水线最好能区分一下，往下细分，前端的 Web 端和客户端最好也能区分，后端的业务层和中台最好也区分开来。 另外我们还希望将运维、自动化测试相关的任务也集成到这个系统中来（目前我们就是使用 Jenkins 完成运维、自动化测试任务的）， 如果没有任何分类，这一大堆流水线将混乱无比。 Argo Workflows 的分类能力当 Workflow 越来越多的时候，如果不做分类，一堆 WorkflowTemplate 堆在一起就会显得特别混乱。（没错，我觉得 Drone 就有这个问题…） Argo 是完全基于 Kubernetes 的，因此目前它也只能通过 namespace/labels 进行分类。 这样的分类结构和 Jenkins 的视图-文件夹体系大相径庭，目前感觉不是很好用（也可能纯粹是 Web UI 的锅）。 ","date":"2021-01-27","objectID":"/posts/expirence-of-argo-workflow/:1:5","series":null,"tags":["云原生","CI","持续集成","流水线"],"title":"云原生流水线 Argo Workflows 的安装、使用以及个人体验","uri":"/posts/expirence-of-argo-workflow/#argo-workflows-的分类能力"},{"categories":["tech"],"content":" 6. 触发构建的方式Argo Workflows 的流水线有多种触发方式： 手动触发：手动提交一个 Workflow，就能触发一次构建。可以通过 workflowTemplateRef 直接引用一个现成的流水线模板。 定时触发：CronWorkflow 通过 Git 仓库变更触发：借助 argo-events 可以实现此功能，详见其文档。 另外目前也不清楚 WebHook 的可靠程度如何，会不会因为宕机、断网等故障，导致 Git 仓库变更了，而 Workflow 却没触发，而且还没有任何显眼的错误通知？如果这个错误就这样藏起来了，就可能会导致很严重的问题！ ","date":"2021-01-27","objectID":"/posts/expirence-of-argo-workflow/:1:6","series":null,"tags":["云原生","CI","持续集成","流水线"],"title":"云原生流水线 Argo Workflows 的安装、使用以及个人体验","uri":"/posts/expirence-of-argo-workflow/#6-触发构建的方式"},{"categories":["tech"],"content":" 7. secrets 管理Argo Workflows 的流水线，可以从 kubernetes secrets/configmap 中获取信息，将信息注入到环境变量中、或者以文件形式挂载到 Pod 中。 Git 私钥、Harbor 仓库凭据、CD 需要的 kubeconfig，都可以直接从 secrets/configmap 中获取到。 另外因为 Vault 很流行，也可以将 secrets 保存在 Vault 中，再通过 vault agent 将配置注入进 Pod。 ","date":"2021-01-27","objectID":"/posts/expirence-of-argo-workflow/:1:7","series":null,"tags":["云原生","CI","持续集成","流水线"],"title":"云原生流水线 Argo Workflows 的安装、使用以及个人体验","uri":"/posts/expirence-of-argo-workflow/#7-secrets-管理"},{"categories":["tech"],"content":" 8. ArtifactsArgo 支持接入对象存储，做全局的 Artifact 仓库，本地可以使用 MinIO. 使用对象存储存储 Artifact，最大的好处就是可以在 Pod 之间随意传数据，Pod 可以完全分布式地运行在 Kubernetes 集群的任何节点上。 另外也可以考虑借助 Artifact 仓库实现跨流水线的缓存复用（未测试），提升构建速度。 ","date":"2021-01-27","objectID":"/posts/expirence-of-argo-workflow/:1:8","series":null,"tags":["云原生","CI","持续集成","流水线"],"title":"云原生流水线 Argo Workflows 的安装、使用以及个人体验","uri":"/posts/expirence-of-argo-workflow/#8-artifacts"},{"categories":["tech"],"content":" 9. 容器镜像的构建借助 Buildkit 等容器镜像构建工具，可以实现容器镜像的分布式构建。 Buildkit 对构建缓存的支持也很好，可以直接将缓存存储在容器镜像仓库中。 不建议使用 Google 的 Kaniko，它对缓存复用的支持不咋地，社区也不活跃。 ","date":"2021-01-27","objectID":"/posts/expirence-of-argo-workflow/:1:9","series":null,"tags":["云原生","CI","持续集成","流水线"],"title":"云原生流水线 Argo Workflows 的安装、使用以及个人体验","uri":"/posts/expirence-of-argo-workflow/#9-容器镜像的构建"},{"categories":["tech"],"content":" 10. 客户端/SDKArgo 有提供一个命令行客户端，也有 HTTP API 可供使用。 如下项目值得试用： argo-client-python: Argo Workflows 的 Python 客户端 说实话，感觉和 kubernetes-client/python 一样难用，毕竟都是 openapi-generator 生成出来的… argo-python-dsl: 使用 Python DSL 编写 Argo Workflows 感觉使用难度比 yaml 高，也不太好用。 couler: 为 Argo/Tekton/Airflow 提供统一的构建与管理接口 理念倒是很好，待研究 感觉 couler 挺不错的，可以直接用 Python 写 WorkflowTemplate，这样就一步到位，所有 CI/CD 代码全部是 Python 了。 此外，因为 Argo Workflows 是 kubernetes 自定义资源 CR，也可以使用 helm/kustomize 来做 workflow 的生成。 目前我们一些步骤非常多，但是重复度也很高的 Argo 流水线配置，就是使用 helm 生成的——关键数据抽取到 values.yaml 中，使用 helm 模板 + range 循环来生成 workflow 配置。 ","date":"2021-01-27","objectID":"/posts/expirence-of-argo-workflow/:1:10","series":null,"tags":["云原生","CI","持续集成","流水线"],"title":"云原生流水线 Argo Workflows 的安装、使用以及个人体验","uri":"/posts/expirence-of-argo-workflow/#10-客户端sdk"},{"categories":["tech"],"content":" 二、安装 Argo Workflows 参考官方文档：https://argoproj.github.io/argo-workflows/installation/ 安装一个集群版(cluster wide)的 Argo Workflows，使用 MinIO 做 artifacts 存储： kubectl apply -f https://raw.githubusercontent.com/argoproj/argo/stable/manifests/install.yaml 部署 MinIO: helm repo add minio https://helm.min.io/ # official minio Helm charts # 查看历史版本 helm search repo minio/minio -l | head # 下载并解压 chart helm pull minio/minio --untar --version 8.0.9 # 编写 custom-values.yaml，然后部署 minio kubectl create namespace minio helm install minio ./minio -n argo -f custom-values.yaml minio 部署好后，它会将默认的 accesskey 和 secretkey 保存在名为 minio 的 secret 中。 我们需要修改 argo 的配置，将 minio 作为它的默认 artifact 仓库。 在 configmap workflow-controller-configmap 的 data 中添加如下字段： artifactRepository: | # 是否将 main 容器的日志保存为 artifact，这样 pod 被删除后，仍然可以在 artifact 中找到日志 archiveLogs: true s3: bucket: argo-bucket # bucket 名称，这个 bucket 需要先手动创建好！ endpoint: minio:9000 # minio 地址 insecure: true # 从 minio 这个 secret 中获取 key/secret accessKeySecret: name: minio key: accesskey secretKeySecret: name: minio key: secretkey 现在还差最后一步：手动进入 minio 的 Web UI，创建好 argo-bucket 这个 bucket. 直接访问 minio 的 9000 端口（需要使用 nodeport/ingress 等方式暴露此端口）就能进入 Web UI，使用前面提到的 secret minio 中的 key/secret 登录，就能创建 bucket. ","date":"2021-01-27","objectID":"/posts/expirence-of-argo-workflow/:2:0","series":null,"tags":["云原生","CI","持续集成","流水线"],"title":"云原生流水线 Argo Workflows 的安装、使用以及个人体验","uri":"/posts/expirence-of-argo-workflow/#二安装-argo-workflows"},{"categories":["tech"],"content":" ServiceAccount 配置 https://argoproj.github.io/argo-workflows/service-accounts/ Argo Workflows 依赖于 ServiceAccount 进行验证与授权，而且默认情况下，它使用所在 namespace 的 default ServiceAccount 运行 workflow. 可 default 这个 ServiceAccount 默认根本没有任何权限！所以 Argo 的 artifacts, outputs, access to secrets 等功能全都会因为权限不足而无法使用！ 为此，Argo 的官方文档提供了两个解决方法。 方法一，直接给 default 绑定 cluster-admin ClusterRole，给它集群管理员的权限，只要一行命令（但是显然安全性堪忧）： kubectl create rolebinding default-admin --clusterrole=admin --serviceaccount=\u003cnamespace\u003e:default -n \u003cnamespace\u003e 方法二，官方给出了Argo Workflows 需要的最小权限的 Role 定义，方便起见我将它改成一个 ClusterRole: apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: name: argo-workflow-role rules: # pod get/watch is used to identify the container IDs of the current pod # pod patch is used to annotate the step's outputs back to controller (e.g. artifact location) - apiGroups: - \"\" resources: - pods verbs: - get - watch - patch # logs get/watch are used to get the pods logs for script outputs, and for log archival - apiGroups: - \"\" resources: - pods/log verbs: - get - watch 创建好上面这个最小的 ClusterRole，然后为每个名字空间，跑一下如下命令，给 default 账号绑定这个 clusterrole: kubectl create rolebinding default-argo-workflow --clusterrole=argo-workflow-role --serviceaccount=\u003cnamespace\u003e:default -n \u003cnamespace\u003e 这样就能给 default 账号提供最小的 workflow 运行权限。 或者如果你希望使用别的 ServiceAccount 来运行 workflow，也可以自行创建 ServiceAccount，然后再走上面方法二的流程，但是最后，要记得在 workflow 的 spec.serviceAccountName 中设定好 ServiceAccount 名称。 ","date":"2021-01-27","objectID":"/posts/expirence-of-argo-workflow/:2:1","series":null,"tags":["云原生","CI","持续集成","流水线"],"title":"云原生流水线 Argo Workflows 的安装、使用以及个人体验","uri":"/posts/expirence-of-argo-workflow/#serviceaccount-配置"},{"categories":["tech"],"content":" Workflow Executors https://argoproj.github.io/argo-workflows/workflow-executors/ Workflow Executor 是符合特定接口的一个进程(Process)，Argo 可以通过它执行一些动作，如监控 Pod 日志、收集 Artifacts、管理容器生命周期等等… Workflow Executor 有多种实现，可以通过前面提到的 configmap workflow-controller-configmap 来选择。 可选项如下： docker(默认): 目前使用范围最广，但是安全性最差。它要求一定要挂载访问 docker.sock，因此一定要 root 权限！ kubelet: 应用非常少，目前功能也有些欠缺，目前也必须提供 root 权限 Kubernetes API (k8sapi): 直接通过调用 k8sapi 实现日志监控、Artifacts 手机等功能，非常安全，但是性能欠佳。 Process Namespace Sharing (pns): 安全性比 k8sapi 差一点，因为 Process 对其他所有容器都可见了。但是相对的性能好很多。 在 docker 被 kubernetes 抛弃的当下，如果你已经改用 containerd 做为 kubernetes 运行时，那 argo 将会无法工作，因为它默认使用 docker 作为运行时！ 我们建议将 workflow executore 改为 pns，兼顾安全性与性能，workflow-controller-configmap 按照如下方式修改： apiVersion: v1 kind: ConfigMap metadata: name: workflow-controller-configmap data: config: | # ...省略若干配置... # Specifies the container runtime interface to use (default: docker) # must be one of: docker, kubelet, k8sapi, pns containerRuntimeExecutor: pns # ... ","date":"2021-01-27","objectID":"/posts/expirence-of-argo-workflow/:2:2","series":null,"tags":["云原生","CI","持续集成","流水线"],"title":"云原生流水线 Argo Workflows 的安装、使用以及个人体验","uri":"/posts/expirence-of-argo-workflow/#workflow-executors"},{"categories":["tech"],"content":" 三、使用 Argo Workflows 做 CI 工具官方的 Reference 还算详细，也有提供非常多的 examples 供我们参考，这里提供我们几个常用的 workflow 定义。 使用 buildkit 构建镜像：https://github.com/argoproj/argo-workflows/blob/master/examples/buildkit-template.yaml buildkit 支持缓存，可以在这个 example 的基础上自定义参数 注意使用 PVC 来跨 step 共享存储空间这种手段，速度会比通过 artifacts 高很多。 ","date":"2021-01-27","objectID":"/posts/expirence-of-argo-workflow/:3:0","series":null,"tags":["云原生","CI","持续集成","流水线"],"title":"云原生流水线 Argo Workflows 的安装、使用以及个人体验","uri":"/posts/expirence-of-argo-workflow/#三使用-argo-workflows-做-ci-工具"},{"categories":["tech"],"content":" 四、常见问题","date":"2021-01-27","objectID":"/posts/expirence-of-argo-workflow/:4:0","series":null,"tags":["云原生","CI","持续集成","流水线"],"title":"云原生流水线 Argo Workflows 的安装、使用以及个人体验","uri":"/posts/expirence-of-argo-workflow/#四常见问题"},{"categories":["tech"],"content":" 1. workflow 默认使用 root 账号？workflow 的流程默认使用 root 账号，如果你的镜像默认使用非 root 账号，而且要修改文件，就很可能遇到 Permission Denined 的问题。 解决方法：通过 Pod Security Context 手动设定容器的 user/group: Workflow Pod Security Context 安全起见，我建议所有的 workflow 都手动设定 securityContext，示例： apiVersion: argoproj.io/v1alpha1 kind: WorkflowTemplate metadata: name: xxx spec: securityContext: runAsNonRoot: true runAsUser: 1000 或者也可以通过 workflow-controller-configmap 的 workflowDefaults 设定默认的 workflow 配置。 ","date":"2021-01-27","objectID":"/posts/expirence-of-argo-workflow/:4:1","series":null,"tags":["云原生","CI","持续集成","流水线"],"title":"云原生流水线 Argo Workflows 的安装、使用以及个人体验","uri":"/posts/expirence-of-argo-workflow/#1-workflow-默认使用-root-账号"},{"categories":["tech"],"content":" 2. 如何从 hashicorp vault 中读取 secrets? 参考 Support to get secrets from Vault hashicorp vault 目前可以说是云原生领域最受欢迎的 secrets 管理工具。 我们在生产环境用它做为分布式配置中心，同时在本地 CI/CD 中，也使用它存储相关的敏感信息。 现在迁移到 argo，我们当然希望能够有一个好的方法从 vault 中读取配置。 目前最推荐的方法，是使用 vault 的 vault-agent，将 secrets 以文件的形式注入到 pod 中。 通过 valut-policy - vault-role - k8s-serviceaccount 一系列认证授权配置，可以制定非常细粒度的 secrets 权限规则，而且配置信息阅后即焚，安全性很高。 ","date":"2021-01-27","objectID":"/posts/expirence-of-argo-workflow/:4:2","series":null,"tags":["云原生","CI","持续集成","流水线"],"title":"云原生流水线 Argo Workflows 的安装、使用以及个人体验","uri":"/posts/expirence-of-argo-workflow/#2-如何从-hashicorp-vault-中读取-secrets"},{"categories":["tech"],"content":" 3. 如何在多个名字空间中使用同一个 secrets?使用 Namespace 对 workflow 进行分类时，遇到的一个常见问题就是，如何在多个名字空间使用 private-git-creds/docker-config/minio/vault 等 workflow 必要的 secrets. 常见的方法是把 secrets 在所有名字空间 create 一次。 但是也有更方便的 secrets 同步工具： 比如，使用 kyverno 进行 secrets 同步的配置： apiVersion: kyverno.io/v1 kind: ClusterPolicy metadata: name: sync-secrets spec: background: false rules: # 将 secret vault 从 argo Namespace 同步到其他所有 Namespace - name: sync-vault-secret match: resources: kinds: - Namespace generate: kind: Secret name: regcred namespace: \"{{request.object.metadata.name}}\" synchronize: true clone: namespace: argo name: vault # 可以配置多个 rules，每个 rules 同步一个 secret 上面提供的 kyverno 配置，会实时地监控所有 Namespace 变更，一但有新 Namespace 被创建，它就会立即将 vault secret 同步到该 Namespace. 或者，使用专门的 secrets/configmap 复制工具：kubernetes-replicator ","date":"2021-01-27","objectID":"/posts/expirence-of-argo-workflow/:4:3","series":null,"tags":["云原生","CI","持续集成","流水线"],"title":"云原生流水线 Argo Workflows 的安装、使用以及个人体验","uri":"/posts/expirence-of-argo-workflow/#3-如何在多个名字空间中使用同一个-secrets"},{"categories":["tech"],"content":" 4. Argo 对 CR 资源的验证不够严谨，写错了 key 都不报错待研究 ","date":"2021-01-27","objectID":"/posts/expirence-of-argo-workflow/:4:4","series":null,"tags":["云原生","CI","持续集成","流水线"],"title":"云原生流水线 Argo Workflows 的安装、使用以及个人体验","uri":"/posts/expirence-of-argo-workflow/#4-argo-对-cr-资源的验证不够严谨写错了-key-都不报错"},{"categories":["tech"],"content":" 5. 如何归档历史数据？Argo 用的时间长了，跑过的 Workflows/Pods 全都保存在 Kubernetes/Argo Server 中，导致 Argo 越用越慢。 为了解决这个问题，Argo 提供了一些配置来限制 Workflows 和 Pods 的数量，详见：Limit The Total Number Of Workflows And Pods 这些限制都是 Workflow 的参数，如果希望设置一个全局默认的限制，可以按照如下示例修改 argo 的 workflow-controller-configmap 这个 configmap: apiVersion: v1 kind: ConfigMap metadata: name: workflow-controller-configmap data: config: | # Default values that will apply to all Workflows from this controller, unless overridden on the Workflow-level # See more: docs/default-workflow-specs.md workflowDefaults: spec: # must complete in 8h (28,800 seconds) activeDeadlineSeconds: 28800 # keep workflows for 1d (86,400 seconds) ttlStrategy: secondsAfterCompletion: 86400 # secondsAfterSuccess: 5 # secondsAfterFailure: 500 # delete all pods as soon as they complete podGC: # 可选项：\"OnPodCompletion\", \"OnPodSuccess\", \"OnWorkflowCompletion\", \"OnWorkflowSuccess\" strategy: OnPodCompletion ","date":"2021-01-27","objectID":"/posts/expirence-of-argo-workflow/:4:5","series":null,"tags":["云原生","CI","持续集成","流水线"],"title":"云原生流水线 Argo Workflows 的安装、使用以及个人体验","uri":"/posts/expirence-of-argo-workflow/#5-如何归档历史数据"},{"categories":["tech"],"content":" 6. Argo 的其他进阶配置Argo Workflows 的配置，都保存在 workflow-controller-configmap 这个 configmap 中，我们前面已经接触到了它的部分内容。 这里给出此配置文件的完整 examples: https://github.com/argoproj/argo-workflows/blob/master/docs/workflow-controller-configmap.yaml 其中一些可能需要自定义的参数如下： parallelism: workflow 的最大并行数量 persistence: 将完成的 workflows 保存到 postgresql/mysql 中，这样即使 k8s 中的 workflow 被删除了，还能查看 workflow 记录 也支持配置过期时间 sso: 启用单点登录 ","date":"2021-01-27","objectID":"/posts/expirence-of-argo-workflow/:4:6","series":null,"tags":["云原生","CI","持续集成","流水线"],"title":"云原生流水线 Argo Workflows 的安装、使用以及个人体验","uri":"/posts/expirence-of-argo-workflow/#6-argo-的其他进阶配置"},{"categories":["tech"],"content":" 7. 是否应该尽量使用 CI/CD 工具提供的功能？我从同事以及网络上，了解到部分 DevOps 人员主张尽量自己使用 Python/Go 来实现 CI/CD 流水线，CI/CD 工具提供的功能能不使用就不要使用。 因此有此一问。下面做下详细的分析： 尽量使用 CI/CD 工具提供的插件/功能，好处是不需要自己去实现，可以降低维护成本。 但是相对的运维人员就需要深入学习这个 CI/CD 工具的使用，另外还会和 CI/CD 工具绑定，会增加迁移难度。 而尽量自己用 Python 等代码去实现流水线，让 CI/CD 工具只负责调度与运行这些 Python 代码， 那 CI/CD 就可以很方便地随便换，运维人员也不需要去深入学习 CI/CD 工具的使用。 缺点是可能会增加 CI/CD 代码的复杂性。 我观察到 argo/drone 的一些 examples，发现它们的特征是： 所有 CI/CD 相关的逻辑，全都实现在流水线中，不需要其他构建代码 每一个 step 都使用专用镜像：golang/nodejs/python 比如先使用 golang 镜像进行测试、构建，再使用 kaniko 将打包成容器镜像 那是否应该尽量使用 CI/CD 工具提供的功能呢？ 其实这就是有多种方法实现同一件事，该用哪种方法的问题。这个问题在各个领域都很常见。 以我目前的经验来看，需要具体问题具体分析，以 Argo Workflows 为例： 流水线本身非常简单，那完全可以直接使用 argo 来实现，没必要自己再搞个 python 脚本 简单的流水线，迁移起来往往也非常简单。没必要为了可迁移性，非要用 argo 去调用 python 脚本。 流水线的步骤之间包含很多逻辑判断/数据传递，那很可能是你的流水线设计有问题！ 流水线的步骤之间传递的数据应该尽可能少！复杂的逻辑判断应该尽量封装在其中一个步骤中！ 这种情况下，就应该使用 python 脚本来封装复杂的逻辑，而不应该将这些逻辑暴露到 Argo Workflows 中！ 我需要批量运行很多的流水线，而且它们之间还有复杂的依赖关系：那显然应该利用上 argo wrokflow 的高级特性。 argo 的 dag/steps 和 workflow of workflows 这两个功能结合，可以简单地实现上述功能。 ","date":"2021-01-27","objectID":"/posts/expirence-of-argo-workflow/:4:7","series":null,"tags":["云原生","CI","持续集成","流水线"],"title":"云原生流水线 Argo Workflows 的安装、使用以及个人体验","uri":"/posts/expirence-of-argo-workflow/#7-是否应该尽量使用-cicd-工具提供的功能"},{"categories":["tech"],"content":" 8. 如何提升 Argo Workflows 的创建和销毁速度？我们发现 workflow 的 pod，创建和销毁消耗了大量时间，尤其是销毁。 这导致我们单个流水线在 argo 上跑，还没在 jenkins 上跑更快。 ","date":"2021-01-27","objectID":"/posts/expirence-of-argo-workflow/:5:0","series":null,"tags":["云原生","CI","持续集成","流水线"],"title":"云原生流水线 Argo Workflows 的安装、使用以及个人体验","uri":"/posts/expirence-of-argo-workflow/#8-如何提升-argo-workflows-的创建和销毁速度"},{"categories":["tech"],"content":" 使用体验目前已经使用 Argo Workflows 一个月多了，总的来说，最难用的就是 Web UI。 其他的都是小问题，只有 Web UI 是真的超难用，感觉根本就没有好好做过设计… 急需一个第三方 Web UI… ","date":"2021-01-27","objectID":"/posts/expirence-of-argo-workflow/:6:0","series":null,"tags":["云原生","CI","持续集成","流水线"],"title":"云原生流水线 Argo Workflows 的安装、使用以及个人体验","uri":"/posts/expirence-of-argo-workflow/#使用体验"},{"categories":["tech"],"content":" 画外 - 如何处理其他 Kubernetes 资源之间的依赖关系Argo 相比其他 CI 工具，最大的特点，是它假设「任务」之间是有依赖关系的，因此它提供了多种协调编排「任务」的方法。 但是貌似 Argo CD 并没有继承这个理念，Argo CD 部署时，并不能在 kubernetes 资源之间，通过 DAG 等方法定义依赖关系。 微服务之间存在依赖关系，希望能按依赖关系进行部署，而 ArgoCD/FluxCD 部署 kubernetes yaml 时都是不考虑任何依赖关系的。这里就存在一些矛盾。 解决这个矛盾的方法有很多，我查阅了很多资料，也自己做了一些思考，得到的最佳实践来自解决服务依赖 - 阿里云 ACK 容器服务，它给出了两种方案： 应用端服务依赖检查: 即在微服务的入口添加依赖检查逻辑，确保所有依赖的微服务/数据库都可访问了，就续探针才能返回 200. 如果超时就直接 Crash 独立的服务依赖检查逻辑: 部分遗留代码使用方法一改造起来或许会很困难，这时可以考虑使用 pod initContainer 或者容器的启动脚本中，加入依赖检查逻辑。 但是这两个方案也还是存在一些问题，在说明问题前，我先说明一下我们「按序部署」的应用场景。 我们是一个很小的团队，后端做 RPC 接口升级时，通常是直接在开发环境做全量升级+测试。 因此运维这边也是，每次都是做全量升级。 因为没有协议协商机制，新的微服务的「RPC 服务端」将兼容 v1 v2 新旧两种协议，而新的「RPC 客户端」将直接使用 v2 协议去请求其他微服务。 这就导致我们必须先升级「RPC 服务端」，然后才能升级「RPC 客户端」。 为此，在进行微服务的全量升级时，就需要沿着 RPC 调用链路按序升级，这里就涉及到了 Kubernetes 资源之间的依赖关系。 我目前获知的关键问题在于：我们使用的并不是真正的微服务开发模式，而是在把整个微服务系统当成一个「单体服务」在看待，所以引申出了这样的依赖关键的问题。 我进入的新公司完全没有这样的问题，所有的服务之间在 CI/CD 这个阶段都是解耦的，CI/CD 不需要考虑服务之间的依赖关系，也没有自动按照依赖关系进行微服务批量发布的功能，这些都由开发人员自行维护。 或许这才是正确的使用姿势，如果动不动就要批量更新一大批服务，那微服务体系的设计、拆分肯定是有问题了，生产环境也不会允许这么轻率的更新。 前面讲了，阿里云提供的「应用端服务依赖检查」和「独立的服务依赖检查逻辑」是最佳实践。它们的优点有： 简化部署逻辑，每次直接做全量部署就 OK。 提升部署速度，具体体现在：GitOps 部署流程只需要走一次（按序部署要很多次）、所有镜像都提前拉取好了、所有 Pod 也都提前启动了。 但是这里有个问题是「灰度发布」或者「滚动更新」，这两种情况下都存在新旧版本共存的问题。 如果出现了 RPC 接口升级，那就必须先完成「RPC 服务端」的「灰度发布」或者「滚动更新」，再去更新「RPC 客户端」。 否则如果直接对所有微服务做灰度更新，只依靠「服务依赖检查」，就会出现这样的问题——「RPC 服务端」处于「薛定谔」状态，你调用到的服务端版本是新还是旧，取决于负载均衡的策略和概率。 **因此在做 RPC 接口的全量升级时，只依靠「服务依赖检查」是行不通的。**我目前想到的方案，有如下几种： 我们当前的使用方案：直接在 yaml 部署这一步实现按序部署，每次部署后就轮询 kube-apiserver，确认全部灰度完成，再进行下一阶段的 yaml 部署。 让后端加个参数来控制客户端使用的 RPC 协议版本，或者搞一个协议协商。这样就不需要控制微服务发布顺序了。 社区很多有状态应用的部署都涉及到部署顺序等复杂操作，目前流行的解决方案是使用 Operator+CRD 来实现这类应用的部署。Operator 会自行处理好各个组件的部署顺序。 ","date":"2021-01-27","objectID":"/posts/expirence-of-argo-workflow/:7:0","series":null,"tags":["云原生","CI","持续集成","流水线"],"title":"云原生流水线 Argo Workflows 的安装、使用以及个人体验","uri":"/posts/expirence-of-argo-workflow/#画外---如何处理其他-kubernetes-资源之间的依赖关系"},{"categories":["tech"],"content":" 参考文档 Argo加入CNCF孵化器，一文解析Kubernetes原生工作流 视频: How to Multiply the Power of Argo Projects By Using Them Together - Hong Wang ","date":"2021-01-27","objectID":"/posts/expirence-of-argo-workflow/:8:0","series":null,"tags":["云原生","CI","持续集成","流水线"],"title":"云原生流水线 Argo Workflows 的安装、使用以及个人体验","uri":"/posts/expirence-of-argo-workflow/#参考文档"},{"categories":["tech"],"content":"Vault 是 hashicorp 推出的 secrets 管理、加密即服务与权限管理工具。它的功能简介如下： secrets 管理：支持保存各种自定义信息、自动生成各类密钥，vault 自动生成的密钥还能自动轮转(rotate) 认证方式：支持接入各大云厂商的账号体系（比如阿里云RAM子账号体系）或者 LDAP 等进行身份验证，不需要创建额外的账号体系。 权限管理：通过 policy，可以设定非常细致的 ACL 权限。 密钥引擎：也支持接管各大云厂商的账号体系（比如阿里云RAM子账号体系），实现 API Key 的自动轮转。 支持接入 kubernetes rbac 认证体系，通过 serviceaccount+role 为每个 Pod 单独配置认证角色。 支持通过 sidecar/init-container 将 secrets 注入到 pod 中，或者通过 k8s operator 将 vault 数据同步到 k8s secrets 中 在使用 Vault 之前，我们是以携程开源的 Apollo 作为微服务的分布式配置中心。 Apollo 在国内非常流行。它功能强大，支持配置的继承，也有提供 HTTP API 方便自动化。 缺点是权限管理和 secrets 管理比较弱，也不支持信息加密，不适合直接存储敏感信息。因此我们现在切换到了 Vault. 目前我们本地的 CI/CD 流水线和云上的微服务体系，都是使用的 Vault 做 secrets 管理. ","date":"2021-01-24","objectID":"/posts/expirence-of-vault/:0:0","series":null,"tags":["Vault","云原生","Secrets","配置","配置管理"],"title":"secrets 管理工具 Vault 的介绍、安装及使用","uri":"/posts/expirence-of-vault/#"},{"categories":["tech"],"content":" 一、Vault 基础概念 「基本概念」这一节，基本都翻译自官方文档: https://www.vaultproject.io/docs/internals/architecture 首先看一下 Vault 的架构图： vault layers 可以看到，几乎所有的 Vault 组件都被统称为「屏障（Barrier）」。 Vault 可以简单地被划分为存储后端（Storage Backend）、屏障（Barrier）和 HTTP/S API 三个部分。 Vault，翻译成中文就是金库。类比银行金库，「屏障」就是用于保护金库的合金大门和钢筋混凝土，存储后端和客户端之间的所有数据流动都需要经过它。 「屏障」确保只有加密数据会被写入存储后端，加密数据在经过「屏障」被读出的过程中被验证与解密。 和银行金库的大门非常类似，「屏障」也必须先解封，才能解密存储后端中的数据。 ","date":"2021-01-24","objectID":"/posts/expirence-of-vault/:1:0","series":null,"tags":["Vault","云原生","Secrets","配置","配置管理"],"title":"secrets 管理工具 Vault 的介绍、安装及使用","uri":"/posts/expirence-of-vault/#一vault-基础概念"},{"categories":["tech"],"content":" 1. 数据存储及加密解密存储后端（Storage Backend）: Vault 自身不存储数据，因此需要为它配置一个存储后端。 存储后端是不受信任的，只用于存储加密数据。 初始化（Initialization）: Vault 在首次启动时需要初始化，这一步生成一个加密密钥（Encryption Key）用于加密数据，加密完成的数据才能被保存到存储后端。 解封（Unseal）: Vault 启动后，因为不知道加密密钥所以无法解密数据，这种状态被形象得称作已封印（Sealed）。在解封前 Vault 无法进行任何操作。 加密密钥被主密钥（Master Key）保护，我们必须提供主密钥才能解密出 Vault 的加密密钥，从而完成解封操作。 默认情况下，Vault 使用沙米尔密钥分割算法 将主密钥分割成五个分割密钥（Key Shares），必须要提供其中任意三个分割密钥才能重建出主密钥，完成解封操作。 vault-shamir-secret-sharing 分割密钥的总数，以及重建主密钥最少需要的分割密钥数量，都是可以调整的。 沙米尔密钥分割算法也可以关闭，这样主密钥将被直接提供给管理员，管理员可直接使用它进行解封操作。 ","date":"2021-01-24","objectID":"/posts/expirence-of-vault/:1:1","series":null,"tags":["Vault","云原生","Secrets","配置","配置管理"],"title":"secrets 管理工具 Vault 的介绍、安装及使用","uri":"/posts/expirence-of-vault/#1-数据存储及加密解密"},{"categories":["tech"],"content":" 2. 认证系统及权限系统在解封完成后，Vault 就可以开始处理请求了。 HTTP 请求进入后的整个处理流程都由 vault core 管理，core 会强制进行 ACL 检查，并确保审计日志(audit logging)完成记录。 客户端首次连接 vault 时，需要先完成身份认证，vault 的 auth methods 模块有很多身份认证方法可选： 用户友好的认证方法，适合管理员使用：username/password、云服务商、ldap 在创建 user 的时候，需要为 user 绑定 policy，给予合适的权限。 应用友好的方法，适合应用程序使用：public/private keys、tokens、kubernetes、jwt 身份验证请求流经 core 并进入 auth methods，auth methods 确定请求是否有效并返回「关联策略(policies)」的列表。 ACL 策略由 policy store 负责管理与存储，由 core 进行 ACL 检查。 ACL 的默认行为是拒绝，这意味着除非明确配置 policy 允许某项操作，否则该操作将被拒绝。 在通过 auth methods 完成了身份认证，并且返回的关联策略也没毛病之后，token store 将会生成并管理一个新的凭证（token）， 这个 token 会被返回给客户端，用于进行后续请求。 类似 web 网站的 cookie，token 也都存在一个租期（lease）或者说有效期，这加强了安全性。 token 关联了相关的策略 policies，这些策略将被用于验证请求的权限。 请求经过验证后，将被路由到 secret engine。如果 secret engine 返回了一个 secret（由 vault 自动生成的 secret）， core 会将其注册到 expiration manager，并给它附加一个 lease ID。lease ID 被客户端用于更新(renew)或吊销(revoke)它得到的 secret. 如果客户端允许租约(lease)到期，expiration manager 将自动吊销这个 secret. core 还负责处理审核代理 audit broker的请求及响应日志，将请求发送到所有已配置的审核设备 audit devices. 不过默认情况下这个功能貌似是关闭的。 ","date":"2021-01-24","objectID":"/posts/expirence-of-vault/:1:2","series":null,"tags":["Vault","云原生","Secrets","配置","配置管理"],"title":"secrets 管理工具 Vault 的介绍、安装及使用","uri":"/posts/expirence-of-vault/#2-认证系统及权限系统"},{"categories":["tech"],"content":" 3. Secret EngineSecret Engine 是保存、生成或者加密数据的组件，它非常灵活。 有的 Secret Engines 只是单纯地存储与读取数据，比如 kv 就可以看作一个加密的 Redis。 而其他的 Secret Engines 则连接到其他的服务并按需生成动态凭证。 还有些 Secret Engines 提供「加密即服务(encryption as a service)」的能力，如 transit、证书管理等。 常用的 engine 举例： AliCloud Secrets Engine: 基于 RAM 策略动态生成 AliCloud Access Token，或基于 RAM 角色动态生成 AliCloud STS 凭据 Access Token 会自动更新(Renew)，而 STS 凭据是临时使用的，过期后就失效了。 kv: 键值存储，可用于存储一些静态的配置。它一定程度上能替代掉携程的 Apollo 配置中心。 Transit Secrets Engine: 提供加密即服务的功能，它只负责加密和解密，不负责存储。主要应用场景是帮 app 加解密数据，但是数据仍旧存储在 MySQL 等数据库中。 ","date":"2021-01-24","objectID":"/posts/expirence-of-vault/:1:3","series":null,"tags":["Vault","云原生","Secrets","配置","配置管理"],"title":"secrets 管理工具 Vault 的介绍、安装及使用","uri":"/posts/expirence-of-vault/#3-secret-engine"},{"categories":["tech"],"content":" 二、部署 Vault官方建议通过 Helm 部署 vault，大概流程： 使用 helm/docker 部署运行 vault. 初始化/解封 vault: vault 安全措施，每次重启必须解封(可设置自动解封). ","date":"2021-01-24","objectID":"/posts/expirence-of-vault/:2:0","series":null,"tags":["Vault","云原生","Secrets","配置","配置管理"],"title":"secrets 管理工具 Vault 的介绍、安装及使用","uri":"/posts/expirence-of-vault/#二部署-vault"},{"categories":["tech"],"content":" 0. 如何选择存储后端？首先，我们肯定需要高可用 HA，至少要保留能升级到 HA 的能力，所以不建议选择不支持 HA 的后端。 而具体的选择，就因团队经验而异了，人们往往倾向于使用自己熟悉的、知根知底的后端，或者选用云服务。 比如我们对 MySQL/PostgreSQL 比较熟悉，而且使用云服务提供的数据库不需要考虑太多的维护问题，MySQL/PostgreSQL 作为一个通用协议也不会被云厂商绑架，那我们就倾向于使用这两者之一。 而如果你们是本地自建，那你可能更倾向于使用 Etcd/Consul/Raft 做后端存储。 ","date":"2021-01-24","objectID":"/posts/expirence-of-vault/:2:1","series":null,"tags":["Vault","云原生","Secrets","配置","配置管理"],"title":"secrets 管理工具 Vault 的介绍、安装及使用","uri":"/posts/expirence-of-vault/#0-如何选择存储后端"},{"categories":["tech"],"content":" 1. docker-compose 部署（非 HA） 推荐用于本地开发测试环境，或者其他不需要高可用的环境。 docker-compose.yml 示例如下： version: '3.3' services: vault: # 文档：https://hub.docker.com/_/vault image: vault:1.6.0 container_name: vault ports: # rootless 容器，内部不能使用标准端口 443 - \"443:8200\" restart: always volumes: # 审计日志存储目录（`file` audit backend） - ./logs:/vault/logs # 当使用 file data storage 插件时，数据被存储在这里。默认不往这写任何数据。 - ./file:/vault/file # vault 配置 - ./config.hcl:/vault/config/config.hcl # TLS 证书 - ./certs:/certs # vault 需要锁定内存以防止敏感值信息被交换(swapped)到磁盘中 # 为此需要添加如下 capability cap_add: - IPC_LOCK # 必须设定 entrypoint，否则 vault 容器默认以 development 模式运行 entrypoint: vault server -config /vault/config/config.hcl config.hcl 内容如下： ui = true // 使用文件做数据存储（单节点） storage \"file\" { path = \"/vault/file\" } listener \"tcp\" { address = \"[::]:8200\" tls_disable = false tls_cert_file = \"/certs/server.crt\" tls_key_file = \"/certs/server.key\" } 将如上两份配置保存在同一文件夹内，同时在 ./certs 中提供 TLS 证书 server.crt 和私钥 server.key。 然后 docker-compose up -d 就能启动运行一个 vault 实例。 ","date":"2021-01-24","objectID":"/posts/expirence-of-vault/:2:2","series":null,"tags":["Vault","云原生","Secrets","配置","配置管理"],"title":"secrets 管理工具 Vault 的介绍、安装及使用","uri":"/posts/expirence-of-vault/#1-docker-compose-部署非-ha"},{"categories":["tech"],"content":" 2. 通过 helm 部署高可用的 vault 推荐用于生产环境 通过 helm 部署： # 添加 valut 仓库 helm repo add hashicorp https://helm.releases.hashicorp.com # 查看 vault 版本号 helm search repo hashicorp/vault -l | head # 下载某个版本号的 vault helm pull hashicorp/vault --version 0.11.0 --untar 参照下载下来的 ./vault/values.yaml 编写 custom-values.yaml， 部署一个以 mysql 为后端存储的 HA vault，配置示例如下: 配置内容虽然多，但是大都是直接拷贝自 ./vault/values.yaml，改动很少。 测试 Vault 时可以忽略掉其中大多数的配置项。 global: # enabled is the master enabled switch. Setting this to true or false # will enable or disable all the components within this chart by default. enabled: true # TLS for end-to-end encrypted transport tlsDisable: false injector: # True if you want to enable vault agent injection. enabled: true replicas: 1 # If true, will enable a node exporter metrics endpoint at /metrics. metrics: enabled: false # Mount Path of the Vault Kubernetes Auth Method. authPath: \"auth/kubernetes\" certs: # secretName is the name of the secret that has the TLS certificate and # private key to serve the injector webhook. If this is null, then the # injector will default to its automatic management mode that will assign # a service account to the injector to generate its own certificates. secretName: null # caBundle is a base64-encoded PEM-encoded certificate bundle for the # CA that signed the TLS certificate that the webhook serves. This must # be set if secretName is non-null. caBundle: \"\" # certName and keyName are the names of the files within the secret for # the TLS cert and private key, respectively. These have reasonable # defaults but can be customized if necessary. certName: tls.crt keyName: tls.key server: # Resource requests, limits, etc. for the server cluster placement. This # should map directly to the value of the resources field for a PodSpec. # By default no direct resource request is made. # Enables a headless service to be used by the Vault Statefulset service: enabled: true # Port on which Vault server is listening port: 8200 # Target port to which the service should be mapped to targetPort: 8200 # This configures the Vault Statefulset to create a PVC for audit # logs. Once Vault is deployed, initialized and unseal, Vault must # be configured to use this for audit logs. This will be mounted to # /vault/audit # See https://www.vaultproject.io/docs/audit/index.html to know more auditStorage: enabled: false # Run Vault in \"HA\" mode. There are no storage requirements unless audit log # persistence is required. In HA mode Vault will configure itself to use Consul # for its storage backend. The default configuration provided will work the Consul # Helm project by default. It is possible to manually configure Vault to use a # different HA backend. ha: enabled: true replicas: 3 # Set the api_addr configuration for Vault HA # See https://www.vaultproject.io/docs/configuration#api_addr # If set to null, this will be set to the Pod IP Address apiAddr: null # config is a raw string of default configuration when using a Stateful # deployment. Default is to use a Consul for its HA storage backend. # This should be HCL. # Note: Configuration files are stored in ConfigMaps so sensitive data # such as passwords should be either mounted through extraSecretEnvironmentVars # or through a Kube secret. For more information see: # https://www.vaultproject.io/docs/platform/k8s/helm/run#protecting-sensitive-vault-configurations config: | ui = true listener \"tcp\" { address = \"[::]:8200\" cluster_address = \"[::]:8201\" # 注意，这个值要和 helm 的参数 global.tlsDisable 一致 tls_disable = false tls_cert_file = \"/etc/certs/vault.crt\" tls_key_file = \"/etc/certs/vault.key\" } # storage \"postgresql\" { # connection_url = \"postgres://username:password@\u003chost\u003e:5432/vault?sslmode=disable\" # ha_enabled = true # } service_registration \"kubernetes\" {} # Example configuration for using auto-unseal, using AWS KMS. # the cluster must have a service account that is authorized to access AWS KMS, throught an IAM Role. # seal \"awskms\" { # region = \"us-east-1\" # kms_key_id = \"\u003csome-key-id\u003e\" # 默认","date":"2021-01-24","objectID":"/posts/expirence-of-vault/:2:3","series":null,"tags":["Vault","云原生","Secrets","配置","配置管理"],"title":"secrets 管理工具 Vault 的介绍、安装及使用","uri":"/posts/expirence-of-vault/#install-by-helm"},{"categories":["tech"],"content":" 3. 初始化并解封 vault 官方文档：Initialize and unseal Vault - Vault on Kubernetes Deployment Guide 通过 helm 部署 vault，默认会部署一个三副本的 StatefulSet，但是这三个副本都会处于 NotReady 状态（docker 方式部署的也一样）。 接下来还需要手动初始化并解封 vault，才能 Ready: 第一步：从三个副本中随便选择一个，运行 vault 的初始化命令：kubectl exec -ti vault-0 -- vault operator init 初始化操作会返回 5 个 unseal keys，以及一个 Initial Root Token，这些数据非常敏感非常重要，一定要保存到安全的地方！ 第二步：在每个副本上，使用任意三个 unseal keys 进行解封操作。 一共有三个副本，也就是说要解封 3*3 次，才能完成 vault 的完整解封！ # 每个实例都需要解封三次！ ## Unseal the first vault server until it reaches the key threshold $ kubectl exec -ti vault-0 -- vault operator unseal # ... Unseal Key 1 $ kubectl exec -ti vault-0 -- vault operator unseal # ... Unseal Key 2 $ kubectl exec -ti vault-0 -- vault operator unseal # ... Unseal Key 3 这样就完成了部署，但是要注意，vault 实例每次重启后，都需要重新解封！也就是重新进行第二步操作！ ","date":"2021-01-24","objectID":"/posts/expirence-of-vault/:2:4","series":null,"tags":["Vault","云原生","Secrets","配置","配置管理"],"title":"secrets 管理工具 Vault 的介绍、安装及使用","uri":"/posts/expirence-of-vault/#3-初始化并解封-vault"},{"categories":["tech"],"content":" 4. 初始化并设置自动解封在未设置 auto unseal 的情况下，vault 每次重启都要手动解封所有 vault 实例，实在是很麻烦，在云上自动扩缩容的情况下，vault 实例会被自动调度，这种情况就更麻烦了。 为了简化这个流程，可以考虑配置 auto unseal 让 vault 自动解封。 自动解封目前有两种方法： 使用阿里云/AWS/Azure 等云服务提供的密钥库来管理 encryption key AWS: awskms Seal 如果是 k8s 集群，vault 使用的 ServiceAccount 需要有权限使用 AWS KMS，它可替代掉 config.hcl 中的 access_key/secret_key 两个属性 阿里云：alicloudkms Seal 如果你不想用云服务，那可以考虑 autounseal-transit，这种方法使用另一个 vault 实例提供的 transit 引擎来实现 auto-unseal. 简单粗暴：直接写个 crontab 或者在 CI 平台上加个定时任务去执行解封命令，以实现自动解封。不过这样安全性就不好说了。 以使用 awskms 为例，首先创建 aws IAM 的 policy 内容如下: { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Sid\": \"VaultKMSUnseal\", \"Effect\": \"Allow\", \"Action\": [ \"kms:Decrypt\", \"kms:Encrypt\", \"kms:DescribeKey\" ], \"Resource\": \"*\" } ] } 然后创建 IAM Role 绑定上面的 policy，并为 vault 的 k8s serviceaccount 创建一个 IAM Role，绑定上这个 policy. 这样 vault 使用的 serviceaccount 自身就拥有了访问 awskms 的权限，也就不需要额外通过 access_key/secret_key 来访问 awskms. 关于 IAM Role 和 k8s serviceaccount 如何绑定，参见官方文档：IAM roles for EKS service accounts 完事后再修改好前面提供的 helm 配置，部署它，最后使用如下命令初始化一下： # 初始化命令和普通模式并无不同 kubectl exec -ti vault-0 -- vault operator init # 会打印出一个 root token，以及五个 Recovery Key（而不是 Unseal Key） # Recover Key 不再用于解封，但是重新生成 root token 等操作仍然会需要用到它. 然后就大功告成了，可以尝试下删除 vault 的 pod，新建的 Pod 应该会自动解封。 ","date":"2021-01-24","objectID":"/posts/expirence-of-vault/:2:5","series":null,"tags":["Vault","云原生","Secrets","配置","配置管理"],"title":"secrets 管理工具 Vault 的介绍、安装及使用","uri":"/posts/expirence-of-vault/#4-初始化并设置自动解封"},{"categories":["tech"],"content":" 三、Vault 自身的配置管理Vault 本身是一个复杂的 secrets 工具，它提供了 Web UI 和 CLI 用于手动管理与查看 Vault 的内容。 但是作为一名 DevOps，我们当然更喜欢更自治的方法，这有两种选择: 使用 vault 的 sdk: python-hvac 使用 terraform-provider-vault 或者 pulumi-vault 实现 vault 配置的自动化管理。 Web UI 适合手工操作，而 sdk/terraform-provider-vault 则适合用于自动化管理 vault. 我们的测试环境就是使用 pulumi-vault 完成的自动化配置 vault policy 和 kubernetes role，然后自动化注入所有测试用的 secrets. ","date":"2021-01-24","objectID":"/posts/expirence-of-vault/:3:0","series":null,"tags":["Vault","云原生","Secrets","配置","配置管理"],"title":"secrets 管理工具 Vault 的介绍、安装及使用","uri":"/posts/expirence-of-vault/#三vault-自身的配置管理"},{"categories":["tech"],"content":" 1. 使用 pulumi 自动化配置 vault使用 pulumi 管理 vault 配置的优势是很大的，因为云上资源的敏感信息（数据库账号密码、资源 ID、RAM子账号）都是 pulumi 创建的。 再结合使用 pulumi_valut，就能实现敏感信息自动生成后，立即保存到 vault 中，实现完全自动化。 后续微服务就可以通过 kubernetes 认证，直接从 vault 读取敏感信息。 或者是写入到本地的 vault 中留做备份，在需要的时候，管理员能登入进去查看相关敏感信息。 1.1 Token 的生成pulumi_vault 本身挺简单的，声明式的配置嘛，直接用就是了。 但是它一定要求提供 VAULT_TOKEN 作为身份认证的凭证（实测 userpass/approle 都不能直接使用，会报错 no vault token found），而且 pulumi 还会先生成临时用的 child token，然后用这个 child token 进行后续的操作。 首先安全起见，肯定不应该直接提供 root token！root token 应该封存，除了紧急情况不应该启用。 那么应该如何生成一个权限有限的 token 给 vault 使用呢？ 我的方法是创建一个 userpass 账号，通过 policy 给予它有限的权限。 然后先手动(或者自动)登录获取到 token，再将 token 提供给 pulumi_vault 使用。 这里面有个坑，就是必须给 userpass 账号创建 child token 的权限： path \"local/*\" { capabilities = [\"read\", \"list\"] } // 允许创建 child token path \"auth/token/create\" { capabilities = [\"create\", \"read\", \"update\", \"delete\", \"list\"] } 不给这个权限，pulumi_vault 就会一直报错。。 然后还得给它「自动化配置」需要的权限，比如自动创建/更新 policy/secrets/kubernetes 等等，示例如下: # To list policies - Step 3 path \"sys/policy\" { capabilities = [\"read\"] } # Create and manage ACL policies broadly across Vault path \"sys/policy/*\" { capabilities = [\"create\", \"read\", \"update\", \"delete\", \"list\", \"sudo\"] } # List, create, update, and delete key/value secrets path \"secret/*\" { capabilities = [\"create\", \"read\", \"update\", \"delete\", \"list\", \"sudo\"] } path \"auth/kubernetes/role/*\" { capabilities = [\"create\", \"read\", \"update\", \"list\"] } ","date":"2021-01-24","objectID":"/posts/expirence-of-vault/:3:1","series":null,"tags":["Vault","云原生","Secrets","配置","配置管理"],"title":"secrets 管理工具 Vault 的介绍、安装及使用","uri":"/posts/expirence-of-vault/#1-使用-pulumi-自动化配置-vault"},{"categories":["tech"],"content":" 1. 使用 pulumi 自动化配置 vault使用 pulumi 管理 vault 配置的优势是很大的，因为云上资源的敏感信息（数据库账号密码、资源 ID、RAM子账号）都是 pulumi 创建的。 再结合使用 pulumi_valut，就能实现敏感信息自动生成后，立即保存到 vault 中，实现完全自动化。 后续微服务就可以通过 kubernetes 认证，直接从 vault 读取敏感信息。 或者是写入到本地的 vault 中留做备份，在需要的时候，管理员能登入进去查看相关敏感信息。 1.1 Token 的生成pulumi_vault 本身挺简单的，声明式的配置嘛，直接用就是了。 但是它一定要求提供 VAULT_TOKEN 作为身份认证的凭证（实测 userpass/approle 都不能直接使用，会报错 no vault token found），而且 pulumi 还会先生成临时用的 child token，然后用这个 child token 进行后续的操作。 首先安全起见，肯定不应该直接提供 root token！root token 应该封存，除了紧急情况不应该启用。 那么应该如何生成一个权限有限的 token 给 vault 使用呢？ 我的方法是创建一个 userpass 账号，通过 policy 给予它有限的权限。 然后先手动(或者自动)登录获取到 token，再将 token 提供给 pulumi_vault 使用。 这里面有个坑，就是必须给 userpass 账号创建 child token 的权限： path \"local/*\" { capabilities = [\"read\", \"list\"] } // 允许创建 child token path \"auth/token/create\" { capabilities = [\"create\", \"read\", \"update\", \"delete\", \"list\"] } 不给这个权限，pulumi_vault 就会一直报错。。 然后还得给它「自动化配置」需要的权限，比如自动创建/更新 policy/secrets/kubernetes 等等，示例如下: # To list policies - Step 3 path \"sys/policy\" { capabilities = [\"read\"] } # Create and manage ACL policies broadly across Vault path \"sys/policy/*\" { capabilities = [\"create\", \"read\", \"update\", \"delete\", \"list\", \"sudo\"] } # List, create, update, and delete key/value secrets path \"secret/*\" { capabilities = [\"create\", \"read\", \"update\", \"delete\", \"list\", \"sudo\"] } path \"auth/kubernetes/role/*\" { capabilities = [\"create\", \"read\", \"update\", \"list\"] } ","date":"2021-01-24","objectID":"/posts/expirence-of-vault/:3:1","series":null,"tags":["Vault","云原生","Secrets","配置","配置管理"],"title":"secrets 管理工具 Vault 的介绍、安装及使用","uri":"/posts/expirence-of-vault/#11-token-的生成"},{"categories":["tech"],"content":" 四、在 Kubernetes 中使用 vault 注入 secrets vault-k8s-auth-workflow 前面提到过 vault 支持通过 Kubernetes 的 ServiceAccount 为每个 Pod 单独分配权限。 应用程序有两种方式去读取 vault 中的配置： 借助 Vault Sidecar，将 secrets 以文件的形式自动注入到 Pod 中，比如 /vault/secrets/config.json vault sidecar 在常驻模式下每 15 秒更新一次配置，应用程序可以使用 watchdog 实时监控 secrets 文件的变更。 应用程序自己使用 SDK 直接访问 vault api 获取 secrets 上述两种方式，都可以借助 Kubernetes ServiceAccount 进行身份验证和权限分配。 下面以 Sidecar 模式为例，介绍如何将 secrets 以文件形式注入到 Pod 中。 ","date":"2021-01-24","objectID":"/posts/expirence-of-vault/:4:0","series":null,"tags":["Vault","云原生","Secrets","配置","配置管理"],"title":"secrets 管理工具 Vault 的介绍、安装及使用","uri":"/posts/expirence-of-vault/#四在-kubernetes-中使用-vault-注入-secrets"},{"categories":["tech"],"content":" 1. 部署并配置 vault agent首先启用 Vault 的 Kubernetes 身份验证: # 配置身份认证需要在 vault pod 中执行，启动 vault-0 的交互式会话 kubectl exec -n vault -it vault-0 -- /bin/sh export VAULT_TOKEN='\u003cyour-root-token\u003e' export VAULT_ADDR='http://localhost:8200' # 启用 Kubernetes 身份验证 vault auth enable kubernetes # kube-apiserver API 配置，vault 需要通过 kube-apiserver 完成对 serviceAccount 的身份验证 vault write auth/kubernetes/config \\ token_reviewer_jwt=\"$(cat /var/run/secrets/kubernetes.io/serviceaccount/token)\" \\ kubernetes_host=\"https://$KUBERNETES_PORT_443_TCP_ADDR:443\" \\ kubernetes_ca_cert=@/var/run/secrets/kubernetes.io/serviceaccount/ca.crt 1.1 使用集群外部的 valut 实例 如果你没这个需求，请跳过这一节。 详见 Install the Vault Helm chart configured to address an external Vault kubernetes 也可以和外部的 vault 实例集成，集群中只部署 vault-agent. 这适用于多个 kubernetes 集群以及其他 APP 共用一个 vault 实例的情况，比如我们本地的多个开发测试集群，就都共用着同一个 vault 实例，方便统一管理应用的 secrets. 首先，使用 helm chart 部署 vault-agent，接入外部的 vault 实例。使用的 custom-values.yaml 示例如下： global: # enabled is the master enabled switch. Setting this to true or false # will enable or disable all the components within this chart by default. enabled: true # TLS for end-to-end encrypted transport tlsDisable: false injector: # True if you want to enable vault agent injection. enabled: true replicas: 1 # If multiple replicas are specified, by default a leader-elector side-car # will be created so that only one injector attempts to create TLS certificates. leaderElector: enabled: true image: repository: \"gcr.io/google_containers/leader-elector\" tag: \"0.4\" ttl: 60s # If true, will enable a node exporter metrics endpoint at /metrics. metrics: enabled: false # External vault server address for the injector to use. Setting this will # disable deployment of a vault server along with the injector. # TODO 这里的 https ca.crt 要怎么设置？mTLS 又该如何配置？ externalVaultAddr: \"https://\u003cexternal-vault-url\u003e\" # Mount Path of the Vault Kubernetes Auth Method. authPath: \"auth/kubernetes\" certs: # secretName is the name of the secret that has the TLS certificate and # private key to serve the injector webhook. If this is null, then the # injector will default to its automatic management mode that will assign # a service account to the injector to generate its own certificates. secretName: null # caBundle is a base64-encoded PEM-encoded certificate bundle for the # CA that signed the TLS certificate that the webhook serves. This must # be set if secretName is non-null. caBundle: \"\" # certName and keyName are the names of the files within the secret for # the TLS cert and private key, respectively. These have reasonable # defaults but can be customized if necessary. certName: tls.crt keyName: tls.key 部署命令和 通过 helm 部署 vault 一致，只要更换 custom-values.yaml 就行。 vault-agent 部署完成后，第二步是为 vault 创建 serviceAccount、secret 和 ClusterRoleBinding，以允许 vault 审查 kubernetes 的 token, 完成对 pod 的身份验证. yaml 配置如下： --- apiVersion: v1 kind: ServiceAccount metadata: name: vault-auth namespace: vault --- apiVersion: v1 kind: Secret metadata: name: vault-auth namespace: vault annotations: kubernetes.io/service-account.name: vault-auth type: kubernetes.io/service-account-token --- apiVersion: rbac.authorization.k8s.io/v1beta1 kind: ClusterRoleBinding metadata: name: role-tokenreview-binding roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: system:auth-delegator subjects: - kind: ServiceAccount name: vault-auth namespace: vault 现在在 vault 实例这边，启用 kubernetes 身份验证，在 vault 实例内，执行如下命令： vault 实例内显然没有 kubectl 和 kubeconfig，简便起见，下列的 vault 命令也可以通过 Web UI 完成。 export VAULT_TOKEN='\u003cyour-root-token\u003e' export VAULT_ADDR='http://localhost:8200' # 启用 Kubernetes 身份验证 vault auth enable kubernetes # kube-apiserver API 配置，vault 需要通过 kube-apiserver 完成对 serviceAccount 的身份验证 # TOKEN_REVIEW_JWT: 就是我们前面创建的 secret `vault-auth` TOKEN_REVIEW_JWT=$(kubectl -n vault get secret vault-auth -o go-template='{{ .data.token }}' | base64 --decode) # kube-apiserver 的 ca 证书 KUBE_CA_CERT=$(kubectl -n vault config view --raw --minify --flatten -o jsonpath='{.clusters[].cluster.certificate-aut","date":"2021-01-24","objectID":"/posts/expirence-of-vault/:4:1","series":null,"tags":["Vault","云原生","Secrets","配置","配置管理"],"title":"secrets 管理工具 Vault 的介绍、安装及使用","uri":"/posts/expirence-of-vault/#1-部署并配置-vault-agent"},{"categories":["tech"],"content":" 1. 部署并配置 vault agent首先启用 Vault 的 Kubernetes 身份验证: # 配置身份认证需要在 vault pod 中执行，启动 vault-0 的交互式会话 kubectl exec -n vault -it vault-0 -- /bin/sh export VAULT_TOKEN='' export VAULT_ADDR='http://localhost:8200' # 启用 Kubernetes 身份验证 vault auth enable kubernetes # kube-apiserver API 配置，vault 需要通过 kube-apiserver 完成对 serviceAccount 的身份验证 vault write auth/kubernetes/config \\ token_reviewer_jwt=\"$(cat /var/run/secrets/kubernetes.io/serviceaccount/token)\" \\ kubernetes_host=\"https://$KUBERNETES_PORT_443_TCP_ADDR:443\" \\ kubernetes_ca_cert=@/var/run/secrets/kubernetes.io/serviceaccount/ca.crt 1.1 使用集群外部的 valut 实例 如果你没这个需求，请跳过这一节。 详见 Install the Vault Helm chart configured to address an external Vault kubernetes 也可以和外部的 vault 实例集成，集群中只部署 vault-agent. 这适用于多个 kubernetes 集群以及其他 APP 共用一个 vault 实例的情况，比如我们本地的多个开发测试集群，就都共用着同一个 vault 实例，方便统一管理应用的 secrets. 首先，使用 helm chart 部署 vault-agent，接入外部的 vault 实例。使用的 custom-values.yaml 示例如下： global: # enabled is the master enabled switch. Setting this to true or false # will enable or disable all the components within this chart by default. enabled: true # TLS for end-to-end encrypted transport tlsDisable: false injector: # True if you want to enable vault agent injection. enabled: true replicas: 1 # If multiple replicas are specified, by default a leader-elector side-car # will be created so that only one injector attempts to create TLS certificates. leaderElector: enabled: true image: repository: \"gcr.io/google_containers/leader-elector\" tag: \"0.4\" ttl: 60s # If true, will enable a node exporter metrics endpoint at /metrics. metrics: enabled: false # External vault server address for the injector to use. Setting this will # disable deployment of a vault server along with the injector. # TODO 这里的 https ca.crt 要怎么设置？mTLS 又该如何配置？ externalVaultAddr: \"https://\" # Mount Path of the Vault Kubernetes Auth Method. authPath: \"auth/kubernetes\" certs: # secretName is the name of the secret that has the TLS certificate and # private key to serve the injector webhook. If this is null, then the # injector will default to its automatic management mode that will assign # a service account to the injector to generate its own certificates. secretName: null # caBundle is a base64-encoded PEM-encoded certificate bundle for the # CA that signed the TLS certificate that the webhook serves. This must # be set if secretName is non-null. caBundle: \"\" # certName and keyName are the names of the files within the secret for # the TLS cert and private key, respectively. These have reasonable # defaults but can be customized if necessary. certName: tls.crt keyName: tls.key 部署命令和 通过 helm 部署 vault 一致，只要更换 custom-values.yaml 就行。 vault-agent 部署完成后，第二步是为 vault 创建 serviceAccount、secret 和 ClusterRoleBinding，以允许 vault 审查 kubernetes 的 token, 完成对 pod 的身份验证. yaml 配置如下： --- apiVersion: v1 kind: ServiceAccount metadata: name: vault-auth namespace: vault --- apiVersion: v1 kind: Secret metadata: name: vault-auth namespace: vault annotations: kubernetes.io/service-account.name: vault-auth type: kubernetes.io/service-account-token --- apiVersion: rbac.authorization.k8s.io/v1beta1 kind: ClusterRoleBinding metadata: name: role-tokenreview-binding roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: system:auth-delegator subjects: - kind: ServiceAccount name: vault-auth namespace: vault 现在在 vault 实例这边，启用 kubernetes 身份验证，在 vault 实例内，执行如下命令： vault 实例内显然没有 kubectl 和 kubeconfig，简便起见，下列的 vault 命令也可以通过 Web UI 完成。 export VAULT_TOKEN='' export VAULT_ADDR='http://localhost:8200' # 启用 Kubernetes 身份验证 vault auth enable kubernetes # kube-apiserver API 配置，vault 需要通过 kube-apiserver 完成对 serviceAccount 的身份验证 # TOKEN_REVIEW_JWT: 就是我们前面创建的 secret `vault-auth` TOKEN_REVIEW_JWT=$(kubectl -n vault get secret vault-auth -o go-template='{{ .data.token }}' | base64 --decode) # kube-apiserver 的 ca 证书 KUBE_CA_CERT=$(kubectl -n vault config view --raw --minify --flatten -o jsonpath='{.clusters[].cluster.certificate-aut","date":"2021-01-24","objectID":"/posts/expirence-of-vault/:4:1","series":null,"tags":["Vault","云原生","Secrets","配置","配置管理"],"title":"secrets 管理工具 Vault 的介绍、安装及使用","uri":"/posts/expirence-of-vault/#11-使用集群外部的-valut-实例"},{"categories":["tech"],"content":" 2. 关联 k8s rbac 权限系统和 vault接下来需要做的事： 通过 vault policy 定义好每个 role（微服务）能访问哪些资源。 为每个微服务生成一个 role，这个 role 需要绑定对应的 vault policy 及 kubernetes serviceaccount 这个 role 是 vault 的 kubernetes 插件自身的属性，它和 kubernetes role 没有半毛钱关系。 创建一个 ServiceAccount，并使用这个 使用这个 ServiceAccount 部署微服务 其中第一步和第二步都可以通过 vault api 自动化完成. 第三步可以通过 kubectl 部署时完成。 方便起见，vault policy / role / k8s serviceaccount 这三个配置，都建议和微服务使用相同的名称。 上述配置中，role 起到一个承上启下的作用，它关联了 k8s serviceaccount 和 vault policy 两个配置。 比如创建一个名为 my-app-policy 的 vault policy，内容为: # 允许读取数据 path \"my-app/data/*\" { capabilities = [\"read\", \"list\"] } // 允许列出 myapp 中的所有数据(kv v2) path \"myapp/metadata/*\" { capabilities = [\"read\", \"list\"] } 然后在 vault 的 kuberntes 插件配置中，创建 role my-app-role，配置如下: 关联 k8s default 名字空间中的 serviceaccount my-app-account，并创建好这个 serviceaccount. 关联 vault token policy，这就是前面创建的 my-app-policy 设置 token period（有效期） 这之后，每个微服务就能通过 serviceaccount 从 vault 中读取 my-app 中的所有信息了。 ","date":"2021-01-24","objectID":"/posts/expirence-of-vault/:4:2","series":null,"tags":["Vault","云原生","Secrets","配置","配置管理"],"title":"secrets 管理工具 Vault 的介绍、安装及使用","uri":"/posts/expirence-of-vault/#2-关联-k8s-rbac-权限系统和-vault"},{"categories":["tech"],"content":" 3. 部署 Pod 参考文档：https://www.vaultproject.io/docs/platform/k8s/injector 下一步就是将配置注入到微服务容器中，这需要使用到 Agent Sidecar Injector。 vault 通过 sidecar 实现配置的自动注入与动态更新。 具体而言就是在 Pod 上加上一堆 Agent Sidecar Injector 的注解，如果配置比较多，也可以使用 configmap 保存，在注解中引用。 需要注意的是 vault-inject-agent 有两种运行模式： init 模式: 仅在 Pod 启动前初始化一次，跑完就退出（Completed） 常驻模式: 容器不退出，持续监控 vault 的配置更新，维持 Pod 配置和 vualt 配置的同步。 示例： apiVersion: apps/v1 kind: Deployment metadata: labels: app: my-app name: my-app namespace: default spec: minReadySeconds: 3 progressDeadlineSeconds: 60 revisionHistoryLimit: 3 selector: matchLabels: app: my-app strategy: rollingUpdate: maxUnavailable: 1 type: RollingUpdate template: metadata: annotations: vault.hashicorp.com/agent-init-first: 'true' # 是否使用 initContainer 提前初始化配置文件 vault.hashicorp.com/agent-inject: 'true' vault.hashicorp.com/secret-volume-path: vault vault.hashicorp.com/role: \"my-app-role\" # vault kubernetes 插件的 role 名称 vault.hashicorp.com/agent-inject-template-config.json: | # 渲染模板的语法在后面介绍 vault.hashicorp.com/agent-limits-cpu: 250m vault.hashicorp.com/agent-requests-cpu: 100m # 包含 vault 配置的 configmap，可以做更精细的控制 # vault.hashicorp.com/agent-configmap: my-app-vault-config labels: app: my-app spec: containers: - image: registry.svc.local/xx/my-app:latest imagePullPolicy: IfNotPresent # 此处省略若干配置... serviceAccountName: my-app-account 常见错误： vault-agent(sidecar) 报错: namespace not authorized auth/kubernetes/config 中的 role 没有绑定 Pod 的 namespace vault-agent(sidecar) 报错: permission denied 检查 vault 实例的日志，应该有对应的错误日志，很可能是 auth/kubernetes/config 没配对，vault 无法验证 kube-apiserver 的 tls 证书，或者使用的 kubernetes token 没有权限。 vault-agent(sidecar) 报错: service account not authorized auth/kubernetes/config 中的 role 没有绑定 Pod 使用的 serviceAccount ","date":"2021-01-24","objectID":"/posts/expirence-of-vault/:4:3","series":null,"tags":["Vault","云原生","Secrets","配置","配置管理"],"title":"secrets 管理工具 Vault 的介绍、安装及使用","uri":"/posts/expirence-of-vault/#3-部署-pod"},{"categories":["tech"],"content":" 4. vault agent 配置vault-agent 的配置，需要注意的有： 如果使用 configmap 提供完整的 config.hcl 配置，注意 agent-init vautl-agent 的 template 说明： 目前来说最流行的配置文件格式应该是 json/yaml，以 json 为例， 对每个微服务的 kv 数据，可以考虑将它所有的个性化配置都保存在 \u003cengine-name\u003e/\u003cservice-name\u003e/ 下面，然后使用如下 template 注入配置： { {{ range secrets \"\u003cengine-name\u003e/metadata/\u003cservice-name\u003e/\" }} \"{{ printf \"%s\" . }}\": {{ with secret (printf \"\u003cengine-name\u003e/\u003cservice-name\u003e/%s\" .) }} {{ .Data.data | toJSONPretty }}, {{ end }} {{ end }} } template 的详细语法参见: https://github.com/hashicorp/consul-template#secret 注意：v2 版本的 kv secrets，它的 list 接口有变更，因此在遍历 v2 kv secrets 时， 必须要写成 range secrets \"\u003cengine-name\u003e/metadata/\u003cservice-name\u003e/\"，也就是中间要插入 metadata，而且 policy 中必须开放 \u003cengine-name\u003e/metadata/\u003cservice-name\u003e/ 的 read/list 权限！ 官方文档完全没提到这一点，我通过 wireshark 抓包调试，对照官方的 KV Secrets Engine - Version 2 (API) 才搞明白这个。 这样生成出来的内容将是 json 格式，不过有个不兼容的地方：最后一个 secrets 的末尾有逗号 , 渲染出的效果示例： { \"secret-a\": { \"a\": \"b\", \"c\": \"d\" }, \"secret-b\": { \"v\": \"g\", \"r\": \"c\" }, } 因为存在尾部逗号(trailing comma)，直接使用 json 标准库解析它会报错。 那该如何去解析它呢？我在万能的 stackoverflow 上找到了解决方案：yaml 完全兼容 json 语法，并且支持尾部逗号！ 以 python 为例，直接 yaml.safe_load() 就能完美解析 vault 生成出的 json 内容。 ","date":"2021-01-24","objectID":"/posts/expirence-of-vault/:4:4","series":null,"tags":["Vault","云原生","Secrets","配置","配置管理"],"title":"secrets 管理工具 Vault 的介绍、安装及使用","uri":"/posts/expirence-of-vault/#4-vault-agent-配置"},{"categories":["tech"],"content":" 5. 拓展：在 kubernetes 中使用 vault 的其他姿势除了使用官方提供的 sidecar 模式进行 secrets 注入，社区也提供了一些别的方案，可以参考： hashicorp/vault-csi-provider: 官方的 Beta 项目，通过 Secrets Store CSI 驱动将 vault secrets 以数据卷的形式挂载到 pod 中 kubernetes-external-secrets: 提供 CRD 定义，根据定义将 secret 从 vault 中同步到 kubernetes secrets 官方的 sidecar/init-container 模式仍然是最推荐使用的。 ","date":"2021-01-24","objectID":"/posts/expirence-of-vault/:4:5","series":null,"tags":["Vault","云原生","Secrets","配置","配置管理"],"title":"secrets 管理工具 Vault 的介绍、安装及使用","uri":"/posts/expirence-of-vault/#5-拓展在-kubernetes-中使用-vault-的其他姿势"},{"categories":["tech"],"content":" 五、使用 vault 实现 AWS IAM Credentials 的自动轮转待续。。。 ","date":"2021-01-24","objectID":"/posts/expirence-of-vault/:5:0","series":null,"tags":["Vault","云原生","Secrets","配置","配置管理"],"title":"secrets 管理工具 Vault 的介绍、安装及使用","uri":"/posts/expirence-of-vault/#五使用-vault-实现-aws-iam-credentials-的自动轮转"},{"categories":["tech"],"content":" QEMU/KVM 虚拟化QEMU/KVM 是目前最流行的虚拟化技术，它基于 Linux 内核提供的 kvm 模块，结构精简，性能损失小，而且开源免费（对比收费的 vmware），因此成了大部分企业的首选虚拟化方案。 目前各大云厂商的虚拟化方案，新的服务器实例基本都是用的 KVM 技术。即使是起步最早，一直重度使用 Xen 的 AWS，从 EC2 C5 开始就改用了基于 KVM 定制的 Nitro 虚拟化技术。 但是 KVM 作为一个企业级的底层虚拟化技术，却没有对桌面使用做深入的优化，因此如果想把它当成桌面虚拟化软件来使用，替代掉 VirtualBox/VMware，有一定难度。 本文是我个人学习 KVM 的一个总结性文档，其目标是使用 KVM 作为桌面虚拟化软件。 ","date":"2021-01-17","objectID":"/posts/qemu-kvm-usage/:1:0","series":null,"tags":["虚拟化","KVM","libvirt","QEMU"],"title":"QEMU-KVM 虚拟化环境的搭建与使用","uri":"/posts/qemu-kvm-usage/#qemukvm-虚拟化"},{"categories":["tech"],"content":" 一、安装 QUEU/KVMQEMU/KVM 环境需要安装很多的组件，它们各司其职： qemu: 模拟各类输入输出设备（网卡、磁盘、USB端口等） qemu 底层使用 kvm 模拟 CPU 和 RAM，比软件模拟的方式快很多。 libvirt: 提供简单且统一的工具和 API，用于管理虚拟机，屏蔽了底层的复杂结构。（支持 qemu-kvm/virtualbox/vmware） ovmf: 为虚拟机启用 UEFI 支持 virt-manager: 用于管理虚拟机的 GUI 界面（可以管理远程 kvm 主机）。 virt-viewer: 通过 GUI 界面直接与虚拟机交互（可以管理远程 kvm 主机）。 dnsmasq vde2 bridge-utils openbsd-netcat: 网络相关组件，提供了以太网虚拟化、网络桥接、NAT网络等虚拟网络功能。 dnsmasq 提供了 NAT 虚拟网络的 DHCP 及 DNS 解析功能。 vde2: 以太网虚拟化 bridge-utils: 顾名思义，提供网络桥接相关的工具。 openbsd-netcat: TCP/IP 的瑞士军刀，详见 socat \u0026 netcat，这里不清楚是哪个网络组件会用到它。 安装命令： # archlinux/manjaro sudo pacman -S qemu virt-manager virt-viewer dnsmasq vde2 bridge-utils openbsd-netcat # ubuntu,参考了官方文档，但未测试 sudo apt install qemu-kvm libvirt-daemon-system virt-manager virt-viewer virtinst bridge-utils # centos,参考了官方文档，但未测试 sudo yum groupinstall \"Virtualization Host\" sudo yum install virt-manager virt-viewer virt-install # opensuse # see: https://doc.opensuse.org/documentation/leap/virtualization/html/book-virt/cha-vt-installation.html sudo yast2 virtualization # enter to terminal ui, select kvm + kvm tools, and then install it. 安装完成后，还不能直接使用，需要做些额外的工作。请继续往下走。 ","date":"2021-01-17","objectID":"/posts/qemu-kvm-usage/:2:0","series":null,"tags":["虚拟化","KVM","libvirt","QEMU"],"title":"QEMU-KVM 虚拟化环境的搭建与使用","uri":"/posts/qemu-kvm-usage/#一安装-queukvm"},{"categories":["tech"],"content":" 1. libguestfs - 虚拟机磁盘映像处理工具libguestfs 是一个虚拟机磁盘映像处理工具，可用于直接修改/查看/虚拟机映像、转换映像格式等。 它提供的命令列表如下： virt-df centos.img: 查看硬盘使用情况 virt-ls centos.img /: 列出目录文件 virt-copy-out -d domain /etc/passwd /tmp：在虚拟映像中执行文件复制 virt-list-filesystems /file/xx.img：查看文件系统信息 virt-list-partitions /file/xx.img：查看分区信息 guestmount -a /file/xx.qcow2(raw/qcow2都支持) -m /dev/VolGroup/lv_root --rw /mnt：直接将分区挂载到宿主机 guestfish: 交互式 shell，可运行上述所有命令。 virt-v2v: 将其他格式的虚拟机(比如 ova) 转换成 kvm 虚拟机。 virt-p2v: 将一台物理机转换成虚拟机。 学习过程中可能会使用到上述命令，提前安装好总不会有错，安装命令如下： # opensuse sudo zypper install libguestfs # archlinux/manjaro，目前缺少 virt-v2v/virt-p2v 组件 sudo pacman -S libguestfs # ubuntu sudo apt install libguestfs-tools # centos sudo yum install libguestfs-tools ","date":"2021-01-17","objectID":"/posts/qemu-kvm-usage/:2:1","series":null,"tags":["虚拟化","KVM","libvirt","QEMU"],"title":"QEMU-KVM 虚拟化环境的搭建与使用","uri":"/posts/qemu-kvm-usage/#1-libguestfs---虚拟机磁盘映像处理工具"},{"categories":["tech"],"content":" 2. 启动 QEMU/KVM通过 systemd 启动 libvirtd 后台服务： sudo systemctl enable libvirtd.service sudo systemctl start libvirtd.service ","date":"2021-01-17","objectID":"/posts/qemu-kvm-usage/:2:2","series":null,"tags":["虚拟化","KVM","libvirt","QEMU"],"title":"QEMU-KVM 虚拟化环境的搭建与使用","uri":"/posts/qemu-kvm-usage/#2-启动-qemukvm"},{"categories":["tech"],"content":" 3. 让非 root 用户能正常使用 kvmqumu/kvm 装好后，默认情况下需要 root 权限才能正常使用它。 为了方便使用，首先编辑文件 /etc/libvirt/libvirtd.conf: unix_sock_group = \"libvirt\"，取消这一行的注释，使 libvirt 用户组能使用 unix 套接字。 unix_sock_rw_perms = \"0770\"，取消这一行的注释，使用户能读写 unix 套接字。 然后新建 libvirt 用户组，将当前用户加入该组： newgrp libvirt sudo usermod -aG libvirt $USER 最后重启 libvirtd 服务，应该就能正常使用了： sudo systemctl restart libvirtd.service ","date":"2021-01-17","objectID":"/posts/qemu-kvm-usage/:2:3","series":null,"tags":["虚拟化","KVM","libvirt","QEMU"],"title":"QEMU-KVM 虚拟化环境的搭建与使用","uri":"/posts/qemu-kvm-usage/#3-让非-root-用户能正常使用-kvm"},{"categories":["tech"],"content":" 3. 启用嵌套虚拟化如果你需要在虚拟机中运行虚拟机（比如在虚拟机里测试 katacontainers 等安全容器技术），那就需要启用内核模块 kvm_intel 实现嵌套虚拟化。 # 临时启用 kvm_intel 嵌套虚拟化 sudo modprobe -r kvm_intel sudo modprobe kvm_intel nested=1 # 修改配置，永久启用嵌套虚拟化 echo \"options kvm-intel nested=1\" | sudo tee /etc/modprobe.d/kvm-intel.conf 验证嵌套虚拟化已经启用： $ cat /sys/module/kvm_intel/parameters/nested Y 至此，KVM 的安装就大功告成啦，现在应该可以在系统中找到 virt-manager 的图标，进去就可以使用了。 virt-manager 的使用方法和 virtualbox/vmware workstation 大同小异，这里就不详细介绍了，自己摸索摸索应该就会了。 如下内容是进阶篇，主要介绍如何通过命令行来管理虚拟机磁盘，以及 KVM。 如果你还是 kvm 新手，建议先通过图形界面 virt-manager 熟悉熟悉，再往下继续读。 ","date":"2021-01-17","objectID":"/posts/qemu-kvm-usage/:2:4","series":null,"tags":["虚拟化","KVM","libvirt","QEMU"],"title":"QEMU-KVM 虚拟化环境的搭建与使用","uri":"/posts/qemu-kvm-usage/#3-启用嵌套虚拟化"},{"categories":["tech"],"content":" 二、虚拟机磁盘映像管理这需要用到两个工具： libguestfs: 虚拟机磁盘映像管理工具，前面介绍过了 qemu-img: qemu 的磁盘映像管理工具，用于创建磁盘、扩缩容磁盘、生成磁盘快照、查看磁盘信息、转换磁盘格式等等。 # 创建磁盘 qemu-img create -f qcow2 -o cluster_size=128K virt_disk.qcow2 20G # 扩容磁盘 qemu-img resize ubuntu-server-cloudimg-amd64.img 30G # 查看磁盘信息 qemu-img info ubuntu-server-cloudimg-amd64.img # 转换磁盘格式 qemu-img convert -f raw -O qcow2 vm01.img vm01.qcow2 # raw =\u003e qcow2 qemu-img convert -f qcow2 -O raw vm01.qcow2 vm01.img # qcow2 =\u003e raw ","date":"2021-01-17","objectID":"/posts/qemu-kvm-usage/:3:0","series":null,"tags":["虚拟化","KVM","libvirt","QEMU"],"title":"QEMU-KVM 虚拟化环境的搭建与使用","uri":"/posts/qemu-kvm-usage/#二虚拟机磁盘映像管理"},{"categories":["tech"],"content":" 1. 导入 vmware 镜像直接从 vmware ova 文件导入 kvm，这种方式转换得到的镜像应该能直接用（网卡需要重新配置）： virt-v2v -i ova centos7-test01.ova -o local -os /vmhost/centos7-01 -of qcow2 也可以先从 ova 中解压出 vmdk 磁盘映像，将 vmware 的 vmdk 文件转换成 qcow2 格式，然后再导入 kvm（网卡需要重新配置）： # 转换映像格式 qemu-img convert -p -f vmdk -O qcow2 centos7-test01-disk1.vmdk centos7-test01.qcow2 # 查看转换后的映像信息 qemu-img info centos7-test01.qcow2 直接转换 vmdk 文件得到的 qcow2 镜像，启会报错，比如「磁盘无法挂载」。 根据 Importing Virtual Machines and disk images - ProxmoxVE Docs 文档所言，需要在网上下载安装 MergeIDE.zip 组件， 另外启动虚拟机前，需要将硬盘类型改为 IDE，才能解决这个问题。 ","date":"2021-01-17","objectID":"/posts/qemu-kvm-usage/:3:1","series":null,"tags":["虚拟化","KVM","libvirt","QEMU"],"title":"QEMU-KVM 虚拟化环境的搭建与使用","uri":"/posts/qemu-kvm-usage/#1-导入-vmware-镜像"},{"categories":["tech"],"content":" 2. 导入 img 镜像img 镜像文件，就是所谓的 raw 格式镜像，也被称为裸镜像，IO 速度比 qcow2 快，但是体积大，而且不支持快照等高级特性。 如果不追求 IO 性能的话，建议将它转换成 qcow2 再使用。 qemu-img convert -f raw -O qcow2 vm01.img vm01.qcow2 ","date":"2021-01-17","objectID":"/posts/qemu-kvm-usage/:3:2","series":null,"tags":["虚拟化","KVM","libvirt","QEMU"],"title":"QEMU-KVM 虚拟化环境的搭建与使用","uri":"/posts/qemu-kvm-usage/#2-导入-img-镜像"},{"categories":["tech"],"content":" 三、虚拟机管理虚拟机管理可以使用命令行工具 virsh/virt-install，也可以使用 GUI 工具 virt-manager. GUI 很傻瓜式，就不介绍了，这里主要介绍命令行工具 virsh/virt-install 先介绍下 libvirt 中的几个概念： Domain: 指代运行在虚拟机器上的操作系统的实例 - 一个虚拟机，或者用于启动虚拟机的配置。 Guest OS: 运行在 domain 中的虚拟操作系统。 大部分情况下，你都可以把下面命令中涉及到的 domain 理解成虚拟机。 ","date":"2021-01-17","objectID":"/posts/qemu-kvm-usage/:4:0","series":null,"tags":["虚拟化","KVM","libvirt","QEMU"],"title":"QEMU-KVM 虚拟化环境的搭建与使用","uri":"/posts/qemu-kvm-usage/#三虚拟机管理"},{"categories":["tech"],"content":" 0. 设置默认 URIvirsh/virt-install/virt-viewer 等一系列 libvirt 命令，sudo virsh net-list –all 默认情况下会使用 qemu:///session 作为 URI 去连接 QEMU/KVM，只有 root 账号才会默认使用 qemu:///system. 另一方面 virt-manager 这个 GUI 工具，默认也会使用 qemu:///system 去连接 QEMU/KVM（和 root 账号一致） qemu:///system 是系统全局的 qemu 环境，而 qemu:///session 的环境是按用户隔离的。 另外 qemu:///session 没有默认的 network，创建虚拟机时会出毛病。。。 因此，你需要将默认的 URI 改为 qemu:///system，否则绝对会被坑: echo 'export LIBVIRT_DEFAULT_URI=\"qemu:///system\"' \u003e\u003e ~/.bashrc ","date":"2021-01-17","objectID":"/posts/qemu-kvm-usage/:4:1","series":null,"tags":["虚拟化","KVM","libvirt","QEMU"],"title":"QEMU-KVM 虚拟化环境的搭建与使用","uri":"/posts/qemu-kvm-usage/#0-设置默认-uri"},{"categories":["tech"],"content":" 1. 虚拟机网络qemu-kvm 安装完成后，qemu:///system 环境中默认会创建一个 default 网络，而 qemu:///session 不提供默认的网络，需要手动创建。 我们通常使用 qemu:///system 环境就好，可以使用如下方法查看并启动 default 网络，这样后面创建虚拟机时才有网络可用。 # 列出所有虚拟机网络 $ sudo virsh net-list --all Name State Autostart Persistent ---------------------------------------------- default inactive no yes # 启动默认网络 $ virsh net-start default Network default started # 将 default 网络设为自启动 $ virsh net-autostart --network default Network default marked as autostarted # 再次检查网络状况，已经是 active 了 $ sudo virsh net-list --all Name State Autostart Persistent -------------------------------------------- default active yes yes 也可以创建新的虚拟机网络，这需要手动编写网络的 xml 配置，然后通过 virsh net-define --file my-network.xml 创建，这里就不详细介绍了，因为暂时用不到… ","date":"2021-01-17","objectID":"/posts/qemu-kvm-usage/:4:2","series":null,"tags":["虚拟化","KVM","libvirt","QEMU"],"title":"QEMU-KVM 虚拟化环境的搭建与使用","uri":"/posts/qemu-kvm-usage/#1-虚拟机网络"},{"categories":["tech"],"content":" 2. 创建虚拟机 - virt-intall # 使用 iso 镜像创建全新的 proxmox 虚拟机，自动创建一个 60G 的磁盘。 virt-install --virt-type kvm \\ --name pve-1 \\ --vcpus 4 --memory 8096 \\ --disk size=60 \\ --network network=default,model=virtio \\ --os-type linux \\ --os-variant generic \\ --graphics vnc \\ --cdrom proxmox-ve_6.3-1.iso # 使用已存在的 opensuse cloud 磁盘创建虚拟机 virt-install --virt-type kvm \\ --name opensuse15-2 \\ --vcpus 2 --memory 2048 \\ --disk opensuse15.2-openstack.qcow2,device=disk,bus=virtio \\ --disk seed.iso,device=cdrom \\ --os-type linux \\ --os-variant opensuse15.2 \\ --network network=default,model=virtio \\ --graphics vnc \\ --import 其中的 --os-variant 用于设定 OS 相关的优化配置，官方文档强烈推荐设定，其可选参数可以通过 osinfo-query os 查看。 ","date":"2021-01-17","objectID":"/posts/qemu-kvm-usage/:4:3","series":null,"tags":["虚拟化","KVM","libvirt","QEMU"],"title":"QEMU-KVM 虚拟化环境的搭建与使用","uri":"/posts/qemu-kvm-usage/#2-创建虚拟机---virt-intall"},{"categories":["tech"],"content":" 3. 虚拟机管理 - virsh虚拟机创建好后，可使用 virsh 管理虚拟机： 查看虚拟机列表： # 查看正在运行的虚拟机 virsh list # 查看所有虚拟机，包括 inactive 的虚拟机 virsh list --all 使用 virt-viewer 以 vnc 协议登入虚拟机终端： # 使用虚拟机 ID 连接 virt-viewer 8 # 使用虚拟机名称连接，并且等待虚拟机启动 virt-viewer --wait opensuse15 启动、关闭、暂停(休眠)、重启虚拟机： virsh start opensuse15 virsh suuspend opensuse15 virsh resume opensuse15 virsh reboot opensuse15 # 优雅关机 virsh shutdown opensuse15 # 强制关机 virsh destroy opensuse15 # 启用自动开机 virsh autostart opensuse15 # 禁用自动开机 virsh autostart --disable opensuse15 虚拟机快照管理： # 列出一个虚拟机的所有快照 virsh snapshot-list --domain opensuse15 # 给某个虚拟机生成一个新快照 virsh snapshot-create \u003cdomain\u003e # 使用快照将虚拟机还原 virsh snapshot-restore \u003cdomain\u003e \u003csnapshotname\u003e # 删除快照 virsh snapshot-delete \u003cdomain\u003e \u003csnapshotname\u003e 删除虚拟机： virsh undefine opensuse15 迁移虚拟机： # 使用默认参数进行离线迁移，将已关机的服务器迁移到另一个 qemu 实例 virsh migrate 37 qemu+ssh://tux@jupiter.example.com/system # 还支持在线实时迁移，待续 cpu/内存修改： # 改成 4 核 virsh setvcpus opensuse15 4 # 改成 4G virsh setmem opensuse15 4096 虚拟机监控： # 待续 修改磁盘、网络及其他设备： # 添加新设备 virsh attach-device virsh attach-disk virsh attach-interface # 删除设备 virsh detach-disk virsh detach-device virsh detach-interface ","date":"2021-01-17","objectID":"/posts/qemu-kvm-usage/:4:4","series":null,"tags":["虚拟化","KVM","libvirt","QEMU"],"title":"QEMU-KVM 虚拟化环境的搭建与使用","uri":"/posts/qemu-kvm-usage/#3-虚拟机管理---virsh"},{"categories":["tech"],"content":" 参考 Virtualization Guide - OpenSUSE Complete Installation of KVM, QEMU and Virt Manager on Arch Linux and Manjaro virtualization-libvirt - ubuntu docs RedHat Docs - KVM ","date":"2021-01-17","objectID":"/posts/qemu-kvm-usage/:5:0","series":null,"tags":["虚拟化","KVM","libvirt","QEMU"],"title":"QEMU-KVM 虚拟化环境的搭建与使用","uri":"/posts/qemu-kvm-usage/#参考"},{"categories":null,"content":" 本站的文章可能以中文或英文两种语言书写，其中部分文章可能是双语，也有部分文章只有中文或者英文版本，请读者按需阅读。 The articles on this site may be written in Chinese or English, some of them may be bilingual, and some of them are only available in Chinese or English. Please read them as needed. ","date":"2021-01-16","objectID":"/about/:0:0","series":null,"tags":null,"title":"关于","uri":"/about/#"},{"categories":null,"content":" 关于我 有很多的绝望，但也有美的时刻，只不过在美的时刻，时间是不同于以往的。 ──《刺猬的优雅》 昵称：中文昵称「於清樂」「二花」，英文 ID「ryan4yin」「ryan_yin」 性别：He/Him/他，异性恋 工作与学习经历 2012-6 ~ 2015-6：在（湖南邵阳）武冈一中读高中 2015-6 ~ 2019-6：在安徽建筑大学读声学专业，没错就是初中物理课上敲音叉的那个声学（不是音乐…）。本人专业知识战五渣，学位证都没拿到 emmm 2019-6 ~ 2021-2：在某不知名创业公司做全能运维（全干工程师），上至搭建阿里云生产环境、搞 Kubernetes 集群、Istio 服务网格、CICD、性能测试，下至搞洋垃圾戴尔服务器与 vSphere、装水管、修门禁、扫地拖地、当苦力搬运货物，反正没人干的就运维干呗 2021-3 ~ 至今：在 Mobiuspace 担任 SRE 工程师，主要负责维护与优化 K8s 服务平台及流量链路，分析与管控 AWS/GCP/Tencent 云计算成本 音乐： 喜欢听后摇、蓝草、民谣、器乐 最喜欢的歌手是虚拟歌手洛天依 有在断断续续地学习竹笛跟口琴（复音、蓝调都有在学），另外也有在学习使用 Synthesizer V/ACE 跟 Reaper 运动：喜欢轮滑以及游泳，哦还有 VR 游戏《Beat Saber》《Pistol Whip》，但是目前都是半吊子哈哈~ 茶：2021 年在朋友家喝过一次青钱柳后就一直念念不忘，年底就入坑了凤牌滇红、天之红祁门红茶、极白安吉白茶，目前比较喜欢喝红茶 书籍：读得最多的正经书是 IT 技术书籍，另外也喜欢看科幻，以及戒不掉的网文/轻小说 影视：看得最多的是动漫，另外就是欧美科幻片、温情片 中文输入方案：小鹤音形 自然语言 English: Good at reading technical articles, but weak in writing, listening and speaking 中文：母语，高中语文中等水准。希望能学会用中文写小说，就先从短篇开始吧 编程语言 Go/Python: 目前的主力，也是我最熟悉的语言 C: 荒废比较久了，为了学习 Linux 正在重新学习 Rust: 学习中，大量函数式的语法糖，贴心的编译器提示，感觉很好用（当然也比较折磨人…） Java: 大三学过，但是很久没用荒废地差不多了 Lua: 谈不上感兴趣，只是为了搞 OpenResty/APISIX 必须得学，目前处于会复制粘贴的程度… 熟悉的技术： Kubernetes Istio Linux 想学的技术（按感兴趣程度降序排列，想学的太多，学会的太少） Linux 内核技术、eBPF Kubernetes/Istio/Nginx 源码 机器学习、深度学习 区块链 3D 建模与渲染 Reaper 编曲、ACE/SynthV 调教 蓝调口琴、竹笛、键盘 联系方式 邮件：eWVzIG15IGVtYWlsIGlzIHhpYW95aW5fY0BxcS5jb20K 豆瓣：本人是豆瓣书影音标记的重度使用者，从 2015 年以来一直使用豆瓣标记读过的书看过的电影。 Twitter: 自 2021 年起在 Twitter 上发布一些生活动态与技术内容，同时也很喜欢看 Twitter 上各位画手大触的作品。是我日常消遣与接触碎片化信息的重要渠道之一。 Bilibili: 日常用 Bilibili 消遣，日常喜欢看 MMD 舞蹈、虚拟歌姬、硬核技术相关内容。 Github Issues: 也可以考虑直接在此仓库的 Issue 区联系我，我在 Github 上还挺活跃的，一般不会漏消息。 ","date":"2021-01-16","objectID":"/about/:1:0","series":null,"tags":null,"title":"关于","uri":"/about/#关于我"},{"categories":null,"content":" 关于此博客 “对我来说，博客首先是一种知识管理工具，其次才是传播工具。我的技术文章，主要用来整理我还不懂的知识。我只写那些我还没有完全掌握的东西，那些我精通的东西，往往没有动力写。炫耀从来不是我的动机，好奇才是。\" ──阮一峰 ","date":"2021-01-16","objectID":"/about/:2:0","series":null,"tags":null,"title":"关于","uri":"/about/#关于此博客"},{"categories":null,"content":" 博客内容2020 年及之前的技术文章，都搬运自我的博客园 https://www.cnblogs.com/kirito-c/。 2021 年开始的技术内容，大都来自我的个人笔记 ryan4yin/knowledge，我会不定期从这个笔记中找些有意思的内容，整理润色后，拿出来和大家分享，同时查漏补缺。 而另一部分生活类的随笔，则是我的闲言碎语，许多都来自我的印象笔记。 ","date":"2021-01-16","objectID":"/about/:2:1","series":null,"tags":null,"title":"关于","uri":"/about/#博客内容"},{"categories":null,"content":" 我曾用过的箴言这些箴言曾经陪伴我与这个博客度过了许多春夏秋冬，它们都曾在某个时期给过我力量，每每看到都令人怀念。 在这里也将它们送给各位读者，希望它们也能给你以力量！ 2021-02-06 ~ 2022-01-03 拆破玉笼飞彩凤，顿开金锁走蛟龙。 2021-01-16 - 2022-04-04 双手合十 闭上眼睛 心里什么也不去想 嘴角就高高扬起 笑出声来 赞美快乐~ 2022-04-04 ~ now 我错过花，却看见海。 ","date":"2021-01-16","objectID":"/about/:2:2","series":null,"tags":null,"title":"关于","uri":"/about/#我曾用过的箴言"},{"categories":null,"content":" 博客时间线 博客时间线 2016-06-17：（大一下学期）在博客园创建博客 https://www.cnblogs.com/kirito-c/ 第一篇博文是贪吃蛇—C—基于easyx图形库，现在还能回忆起收到第一条评论时的兴奋之情。 2021-01-16：（工作一年多后）申请域名并开设独立博客 https://ryan4yin.space/ 2022-01-28：站点从 Cloudflare + Github Gages 迁移到 Vercel，国内访问速度有一定提升。 2022-02-07：（第一份工作结束后赋闲）将博客主域名切换为 https://thiscute.world/，另外新增备用域名 https://writefor.fun 2022-02-07：站点添加「阅读排行」页，展示从 Google Analytics 拉取的站点统计数据。 2022-02-16：站点通过十年之约审核，正式加入十年之约 2022-08-12：站点架构升级为 Azure Front Door + Vercel，国内访问速度显著提升。 博客快照-2020-01-21 博客园快照 博客快照-2022-07-31 ThisCute.World 快照 ","date":"2021-01-16","objectID":"/about/:2:3","series":null,"tags":null,"title":"关于","uri":"/about/#博客时间线"},{"categories":null,"content":" 注意事项本站所有技术内容均为个人观点，不保证正确，另外随着时间变化部分技术内容也可能会失效，请读者自行甄别。 另外本站使用的许多配图都来源于网络，如有侵权，请联系我删除。 ","date":"2021-01-16","objectID":"/about/:3:0","series":null,"tags":null,"title":"关于","uri":"/about/#注意事项"},{"categories":null,"content":" 画外互联网浩如烟海，这个小站偏安一隅，如果它有幸被你发现，而且其中文字对你还有些帮助，那可真是太棒了！感谢有你~ ","date":"2021-01-16","objectID":"/about/:4:0","series":null,"tags":null,"title":"关于","uri":"/about/#画外"},{"categories":null,"content":" 感谢 @芝士部落格 提供了友链页面模板~ 在友链形成的网络中漫游，是一件很有意思的事情。 以前的人们通过信笺交流，而我们通过友链串联起一个「世界」。希望你我都能在这个「世界」中有所收获 注： 下方友链次序每次刷新页面随机排列。 ","date":"2021-01-16","objectID":"/friends/:0:0","series":null,"tags":null,"title":"我的小伙伴们","uri":"/friends/#"},{"categories":null,"content":" 交换友链如果你觉得我的博客有些意思，而且也有自己的博客，并且博客运行时间超过半年，至少有 6 篇自认为有价值的原创文章，欢迎与我交换友链~ 可通过 Issues 或者评论区提交友链申请，格式如下： 站点名称：This Cute World 站点地址：https://thiscute.world/ 个人形象：https://thiscute.world/avatar/myself.webp 站点描述：赞美快乐~ ","date":"2021-01-16","objectID":"/friends/:1:0","series":null,"tags":null,"title":"我的小伙伴们","uri":"/friends/#交换友链"},{"categories":["tech"],"content":"Pulumi 是一个基础设施的自动管理工具，使用 Python/TypeScript/Go/Dotnet 编写好声明式的资源配置，就能实现一键创建/修改/销毁各类资源，这里的资源可以是： AWS/阿里云等云上的负载均衡、云服务器、TLS 证书、DNS、CDN、OSS、数据库…几乎所有的云上资源 本地自建的 vSphere/Kubernetes/ProxmoxVE/libvirt 环境中的虚拟机、容器等资源 相比直接调用 AWS/阿里云/Kubernetes 的 API，使用 pulumi 的好处有： 声明式配置：你只需要声明你的资源属性就 OK，所有的状态管理、异常处理都由 pulumi 完成。 统一的配置方式：提供统一的配置方法，来声明式的配置所有 AWS/阿里云/Kubernetes 资源。 声明式配置的可读性更好，更便于维护 试想一下，通过传统的手段去从零搭建一个云上测试环境、或者本地开发环境，需要手工做多少繁琐的工作。 而依靠 Pulumi 这类「基础设施即代码（Infrastructure as Code, IaC）」的工具，只需要一行命令就能搭建好一个可复现的云上测试环境或本地开发环境。 比如我们的阿里云测试环境，包括两个 kubernetes 集群、负载均衡、VPC 网络、数据库、云监控告警/日志告警、RAM账号权限体系等等，是一个比较复杂的体系。 人工去配置这么多东西，想要复现是很困难的，非常繁琐而且容易出错。 但是使用 pulumi，只需要一行命令，就能创建并配置好这五花八门一大堆的玩意儿。 销毁整个测试环境也只需要一行命令。 实际使用体验：我们使用 Pulumi 自动化了阿里云测试环境搭建 95%+ 的操作，这个比例随着阿里云的 pulumi provider 的完善，还可以进一步提高！ ","date":"2021-01-08","objectID":"/posts/expirence-of-pulumi/:0:0","series":null,"tags":["基础设施即代码","云原生","Pulumi","Terraform"],"title":"Pulumi 使用体验 - 基础设施代码化","uri":"/posts/expirence-of-pulumi/#"},{"categories":["tech"],"content":" Pulumi vs Terraform vs CloudFormation先介绍下 CloudFormation，它是 AWS 提供的一个 IaC 工具， 它使用 json/yaml 编写声明式配置文件，然后完全在 AWS 云上进行资源的创建、管理、销毁。 其所创建的资源跟 CloudFormation Task 同生命周期，因此删除该 CloudFormation Task 就会自动销毁所有相关资源。 因此它的好处应该是可以完全在云上运行，本地客户端只是一个提交配置的工具。而缺点则是只能在 AWS 上使用。 而在通用的「基础设施即代码」领域，有一个工具比 Pulumi 更流行，它就是 Terraform. 实际上我们一开始使用的也是 Terraform，但是后来使用 Pulumi 完全重写了一遍。 主要原因是，Pulumi 解决了 Terraform 配置的一个痛点：配置语法太过简单，导致配置繁琐。而且还要额外学习一门 DSL - HCL Terraform 虽然应用广泛，但是它默认使用的 HCL 语言太简单，表现力不够强。 这就导致在一些场景下使用 Terraform，会出现大量的重复配置。 一个典型的场景是「批量创建资源，动态生成资源参数」。比如批量创建一批名称类似的 ECS 服务器/VPC交换机。如果使用 terraform，就会出现大量的重复配置。 改用 terraform 提供的 module 能在一定程度上实现配置的复用，但是它还是解决不了问题。 要使用 module，你需要付出时间去学习 module 的概念，为了拼接参数，你还需要学习 HCL 的一些高级用法。 但是付出了这么多，最后写出的 module 还是不够灵活——它被 HCL 局限住了。 为了实现如此的参数化动态化，我们不得不引入 Python 等其他编程语言。于是构建流程就变成了： 借助 Python 等其他语言先生成出 HCL 配置 通过 terraform 命令行进行 plan 与 apply 通过 Python 代码解析 terraform.tfstat，获取 apply 结果，再进行进一步操作。 这显然非常繁琐，主要困难就在于 Python 和 Terraform 之间的交互。 进一步思考，既然其他编程语言如 Python/Go 的引入不可避免，那是不是能使用它们彻底替代掉 HCL 呢？能不能直接使用 Python/Go 编写配置？如果 Terraform 原生就支持 Python/Go 来编写配置，那就不存在交互问题了。 相比于使用领域特定语言 HCL，使用通用编程语言编写配置，好处有： Python/Go/TypeScript 等通用的编程语言，也支持 Yaml 这样方便自动化生成的配置语言，能满足你的一切需求。 作为一个开发人员/DevOps，你应该对 Python/Go 等语言相当熟悉，可以直接利用上已有的经验。 更方便测试：可以使用各编程语言中流行的测试框架来测试 pulumi 配置！ 于是 Pulumi 横空出世。 另一个和 Pulumi 功能类似的工具，是刚出炉没多久的 terraform-cdk，但是目前它还很不成熟。 ","date":"2021-01-08","objectID":"/posts/expirence-of-pulumi/:1:0","series":null,"tags":["基础设施即代码","云原生","Pulumi","Terraform"],"title":"Pulumi 使用体验 - 基础设施代码化","uri":"/posts/expirence-of-pulumi/#pulumi-vs-terraform-vs-cloudformation"},{"categories":["tech"],"content":" Pulumi 特点介绍 原生支持通过 Python/Go/TypeScript/Dotnet 等语言编写配置，也就完全解决了上述的 terraform 和 python 的交互问题。 pulumi 是目前最流行的 真-IaaS 工具，对各语言的支持都很成熟。 兼容 terraform 的所有 provider，只是需要自行使用 pulumi-tf-provider-boilerplate 重新打包，有些麻烦。 pulumi 官方的 provider 几乎全都是封装的 terraform provider，包括 aws/azure/alicloud，目前只发现 kubernetes 是原生的（独苗啊）。 状态管理和 secrets 管理有如下几种选择： 使用 app.pulumi.com（默认）:免费版提供 stack 历史管理，可以看到所有的历史记录。另外还提供一个资源关系的可视化面板。总之很方便，但是多人合作就需要收费。 本地文件存储：pulumi login file:///app/data 云端对象存储，支持 s3 等对象存储协议，因此可以使用 AWS 或者本地的 MinIO 来做 Backend. pulumi login 's3://\u003cbucket-path\u003e?endpoint=my.minio.local:8080\u0026disableSSL=true\u0026s3ForcePathStyle=true' minio/aws 的 creadential 可以通过 AWS_ACCESS_KEY_ID 和 AWS_SECRET_ACCESS_KEY 两个环境变量设置。另外即使是使用 MinIO，AWS_REGION 这个没啥用的环境变量也必须设置！否则会报错。 gitlab 13 支持 Terraform HTTP State 协议，等这个 pr 合并，pulumi 也能以 gitlab 为 backend 了。 使用 pulumi 企业版（自建服务）：比 app.pulumi.com 提供更多的特性，但是显然是收费的。。 总之，非常香，强烈推荐各位 DevOps 试用。 以下内容是我对 pulumi 的一些思考，以及使用 pulumi 遇到的各种问题+解决方法，适合对 pulumi 有一定了解的同学阅读。 如果你刚接触 Pulumi 而且有兴趣学习，建议先移步 pulumi get started 入个门，再接着看下面的内容。 ","date":"2021-01-08","objectID":"/posts/expirence-of-pulumi/:2:0","series":null,"tags":["基础设施即代码","云原生","Pulumi","Terraform"],"title":"Pulumi 使用体验 - 基础设施代码化","uri":"/posts/expirence-of-pulumi/#pulumi-特点介绍"},{"categories":["tech"],"content":" 使用建议 建议查看对应的 terraform provider 文档：pulumi 的 provider 基本都是封装的 terraform 版本，而且文档是自动生成的，比（简）较（直）难（一）看（坨）懂（shi），examples 也少。 stack: pulumi 官方提供了两种 stack 用法：「单体」和「微-stack」 单体: one stack rule them all，通过 stack 参数来控制步骤。stack 用来区分环境 dev/pro 等。 微-stack: 每一个 stack 是一个步骤，所有 stack 组成一个完整的项目。 实际使用中，我发现「微-stack」模式需要使用到 pulumi 的 inter-stack dependencies，报一堆的错，而且不够灵活。因此目前更推荐「单体」模式。 我们最近使用 pulumi 完全重写了以前用 terraform 编写的云上配置，简化了很多繁琐的配置，也降低了我们 Python 运维代码和 terraform 之间的交互难度。 另外我们还充分利用上了 Python 的类型检查和语法检查，很多错误 IDE 都能直接给出提示，强化了配置的一致性和可维护性。 不过由于阿里云 provider 暂时还： 不支持管理 ASM 服务网格、DTS 数据传输等资源 OSS 等产品的部分参数也暂时不支持配置（比如 OSS 不支持配置图片样式、ElasticSearch 暂时不支持自动创建 7.x 版本） 不支持创建 ElasticSearch 7.x 这些问题，导致我们仍然有部分配置需要手动处理，另外一些耗时长的资源，需要单独去创建。 因此还不能实现完全的「一键」。 ","date":"2021-01-08","objectID":"/posts/expirence-of-pulumi/:3:0","series":null,"tags":["基础设施即代码","云原生","Pulumi","Terraform"],"title":"Pulumi 使用体验 - 基础设施代码化","uri":"/posts/expirence-of-pulumi/#使用建议"},{"categories":["tech"],"content":" 常见问题","date":"2021-01-08","objectID":"/posts/expirence-of-pulumi/:4:0","series":null,"tags":["基础设施即代码","云原生","Pulumi","Terraform"],"title":"Pulumi 使用体验 - 基础设施代码化","uri":"/posts/expirence-of-pulumi/#常见问题"},{"categories":["tech"],"content":" 1. Output 的用法 pulumi 通过资源之间的属性引用（Output[str]）来确定依赖关系，如果你通过自定义的属性(str)解耦了资源依赖，会导致资源创建顺序错误而创建失败。 Output[str] 是一个异步属性，类似 Future，不能被用在 pulumi 参数之外的地方！ Output[str] 提供两种方法能直接对 Output[str] 进行一些操作： Output.concat(\"http://\", domain, \"/\", path): 此方法将 str 与 Output[str] 拼接起来，返回一个新的 Output[str] 对象，可用做 pulumi 属性。 domain.apply(lambda it: print(it)): Output[str] 的 apply 方法接收一个函数。在异步获取到数据后，pulumi 会调用这个函数，把具体的数据作为参数传入。 另外 apply 也会将传入函数的返回值包装成 Output 类型返回出来。 可用于：在获取到数据后，将数据打印出来/发送到邮箱/调用某个 API 上传数据等等。 Output.all(output1, output2, ...).apply(lambda it: print(it)) 可用于将多个 output 值，拼接成一个 Output 类型，其内部的 raw 值为一个 tuple 对象 (str1, str2, ...). 官方举例：connection_string = Output.all(sql_server.name, database.name).apply(lambda args: f\"Server=tcp:{args[0]}.database.windows.net;initial catalog={args[1]}...\") ","date":"2021-01-08","objectID":"/posts/expirence-of-pulumi/:4:1","series":null,"tags":["基础设施即代码","云原生","Pulumi","Terraform"],"title":"Pulumi 使用体验 - 基础设施代码化","uri":"/posts/expirence-of-pulumi/#1-output-的用法"},{"categories":["tech"],"content":" 2. 如何使用多个云账号/多个 k8s 集群？默认情况下 pulumi 使用默认的 provider，但是 pulumi 所有的资源都有一个额外的 opts 参数，可用于设定其他 provider。 通过这个 opts，我们可以实现在一个 pulumi 项目中，使用多个云账号，或者管理多个 k8s 集群。 示例： from pulumi import get_stack, ResourceOptions, StackReference from pulumi_alicloud import Provider, oss # 自定义 provider，key/secret 通过参数设定，而不是从默认的环境变量读取。 # 可以自定义很多个 providers provider = pulumi_alicloud.Provider( \"custom-alicloud-provider\", region=\"cn-hangzhou\", access_key=\"xxx\", secret_key=\"jjj\", ) # 通过 opts，让 pulumi 使用自定义的 provider（替换掉默认的） bucket = oss.Bucket(..., opts=ResourceOptions(provider=provider)) ","date":"2021-01-08","objectID":"/posts/expirence-of-pulumi/:4:2","series":null,"tags":["基础设施即代码","云原生","Pulumi","Terraform"],"title":"Pulumi 使用体验 - 基础设施代码化","uri":"/posts/expirence-of-pulumi/#2-如何使用多个云账号多个-k8s-集群"},{"categories":["tech"],"content":" 3. inter-stack 属性传递 这东西还没搞透，待研究。 多个 stack 之间要互相传递参数，需要通过 pulumi.export 导出属性，通过 stack.require_xxx 获取属性。 从另一个 stack 读取属性的示例： from pulumi import StackReference cfg = pulumi.Config() stack_name = pulumi.get_stack() # stack 名称 project = pulumi.get_project() infra = StackReference(f\"ryan4yin/{project}/{stack_name}\") # 这个属性在上一个 stack 中被 export 出来 vpc_id = infra.require(\"resources.vpc.id\") ","date":"2021-01-08","objectID":"/posts/expirence-of-pulumi/:4:3","series":null,"tags":["基础设施即代码","云原生","Pulumi","Terraform"],"title":"Pulumi 使用体验 - 基础设施代码化","uri":"/posts/expirence-of-pulumi/#3-inter-stack-属性传递"},{"categories":["tech"],"content":" 4. pulumi up 被中断，或者对资源做了手动修改，会发生什么？ 强行中断 pulumi up，会导致资源进入 pending 状态，必须手动修复。 修复方法：pulumi stack export，删除 pending 资源，再 pulumi stack import 手动删除了云上资源，或者修改了一些对资源管理无影响的参数，对 pulumi 没有影响，它能正确检测到这种情况。 可以通过 pulumi refresh 手动从云上拉取最新的资源状态。 手动更改了资源之间的依赖关系（比如绑定 EIP 之类的），很可能导致 pulumi 无法正确管理资源之间的依赖。 这种情况必须先手动还原依赖关系（或者把相关资源全部手动删除掉），然后才能继续使用 pulumi。 ","date":"2021-01-08","objectID":"/posts/expirence-of-pulumi/:4:4","series":null,"tags":["基础设施即代码","云原生","Pulumi","Terraform"],"title":"Pulumi 使用体验 - 基础设施代码化","uri":"/posts/expirence-of-pulumi/#4-pulumi-up-被中断或者对资源做了手动修改会发生什么"},{"categories":["tech"],"content":" 5. 如何手动声明资源间的依赖关系？有时候因为一些问题（比如 pulumi provider 功能缺失，使用了 restful api 实现部分功能），pulumi 可能无法识别到某些资源之间的依赖关系。 这时可以为资源添加 dependsOn 属性，这个属性能显式地声明依赖关系。 ","date":"2021-01-08","objectID":"/posts/expirence-of-pulumi/:4:5","series":null,"tags":["基础设施即代码","云原生","Pulumi","Terraform"],"title":"Pulumi 使用体验 - 基础设施代码化","uri":"/posts/expirence-of-pulumi/#5-如何手动声明资源间的依赖关系"},{"categories":["tech"],"content":" 6. 如何导入已经存在的资源？如果你司不是一开始就使用了 pulumi 这类工具，那通常绝大部分云上资源都是手动管理、或者由其他工具自动化管理的，该如何将它们纳入 pulumi 管辖呢？ 官方有提供一篇相关文档 Importing Infrastructure. 文档有提到两种资源导入的方法，导入成功后都会自动生成资源的状态，以及对应的 pulumi 代码。 第一种是使用 pulumi import 命令，第二种是在代码中使用 import 参数。 除此之外，社区还有几个其他资源导入工具（reverse IaC）值得研究： former2: 为已有的 AWS 资源生成 terraform/pulumi/cloudformation 等配置，但是不支持生成 tfstate 状态 terraformer: 为已有的 AWS/GCP/Azure/Alicloud/DigitalOcean 等多种云资源生成 terraform 配置以及 tfstate 状态 terracognita: 功能跟 terraformer 一样，都支持生成 terraform 配置以及 tfstate 状态，但是它支持 AWS/GCP/Azure 三朵云 pulumi-terraform: 这个 provider 使你可以在 pulumi 项目里使用 tfstate 状态文件 tf2pulumi: 将 terraform 配置转换为 pulumi typescript 配置 6.1 通过 pulumi import 命令导入资源使用 pulumi import 命令导入资源的好处是，不需要为每个资源手写代码，此命令会自动生成资源的 stack state 与配置代码。 使用此命令导入的资源，默认会启用删除保护，你可通过参数 --protect=false 来关闭删除保护。 资源名称可通过命令行参数，或者 Json 文件来指定。 下面我们演示一个导入一个 s3 bucket 的流程： # 导入一个名为 test-sre 的 s3 bucket，资源 ID 为 p-test-sre $ pulumi import aws:s3/bucket:Bucket p-test-sre test-sre ...... Do you want to perform this import? yes Importing (dev): Type Name Status + pulumi:pulumi:Stack pulumi-test-dev created = └─ aws:s3:Bucket p-test-sre imported Resources: + 1 created = 1 imported 2 changes Duration: 8s Please copy the following code into your Pulumi application. Not doing so will cause Pulumi to report that an update will happen on the next update command. Please note that the imported resources are marked as protected. To destroy them you will need to remove the `protect` option and run `pulumi update` *before* the destroy will take effect. import pulumi import pulumi_aws as aws p_test_sre = aws.s3.Bucket(\"p-test-sre\", arn=\"arn:aws:s3:::test-sre\", bucket=\"test-sre\", hosted_zone_id=\"ZZBBCC332211KK\", request_payer=\"BucketOwner\", tags={ \"Name\": \"test-sre\", \"Team\": \"Platform\", }, opts=pulumi.ResourceOptions(protect=True)) 能看到它会自动导入对应资源的 state，并同时打印出对应的 python 代码，要求我们手动将代码复制粘贴到项目中。 而且代码会自带 arn/hosted_zone_id/protect 等属性，说明这个资源实际上是无法像普通 pulumi 资源一样，通过 pulumi up/pulumi destroy 自动创建销毁的。要通过 pulumi 删除该资源，需要首先解除删除保护，然后将对应的代码片段删除掉，最后执行 pulumi up。 也可通过 json 来批量导入资源，首先编写一个 json 资源清单： { \"resources\": [{ \"type\": \"aws:s3/bucket:Bucket\", \"name\": \"s3-bucket_xxx-debug\", \"id\": \"xxx-debug\" }, { \"type\": \"aws:s3/accessPoint:AccessPoint\", \"name\": \"s3-accesspoint_xxx-debug\", \"id\": \"112233445566:xxx-debug\" } ] } 然后执行如下命令批量导入资源： $ pulumi import -f test-resources.json ...... Do you want to perform this import? yes Importing (dev): Type Name Status pulumi:pulumi:Stack pulumi-test-dev = ├─ aws:s3:AccessPoint s3-accesspoint_xxx-debug imported = └─ aws:s3:Bucket s3-bucket_xxx-debug imported Resources: = 2 imported 2 unchanged Duration: 8s Please copy the following code into your Pulumi application. Not doing so will cause Pulumi to report that an update will happen on the next update command. Please note that the imported resources are marked as protected. To destroy them you will need to remove the `protect` option and run `pulumi update` *before* the destroy will take effect. import pulumi import pulumi_aws as aws s3_bucket_snappea_dl_debug = aws.s3.Bucket(\"s3-bucket_xxx-debug\", arn=\"arn:aws:s3:::xxx-debug\", bucket=\"xxx-debug\", hosted_zone_id=\"ZZBBCC332211KK\", request_payer=\"BucketOwner\", tags={ \"Name\": \"xxx-debug\", \"Team\": \"Xxx\", }, opts=pulumi.ResourceOptions(protect=True)) s3_accesspoint_snappea_dl_debug = aws.s3.AccessPoint(\"s3-accesspoint_xxx-debug\", account_id=\"112233445566\", bucket=\"xxx-debug\", name=\"xxx-debug\", public_access_block_configuration=aws.s3.AccessPointPublicAccessBlockConfigurationArgs( block_public_acls=False, block_public_policy=False, ignore_public_acls=False, restrict_public_buckets=False, ), opts=pulumi.ResourceOptions(protect=True)) 能看到同样的生成出了两个资源的 stack 状态，以及对应的代码。 6.2 通过代码导入资源通过代码导入资源，需要你手工为每个资源编写代码，并且确保代码的所有参数与资源本身的状态完全一致。 因此可以看到这种导入方式很不灵活，通常不推荐使用，pulumi import 自动生成代码它不香么 emmmm 大概的流程如下，首先编写一个资源的配置代码，并将其标注为 import: p_test_sre = aws.s3.Bucket(\"p-test-sre\", bucket=\"test-sre\", tags={ \"Name\": \"test-sre\", \"Team\": \"xxx\", # 这里我故意写错了，pulumi 会检测到这里有问题，提示导入将失败 }, opts=pulumi.ResourceOptio","date":"2021-01-08","objectID":"/posts/expirence-of-pulumi/:4:6","series":null,"tags":["基础设施即代码","云原生","Pulumi","Terraform"],"title":"Pulumi 使用体验 - 基础设施代码化","uri":"/posts/expirence-of-pulumi/#6-如何导入已经存在的资源"},{"categories":["tech"],"content":" 6. 如何导入已经存在的资源？如果你司不是一开始就使用了 pulumi 这类工具，那通常绝大部分云上资源都是手动管理、或者由其他工具自动化管理的，该如何将它们纳入 pulumi 管辖呢？ 官方有提供一篇相关文档 Importing Infrastructure. 文档有提到两种资源导入的方法，导入成功后都会自动生成资源的状态，以及对应的 pulumi 代码。 第一种是使用 pulumi import 命令，第二种是在代码中使用 import 参数。 除此之外，社区还有几个其他资源导入工具（reverse IaC）值得研究： former2: 为已有的 AWS 资源生成 terraform/pulumi/cloudformation 等配置，但是不支持生成 tfstate 状态 terraformer: 为已有的 AWS/GCP/Azure/Alicloud/DigitalOcean 等多种云资源生成 terraform 配置以及 tfstate 状态 terracognita: 功能跟 terraformer 一样，都支持生成 terraform 配置以及 tfstate 状态，但是它支持 AWS/GCP/Azure 三朵云 pulumi-terraform: 这个 provider 使你可以在 pulumi 项目里使用 tfstate 状态文件 tf2pulumi: 将 terraform 配置转换为 pulumi typescript 配置 6.1 通过 pulumi import 命令导入资源使用 pulumi import 命令导入资源的好处是，不需要为每个资源手写代码，此命令会自动生成资源的 stack state 与配置代码。 使用此命令导入的资源，默认会启用删除保护，你可通过参数 --protect=false 来关闭删除保护。 资源名称可通过命令行参数，或者 Json 文件来指定。 下面我们演示一个导入一个 s3 bucket 的流程： # 导入一个名为 test-sre 的 s3 bucket，资源 ID 为 p-test-sre $ pulumi import aws:s3/bucket:Bucket p-test-sre test-sre ...... Do you want to perform this import? yes Importing (dev): Type Name Status + pulumi:pulumi:Stack pulumi-test-dev created = └─ aws:s3:Bucket p-test-sre imported Resources: + 1 created = 1 imported 2 changes Duration: 8s Please copy the following code into your Pulumi application. Not doing so will cause Pulumi to report that an update will happen on the next update command. Please note that the imported resources are marked as protected. To destroy them you will need to remove the `protect` option and run `pulumi update` *before* the destroy will take effect. import pulumi import pulumi_aws as aws p_test_sre = aws.s3.Bucket(\"p-test-sre\", arn=\"arn:aws:s3:::test-sre\", bucket=\"test-sre\", hosted_zone_id=\"ZZBBCC332211KK\", request_payer=\"BucketOwner\", tags={ \"Name\": \"test-sre\", \"Team\": \"Platform\", }, opts=pulumi.ResourceOptions(protect=True)) 能看到它会自动导入对应资源的 state，并同时打印出对应的 python 代码，要求我们手动将代码复制粘贴到项目中。 而且代码会自带 arn/hosted_zone_id/protect 等属性，说明这个资源实际上是无法像普通 pulumi 资源一样，通过 pulumi up/pulumi destroy 自动创建销毁的。要通过 pulumi 删除该资源，需要首先解除删除保护，然后将对应的代码片段删除掉，最后执行 pulumi up。 也可通过 json 来批量导入资源，首先编写一个 json 资源清单： { \"resources\": [{ \"type\": \"aws:s3/bucket:Bucket\", \"name\": \"s3-bucket_xxx-debug\", \"id\": \"xxx-debug\" }, { \"type\": \"aws:s3/accessPoint:AccessPoint\", \"name\": \"s3-accesspoint_xxx-debug\", \"id\": \"112233445566:xxx-debug\" } ] } 然后执行如下命令批量导入资源： $ pulumi import -f test-resources.json ...... Do you want to perform this import? yes Importing (dev): Type Name Status pulumi:pulumi:Stack pulumi-test-dev = ├─ aws:s3:AccessPoint s3-accesspoint_xxx-debug imported = └─ aws:s3:Bucket s3-bucket_xxx-debug imported Resources: = 2 imported 2 unchanged Duration: 8s Please copy the following code into your Pulumi application. Not doing so will cause Pulumi to report that an update will happen on the next update command. Please note that the imported resources are marked as protected. To destroy them you will need to remove the `protect` option and run `pulumi update` *before* the destroy will take effect. import pulumi import pulumi_aws as aws s3_bucket_snappea_dl_debug = aws.s3.Bucket(\"s3-bucket_xxx-debug\", arn=\"arn:aws:s3:::xxx-debug\", bucket=\"xxx-debug\", hosted_zone_id=\"ZZBBCC332211KK\", request_payer=\"BucketOwner\", tags={ \"Name\": \"xxx-debug\", \"Team\": \"Xxx\", }, opts=pulumi.ResourceOptions(protect=True)) s3_accesspoint_snappea_dl_debug = aws.s3.AccessPoint(\"s3-accesspoint_xxx-debug\", account_id=\"112233445566\", bucket=\"xxx-debug\", name=\"xxx-debug\", public_access_block_configuration=aws.s3.AccessPointPublicAccessBlockConfigurationArgs( block_public_acls=False, block_public_policy=False, ignore_public_acls=False, restrict_public_buckets=False, ), opts=pulumi.ResourceOptions(protect=True)) 能看到同样的生成出了两个资源的 stack 状态，以及对应的代码。 6.2 通过代码导入资源通过代码导入资源，需要你手工为每个资源编写代码，并且确保代码的所有参数与资源本身的状态完全一致。 因此可以看到这种导入方式很不灵活，通常不推荐使用，pulumi import 自动生成代码它不香么 emmmm 大概的流程如下，首先编写一个资源的配置代码，并将其标注为 import: p_test_sre = aws.s3.Bucket(\"p-test-sre\", bucket=\"test-sre\", tags={ \"Name\": \"test-sre\", \"Team\": \"xxx\", # 这里我故意写错了，pulumi 会检测到这里有问题，提示导入将失败 }, opts=pulumi.ResourceOptio","date":"2021-01-08","objectID":"/posts/expirence-of-pulumi/:4:6","series":null,"tags":["基础设施即代码","云原生","Pulumi","Terraform"],"title":"Pulumi 使用体验 - 基础设施代码化","uri":"/posts/expirence-of-pulumi/#61-通过-pulumi-import-命令导入资源"},{"categories":["tech"],"content":" 6. 如何导入已经存在的资源？如果你司不是一开始就使用了 pulumi 这类工具，那通常绝大部分云上资源都是手动管理、或者由其他工具自动化管理的，该如何将它们纳入 pulumi 管辖呢？ 官方有提供一篇相关文档 Importing Infrastructure. 文档有提到两种资源导入的方法，导入成功后都会自动生成资源的状态，以及对应的 pulumi 代码。 第一种是使用 pulumi import 命令，第二种是在代码中使用 import 参数。 除此之外，社区还有几个其他资源导入工具（reverse IaC）值得研究： former2: 为已有的 AWS 资源生成 terraform/pulumi/cloudformation 等配置，但是不支持生成 tfstate 状态 terraformer: 为已有的 AWS/GCP/Azure/Alicloud/DigitalOcean 等多种云资源生成 terraform 配置以及 tfstate 状态 terracognita: 功能跟 terraformer 一样，都支持生成 terraform 配置以及 tfstate 状态，但是它支持 AWS/GCP/Azure 三朵云 pulumi-terraform: 这个 provider 使你可以在 pulumi 项目里使用 tfstate 状态文件 tf2pulumi: 将 terraform 配置转换为 pulumi typescript 配置 6.1 通过 pulumi import 命令导入资源使用 pulumi import 命令导入资源的好处是，不需要为每个资源手写代码，此命令会自动生成资源的 stack state 与配置代码。 使用此命令导入的资源，默认会启用删除保护，你可通过参数 --protect=false 来关闭删除保护。 资源名称可通过命令行参数，或者 Json 文件来指定。 下面我们演示一个导入一个 s3 bucket 的流程： # 导入一个名为 test-sre 的 s3 bucket，资源 ID 为 p-test-sre $ pulumi import aws:s3/bucket:Bucket p-test-sre test-sre ...... Do you want to perform this import? yes Importing (dev): Type Name Status + pulumi:pulumi:Stack pulumi-test-dev created = └─ aws:s3:Bucket p-test-sre imported Resources: + 1 created = 1 imported 2 changes Duration: 8s Please copy the following code into your Pulumi application. Not doing so will cause Pulumi to report that an update will happen on the next update command. Please note that the imported resources are marked as protected. To destroy them you will need to remove the `protect` option and run `pulumi update` *before* the destroy will take effect. import pulumi import pulumi_aws as aws p_test_sre = aws.s3.Bucket(\"p-test-sre\", arn=\"arn:aws:s3:::test-sre\", bucket=\"test-sre\", hosted_zone_id=\"ZZBBCC332211KK\", request_payer=\"BucketOwner\", tags={ \"Name\": \"test-sre\", \"Team\": \"Platform\", }, opts=pulumi.ResourceOptions(protect=True)) 能看到它会自动导入对应资源的 state，并同时打印出对应的 python 代码，要求我们手动将代码复制粘贴到项目中。 而且代码会自带 arn/hosted_zone_id/protect 等属性，说明这个资源实际上是无法像普通 pulumi 资源一样，通过 pulumi up/pulumi destroy 自动创建销毁的。要通过 pulumi 删除该资源，需要首先解除删除保护，然后将对应的代码片段删除掉，最后执行 pulumi up。 也可通过 json 来批量导入资源，首先编写一个 json 资源清单： { \"resources\": [{ \"type\": \"aws:s3/bucket:Bucket\", \"name\": \"s3-bucket_xxx-debug\", \"id\": \"xxx-debug\" }, { \"type\": \"aws:s3/accessPoint:AccessPoint\", \"name\": \"s3-accesspoint_xxx-debug\", \"id\": \"112233445566:xxx-debug\" } ] } 然后执行如下命令批量导入资源： $ pulumi import -f test-resources.json ...... Do you want to perform this import? yes Importing (dev): Type Name Status pulumi:pulumi:Stack pulumi-test-dev = ├─ aws:s3:AccessPoint s3-accesspoint_xxx-debug imported = └─ aws:s3:Bucket s3-bucket_xxx-debug imported Resources: = 2 imported 2 unchanged Duration: 8s Please copy the following code into your Pulumi application. Not doing so will cause Pulumi to report that an update will happen on the next update command. Please note that the imported resources are marked as protected. To destroy them you will need to remove the `protect` option and run `pulumi update` *before* the destroy will take effect. import pulumi import pulumi_aws as aws s3_bucket_snappea_dl_debug = aws.s3.Bucket(\"s3-bucket_xxx-debug\", arn=\"arn:aws:s3:::xxx-debug\", bucket=\"xxx-debug\", hosted_zone_id=\"ZZBBCC332211KK\", request_payer=\"BucketOwner\", tags={ \"Name\": \"xxx-debug\", \"Team\": \"Xxx\", }, opts=pulumi.ResourceOptions(protect=True)) s3_accesspoint_snappea_dl_debug = aws.s3.AccessPoint(\"s3-accesspoint_xxx-debug\", account_id=\"112233445566\", bucket=\"xxx-debug\", name=\"xxx-debug\", public_access_block_configuration=aws.s3.AccessPointPublicAccessBlockConfigurationArgs( block_public_acls=False, block_public_policy=False, ignore_public_acls=False, restrict_public_buckets=False, ), opts=pulumi.ResourceOptions(protect=True)) 能看到同样的生成出了两个资源的 stack 状态，以及对应的代码。 6.2 通过代码导入资源通过代码导入资源，需要你手工为每个资源编写代码，并且确保代码的所有参数与资源本身的状态完全一致。 因此可以看到这种导入方式很不灵活，通常不推荐使用，pulumi import 自动生成代码它不香么 emmmm 大概的流程如下，首先编写一个资源的配置代码，并将其标注为 import: p_test_sre = aws.s3.Bucket(\"p-test-sre\", bucket=\"test-sre\", tags={ \"Name\": \"test-sre\", \"Team\": \"xxx\", # 这里我故意写错了，pulumi 会检测到这里有问题，提示导入将失败 }, opts=pulumi.ResourceOptio","date":"2021-01-08","objectID":"/posts/expirence-of-pulumi/:4:6","series":null,"tags":["基础设施即代码","云原生","Pulumi","Terraform"],"title":"Pulumi 使用体验 - 基础设施代码化","uri":"/posts/expirence-of-pulumi/#62-通过代码导入资源"},{"categories":["tech"],"content":" 6.3 如何从 pulumi 中移除被导入的资源格式如下： pulumi state delete \u003cresource URN\u003e [flags] 比如要删除先前导入的 arn:aws:s3:::test-sre，首先删除对应的代码，然后执行 pulumi preview，就会报错并打印出对应的资源 urn: $ pulumi preview ... Diagnostics: aws:s3:Bucket (p-test-sre): error: Preview failed: unable to delete resource \"urn:pulumi:dev::pulumi-test::aws:s3/bucket:Bucket::p-test-sre\" as it is currently marked for protection. To unprotect the resource, either remove the `protect` flag from the resource in your Pulumi program and run `pulumi up` or use the command: `pulumi state unprotect 'urn:pulumi:dev::pulumi-test::aws:s3/bucket:Bucket::p-test-sre'` 接下来使用如下命令强制从 state 文件中移除此资源（仅修改配置，对实际资源无任何影响）： pulumi state delete urn:pulumi:dev::pulumi-test::aws:s3/bucket:Bucket::p-test-sre --force ","date":"2021-01-08","objectID":"/posts/expirence-of-pulumi/:4:7","series":null,"tags":["基础设施即代码","云原生","Pulumi","Terraform"],"title":"Pulumi 使用体验 - 基础设施代码化","uri":"/posts/expirence-of-pulumi/#63-如何从-pulumi-中移除被导入的资源"},{"categories":["tech"],"content":" 5. pulumi-kubernetes？pulumi-kubernetes 是一条龙服务： 在 yaml 配置生成这一步，它能结合/替代掉 helm/kustomize，或者你高度自定义的 Python 脚本。 在 yaml 部署这一步，它能替代掉 argo-cd 这类 gitops 工具。 强大的状态管理，argo-cd 也有状态管理，可以对比看看。 也可以仅通过 kubernetes_pulumi 生成 yaml，再通过 argo-cd 部署，这样 pulumi_kubernetes 就仅用来简化 yaml 的编写，仍然通过 gitops 工具/kubectl 来部署。 使用 pulumi-kubernetes 写配置，要警惕逻辑和数据的混合程度。 因为 kubernetes 的配置复杂度比较高，如果动态配置比较多，很容易就会写出难以维护的 python 代码来。 渲染 yaml 的示例： from pulumi import get_stack, ResourceOptions, StackReference from pulumi_kubernetes import Provider from pulumi_kubernetes.apps.v1 import Deployment, DeploymentSpecArgs from pulumi_kubernetes.core.v1 import ( ContainerArgs, ContainerPortArgs, EnvVarArgs, PodSpecArgs, PodTemplateSpecArgs, ResourceRequirementsArgs, Service, ServicePortArgs, ServiceSpecArgs, ) from pulumi_kubernetes.meta.v1 import LabelSelectorArgs, ObjectMetaArgs provider = Provider( \"render-yaml\", render_yaml_to_directory=\"rendered\", ) deployment = Deployment( \"redis\", spec=DeploymentSpecArgs(...), opts=ResourceOptions(provider=provider), ) 如示例所示，pulumi-kubernetes 的配置是完全结构化的，比 yaml/helm/kustomize 要灵活非常多。 总之它非常灵活，既可以和 helm/kustomize 结合使用，替代掉 argocd/kubectl。 也可以和 argocd/kubectl 使用，替代掉 helm/kustomize。 具体怎么使用好？我也还在研究。 ","date":"2021-01-08","objectID":"/posts/expirence-of-pulumi/:4:8","series":null,"tags":["基础设施即代码","云原生","Pulumi","Terraform"],"title":"Pulumi 使用体验 - 基础设施代码化","uri":"/posts/expirence-of-pulumi/#5-pulumi-kubernetes"},{"categories":["tech"],"content":" 6. 阿里云资源 replace 报错？阿里云有部分资源，只能创建删除，不允许修改，比如「资源组」。 对这类资源做变更时，pulumi 会直接报错：「Resources aleardy exists」， 这类资源，通常都有一个「force」参数，指示是否强制修改——即先删除再重建。 ","date":"2021-01-08","objectID":"/posts/expirence-of-pulumi/:4:9","series":null,"tags":["基础设施即代码","云原生","Pulumi","Terraform"],"title":"Pulumi 使用体验 - 基础设施代码化","uri":"/posts/expirence-of-pulumi/#6-阿里云资源-replace-报错"},{"categories":["tech"],"content":" 7. 有些资源属性无法使用 pulumi 配置？这得看各云服务提供商的支持情况。 比如阿里云很多资源的属性，pulumi 都无法完全配置，因为 alicloud provider 的功能还不够全面。 目前我们生产环境，大概 95%+ 的东西，都可以使用 pulumi 实现自动化配置。 而其他 OSS 的高级参数、新出的 ASM 服务网格、kubernetes 的授权管理、ElasticSearch7 等资源，还是需要手动配置。 这个没办法，只能等阿里云提供支持。 ","date":"2021-01-08","objectID":"/posts/expirence-of-pulumi/:4:10","series":null,"tags":["基础设施即代码","云原生","Pulumi","Terraform"],"title":"Pulumi 使用体验 - 基础设施代码化","uri":"/posts/expirence-of-pulumi/#7-有些资源属性无法使用-pulumi-配置"},{"categories":["tech"],"content":" 8. CI/CD 中如何使 pulumi 将状态保存到文件？CI/CD 中我们可能会希望 pulumi 将状态保存到本地，避免连接 pulumi 中心服务器。 这一方面能加快速度，另一方面一些临时状态我们可能根本不想存储，可以直接丢弃。 方法： # 指定状态文件路径 pulumi login file://\u003cfile-path\u003e # 保存到默认位置: ~/.pulumi/credentials.json pulumi login --local # 保存到远程 S3 存储（minio/ceph 或者各类云对象存储服务，都兼容 aws 的 s3 协议） pulumi login s3://\u003cbucket-path\u003e 登录完成后，再进行 pulumi up 操作，数据就会直接保存到你设定的路径下。 ","date":"2021-01-08","objectID":"/posts/expirence-of-pulumi/:4:11","series":null,"tags":["基础设施即代码","云原生","Pulumi","Terraform"],"title":"Pulumi 使用体验 - 基础设施代码化","uri":"/posts/expirence-of-pulumi/#8-cicd-中如何使-pulumi-将状态保存到文件"},{"categories":["tech"],"content":" 9. 如何估算资源变更导致的成本变化？目前 pulumi 貌似没有类似的工具，但是 terraform 有一个 infracost 可以干这个活，值得关注。 ","date":"2021-01-08","objectID":"/posts/expirence-of-pulumi/:4:12","series":null,"tags":["基础设施即代码","云原生","Pulumi","Terraform"],"title":"Pulumi 使用体验 - 基础设施代码化","uri":"/posts/expirence-of-pulumi/#9-如何估算资源变更导致的成本变化"},{"categories":["tech"],"content":" 缺点","date":"2021-01-08","objectID":"/posts/expirence-of-pulumi/:5:0","series":null,"tags":["基础设施即代码","云原生","Pulumi","Terraform"],"title":"Pulumi 使用体验 - 基础设施代码化","uri":"/posts/expirence-of-pulumi/#缺点"},{"categories":["tech"],"content":" 1. 报错信息不直观pulumi 和 terraform 都有一个缺点，就是封装层次太高了。 封装的层次很高，优点是方便了我们使用，可以使用很统一很简洁的声明式语法编写配置。 而缺点，则是出了 bug，报错信息往往不够直观，导致问题不好排查。 ","date":"2021-01-08","objectID":"/posts/expirence-of-pulumi/:5:1","series":null,"tags":["基础设施即代码","云原生","Pulumi","Terraform"],"title":"Pulumi 使用体验 - 基础设施代码化","uri":"/posts/expirence-of-pulumi/#1-报错信息不直观"},{"categories":["tech"],"content":" 2. 资源状态被破坏时，修复起来非常麻烦在很多情况下，都可能发生资源状态被破坏的问题： 在创建资源 A，因为参数是已知的，你直接使用了常量而不是 Output。这会导致 pulumi 无法识别到依赖关系！从而创建失败，或者删除时资源状态被破坏！ 有一个 pulumi stack 一次在三台物理机上创建资源。你白天创建资源晚上删除资源，但是某一台物理机晚上会关机。这将导致 pulumi 无法查询到这台物理机上的资源状态，这个 pulumi stack 在晚上就无法使用，它会一直报错！ ","date":"2021-01-08","objectID":"/posts/expirence-of-pulumi/:5:2","series":null,"tags":["基础设施即代码","云原生","Pulumi","Terraform"],"title":"Pulumi 使用体验 - 基础设施代码化","uri":"/posts/expirence-of-pulumi/#2-资源状态被破坏时修复起来非常麻烦"},{"categories":["tech"],"content":" 常用 Provider pulumi-alicloud: 管理阿里云资源 pulumi-vault: 我这边用它来快速初始化 vault，创建与管理 vault 的所有配置。 ","date":"2021-01-08","objectID":"/posts/expirence-of-pulumi/:6:0","series":null,"tags":["基础设施即代码","云原生","Pulumi","Terraform"],"title":"Pulumi 使用体验 - 基础设施代码化","uri":"/posts/expirence-of-pulumi/#常用-provider"},{"categories":["tech"],"content":" 我创建维护的 Provider由于 Pulumi 生态还比较小，有些 provider 只有 terraform 才有。 我为了造(方)福(便)大(自)众(己)，创建并维护了两个本地虚拟机相关的 Providers: ryan4yin/pulumi-proxmox: 目前只用来自动创建 PVE 虚拟机 可以考虑结合 kubespray/kubeadm 快速创建 k8s 集群 ryan4yin/pulumi-libvirt: 快速创建 kvm 虚拟机 可以考虑结合 kubespray/kubeadm 快速创建 k8s 集群 ","date":"2021-01-08","objectID":"/posts/expirence-of-pulumi/:7:0","series":null,"tags":["基础设施即代码","云原生","Pulumi","Terraform"],"title":"Pulumi 使用体验 - 基础设施代码化","uri":"/posts/expirence-of-pulumi/#我创建维护的-provider"},{"categories":["tech"],"content":"openSUSE 是一个基于 RPM 的发行版，这和 RHEL/CentOS 一致。 但是它的官方包管理器是专有的 zypper，挺好用的，软件也很新。 我最近从 Manjaro 切换到了 openSUSE，发现 KDE 桌面确实比 Manjaro 更丝滑，而且社区源 OBS 体验下来比 AUR 更舒服。 尤其是容器/Kubernetes 方面，源里面的东西比 AUR 更丰富，而且是官方维护的。 本文算是对迁移流程做的一个总结。 本文以 openSUSE Tumbleweed 为基础编写，这是一个和 Manjaro/Arch 一样的滚动发行版，软件源都很新。 openSUSE 社区的大部分用户都是使用的 Tumbleweed. 它的硬件兼容性也要比 openSUSE Leap（稳定版）好——实测小米游戏本安装 Leap，休眠后 Touchpad 会失灵。 ","date":"2021-01-04","objectID":"/posts/opensuse-instruction/:0:0","series":null,"tags":["openSUSE","Linux"],"title":"openSUSE 使用指南","uri":"/posts/opensuse-instruction/#"},{"categories":["tech"],"content":" 一、zypper 的基础命令zypper 的源在国内比较慢，但实际上下载的时候，zypper 会智能选择最快的镜像源下载软件包，比如国内的清华源等。 但是我发现官方的源索引更新太慢，甚至经常失败。因此没办法，还是得手动设置镜像源： # 禁用原有的官方软件源 sudo zypper mr --disable repo-oss repo-non-oss repo-update repo-update-non-oss repo-debug # 添加北外镜像源 sudo zypper ar -fcg https://mirrors.bfsu.edu.cn/opensuse/tumbleweed/repo/oss/ bfsu-oss sudo zypper ar -fcg https://mirrors.bfsu.edu.cn/opensuse/tumbleweed/repo/non-oss/ bfsu-non-oss 然后就是 zypper 的常用命令： sudo zypper refresh # refresh all repos sudo zypper update # update all softwares sudo zypper search --installed-only \u003cpackage-name\u003e # 查找本地安装的程序 sudo zypper search \u003cpackage-name\u003e # 查找本地和软件源中的程序 sudo zypper install \u003cpackage-name\u003e # 安装程序 sudo zypper remove --clean-deps \u003cpackage-name\u003e # 卸载程序，注意添加 --clean-deps 或者 -u，否则不会卸载依赖项！ sudo zypper clean # 清理本地的包缓存 ","date":"2021-01-04","objectID":"/posts/opensuse-instruction/:1:0","series":null,"tags":["openSUSE","Linux"],"title":"openSUSE 使用指南","uri":"/posts/opensuse-instruction/#一zypper-的基础命令"},{"categories":["tech"],"content":" Install Softwares 这里需要用到 OBS(Open Build Service, 类似 arch 的 AUR，但是是预编译的包)，因为 OBS 东西太多了，因此不存在完整的国内镜像，平均速度大概 300kb/s。 建议有条件可以在路由器上加智能代理提速。 安装需要用到的各类软件: # 启用 Packman 仓库，使用北交镜像源 sudo zypper ar -cfp 90 'https://mirror.bjtu.edu.cn/packman/suse/openSUSE_Tumbleweed/' packman-bjtu # install video player and web browser sudo zypper install mpv ffmpeg-4 chromium firefox # install screenshot and other utils # 安装好后可以配个截图快捷键 alt+a =\u003e `flameshot gui` sudo zypper install flameshot peek nomacs # install git clang/make/cmake sudo zypper install git gcc clang make cmake # install wireshark sudo zypper install wireshark sudo gpasswd --add $USER wireshark # 将你添加到 wireshark 用户组中 ","date":"2021-01-04","objectID":"/posts/opensuse-instruction/:2:0","series":null,"tags":["openSUSE","Linux"],"title":"openSUSE 使用指南","uri":"/posts/opensuse-instruction/#install-softwares"},{"categories":["tech"],"content":" IDE + 编程语言 # install vscode: https://en.openSUSE.org/Visual_Studio_Code sudo rpm --import https://packages.microsoft.com/keys/microsoft.asc sudo zypper addrepo https://packages.microsoft.com/yumrepos/vscode vscode sudo zypper refresh sudo zypper install code # 安装 dotnet 5: https://docs.microsoft.com/en-us/dotnet/core/install/linux-openSUSE#openSUSE-15- sudo rpm --import https://packages.microsoft.com/keys/microsoft.asc sudo zypper addrepo https://packages.microsoft.com/openSUSE/15/prod/ microsoft-prod sudo zypper refresh sudo zypper install dotnet-sdk-5.0 # 安装新版本的 go（源中的版本比较低，更建议从 go 官网下载安装） sudo zypper install go 通过 tarball/script 安装： # rustup，rust 环境管理器 curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh # jetbrains toolbox app，用于安装和管理 pycharm/idea/goland/android studio 等 IDE # 参见：https://www.jetbrains.com/toolbox-app/ # 不使用系统 python，改用 miniconda 装 python3.8 # 参考：https://github.com/ContinuumIO/docker-images/blob/master/miniconda3/debian/Dockerfile wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh -O /tmp/miniconda.sh sudo /bin/bash /tmp/miniconda.sh -b -p /opt/conda rm /tmp/miniconda.sh sudo /opt/conda/bin/conda clean -tipsy sudo ln -s /opt/conda/etc/profile.d/conda.sh /etc/profile.d/conda.sh echo \". /opt/conda/etc/profile.d/conda.sh\" \u003e\u003e ~/.bashrc echo \"conda activate base\" \u003e\u003e ~/.bashrc # miniconda 的 entrypoint 默认安装在如下目录，添加到 PATH 中 echo \"export PATH=\\$PATH:\\$HOME/.local/bin\" \u003e\u003e ~/.bashrc 接下来安装 VSCode 插件，下列是我的插件列表： 语言： python/go/rust/c#/julia/flutter xml/yaml/toml vscode proto3 ansible/terraform markdown all in one + Markdown Preview Enhanced 美化： community material theme vscode icons glasslt-vsc docker/kubernetes IntelliJ IDEA Keybindings gitlens prettier utils comment translate path intellisense svg visual studio intellicode antlr4 remote ssh + remote containers rest client vscode databases ","date":"2021-01-04","objectID":"/posts/opensuse-instruction/:2:1","series":null,"tags":["openSUSE","Linux"],"title":"openSUSE 使用指南","uri":"/posts/opensuse-instruction/#ide--编程语言"},{"categories":["tech"],"content":" 容器 + Kubernetes # 时髦的新容器套装: https://documentation.suse.com/sles/15-SP2/html/SLES-all/cha-podman-overview.html sudo zypper in podman kompose skopeo buildah katacontainers # 安装 kubernetes 相关工具，tumbleweed 官方仓库的包都非常新！很舒服 sudo zypper in helm k9s kubernetes-client # 本地测试目前还是 docker-compose 最方便，docker 仍有必要安装 sudo zypper in docker sudo gpasswd --add $USER docker sudo systemctl enable docker sudo systemctl start docker # 简单起见，直接用 pip 安装 docker-compose 和 podman-compose sudo pip install docker-compose podman-compose ","date":"2021-01-04","objectID":"/posts/opensuse-instruction/:2:2","series":null,"tags":["openSUSE","Linux"],"title":"openSUSE 使用指南","uri":"/posts/opensuse-instruction/#容器--kubernetes"},{"categories":["tech"],"content":" 办公、音乐、聊天 # 添加 openSUSE_zh 源：https://build.opensuse.org/project/show/home:opensuse_zh sudo zypper addrepo 'https://download.opensuse.org/repositories/home:/opensuse_zh/openSUSE_Tumbleweed' openSUSE_zh sudo zypper refresh sudo zypper install wps-office netease-cloud-music # linux qq: https://im.qq.com/linuxqq/download.html # 虽然简陋但也够用，发送文件比 KDE Connect 要方便一些。 sudo rpm -ivh linux_qq.rpm ","date":"2021-01-04","objectID":"/posts/opensuse-instruction/:2:3","series":null,"tags":["openSUSE","Linux"],"title":"openSUSE 使用指南","uri":"/posts/opensuse-instruction/#办公音乐聊天"},{"categories":["tech"],"content":" 安装输入法我用的输入法是小鹤音形，首先安装 fcitx-rime: # 添加 m17n obs 源：https://build.openSUSE.org/repositories/M17N sudo zypper addrepo 'https://download.opensuse.org/repositories/M17N/openSUSE_Tumbleweed' m17n sudo zypper refresh sudo zypper install fcitx5 fcitx5-configtool fcitx5-qt5 fcitx5-rime 然后，从 http://flypy.ys168.com/ 下载最新的鼠须管（MacOS）配置文件，将解压得到的 rime 文件夹拷贝到 ~/.local/share/fcitx5/ 下： mv rime ~/.local/share/fcitx5/ 现在重启系统，在 fcitx5 配置里面添加 rime「中州韵」，就可以正常使用小鹤音形了。 ","date":"2021-01-04","objectID":"/posts/opensuse-instruction/:2:4","series":null,"tags":["openSUSE","Linux"],"title":"openSUSE 使用指南","uri":"/posts/opensuse-instruction/#安装输入法"},{"categories":["tech"],"content":" QEMU/KVM不得不说，openSUSE 安装 KVM 真的超方便，纯 GUI 操作： # see: https://doc.openSUSE.org/documentation/leap/virtualization/html/book-virt/cha-vt-installation.html sudo yast2 virtualization # enter to terminal ui, select kvm + kvm tools, and then install it. KVM 的详细文档参见 KVM/README.md ","date":"2021-01-04","objectID":"/posts/opensuse-instruction/:2:5","series":null,"tags":["openSUSE","Linux"],"title":"openSUSE 使用指南","uri":"/posts/opensuse-instruction/#qemukvm"},{"categories":["tech"],"content":" KDE ConnectKDE Connect 是一个 PC 手机协同工具，可以在电脑和手机之间共享剪切版、远程输入、发送文件、共享文件夹、通知同步等等。 总而言之非常好用，只要手机和 PC 处于同一个局域网就行，不需要什么数据线。 如果安装系统时选择了打开防火墙，KDE Connect 是连不上的，需要手动开放端口号： # see: https://userbase.kde.org/KDEConnect#firewalld # 还可以使用 --add-source=xx.xx.xx.xx/xx 设置 ip 白名单 sudo firewall-cmd --zone=public --permanent --add-port=1714-1764/tcp sudo firewall-cmd --zone=public --permanent --add-port=1714-1764/udp sudo systemctl restart firewalld.service 然后手机（Android）安装好 KDE Connect，就能开始享受了。 目前存在的 Bug: Android 10 禁止了后台应用读取剪切版，这导致 KDE Connect 只能从 PC 同步到手机，而无法反向同步。 如果你有 ROOT 权限，可以参考 Fix clipboard permission on Android 10 的方法，安装 ClipboardWhitelist 来打开权限。 否则，貌似就只能使用手机端的「远程输入」模块来手动传输文本了。 ","date":"2021-01-04","objectID":"/posts/opensuse-instruction/:2:6","series":null,"tags":["openSUSE","Linux"],"title":"openSUSE 使用指南","uri":"/posts/opensuse-instruction/#kde-connect"},{"categories":["tech"],"content":" Qv2ray 代理Qv2ray 是我用过的比较好用的 GUI 代理工具，通过插件可支持常见的所有代理协议。 # see: https://build.openSUSE.org/repositories/home:zzndb sudo zypper addrepo 'https://download.opensuse.org/repositories/home:/zzndb/openSUSE_Tumbleweed' qv2ray sudo zypper refresh sudo zypper install Qv2ray QvPlugin-Trojan QvPlugin-SS ","date":"2021-01-04","objectID":"/posts/opensuse-instruction/:2:7","series":null,"tags":["openSUSE","Linux"],"title":"openSUSE 使用指南","uri":"/posts/opensuse-instruction/#qv2ray-代理"},{"categories":["tech"],"content":" VPN 连接与防火墙防火墙默认会禁用 pptp 等 vpn 协议的端口，需要手动打开. 允许使用 PPTP 协议： # 允许 gre 数据包流入网络 sudo firewall-cmd --permanent --zone=public --direct --add-rule ipv4 filter INPUT 0 -p gre -j ACCEPT sudo firewall-cmd --permanent --zone=public --direct --add-rule ipv6 filter INPUT 0 -p gre -j ACCEPT # masquerade: 自动使用 interface 地址伪装所有流量（将主机当作路由器使用，vpn 是虚拟网络，需要这个功能） sudo firewall-cmd --permanent --zone=public --add-masquerade # pptp 客户端使用固定端口 1723/tcp 通信 firewall-cmd --add-port=1723/tcp --permanent sudo firewall-cmd --reload 允许使用 wireguard 协议，此协议只使用 tcp 协议，而且可以端口号可以自定义。不过 wireguard 自身的配置文件 /etc/wireguard/xxx.conf 就能配置 iptables 参数放行相关端口，这里就不赘述了。 ","date":"2021-01-04","objectID":"/posts/opensuse-instruction/:2:8","series":null,"tags":["openSUSE","Linux"],"title":"openSUSE 使用指南","uri":"/posts/opensuse-instruction/#vpn-连接与防火墙"},{"categories":["tech"],"content":" 其他设置从 Windows 带过来的习惯是单击选中文件，双击才打开，这个可以在「系统设置」-「工作空间行为」-「常规行为」-「点击行为」中修改。 ","date":"2021-01-04","objectID":"/posts/opensuse-instruction/:3:0","series":null,"tags":["openSUSE","Linux"],"title":"openSUSE 使用指南","uri":"/posts/opensuse-instruction/#其他设置"},{"categories":["life","tech"],"content":" 闲言碎语一晃一年又过去了，今年可真是魔幻的一年，口罩带了一年没能摘下来，美国疫情感染人数 1500 万。 上面这段话要是让去年的我看到了，没准都以为今年真的生化危机了hhh… 言归正传，从去年 6 月底入职，到现在有一年半了，这一年半学到的东西真的非常多，完全重塑了我的技术栈。现在我的整个技术栈，基本都是围绕着云原生这一块发展了。 ","date":"2020-12-12","objectID":"/posts/2020-summary/:1:0","series":null,"tags":["总结"],"title":"2020 年年终总结","uri":"/posts/2020-summary/#闲言碎语"},{"categories":["life","tech"],"content":" 活动今年也参加了几个技术沙龙，有些收获，但是没去年那么新奇了，主要是很多东西自己已经懂了hhh。大概有这么几个活动： 2019 年腾讯蓝鲸第5届运维技术沙龙：在深圳腾讯大厦参加的，点心和咖啡很棒，讲的东西里，腾讯自己分享的「研发运维一体化平台」比较有收获，我收藏了那一份 PPT Rancher - 企业云原生的探索与落地：去年参加 Rancher 的沙龙觉得很高大上，因为自己很多东西都不懂。但是今年来听，明显就感觉他们讲的很基础，对我没什么价值了。也侧面说明我确实进步了非常多哈哈。 2020 PyconChina 深圳场：额，也觉得没什么干货，好几个都是在推销自家的产品（Azure AI 平台和一个 Django 写的低代码平台），有个讲 Nix 包管理的大佬但是没讲好，后面我们就直接溜了… 另外就是，今年心血来潮买了四张 Live House 的演出票，体验下来觉得钱花得很值，给我充值了不少正能量。 景德镇文艺复兴《小歌行》：这是我超级喜欢的一个乐队，演出效果超棒！听到了完整的故事，而且见到了九三姑娘本人，太高兴了！ 徐海俏 - 游离片刻：这位歌手我之前其实没接触过，但是听了下她的《空》发现很不错很帅气，就买了。但是整场下来感觉俏俏状态不佳，有点唱不动的感觉。中场问歌迷们有没有带野格酒，末了又问深圳现在能游泳么哈哈，是个很随性的歌手。后面可能还真游泳去了。 夏小虎 - 逝年：这是个民谣歌手，以前上大学的时候听过，只有吉他和人声，其实是有些伤感的歌。因为我最近状态很好，我去之前还担心氛围不适合我。然后夏小虎说开心最重要，带了个乐队来伴奏，架子鼓就是灵魂，整个演出都因鼓点而欢快了起来。效果也非常棒！ 时光胶囊乐队：这也是一个国风后摇乐队，在一个深圳福田一个小酒吧「红糖罐上步店」演出的，比较简陋，人也不多，就四五十人的样子（出乎意料）。但是演出效果很棒，《旅途》《忆长安》《磐石》都非常好听。尤其是在这样的场合唱《我不知道你的名字》，挺有感触的。 ","date":"2020-12-12","objectID":"/posts/2020-summary/:2:0","series":null,"tags":["总结"],"title":"2020 年年终总结","uri":"/posts/2020-summary/#活动"},{"categories":["life","tech"],"content":" 技术能力总结今年我的工作重点有这么几个： 重构及维护 CI/CD 代码，让它能在多个产品线上复用 云上环境管理：今年熟悉了阿里云这一套东西，并且用上了自动化工具对云上环境进行管理。 一开始是使用 terraform，但是 terraform 的 hcl 语法不够灵活，最近切换到了 pulumi+python，不得不说真香。目前云上的资源及配置 95% 都完全用 pulumi 管理了（还剩大概 5% 因为各种原因，需要手动配置）。 kubernetes云原生: 今年我在这个领域的进步最大，熟悉了 k8s/istio/flagger/vault/prometheus/helm/traefik 等等。不过目前这里面大部分工具还停留在「会用」的状态。 服务器虚拟化系统从 vSphere 切换到 PVE VMware 的 vCenter 吃的资源太多，而且还不能自动扩缩容硬盘，Python SDK 也超难用。因此我在公司尝试使用 PVE 替换 vSphere 这一套，效果很不错。 PVE+pulumi/terraform+cloudinit 能实现自动化部署虚拟机，自动配置网络、账号及SSH密钥、自动扩缩容硬盘，非常方便！ 而且 PVE 不收费，去中心化，一套用下来舒服太多了。只是 pve+cloud-init 门槛稍微高一点，需要一定时间去熟悉。 CI/CD 系统：基于 Jenkins 的 CI/CD 在我司各种水土不服，小毛病不断。Jenkins 本身就存在单点故障，不适合云原生，加上 Jenkinsfile 有学习成本，而且不方便复用，我就想把 Jenkins 换掉。我在这一年里调研了大量的开源 CI/CD 工具，都各有不足。主要还是因为我们当下的 Jenkins 承载了太多的功能，已经是一个CI/CD、自动化测试、自动化运维平台了，另一方面公司后端的流水线还存在依赖关系，需要进行复杂的编排。 目前我就找到 Argo Workflows 的功能很符合我们的需求，目前正在尝试迁移一部分功能到 Argo Workflows 试用。 因为 argo 的 UI 和 jenkins 差别过大，暂定仍以 jenkins 为前端，通过 python 将任务分派给 argo 运行。这样 argo 对使用者而言是隐形的，用户体验基本上没区别。 杂事：修水电、修服务器、组装办公电脑、搬机房… ","date":"2020-12-12","objectID":"/posts/2020-summary/:3:0","series":null,"tags":["总结"],"title":"2020 年年终总结","uri":"/posts/2020-summary/#技术能力总结"},{"categories":["life","tech"],"content":" 今年在技术方面的感受 Podman/Skopeo/Buildah/Kaniko 等技术进一步发展，正在逐渐蚕食 Docker 的地盘. kubernetes 已经弃用 docker-shim，直接对接 containerd，下一步应该是彻底切换到 CRI-O。 Istio 1.5 合并为单体架构效果很明显，各微信公众号三天两头就讲服务网格，服务网格是毋庸置疑的未来 阿里云的 OAM 进一步发展，目前阿里基于 OAM 研发的 Kubevela 致力于封装 Kubernetes 的功能，让小白也能用上 Kubernetes。而这同时还能保留 k8s 完整的能力，值得期待。 云上安全越来越引起重视了，目前 CNCF 社区上容器安全相关的项目在快速发展。包括镜像安全/安全容器(kata containers)等。 使用 Kubernetes 来管理数据库已经是大趋势，毕竟成本优势太明显了。 很多公司已经在使用 docker 运行数据库，毕竟性能没啥损失，就能方便很多。但是仍然手动搭建集群，也不使用分布式存储。 目前好像只有大厂如阿里京东才有这个实力，使用 kubernetes 和分布式存储来跑数据库。容器化的分布式存储系统维护(如 ceph)，其中的难点我目前还不是很清楚，不过无外乎性能、稳定性、故障恢复这些。 ","date":"2020-12-12","objectID":"/posts/2020-summary/:4:0","series":null,"tags":["总结"],"title":"2020 年年终总结","uri":"/posts/2020-summary/#今年在技术方面的感受"},{"categories":["life","tech"],"content":" 明年的展望 Go 语言必须学起来，今年入门了两遍语法，但是没写过啥东西，又忘差不多了。 要进一步熟悉 k8s/istio/flagger/vault/prometheus/helm/traefik/caddy 这些工具，会用还不够，要深入底层。 深入学习计算机网络 + Linux 网络 + Kubernetes 网络！这非常重要。 学习 Podman/Docker 的底层原理，学习 katacontainers 等安全容器技术。 为 kubevela/dapr/knative 等云原生项目做一些贡献，要参与到开源中去。 掌握 Argo Workflows/tekton，将 CI/CD 搬到 k8s 上去。 学习设计模式 有机会的话，熟悉下分布式存储、分布式数据库。这方面我目前还相当欠缺。 学习 KVM 虚拟化 如果学（xian）有（de）余（dan）力（teng）的话，也可以考虑搞搞下面这几个： rust 语言：rust 通过 owner+lifetime 实现内存的智能管理，性能很高，而且编译器提示非常友好，值得一学。 机器学习/深度学习: 这个领域可是当下的大热门，可以用 python/julia 玩一玩，顺便补一补微积分线代概率论。。 《编程语言实现模式》：嗯，希望能自己造轮子，写些简单的 parser。 elixir: ruby 语法+ erlang 并发模型(actor), 如果有时间的话，也可以玩玩，了解下原生分布式的函数式语言的特点。 回看了下去年的总结，发现我 go/c# 都没学多少，设计模式也没动，python 还在原地踏步hhh… 去年的展望很多都没实现。 不过云原生这一块倒是进步很快，总体很满意今年的成果哈哈~ 明年加油！ ","date":"2020-12-12","objectID":"/posts/2020-summary/:5:0","series":null,"tags":["总结"],"title":"2020 年年终总结","uri":"/posts/2020-summary/#明年的展望"},{"categories":["音乐","life"],"content":" 前言2020 年 11 月 28 日，我第一次参加 Live House，演出乐队是「景德镇文艺复兴」。 「景德镇文艺复兴」是我很喜欢的一支后摇乐队，我喜欢上这支乐队，还得从我的昵称「於清樂」说起。 17 年的时候，听了许多后摇，网易云就给我推荐「景德镇文艺复兴」的歌，如此结缘。 其中有一首歌是「满世」，后摇嘛，歌词只有四句： 月下灵鸟吟 花香无处寻 再看破土人 一满又一世 听这首歌的时候，我想起看过韩寒的《长安乱》，里面女主名叫「喜乐」，这名字里寄托着家人对她的期许——平安喜乐。 我心里也升起一个不可能实现的愿望： 希冀能于这尘世之中，享得半世清闲，一生喜乐 这愿望已然不可能实现，过往的岁月里，我有过太多苦恼，做过了太多错事；可遇见的未来，也还没到清闲享乐的时候。 那至少把昵称改成「於清樂」，提醒着自己，你有过这样一个愿望。 ","date":"2020-11-28","objectID":"/posts/jingdezhen-renaissance-band-2020-shenzhen/:1:0","series":null,"tags":["景德镇文艺复兴","后摇"],"title":"「小歌行」-景德镇文艺复兴-2020巡演-深圳","uri":"/posts/jingdezhen-renaissance-band-2020-shenzhen/#前言"},{"categories":["音乐","life"],"content":" 演出演出的内容是《小歌行》这张专辑，乐队通过一个自创的神话故事，将整张专辑串成了一个类似音乐剧的形式，进行演出，效果很棒！ 我用手机录下了几乎全程，因为第一次参加 Live House，又是自己这么喜欢的乐队，想要录下来，留做纪念。 视频已经上传到了 Bilibili: 录到最后手机没电了，为了留点电量刷公交车卡和门禁卡，最后一首《水码头》没有录完。（到家时真的差点刷不了门禁hhh） 好了，下面是演出的照片集锦： Live House 入口的宣传海报 老村长1 老村长2 老村长3 拉小提琴的小哥哥 九三舞蹈 小提琴伴奏 九三是朝廷命官 阿弥陀佛 唱 唱 唱 唱 唱 九三最漂亮的一瞬间 九三超帅气 结束： Live House 后台 结束鞠躬 大合照 签售： 签售 ","date":"2020-11-28","objectID":"/posts/jingdezhen-renaissance-band-2020-shenzhen/:2:0","series":null,"tags":["景德镇文艺复兴","后摇"],"title":"「小歌行」-景德镇文艺复兴-2020巡演-深圳","uri":"/posts/jingdezhen-renaissance-band-2020-shenzhen/#演出"},{"categories":["tech"],"content":" 个人笔记，不保证正确 ","date":"2020-05-31","objectID":"/posts/base64-encoding-is-not-unique/:0:0","series":null,"tags":["Base64","编码"],"title":"Base64 编码并不唯一","uri":"/posts/base64-encoding-is-not-unique/#"},{"categories":["tech"],"content":" 问题我以前只知道 Base64 这个编码算法很常用，自己也经常在 JWT 等场景下使用，但是从来没了解过它的原理，一直先入为主地认为它的编码应该是唯一的。 但是今天测试 JWT 时，发现修改 JWT 的最后一个字符（其实不是我发现的。。），居然有可能不影响 JWT 的正确性。比如下这个使用 HS256 算法的 JWT: eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzdWIiOiIxMjM0NTY3ODkwIiwibmFtZSI6IkpvaG4gRG9lIiwiaWF0IjoxNTE2MjM5MDIyfQ.SflKxwRJSMeKKF2QT4fwpMeJf36POk6yJV_adQssw5c 把它的最后一个字符改成 d e或者 f，都能成功通过 http://jwt.io 的验证。 这让我觉得很奇怪（难道我发现了一个 Bug？），在QQ群里一问，就有大佬找到根本原因：这是 Base64 编码的特性。并且通过 python 进行了实际演示： In [1]: import base64 # 使用 jwt 的 signature 进行验证 In [2]: base64.b64decode(\"SflKxwRJSMeKKF2QT4fwpMeJf36POk6yJV_adQssw5c==\") Out[2]: b'I\\xf9J\\xc7\\x04IH\\xc7\\x8a(]\\x90O\\x87\\xf0\\xa4\\xc7\\x89\\x7f~\\x8f:N\\xb2%V\\x9dB\\xcb0\\xe5' In [3]: base64.b64decode(\"SflKxwRJSMeKKF2QT4fwpMeJf36POk6yJV_adQssw5d==\") Out[3]: b'I\\xf9J\\xc7\\x04IH\\xc7\\x8a(]\\x90O\\x87\\xf0\\xa4\\xc7\\x89\\x7f~\\x8f:N\\xb2%V\\x9dB\\xcb0\\xe5' In [4]: base64.b64decode(\"SflKxwRJSMeKKF2QT4fwpMeJf36POk6yJV_adQssw5e==\") Out[4]: b'I\\xf9J\\xc7\\x04IH\\xc7\\x8a(]\\x90O\\x87\\xf0\\xa4\\xc7\\x89\\x7f~\\x8f:N\\xb2%V\\x9dB\\xcb0\\xe5' In [5]: base64.b64decode(\"SflKxwRJSMeKKF2QT4fwpMeJf36POk6yJV_adQssw5f==\") Out[5]: b'I\\xf9J\\xc7\\x04IH\\xc7\\x8a(]\\x90O\\x87\\xf0\\xa4\\xc7\\x89\\x7f~\\x8f:N\\xb2%V\\x9dB\\xcb0\\xe5' # 两个等于号之后的任何内容，都会被直接丢弃。这个是实现相关的，有的 base64 处理库对这种情况会报错。 In [6]: base64.b64decode(\"SflKxwRJSMeKKF2QT4fwpMeJf36POk6yJV_adQssw5f==fdf=df==dfd=fderwe=r\") Out[6]: b'I\\xf9J\\xc7\\x04IH\\xc7\\x8a(]\\x90O\\x87\\xf0\\xa4\\xc7\\x89\\x7f~\\x8f:N\\xb2%V\\x9dB\\xcb0\\xe5' 可以看到有两个现象： 将同一个 base64 串的最后一个字母分别改成 d e f，解码出来的内容没有任何变化。 在 base64 串末尾 == 后面添加了一堆随机字符，对解码出的内容也没有任何影响。 ","date":"2020-05-31","objectID":"/posts/base64-encoding-is-not-unique/:0:0","series":null,"tags":["Base64","编码"],"title":"Base64 编码并不唯一","uri":"/posts/base64-encoding-is-not-unique/#问题"},{"categories":["tech"],"content":" 原因分析base64 编码将二进制内容(bytes)从左往右每 6 bits 分为一组，每一组编码为一个可打印字符。 bas64 从 ASCII 字符集中选出了 64 个字符（=号除外）进行编码。因为 $2^6=64$，使用 64 个字符才能保证上述编码的唯一性。 但是被编码的二进制内容(bytes)的 bits 数不一定是 6 的倍数，无法被编码为 6 bits 一组。 为了解决这个问题，就需要在这些二进制内容的末尾填充上 2 或 4 个 bit 位，这样才能使用 base64 进行编码。 关于这些被填充的 bits，在 RFC4648 中定义了规范行为：全部补 0. 但是这并不是一个强制的行为，因此实际上你可以随便补，在进行 base64 解析时，被填补的 bits 会被直接忽略掉。 这就导致了上面描述的行为：修改 JWT 的最后一个字符(6 bits，其中可能包含 2 或 4 个填充比特位)可能并不影响被编码的实际内容！ RFC4684 中对这个 bits 填充的描述如下： 3.5. Canonical Encoding The padding step in base 64 and base 32 encoding can, if improperly implemented, lead to non-significant alterations of the encoded data. For example, if the input is only one octet for a base 64 encoding, then all six bits of the first symbol are used, but only the first two bits of the next symbol are used. These pad bits MUST be set to zero by conforming encoders, which is described in the descriptions on padding below. If this property do not hold, there is no canonical representation of base-encoded data, and multiple base- encoded strings can be decoded to the same binary data. If this property (and others discussed in this document) holds, a canonical encoding is guaranteed. In some environments, the alteration is critical and therefore decoders MAY chose to reject an encoding if the pad bits have not been set to zero. The specification referring to this may mandate a specific behaviour. 它讲到在某些环境下，base64 解析器可能会严格检查被填充的这几个 bits，要求它们全部为 0. 但是我测试发现，Python 标准库和 https://jwt.io 都没有做这样的限制。因此我认为绝大部分环境下，被填充的 bits 都是会被忽略的。 ","date":"2020-05-31","objectID":"/posts/base64-encoding-is-not-unique/:1:0","series":null,"tags":["Base64","编码"],"title":"Base64 编码并不唯一","uri":"/posts/base64-encoding-is-not-unique/#原因分析"},{"categories":["tech"],"content":" 问题一：为什么只需要填充 2 或 4 个 bit 位？这是看到「填充上 2 或 4 个 bit 位」时的第一想法——如果要补足到 6 的倍数，不应该是要填充 1-5 个 bit 位么？ 要解答这个问题，我们得看 base64 的定义。在 RFC4648 的 base64 定义中，有如下这样一段话： The Base 64 encoding is designed to represent arbitrary sequences of octets in a form that allows the use of both upper- and lowercase letters but that need not be human readable. 注意重点：octets—— 和 bytes 同义，表示 8 bits 一组的位序列。这表示 base64 只支持编码 bits 数为 8 的倍数的二进制内容，而 $8x \\bmod 6$ 的结果只可能是 0/2/4 三种情况。 因此只需要填充 2 或 4 个 bit 位。 这样的假设也并没有什么问题，因为现代计算机都是统一使用 8 bits(byte) 为最小的可读单位的。即使是 c 语言的「位域」也是如此。 因为 Byte(8 bits) 现代 CPU 数据读写操作的基本单位，学过汇编的对这个应该都有些印象。 你仔细想想，所有文件的最小计量单位，是不是都是 byte？ ","date":"2020-05-31","objectID":"/posts/base64-encoding-is-not-unique/:2:0","series":null,"tags":["Base64","编码"],"title":"Base64 编码并不唯一","uri":"/posts/base64-encoding-is-not-unique/#问题一为什么只需要填充-2-或-4-个-bit-位"},{"categories":["tech"],"content":" 问题二：为什么用 python 测试时可能需要在 JWT signature 的末尾添加多个 =，而 JWT 中不需要？前面已经讲过，base64 的编码步骤是是将字节(byte, 8 bits)序列，从左往右每 6 个 bits 转换成一个可打印字符。 查阅 RFC4648 第 4 小节中 baae64 的定义，能看到它实际上是每次处理 24 bits，因为这是 6 和 8 的最小公倍数，可以刚好用 4 个字符表示。 **在被处理的字节序列的比特(bits)数不是 24 的整数时，就需要在序列末尾填充 0 使末尾的 bits 数是 6 的倍数(6-bit groups)。**有可能会出现三种情况： 被处理的字节序列 S 的比特数刚好是 24 的倍数：不需要补比特位，末尾也就不需要加 = S 的比特数是 $24x+8$: 末尾需要补 4 个 bits，这样末尾剩余的 bits 才是 6-bit groups，才能编码成 base64。然后添加两个 == 使编码后的字符数为 4 的倍数。 S 的比特数为 $24x+16$：末尾需要添加 2 个 bits 才能编码成 base64。然后添加一个 = 使编码后的字符数为 4 的倍数。 其实可以看到，添加 = 的目的只是为了使编码后的字符数为 4 的倍数而已，= 这个 padding 其实是冗余信息，完全可以去掉。 在解码完成后，应用程序会自动去除掉末尾这不足 1 byte 的 2 或 4 个填充位。 因此 JWT 就去掉了它以减少传输的数据量。 可以用前面讲到的 JWT signature 进行验证： In [1]: import base64 In [2]: s = base64.b64decode(\"SflKxwRJSMeKKF2QT4fwpMeJf36POk6yJV_adQssw5c==\") # len(s) * 8 得到 bits 数 In [3]: len(s) * 8 % 24 Out[3]: 8 可以看到这里的被编码内容比特数为 $24x+8$，所以末尾需要添加两个 == 号才符合 RFC4648 的定义。 ","date":"2020-05-31","objectID":"/posts/base64-encoding-is-not-unique/:3:0","series":null,"tags":["Base64","编码"],"title":"Base64 编码并不唯一","uri":"/posts/base64-encoding-is-not-unique/#问题二为什么用-python-测试时可能需要在-jwt-signature-的末尾添加多个-而-jwt-中不需要"},{"categories":["tech"],"content":" 参考 Remove trailing “=” when base64 encoding RFC4648 - base64 定义 Difference betweeen RFC 3548 and RFC 4648 Base64隐写原理和提取脚本 ","date":"2020-05-31","objectID":"/posts/base64-encoding-is-not-unique/:4:0","series":null,"tags":["Base64","编码"],"title":"Base64 编码并不唯一","uri":"/posts/base64-encoding-is-not-unique/#参考"},{"categories":["tech"],"content":" 抓包分析抓包分析工具主要有两种： http/https 网络代理工具：mitmproxy/fiddler 都属于这一类，用于分析 http 非常方便。但是只支持 http/https，有局限性。 tcp/udp/icmp 等网络嗅探工具：tcpdump/tshark 都属于这一类，网络故障分析等场景常用。 这里主要介绍如何使用 tcpdump + wireshark 进行远程实时抓包分析。 而 mitmproxy 抓包和 wireshark 本地抓包都相当简单，就不介绍了。 P.S. tshark 是 wireshark 的命令行版本，用法 tcpdump 非常相似。 ","date":"2020-05-28","objectID":"/posts/tcpdump-and-wireshark/:0:0","series":null,"tags":["网络","Wireshark","tcpdump","抓包分析"],"title":"使用 tcpdump 和 Wireshark 进行远程实时抓包分析","uri":"/posts/tcpdump-and-wireshark/#抓包分析"},{"categories":["tech"],"content":" 一、wireshark 的基本用法WireShark 的 UI 界面如何使用，网上能搜得到各种类型的 wireshark 演示，多看几篇博客就会了。 搜索 [xxx 协议 wireshark 抓包分析] 就能找到各种各样的演示，比如 「gRPC 协议 wireshark 抓包分析」 「WebSocket 协议 wireshark 抓包分析」 「TCP 协议 wireshark 抓包分析」 等等 主要需要介绍的，应该是 wireshark 的数据包过滤器。 wireshark 中有两种包过滤器： 捕获过滤器：在抓包的时候使用它对数据包进行过滤。 显示过滤器：对抓到的所有数据包进行过滤。 显示过滤器是最有用的，下面简要介绍下显示过滤器的语法。 可以直接通过「协议名称」进行过滤： # 只看 tcp 流量 tcp # 只看 http 流量 http # 使用感叹号（或 not）进行反向过滤 !arp # 过滤掉所有 arp 数据包 也可以通过「协议名称.协议属性」和「比较操作符（比如 ==）」进行更精确的过滤： # 通过 ip 的源地址 src 或 dst 进行过滤 ip.src==192.168.1.33 # 通过 IP 地址（ip.addr）进行过滤（匹配 ip.src 或 ip.dst） ip.addr==192.168.0.5 # 上一条过滤表达式等价于： ip.src==192.168.0.5 or ip.dst==192.168.0.5 # 通过 tcp 端口号进行过滤 tcp.port==80 tcp.port\u003e4000 # 通过 http 的 host 属性进行过滤 http.host != \"xxx.baidu.com\" # 通过 http.referer 属性进行过滤 http.referer == \"xxx.baidu.com\" # 多个过滤器之间用 and、or 进行组合 http.host != \"xxx.baidu.com\" and http.referer == \"xxx.baidu.com\" ","date":"2020-05-28","objectID":"/posts/tcpdump-and-wireshark/:1:0","series":null,"tags":["网络","Wireshark","tcpdump","抓包分析"],"title":"使用 tcpdump 和 Wireshark 进行远程实时抓包分析","uri":"/posts/tcpdump-and-wireshark/#一wireshark-的基本用法"},{"categories":["tech"],"content":" 二、tcpdump + ssh + wireshark 远程实时抓包在进行远程网络抓包分析时，我们通常的做法是： 使用 tcpdump 在远程主机上抓包，保存为 pcap 文件。 将 pcap 文件拷贝到本地，使用 wireshark 对其进行分析。 但是这样做没有时效性，而且数据拷贝来去也比较麻烦。 考虑使用流的方式，在远程主机上使用 tcpdump 抓包，本地使用 wireshark 进行实时分析。 使用 ssh 协议进行流式传输的示例如下： # eth0 更换成你的机器 interface 名称，虚拟机可能是 ens33 ssh root@some.host \"tcpdump -i eth0 -l -w -\" | wireshark -k -i - 在不方便使用 ssh 协议的情况下（比如容器抓包、Android 抓包），可以考虑使用 nc(netcat) 进行数据流的转发： # 1. 远程主机抓包：将数据流通过 11111 端口暴露出去 tcpdump -i wlan0 -s0 -w - | nc -l -p 11111 # 2. 本地主机从远程主机的 11111 端口读取数据，提供给 wireshark nc \u003cremote-host\u003e 11111 | wireshark -k -S -i - 如果是抓取 Android 手机的数据，方便起见，可以通过 adb 多进行一次数据转发： # 方案一：root 手机后，将 arm 版的 tcpdump 拷贝到手机内进行抓包 # 1. 在 adb shell 里使用 tcpdump 抓 WiFi 的数据包，转发到 11111 端口 ## 需要先获取到 root 权限，将 tcpdump 拷贝到 /system/bin/ 目录下 tcpdump -i wlan0 -s0 -w - | nc -l -p 11111 # 2. 在本机使用 adb forward 将手机的 11111 端口绑定到本机(PC)的 11111 端口 adb forward tcp:11111 tcp:11111 # 3. 直接从本机(PC)的 11111 端口读取数据，提供给 wireshark nc localhost 11111 | wireshark -k -S -i - ## 通过数据转发，本机 11111 端口的数据，就是安卓手机内 tcmpdump 的 stdout 内容。 # 方案二： # 如果手机不方便 root，推荐 PC 开启 WiFi 热点，手机连上这个热点访问网络。 # 这样手机的数据就一定会走 PC，直接在 PC 上通过 wireshark 抓包就行。 # 如果你只需要简单地抓 http/https 包，请使用 fiddler/mitmproxy 如果需要对 Kubernetes 集群中的容器进行抓包，推荐直接使用 ksniff! ","date":"2020-05-28","objectID":"/posts/tcpdump-and-wireshark/:2:0","series":null,"tags":["网络","Wireshark","tcpdump","抓包分析"],"title":"使用 tcpdump 和 Wireshark 进行远程实时抓包分析","uri":"/posts/tcpdump-and-wireshark/#二tcpdump--ssh--wireshark-远程实时抓包"},{"categories":["tech"],"content":" Windows 系统另外如果你本机是 Windows 系统，要分 shell 讨论： cmd: 可以直接使用上述命令。 powershell: **PowerShell 管道对 native commands 的支持不是很好，管道两边的命令貌似是串行执行的，这会导致 wireshark 无法启动！**目前没有找到好的解决方法。。 另外如果你使用 wsl，那么可以通过如下命令使 wsl 调用 windows 中的 wireshark 进行抓包分析： # 添加软链接 sudo ln -s \"$(which wireshark.exe)\" /usr/local/bin/wireshark 添加了上述软链接后，就可以正常地在 wsl 中使用前面介绍的所有抓包指令了（包括 ksniff）。 它能正常调用 windows 中的 wireshark，数据流也能正常地通过 shell 管道传输。 ","date":"2020-05-28","objectID":"/posts/tcpdump-and-wireshark/:2:1","series":null,"tags":["网络","Wireshark","tcpdump","抓包分析"],"title":"使用 tcpdump 和 Wireshark 进行远程实时抓包分析","uri":"/posts/tcpdump-and-wireshark/#windows-系统"},{"categories":["tech"],"content":" 三、直接在命令行抓包检查","date":"2020-05-28","objectID":"/posts/tcpdump-and-wireshark/:3:0","series":null,"tags":["网络","Wireshark","tcpdump","抓包分析"],"title":"使用 tcpdump 和 Wireshark 进行远程实时抓包分析","uri":"/posts/tcpdump-and-wireshark/#三直接在命令行抓包检查"},{"categories":["tech"],"content":" termshark可以直接使用命令行 UI 工具 termshark 进行实时抓包分析 有的时候，远程实时抓包因为某些原因无法实现，而把 pcap 数据拷贝到本地分析又比较麻烦。 这时你可以考虑直接使用命令行版本的 wireshark UI: termshark，直接在命令行进行实时的抓包分析。 kubectl-debug 默认的调试镜像中，就自带 termshark. ","date":"2020-05-28","objectID":"/posts/tcpdump-and-wireshark/:3:1","series":null,"tags":["网络","Wireshark","tcpdump","抓包分析"],"title":"使用 tcpdump 和 Wireshark 进行远程实时抓包分析","uri":"/posts/tcpdump-and-wireshark/#termshark"},{"categories":["tech"],"content":" tcpdump也可以直接使用 tcpdump 将抓到的数据打印到 stdout 查看，常用命令如下： # 1. 嗅探所有接口，80 端口上所有 HTTP 协议请求与响应的 headers 以及 body tcpdump -A -s 0 'tcp port 80 and (((ip[2:2] - ((ip[0]\u00260xf)\u003c\u003c2)) - ((tcp[12]\u00260xf0)\u003e\u003e2)) != 0)' # 2. 嗅探 eth0 接口，80 端口上所有 HTTP GET 请求（'GET ' =\u003e 0x47455420） tcpdump -A -i eth0 -s 0 'tcp port 80 and tcp[((tcp[12:1] \u0026 0xf0) \u003e\u003e 2):4] = 0x47455420' # 2. 嗅探 eth0 接口，80 端口上所有 HTTP POST 请求（'POST' =\u003e 0x504F5354） tcpdump -A -i eth0 -s 0 'tcp port 80 and tcp[((tcp[12:1] \u0026 0xf0) \u003e\u003e 2):4] = 0x504F5354' ","date":"2020-05-28","objectID":"/posts/tcpdump-and-wireshark/:3:2","series":null,"tags":["网络","Wireshark","tcpdump","抓包分析"],"title":"使用 tcpdump 和 Wireshark 进行远程实时抓包分析","uri":"/posts/tcpdump-and-wireshark/#tcpdump"},{"categories":["tech"],"content":" 参考 WireShark使用教程 Tracing network traffic using tcpdump and tshark Android remote sniffing using Tcpdump, nc and Wireshark 聊聊tcpdump与Wireshark抓包分析 ","date":"2020-05-28","objectID":"/posts/tcpdump-and-wireshark/:4:0","series":null,"tags":["网络","Wireshark","tcpdump","抓包分析"],"title":"使用 tcpdump 和 Wireshark 进行远程实时抓包分析","uri":"/posts/tcpdump-and-wireshark/#参考"},{"categories":["tech"],"content":" 本文基于 Istio1.5 编写测试 Istio 支持使用 JWT 对终端用户进行身份验证（Istio End User Authentication），支持多种 JWT 签名算法。 目前主流的 JWT 算法是 RS256/ES256。（请忽略 HS256，该算法不适合分布式 JWT 验证） 这里以 RSA256 算法为例进行介绍，ES256 的配置方式也是一样的。 ","date":"2020-04-06","objectID":"/posts/use-istio-for-jwt-auth/:0:0","series":null,"tags":["Kubernetes","Istio","服务网格"],"title":"使用 Istio 进行 JWT 身份验证（充当 API 网关）","uri":"/posts/use-istio-for-jwt-auth/#"},{"categories":["tech"],"content":" 1. 介绍 JWK 与 JWKSIstio 要求提供 JWKS 格式的信息，用于 JWT 签名验证。因此这里得先介绍一下 JWK 和 JWKS. JWKS ，也就是 JWK Set，json 结构如下： { \"keys\": [ \u003cjwk-1\u003e, \u003cjwk-2\u003e, ... ]} JWKS 描述一组 JWK 密钥。它能同时描述多个可用的公钥，应用场景之一是密钥的 Rotate. 而 JWK，全称是 Json Web Key，它描述了一个加密密钥（公钥或私钥）的各项属性，包括密钥的值。 Istio 使用 JWK 描述验证 JWT 签名所需要的信息。在使用 RSA 签名算法时，JWK 描述的应该是用于验证的 RSA 公钥。 一个 RSA 公钥的 JWK 描述如下： { \"alg\": \"RS256\", # 算法「可选参数」 \"kty\": \"RSA\", # 密钥类型 \"use\": \"sig\", # 被用于签名「可选参数」 \"kid\": \"NjVBRjY5MDlCMUIwNzU4RTA2QzZFMDQ4QzQ2MDAyQjVDNjk1RTM2Qg\", # key 的唯一 id \"n\": \"yeNlzlub94YgerT030codqEztjfU_S6X4DbDA_iVKkjAWtYfPHDzz_sPCT1Axz6isZdf3lHpq_gYX4Sz-cbe4rjmigxUxr-FgKHQy3HeCdK6hNq9ASQvMK9LBOpXDNn7mei6RZWom4wo3CMvvsY1w8tjtfLb-yQwJPltHxShZq5-ihC9irpLI9xEBTgG12q5lGIFPhTl_7inA1PFK97LuSLnTJzW0bj096v_TMDg7pOWm_zHtF53qbVsI0e3v5nmdKXdFf9BjIARRfVrbxVxiZHjU6zL6jY5QJdh1QCmENoejj_ytspMmGW7yMRxzUqgxcAqOBpVm0b-_mW3HoBdjQ\", \"e\": \"AQAB\" } RSA 是基于大数分解的加密/签名算法，上述参数中，e 是公钥的模数(modulus)，n 是公钥的指数(exponent)，两个参数都是 base64 字符串。 JWK 中 RSA 公钥的具体定义参见 RSA Keys - JSON Web Algorithms (JWA) ","date":"2020-04-06","objectID":"/posts/use-istio-for-jwt-auth/:0:1","series":null,"tags":["Kubernetes","Istio","服务网格"],"title":"使用 Istio 进行 JWT 身份验证（充当 API 网关）","uri":"/posts/use-istio-for-jwt-auth/#1-介绍-jwk-与-jwks"},{"categories":["tech"],"content":" 2. JWK 的生成要生成 JWK 公钥，需要先生成私钥，生成方法参见 JWT 签名算法 HS256、RS256 及 ES256 及密钥生成。 公钥不需要用上述方法生成，因为我们需要的是 JWK 格式的公钥。后面会通过 python 生成出 JWK 公钥。 上面的命令会将生成出的 RSA 私钥写入 key.pem 中，查看一下私钥内容。 ryan@RYAN-MI-DESKTOP:~/istio$ cat key.pem -----BEGIN RSA PRIVATE KEY----- MIIEpAIBAAKCAQEAt1cKkQqPh8iOv5BhKh7Rx6A2+1ldpO/jczML/0GBKu4X+lHr Y8YbJrt29jyAXlWM8vHC7tXsqgUG+WziRD0D8nhnh10XC14SeH+3mVuBqph+TqhX TWsh9gtAIbeUHJjEI4I79QK4/wquPHHIGZBQDQQnuMh6vAS3VaUYJdEIoKvUBnAy Y35kJZgyJSbrxLsEExL2zujUD/OY+/In2bq/3rFtDGNlgHyC7Gu2zXSXvfOA4O5m 9BBXOc7eEqj7PoOKNaTxLN3YcuRtgR6NIXL4KLb6oyvIzoeiprt4+9q7sc3Dnkc5 EV9kwWlEW2DHzhP6VYca0WXIIXc53U1AM3ewxwIDAQABAoIBABIKhaaqJF+XM7zU B0uuxrPfJynqrFVbqcUfQ9H1bzF7Rm7CeuhRiUBxeA5Y+8TMpFcPxT/dWzGL1xja RxWx715/zKg8V9Uth6HF55o2r/bKlLtGw3iBz1C34LKwrul1eu+HlEDS6MNoGKco BynE0qvFOedsCu/Pgv7xhQPLow60Ty1uM0AhbcPgi6yJ5ksRB1XjtEnW0t+c8yQS nU3mU8k230SdMhf4Ifud/5TPLjmXdFpyPi9uYiVdJ5oWsmMWEvekXoBnHWDDF/eT VkVMiTBorT4qn+Ax1VjHL2VOMO5ZbXEcpbIc3Uer7eZAaDQ0NPZK37IkIn9TiZ21 cqzgbCkCgYEA5enHZbD5JgfwSNWCaiNrcBhYjpCtvfbT82yGW+J4/Qe/H+bY/hmJ RRTKf0kVPdRwZzq7GphVMWIuezbOk0aFGhk/SzIveW8QpLY0FV/5xFnGNjV9AuNc xrmgVshUsyQvr1TFkbdkC6yuvNgQfXfnbEoaPsXYEMCii2zqdF5lWGUCgYEAzCR2 6g8vEQx0hdRS5d0zD2/9IRYNzfP5oK0+F3KHH2OuwlmQVIo7IhCiUgqserXNBDef hj+GNcU8O/yXLomAXG7VG/cLWRrpY8d9bcRMrwb0/SkNr0yNrkqHiWQ/PvR+2MLk viWFZTTp8YizPA+8pSC/oFd1jkZF0UhKVAREM7sCgYB5+mfxyczFopyW58ADM7uC g0goixXCnTuiAEfgY+0wwXVjJYSme0HaxscQdOOyJA1ml0BBQeShCKgEcvVyKY3g ZNixunR5hrVbzdcgKAVJaR/CDuq+J4ZHYKByqmJVkLND4EPZpWSM1Rb31eIZzw2W 5FG8UBbr/GfAdQ6GorY+CQKBgQCzWQHkBmz6VG/2t6AQ9LIMSP4hWEfOfh78q9dW MDdIO4JomtkzfLIQ7n49B8WalShGITwUbLDTgrG1neeQahsMmg6+X99nbD5JfBTV H9WjG8CWvb+ZF++NhUroSNtLyu+6LhdaeopkbQVvPwMArG62wDu6ebv8v/5MrG8o uwrUSwKBgQCxV43ZqTRnEuDlF7jMN+2JZWhpbrucTG5INoMPOC0ZVatePszZjYm8 LrmqQZHer2nqtFpyslwgKMWgmVLJTH7sVf0hS9po0+iSYY/r8e/c85UdUreb0xyT x8whrOnMMODCAqu4W/Rx1Lgf2vXIx0pZmlt8Df9i2AVg/ePR6jO3Nw== -----END RSA PRIVATE KEY----- 接下来通过 Python 编程生成 RSA Public Key 和 JWK（jwk 其实就是公钥的另一个表述形式而已）: # 需要先安装依赖: pip install jwcrypto from jwcrypto.jwk import JWK from pathlib import Path private_key = Path(\"key.pem\").read_bytes() jwk = JWK.from_pem(private_key) # 导出公钥 RSA Public Key public_key = jwk.public().export_to_pem() print(public_key) print(\"=\"*30) # 导出 JWK jwk_bytes = jwk.public().export() print(jwk_bytes) Istio 需要 JWK 进行 JWT 验证，而我们手动验证 JWT 时一般需要用到 Public Key. 方便起见，上述代码把这两个都打印了出来。内容如下： # Public Key 内容，不包含这行注释 -----BEGIN PUBLIC KEY----- MIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEAt1cKkQqPh8iOv5BhKh7R x6A2+1ldpO/jczML/0GBKu4X+lHrY8YbJrt29jyAXlWM8vHC7tXsqgUG+WziRD0D 8nhnh10XC14SeH+3mVuBqph+TqhXTWsh9gtAIbeUHJjEI4I79QK4/wquPHHIGZBQ DQQnuMh6vAS3VaUYJdEIoKvUBnAyY35kJZgyJSbrxLsEExL2zujUD/OY+/In2bq/ 3rFtDGNlgHyC7Gu2zXSXvfOA4O5m9BBXOc7eEqj7PoOKNaTxLN3YcuRtgR6NIXL4 KLb6oyvIzoeiprt4+9q7sc3Dnkc5EV9kwWlEW2DHzhP6VYca0WXIIXc53U1AM3ew xwIDAQAB -----END PUBLIC KEY----- # jwk 内容 { 'e': 'AQAB', 'kid': 'oyYwZSLCLVVPHdVp0jXIcLNpGn6dMCumlY-6wSenmFo', 'kty': 'RSA', 'n': 't1cKkQqPh8iOv5BhKh7Rx6A2-1ldpO_jczML_0GBKu4X-lHrY8YbJrt29jyAXlWM8vHC7tXsqgUG-WziRD0D8nhnh10XC14SeH-3mVuBqph-TqhXTWsh9gtAIbeUHJjEI4I79QK4_wquPHHIGZBQDQQnuMh6vAS3VaUYJdEIoKvUBnAyY35kJZgyJSbrxLsEExL2zujUD_OY-_In2bq_3rFtDGNlgHyC7Gu2zXSXvfOA4O5m9BBXOc7eEqj7PoOKNaTxLN3YcuRtgR6NIXL4KLb6oyvIzoeiprt4-9q7sc3Dnkc5EV9kwWlEW2DHzhP6VYca0WXIIXc53U1AM3ewxw' } ","date":"2020-04-06","objectID":"/posts/use-istio-for-jwt-auth/:0:2","series":null,"tags":["Kubernetes","Istio","服务网格"],"title":"使用 Istio 进行 JWT 身份验证（充当 API 网关）","uri":"/posts/use-istio-for-jwt-auth/#2-jwk-的生成"},{"categories":["tech"],"content":" 4. 测试密钥可用性接下来在 jwt.io 中填入测试用的公钥私钥，还有 Header/Payload。一是测试公私钥的可用性，二是生成出 JWT 供后续测试 Istio JWT 验证功能的可用性。 可以看到左下角显示「Signature Verified」，成功地生成出了 JWT。后续可以使用这个 JWT 访问 Istio 网关，测试 Istio JWT 验证功能。 ","date":"2020-04-06","objectID":"/posts/use-istio-for-jwt-auth/:0:3","series":null,"tags":["Kubernetes","Istio","服务网格"],"title":"使用 Istio 进行 JWT 身份验证（充当 API 网关）","uri":"/posts/use-istio-for-jwt-auth/#4-测试密钥可用性"},{"categories":["tech"],"content":" 5. 启用 Istio 的身份验证编写 istio 配置： apiVersion: \"security.istio.io/v1beta1\" kind: \"RequestAuthentication\" metadata: name: \"jwt-example\" namespace: istio-system # istio-system 名字空间中的配置，默认情况下会应用到所有名字空间 spec: selector: matchLabels: istio: ingressgateway # 在带有这些 labels 的 ingressgateway/sidecar 上生效 jwtRules: # issuer 即签发者，需要和 JWT payload 中的 iss 属性完全一致。 - issuer: \"testing@secure.istio.io\" jwks: | { \"keys\": [ { \"e\": \"AQAB\", \"kid\": \"oyYwZSLCLVVPHdVp0jXIcLNpGn6dMCumlY-6wSenmFo\", # kid 需要与 jwt header 中的 kid 完全一致。 \"kty\": \"RSA\", \"n\": \"t1cKkQqPh8iOv5BhKh7Rx6A2-1ldpO_jczML_0GBKu4X-lHrY8YbJrt29jyAXlWM8vHC7tXsqgUG-WziRD0D8nhnh10XC14SeH-3mVuBqph-TqhXTWsh9gtAIbeUHJjEI4I79QK4_wquPHHIGZBQDQQnuMh6vAS3VaUYJdEIoKvUBnAyY35kJZgyJSbrxLsEExL2zujUD_OY-_In2bq_3rFtDGNlgHyC7Gu2zXSXvfOA4O5m9BBXOc7eEqj7PoOKNaTxLN3YcuRtgR6NIXL4KLb6oyvIzoeiprt4-9q7sc3Dnkc5EV9kwWlEW2DHzhP6VYca0WXIIXc53U1AM3ewxw\" } ] } # jwks 或 jwksUri 二选其一 # jwksUri: \"http://nginx.test.local/istio/jwks.json\" 现在 kubectl apply 一下，JWT 验证就添加到全局了。 可以看到 jwtRules 是一个列表，因此可以为每个 issuers 配置不同的 jwtRule. 对同一个 issuers（jwt 签发者），可以通过 jwks 设置多个公钥，以实现JWT签名密钥的轮转。 JWT 的验证规则是： JWT 的 payload 中有 issuer 属性，首先通过 issuer 匹配到对应的 istio 中配置的 jwks。 JWT 的 header 中有 kid 属性，第二步在 jwks 的公钥列表中，中找到 kid 相同的公钥。 使用找到的公钥进行 JWT 签名验证。 配置中的 spec.selector 可以省略，这样会直接在整个 namespace 中生效，而如果是在 istio-system 名字空间，该配置将在全集群的所有 sidecar/ingressgateway 上生效！ ","date":"2020-04-06","objectID":"/posts/use-istio-for-jwt-auth/:0:4","series":null,"tags":["Kubernetes","Istio","服务网格"],"title":"使用 Istio 进行 JWT 身份验证（充当 API 网关）","uri":"/posts/use-istio-for-jwt-auth/#5-启用-istio-的身份验证"},{"categories":["tech"],"content":" 6. 启用 Payload 转发/Authorization 转发默认情况下，Istio 在完成了身份验证之后，会去掉 Authorization 请求头再进行转发。 这将导致我们的后端服务获取不到对应的 Payload，无法判断 End User 的身份。 因此我们需要启用 Istio 的 Authorization 请求头的转发功能，在前述的 RequestAuthentication yaml 配置中添加一个参数就行： apiVersion: \"security.istio.io/v1beta1\" kind: \"RequestAuthentication\" metadata: name: \"jwt-example\" namespace: istio-system spec: selector: matchLabels: istio: ingressgateway jwtRules: - issuer: \"testing@secure.istio.io\" jwks: | { \"keys\": [ { \"e\": \"AQAB\", \"kid\": \"oyYwZSLCLVVPHdVp0jXIcLNpGn6dMCumlY-6wSenmFo\", \"kty\": \"RSA\", \"n\": \"t1cKkQqPh8iOv5BhKh7Rx6A2-1ldpO_jczML_0GBKu4X-lHrY8YbJrt29jyAXlWM8vHC7tXsqgUG-WziRD0D8nhnh10XC14SeH-3mVuBqph-TqhXTWsh9gtAIbeUHJjEI4I79QK4_wquPHHIGZBQDQQnuMh6vAS3VaUYJdEIoKvUBnAyY35kJZgyJSbrxLsEExL2zujUD_OY-_In2bq_3rFtDGNlgHyC7Gu2zXSXvfOA4O5m9BBXOc7eEqj7PoOKNaTxLN3YcuRtgR6NIXL4KLb6oyvIzoeiprt4-9q7sc3Dnkc5EV9kwWlEW2DHzhP6VYca0WXIIXc53U1AM3ewxw\" } ] } # ===================== 添加如下参数=========================== forwardOriginalToken: true # 转发 Authorization 请求头 加了转发后，流程图如下（需要 mermaid 渲染）： ","date":"2020-04-06","objectID":"/posts/use-istio-for-jwt-auth/:0:5","series":null,"tags":["Kubernetes","Istio","服务网格"],"title":"使用 Istio 进行 JWT 身份验证（充当 API 网关）","uri":"/posts/use-istio-for-jwt-auth/#6-启用-payload-转发authorization-转发"},{"categories":["tech"],"content":" 7. 设定强制认证规则Istio 的 JWT 验证规则，默认情况下会直接忽略不带 Authorization 请求头（即 JWT）的流量，因此这类流量能直接进入网格内部。 通常这是没问题的，因为没有 Authorization 的流量即使进入到内部，也会因为无法通过 payload 判别身份而被拒绝操作。 如果需要禁止不带 JWT 的流量，就需要额外配置 AuthorizationPolicy 策略。 比如拒绝任何 JWT 无效的请求（包括 Authorization 的情况）： apiVersion: security.istio.io/v1beta1 kind: AuthorizationPolicy metadata: name: \"deny-requests-with-out-authorization\" namespace: istio-system spec: selector: matchLabels: istio: ingressgateway action: DENY # 拒绝 rules: - from: - source: notRequestPrincipals: [\"*\"] # 不存在任何请求身份（Principal）的 requests 如果仅希望强制要求对部分 path 的请求必须带有 Authorization Header，可以这样设置： apiVersion: security.istio.io/v1beta1 kind: AuthorizationPolicy metadata: name: \"deny-requests-with-out-authorization\" namespace: istio-system spec: selector: matchLabels: istio: ingressgateway action: DENY # 拒绝 rules: - from: - source: notRequestPrincipals: [\"*\"] # 不存在任何请求身份（Principal）的 requests # 仅强制要求如下 host/path 相关的请求，必须带上 JWT token to: - operation: hosts: [\"another-host.com\"] paths: [\"/headers\"] 注意这两个 Istio CR 返回的错误码是不同的： RequestsAuthentication 验证失败的请求，Istio 会返回 401 状态码。 AuthorizationPolicy 验证失败的请求，Istio 会返回 403 状态码。 这会导致在使用 AuthorizationPolicy 禁止了不带 Authorization 头的流量后，这类请求会直接被返回 403，在使用 RESTful API 时，这种情况可能会造成问题。 ","date":"2020-04-06","objectID":"/posts/use-istio-for-jwt-auth/:0:6","series":null,"tags":["Kubernetes","Istio","服务网格"],"title":"使用 Istio 进行 JWT 身份验证（充当 API 网关）","uri":"/posts/use-istio-for-jwt-auth/#7-设定强制认证规则"},{"categories":["tech"],"content":" 8. Response HeadersRequestsAuthentication 不支持自定义响应头信息，这导致对于前后端分离的 Web API 而言， 一旦 JWT 失效，Istio 会直接将 401 返回给前端 Web 页面。 因为响应头中不包含 Access-Crontrol-Allow-Origin，响应将被浏览器拦截！ 这可能需要通过 EnvoyFilter 自定义响应头，添加跨域信息。 ","date":"2020-04-06","objectID":"/posts/use-istio-for-jwt-auth/:0:7","series":null,"tags":["Kubernetes","Istio","服务网格"],"title":"使用 Istio 进行 JWT 身份验证（充当 API 网关）","uri":"/posts/use-istio-for-jwt-auth/#8-response-headers"},{"categories":["tech"],"content":" 参考 JSON Web Key Set Properties - Auth0 JWK - RFC7517 Sample JWT and JWKS data for demo - Istio Security End User Authentication - Istio JWTRule - Istio jwt.io - 动态生成 jwt ","date":"2020-04-06","objectID":"/posts/use-istio-for-jwt-auth/:1:0","series":null,"tags":["Kubernetes","Istio","服务网格"],"title":"使用 Istio 进行 JWT 身份验证（充当 API 网关）","uri":"/posts/use-istio-for-jwt-auth/#参考"},{"categories":["tech"],"content":" 个人笔记，观点不一定正确. 适合对 Kubernetes 有一定了解的同学。 ","date":"2020-03-29","objectID":"/posts/about-dns-protocol/:0:0","series":null,"tags":["DNS","CoreDNS","网络"],"title":"Linux网络学习笔记（二）：域名解析(DNS)——以 CoreDNS 为例","uri":"/posts/about-dns-protocol/#"},{"categories":["tech"],"content":" 前言最近一直在学习 Kubernetes，但是手头没有个自有域名，要测试 ingress 就比较麻烦，每次都是手动改 hosts 文件。。 今天突然想到——K8s 内部就是用 DNS 做的服务发现，我为啥不自己弄一个 DNS 服务器呢？然后所有节点的 DNS 都配成它，这样有需要时直接改这个 DNS 服务器的配置就行， 一劳永逸。 我首先想到的是 群晖/Windows Server 自带的那种自带图形化界面的 DNS 服务器，但是这俩都是平台特定的。 网上搜一圈没找到类似带 UI 的 DNS 工具，搜到的 powerdns/bind 相比 coredns 也没看出啥优势来，所以决定就用 CoreDNS，刚好熟悉一下它的具体使用。 不过讲 CoreDNS 前，我们还是先来熟悉一下 DNS 的基础概念吧。 ","date":"2020-03-29","objectID":"/posts/about-dns-protocol/:1:0","series":null,"tags":["DNS","CoreDNS","网络"],"title":"Linux网络学习笔记（二）：域名解析(DNS)——以 CoreDNS 为例","uri":"/posts/about-dns-protocol/#前言"},{"categories":["tech"],"content":" 一、DNS 是个啥？ 没有写得很清楚，不适合初学。建议先通过别的资料熟悉下 DNS 基础。 DNS，即域名系统（Domain Name System），是一项负责将一个 human readable 的所谓域名，转换成一个 ip 地址的协议。 而域名的好处，有如下几项： 域名对人类更友好，可读的字符串比一串 ip 数字可好记多了。 一个域名可以对应多个 ip，可实现所谓的负载均衡。 多个域名可以对应同一个 ip，以不同的域名访问该 ip，能访问不同的应用。（通过 nginx 做代理实现） DNS 协议是一个基于 UDP 的应用层协议，它默认使用 53 端口进行通信。 应用程序通常将 DNS 解析委派给操作系统的 DNS Resolver 来执行，程序员对它几乎无感知。 DNS 虽然说一般只用来查个 ip 地址，但是它提供的记录类型还蛮多的，主要有如下几种： A 记录：它记录域名与 IPv4 地址的对应关系。目前用的最多的 DNS 记录就是这个。 AAAA 记录：它对应的是 IPv6，可以理解成新一代的 A 记录。以后会用的越来越多的。 NS 记录：记录 DNS 域对应的权威服务器域名，权威服务器域名必须要有对应的 A 记录。 通过这个记录，可以将子域名的解析分配给别的 DNS 服务器。 CNAME 记录: 记录域名与另一个域名的对应关系，用于给域名起别名。这个用得也挺多的。 MX 记录：记录域名对应的邮件服务器域名，邮件服务器的域名必须要有对应的 A 记录。 SRV 记录：SRV 记录用于提供服务发现，看名字也能知道它和 SERVICE 有关。 SRV 记录的内容有固定格式：优先级 权重 端口 目标地址，例如 0 5 5060 sipserver.example.com 主要用于企业域控(AD)、微服务发现（Kubernetes） 上述的所有 DNS 记录，都是属于将域名解析为 IP 地址，或者另一个域名，这被称做** DNS 正向解析**。 除了这个正向解析外，还有个非常冷门的反向解析，基本上只在设置邮件服务器时才会用到。（Kubernetes 可能也有用到） 反向解析主要的记录类型是：PTR 记录，它提供将 IP 地址反向解析为域名的功能。 而且因为域名是从右往左读的（最右侧是根, www.baidu.com.），而 IP 的网段（如 192.168.0.0/16）刚好相反，是左边优先。 因此 PTR 记录的“域名”必须将 IP 地址反着写，末尾再加上 .in-addr.arpa. 表示这是一个反向解析的域名。（ipv6 使用 ip6.arpa.） 拿 baidu.com 的邮件服务器测试一下： PTR 记录查询 其他还有些 TXT、CAA 等奇奇怪怪的记录，就用到的时候自己再查了。 ","date":"2020-03-29","objectID":"/posts/about-dns-protocol/:2:0","series":null,"tags":["DNS","CoreDNS","网络"],"title":"Linux网络学习笔记（二）：域名解析(DNS)——以 CoreDNS 为例","uri":"/posts/about-dns-protocol/#一dns-是个啥"},{"categories":["tech"],"content":" 二、域名的分层结构国际域名系统被分成四层： 根域（Root Zone）：所有域名的根。 根域名服务器负责解析顶级域名，给出顶级域名的 DNS 服务器地址。 全世界仅有十三组根域名服务器，这些服务器的 ip 地址基本不会变动。 它的域名是 “\"，空字符串。而它的**全限定域名（FQDN）**是 .，因为 FQDN 总是以 . 结尾。（FQDN 在后面解释，可暂时忽略） 顶级域（Top Level Domains, TLD）：.com .cn 等国际、国家级的域名 顶级域名服务器负责解析次级域名，给出次级域名的 DNS 服务器地址。 每个顶级域名都对应各自的服务器，它们之间是完全独立的。.cn 的域名解析仅由 .cn 顶级域名服务器提供。 目前国际 DNS 系统中已有上千个 TLD，包括中文「.我爱你」甚至藏文域名，详细列表参见 IANA TLD 数据库 除了国际可用的 TLD，还有一类类似「内网 IP 地址」的“私有 TLD”，最常见的比如 xxx.local xxx.lan，被广泛用在集群通信中。后面详细介绍 次级域（Second Level Domains）：这个才是个人/企业能够买到的域名，比如 baidu.com 每个次级域名都有一到多个权威 DNS 服务器，这些 DNS 服务器会以 NS 记录的形式保存在对应的顶级域名（TLD）服务器中。 权威域名服务器则负责给出最终的解析结果：ip 地址(A 记录 )，另一个域名（CNAME 记录）、另一个 DNS 服务器（NS 记录）等。 子域（Sub Domians）：*.baidu.com 统统都是 baidu.com 的子域。 每一个子域都可以有自己独立的权威 DNS 服务器，这通过在子域中添加 NS 记录实现。 普通用户通常是通过域名提供商如阿里云购买的次级域名，接下来我们以 rea.ink 为例介绍域名的购买到可用的整个流程。 域名的购买与使用流程： 你在某域名提供商处购买了一个域名 rea.ink 域名提供商向 .ink 对应的顶级域名服务器中插入一条以上的 NS 记录，指向它自己的次级 DNS 服务器，如 dns25.hichina.com. 阿里云会向 TLD 中插入几条 NS 记录，指向阿里云的次级 DNS 服务器（如 vip1.alidns.com）。 你在该域名提供商的 DNS 管理界面中添加 A 记录，值为你的服务器 IP。 OK 现在 ping 一下 rea.ink，就会发现它已经解析到你自己的服务器了。 上述流程中忽略了我大天朝的特殊国情——备案，勿介意。 ","date":"2020-03-29","objectID":"/posts/about-dns-protocol/:3:0","series":null,"tags":["DNS","CoreDNS","网络"],"title":"Linux网络学习笔记（二）：域名解析(DNS)——以 CoreDNS 为例","uri":"/posts/about-dns-protocol/#二域名的分层结构"},{"categories":["tech"],"content":" 三、DNS 递归解析器：在浏览器中输入域名后发生了什么？下面的图片拷贝自 Amazon Aws 文档，它展示了在不考虑任何 DNS 缓存的情况下，一次 Web 请求的经过，详细描绘了 DNS 解析的部分。 DNS 解析流程 其中的第 3 4 5 步按顺序向前面讲过的根域名服务器、顶级域名服务器、权威域名服务器发起请求，以获得下一个 DNS 服务器的信息。这很清晰。 图中当前还没介绍的部分，是紫色的 DNS Resolver(域名解析器)，也叫 Recursive DNS resolver（DNS 递归解析器）。 它本身只负责递归地请求 3 4 5 步中的上游服务器，然后把获取的最终结果返回给客户端，同时将记录缓存到本地以加快解析速度。 这个 DNS 解析器，其实就是所谓的公共 DNS 服务器：Google 的 8.8.8.8，国内著名的 114.114.114.114。 这些公共 DNS 用户量大，缓存了大量的 DNS 记录，有效地降低了上游 DNS 服务器的压力，也加快了网络上的 DNS 查询速度。 接下来使用 dig +trace baidu.com 复现一下上述的查询流程（这种情况下 dig 自己就是一个 DNS 递归解析器）： dig +trace baidu.com 另外前面有讲过 DNS 的反向解析，也是同样的层级结构，是从根服务器开始往下查询的，下面拿 baidu 的一个邮件服务器进行测试： 反向解析 dig 工具未来可能会被 drill 取代。 ","date":"2020-03-29","objectID":"/posts/about-dns-protocol/:4:0","series":null,"tags":["DNS","CoreDNS","网络"],"title":"Linux网络学习笔记（二）：域名解析(DNS)——以 CoreDNS 为例","uri":"/posts/about-dns-protocol/#三dns-递归解析器在浏览器中输入域名后发生了什么"},{"categories":["tech"],"content":" DNS 泛解析通配符 *DNS 记录允许使用通配符 *，并且该通配符可匹配任意级数的子域！！！比如 *.example.com 就可以匹配所有的一二三四级域名等等，但是无法匹配 example.com 本身！ ","date":"2020-03-29","objectID":"/posts/about-dns-protocol/:4:1","series":null,"tags":["DNS","CoreDNS","网络"],"title":"Linux网络学习笔记（二）：域名解析(DNS)——以 CoreDNS 为例","uri":"/posts/about-dns-protocol/#dns-泛解析通配符-"},{"categories":["tech"],"content":" TTL（Time To Live）上面讲了公共 DNS 服务器通过缓存技术，降低了上游 DNS 服务器的压力，也加快了网络上的 DNS 查询速度。 可缓存总得有个过期时间吧！为了精确地控制 DNS 记录的过期时间，每条 DNS 记录都要求设置一个时间属性——TTL，单位为秒。这个时间可以自定义。 任何一条 DNS 缓存，在超过过期时间后都必须丢弃！ 另外在没超时的时候，DNS 缓存也可以被主动或者被动地刷新。 ","date":"2020-03-29","objectID":"/posts/about-dns-protocol/:4:2","series":null,"tags":["DNS","CoreDNS","网络"],"title":"Linux网络学习笔记（二）：域名解析(DNS)——以 CoreDNS 为例","uri":"/posts/about-dns-protocol/#ttltime-to-live"},{"categories":["tech"],"content":" 四、本地 DNS 服务器与私有 DNS 域这类服务器只在当前局域网内有效，是一个私有的 DNS 服务器，企业常用。一般通过 DHCP 或者手动配置的方式，使内网的服务器都默认使用局域网 DNS 服务器进行解析。该服务器可以只解析自己的私有 DNS 域，而将其他 DNS 域的解析 forward 到公网 DNS 解析器去。 这个私有 DNS 域，会覆盖掉公网的同名域(如果公网上有这个域的话)。 私有 dns 域也可使用公网不存在的 TLD，比如 xxx.local xxx.lan 等。vmware vcenter 就默认使用 vsphere.local 作为它的 sso (单点登录)系统的域名。kubernetes 默认使用 svc.cluster.local 作为集群内部域名。 私有 DNS 域的选择，参见 DNS 私有域的选择：internal.xxx.com xxx.local 还是 xxx.zone？ 局域网 DNS 服务器的规模与层级，视局域网的大小而定。一般小公司一个就行，要容灾设三个副本也够了。 以 CoreDNS 为例，局域网 DNS 服务器也可以被设置成一个 DNS Resolver，可以设置只转发特定域名的 DNS 解析。这叫将某个域设为「转发区域」。 ","date":"2020-03-29","objectID":"/posts/about-dns-protocol/:5:0","series":null,"tags":["DNS","CoreDNS","网络"],"title":"Linux网络学习笔记（二）：域名解析(DNS)——以 CoreDNS 为例","uri":"/posts/about-dns-protocol/#四本地-dns-服务器与私有-dns-域"},{"categories":["tech"],"content":" 五、操作系统的 DNS 解析器应用程序实际上都是调用的操作系统的 DNS Resolver 进行域名解析的。在 Linux 中 DNS Resolver 由 glibc/musl 提供，配置文件为 /etc/resolv.conf。 比如 Python 的 DNS 解析，就来自于标准库的 socket 包，这个包只是对底层 c 语言库的一个简单封装。 基本上只有专门用于网络诊断的 DNS 工具包，才会自己实现 DNS 协议。 ","date":"2020-03-29","objectID":"/posts/about-dns-protocol/:6:0","series":null,"tags":["DNS","CoreDNS","网络"],"title":"Linux网络学习笔记（二）：域名解析(DNS)——以 CoreDNS 为例","uri":"/posts/about-dns-protocol/#五操作系统的-dns-解析器"},{"categories":["tech"],"content":" 1. hosts 文件操作系统中还有一个特殊文件：Linux 中的 /etc/hosts 和 Windows 中的 C:\\Windows\\System32\\drivers\\etc\\hosts 系统中的 DNS resolver 会首先查看这个 hosts 文件中有没有该域名的记录，如果有就直接返回了。没找到才会去查找本地 DNS 缓存、别的 DNS 服务器。 只有部分专门用于网络诊断的应用程序（e.g. dig）不会依赖 OS 的 DNS 解析器，因此这个 hosts 会失效。hosts 对于绝大部分程序都有效。 移动设备上 hosts 可能会失效，部分 app 会绕过系统，使用新兴的 HTTPDNS 协议进行 DNS 解析。 ","date":"2020-03-29","objectID":"/posts/about-dns-protocol/:6:1","series":null,"tags":["DNS","CoreDNS","网络"],"title":"Linux网络学习笔记（二）：域名解析(DNS)——以 CoreDNS 为例","uri":"/posts/about-dns-protocol/#1-hosts-文件"},{"categories":["tech"],"content":" 2. HTTPDNS传统的 DNS 协议因为使用了明文的 UDP 协议，很容易被劫持。顺应移动互联网的兴起，目前一种新型的 DNS 协议——HTTPDNS 应用越来越广泛，国内的阿里云腾讯云都提供了这项功能。 HTTPDNS 通过 HTTP 协议直接向权威 DNS 服务器发起请求，绕过了一堆中间的 DNS 递归解析器。好处有二： 权威 DNS 服务器能直接获取到客户端的真实 IP（而不是某个中间 DNS 递归解析器的 IP），能实现就近调度。 因为是直接与权威 DNS 服务器连接，避免了 DNS 缓存污染的问题。 HTTPDNS 协议需要程序自己引入 SDK，或者直接请求 HTTP API。 ","date":"2020-03-29","objectID":"/posts/about-dns-protocol/:6:2","series":null,"tags":["DNS","CoreDNS","网络"],"title":"Linux网络学习笔记（二）：域名解析(DNS)——以 CoreDNS 为例","uri":"/posts/about-dns-protocol/#2-httpdns"},{"categories":["tech"],"content":" 3. 默认 DNS 服务器操作系统的 DNS 解析器通常会允许我们配置多个上游 Name Servers，比如 Linux 就是通过 /etc/resolv.conf 配置 DNS 服务器的。 $ cat /etc/resolv.conf nameserver 8.8.8.8 nameserver 8.8.4.4 search lan 不过现在这个文件基本不会手动修改了，各 Linux 发行版都推出了自己的网络配置工具，由这些工具自动生成 Linux 的各种网络配置，更方便。 比如 Ubuntu 就推荐使用 netplan 工具进行网络设置。 Kubernetes 就是通过使用容器卷映射的功能，修改 /etc/resolv.conf，使集群的所有容器都使用集群 DNS 服务器（CoreDNS）进行 DNS 解析。 通过重复使用 nameserver 字段，可以指定多个 DNS 服务器（Linux 最多三个）。DNS 查询会按配置中的顺序选用 DNS 服务器。 **仅在靠前的 DNS 服务器没有响应（timeout）时，才会使用后续的 DNS 服务器！所以指定的服务器中的 DNS 记录最好完全一致！！！**不要把第一个配内网 DNS，第二个配外网！！！ ","date":"2020-03-29","objectID":"/posts/about-dns-protocol/:6:3","series":null,"tags":["DNS","CoreDNS","网络"],"title":"Linux网络学习笔记（二）：域名解析(DNS)——以 CoreDNS 为例","uri":"/posts/about-dns-protocol/#3-默认-dns-服务器"},{"categories":["tech"],"content":" 4. DNS 搜索域上一小节给出的 /etc/resolv.conf 文件内容的末尾，有这样一行: search lan，它指定的，是所谓的 DNS 搜索域。 讲到 DNS 搜索域，就不得不提到一个名词：全限定域名（Full Qulified Domain Name, FQDN），即一个域名的完整名称，www.baidu.com。 一个普通的域名，有下列四种可能： www.baidu.com.: 末尾的 . 表示根域，说明 www.baidu.com 是一个 FQDN，因此不会使用搜索域！ www.baidu.com: 末尾没 .，但是域名包含不止一个 .。首先当作 FQDN 进行查询，没查找再按顺序在各搜索域中查询。 /etc/resolv.conf 的 options 参数中，可以指定域名中包含 . 的临界个数，默认是 1. local: 不包含 .，被当作 host 名称，非 FQDN。首先在 /etc/hosts 中查找，没找到的话，再按顺序在各搜索域中查找。 上述搜索顺序可以通过 host -v \u003cdomain-name\u003e 进行测试，该命令会输出它尝试过的所有 FQDN。 修改 /etc/resolv.conf 中的 search 属性并测试，然后查看输出。 就如上面说例举的，在没有 DNS 搜索域 这个东西的条件下，我们访问任何域名，都必须输入一个全限定域名 FQDN。 有了搜索域我们就可以稍微偷点懒，省略掉域名的一部分后缀，让 DNS Resolver 自己去在各搜索域中搜索。 在 Kubernetes 中就使用到了搜索域，k8s 中默认的域名 FQDN 是 service.namespace.svc.cluster.local， 但是对于 default namespace 中的 service，我们可以直接通过 service 名称查询到它的 IP。 对于其他名字空间中的 service，也可以通过 service.namespace 查询到它们的 IP，不需要给出 FQDN。 Kubernetes 中 /etc/resolv.conf 的示例如下： nameserver 10.43.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5 可以看到 k8s 设置了一系列的搜索域，并且将 . 的临界值设为了 5。 也就是少于 5 个 dots 的域名，都首先当作非 FQDN 看待，优先在搜索域里面查找。 该配置文件的详细描述参见 manpage - resolv.conf，或者在 Linux 中使用 man resolv.conf 命令查看。 ","date":"2020-03-29","objectID":"/posts/about-dns-protocol/:6:4","series":null,"tags":["DNS","CoreDNS","网络"],"title":"Linux网络学习笔记（二）：域名解析(DNS)——以 CoreDNS 为例","uri":"/posts/about-dns-protocol/#4-dns-搜索域"},{"categories":["tech"],"content":" 六、DNS 诊断的命令行工具 dig +trace baidu.com # 诊断 dns 的主要工具，非常强大 host -a baidu.com # host 基本就是 dig 的弱化版，不过 host 有个有点就是能打印出它测试过的所有 FQDN nslookup baidu.com # 和 host 没啥大差别，多个交互式查询不过一般用不到 whois baidu.com # 查询域名注册信息，内网诊断用不到 详细的使用请 man dig ","date":"2020-03-29","objectID":"/posts/about-dns-protocol/:7:0","series":null,"tags":["DNS","CoreDNS","网络"],"title":"Linux网络学习笔记（二）：域名解析(DNS)——以 CoreDNS 为例","uri":"/posts/about-dns-protocol/#六dns-诊断的命令行工具"},{"categories":["tech"],"content":" 七、CoreDNS 的使用主流的本地 DNS 服务器中，提供 UI 界面的有 Windows DNS Server 和群晖 DNS Server，很方便，不过这两个都是操作系统绑定的。 开源的 DNS 服务器里边儿，BIND 好像是最有名的，各大 Linux 发行版自带的 dig/host/nslookup，最初都是 Bind 提供的命令行工具。 不过为了一举两得（DNS+K8s），咱还是直接学习 CoreDNS 的使用。 CoreDNS 最大的特点是灵活，可以很方便地给它编写插件以提供新功能。功能非常强大，相比传统 DNS 服务器，它非常“现代化”。在 K8s 中它被用于提供服务发现功能。 接下来以 CoreDNS 为例，讲述如何配置一个 DNS 服务器，添加私有的 DNS 记录，并设置转发规则以解析公网域名。 ","date":"2020-03-29","objectID":"/posts/about-dns-protocol/:8:0","series":null,"tags":["DNS","CoreDNS","网络"],"title":"Linux网络学习笔记（二）：域名解析(DNS)——以 CoreDNS 为例","uri":"/posts/about-dns-protocol/#七coredns-的使用"},{"categories":["tech"],"content":" 1. 配置文件：CorefileCoreDNS 因为是 Go 语言写的，编译结果是单个可执行文件，它默认以当前文件夹下的 Corefile 为配置文件。以 kubernetes 中的 Corefile 为例： .:53 { errors # 启用错误日志 health # 启用健康检查 api ready # 启用 readiness 就绪 api # 启用 kubernetes 集群支持，详见 https://coredns.io/plugins/kubernetes/ # 此插件只处理 cluster.local 域，以及 PTR 解析 kubernetes cluster.local in-addr.arpa ip6.arpa { pods insecure upstream # fallthrough in-addr.arpa ip6.arpa # 向下传递 DNS 反向查询 ttl 30 # 过期时间 } prometheus :9153 # 启用 prometheus metrics 支持 forward . 114.114.114.114 19.29.29.29 # 将非集群域名的 DNS 请求，转发给公网 DNS 服务器。 cache 30 # 启用前端缓存，缓存的 TTL 设为 30 loop # 检测并停止死循环解析 reload # 支持动态更新 Corefile # 随机化 A/AAAA/MX 记录的顺序以实现负载均衡。 # 因为 DNS resolver 通常使用第一条记录，而第一条记录是随机的。这样客户端的请求就能被随机分配到多个后端。 loadbalance } Corefile 首先定义 DNS 域，域后的代码块内定义需要使用的各种插件。**注意这里的插件顺序是没有任何意义的！**插件的调用链是在 CoreDNS 编译时就定义好的，不能在运行时更改。 通过上述配置启动的 CoreDNS 是无状态的，它以 Kubernetes ApiServer 为数据源，CoreDNS 本身只相当于一个查询器/缓存，因此它可以很方便地扩缩容。 ","date":"2020-03-29","objectID":"/posts/about-dns-protocol/:8:1","series":null,"tags":["DNS","CoreDNS","网络"],"title":"Linux网络学习笔记（二）：域名解析(DNS)——以 CoreDNS 为例","uri":"/posts/about-dns-protocol/#1-配置文件corefile"},{"categories":["tech"],"content":" 2. 将 CoreDNS 设置成一个私有 DNS 服务器现在清楚了 Corefile 的结构，让我们来设计一个通过文件配置 DNS 条目的 Corefile 配置： # 定义可复用 Block (common) { log errors cache loop # 检测并停止死循环解析 } # 本地开发环境的 DNS 解析 dev-env.local:53 { import common # 导入 Block file dev-env.local { # 从文件 `dev-env.local` 中读取 DNS 数据 reload 30s # 每 30s 检查一次配置的 Serial，若该值有变更则重载整个 Zone 的配置。 } } # 本地测试环境 test-env.local:53 { import common file test-env.local { reload 30s } } # 其他 .:53 { forward . 114.114.114.114 # 解析公网域名 log errors cache } 上面的 Corefile 定义了两个本地域名 dev-env.local 和 test-env.local，它们的 DNS 数据分别保存在 file 指定的文件中。 这个 file 指定的文件和 bind9 一样，都是使用在 rfc1035 中定义的 Master File 格式，dig 命令输出的就是这种格式的内容。示例如下： ;; 與整個領域相關性較高的設定包括 NS, A, MX, SOA 等標誌的設定處！ $TTL 30 @ IN SOA dev-env.local. devops.dev-env.local. ( 20200202 ; SERIAL，每次修改此文件，都应该同步修改这个“版本号”，可将它设为修改时间。 7200 ; REFRESH 600 ; RETRY 3600000 ; EXPIRE 60) ; MINIMUM @ IN NS dns1.dev-env.local. ; DNS 伺服器名稱 dns1.dev-env.local. IN A 192.168.23.2 ; DNS 伺服器 IP redis.dev-env.local. IN A 192.168.23.21 mysql.dev-env.local. IN A 192.168.23.22 elasticsearch.dev-env.local. IN A 192.168.23.23 ftp IN A 192.168.23.25 ; 這是簡化的寫法！ 详细的格式说明参见 鳥哥的 Linux 私房菜 - DNS 正解資料庫檔案的設定 test-env.local 也是一样的格式，根据上面的模板修改就行。这两个配置文件和 Corefile 放在同一个目录下： root@test-ubuntu:~/dns-server# tree . ├── coredns # coredns binary ├── Corefile ├── dev-env.local └── test-env.local 然后通过 ./coredns 启动 coredns。通过 dig 检验： DNS 测试 可以看到 ftp.dev-env.local 已经被成功解析了。 ","date":"2020-03-29","objectID":"/posts/about-dns-protocol/:8:2","series":null,"tags":["DNS","CoreDNS","网络"],"title":"Linux网络学习笔记（二）：域名解析(DNS)——以 CoreDNS 为例","uri":"/posts/about-dns-protocol/#2-将-coredns-设置成一个私有-dns-服务器"},{"categories":["tech"],"content":" 3. 可选插件（External Plugins）CoreDNS 提供的预编译版本，不包含 External Plugins 中列出的部分，如果你需要，可以自行修改 plugin.cfg，然后手动编译。 不得不说 Go 语言的编译，比 C 语言是方便太多了。自动拉取依赖，一行命令编译！只要配好 GOPROXY，启用可选插件其实相当简单。 ","date":"2020-03-29","objectID":"/posts/about-dns-protocol/:8:3","series":null,"tags":["DNS","CoreDNS","网络"],"title":"Linux网络学习笔记（二）：域名解析(DNS)——以 CoreDNS 为例","uri":"/posts/about-dns-protocol/#3-可选插件external-pluginshttpscorednsioexplugins"},{"categories":["tech"],"content":" 4. 设置 DNS 集群单台 DNS 服务器的性能是有限的，而且存在单点故障问题。因此在要求高可用或者高性能的情况下，就需要设置 DNS 集群。 虽然说 CoreDNS 本身也支持各种 DNS Zone 传输，主从 DNS 服务器等功能，不过我想最简单的，可能还是直接用 K8s。 直接用 ConfigMap 存配置，通过 Deployment 扩容就行，多方便。 要修改起来更方便，还可以启用可选插件：redis，直接把配置以 json 的形式存在 redis 里，通过 redis-desktop-manager 进行查看与修改。 ","date":"2020-03-29","objectID":"/posts/about-dns-protocol/:8:4","series":null,"tags":["DNS","CoreDNS","网络"],"title":"Linux网络学习笔记（二）：域名解析(DNS)——以 CoreDNS 为例","uri":"/posts/about-dns-protocol/#4-设置-dns-集群"},{"categories":["tech"],"content":" 参考 DNS 原理入门 What Is DNS? | How DNS Works - Cloudflare What is DNS? - Amazon AWS 鸟哥的 Linux 私房菜——主機名稱控制者： DNS 伺服器 CoreDNS - Manual Kubernetes - DNS for Services and Pods Kubernetes - Customizing DNS Service ","date":"2020-03-29","objectID":"/posts/about-dns-protocol/:9:0","series":null,"tags":["DNS","CoreDNS","网络"],"title":"Linux网络学习笔记（二）：域名解析(DNS)——以 CoreDNS 为例","uri":"/posts/about-dns-protocol/#参考"},{"categories":["tech"],"content":" 签名算法介绍具体的 JWT 签名算法前，先解释一下签名、摘要/指纹、加密这几个名词的含义： 数字签名(Digital Signature): 就和我们日常办理各种手续时需要在文件上签上你自己的名字一样，数字签名的主要用途也是用于身份认证。 更准确的讲，数字签名可保证数据的三个特性：真实性（未被伪造）、完整性（不存在缺失）、不可否认性（确实是由你本人认可并签名） 数字摘要(digest)/数字指纹(fingerprint): 指的是数据的 Hash 值。 加密算法：这个应该不需要解释，就是对数据进行加密。。 数字签名的具体实现，通常是先对数据进行一次 Hash 摘要(SHA1/SHA256/SHA512 等)，然后再使用非对称加密算法(RSA/ECDSA 等)的私钥对这个摘要进行加密，这样得到的结果就是原始数据的一个签名。 用户在验证数据时，只需要使用公钥解密出 Hash 摘要，然后自己再对数据进行一次同样的摘要，对比两个摘要是否相同即可。 注意：签名算法是使用私钥加密，确保得到的签名无法被伪造，同时所有人都可以使用公钥解密来验证签名。这和正常的数据加密算法是相反的。（非对称加密算法支持使用密钥对的任何一个加密数据，再用另一个密钥解密） 因为数字签名多了非对称加密这一步，就能保证只有拥有私钥的人才能生成出正确的数字签名，达到了防止伪造签名的目的。 而数字摘要（Hash）则谁都可以计算出来，通常由可信方公布数据的 Hash 值，用户下载数据后，可通过 Hash 值对比来判断数据是否损坏，或者被人调包。 重点在于，Hash 摘要必须由可信方公布出来，否则不能保证安全性。而数字签名可以随数据一起提供，不需要担心被伪造。 JWT 是签名和数据一起提供的，因此必须使用签名才能保证安全性。 P.S. 在 Android/IOS 开发中，经常会遇到各类 API 或者 APP 商店要求提供 APP 的签名，还指明需要的是 MD5/SHA1 值。 这个地方需要填的 MD5/SHA1 值，实际上只是你「签名证书(=公钥+证书拥有者信息)」的「数字指纹/摘要」，和 JWT 的签名不是一回事。 ","date":"2020-03-03","objectID":"/posts/jwt-algorithm-key-generation/:0:0","series":null,"tags":["JWT","算法","OpenSSL"],"title":"JWT 签名算法 HS256、RS256 及 ES256 及密钥生成","uri":"/posts/jwt-algorithm-key-generation/#签名算法"},{"categories":["tech"],"content":" 前言JWT 规范的详细说明请见「参考」部分的链接。这里主要说明一下 JWT 最常见的几种签名算法(JWA)：HS256(HMAC-SHA256) 、RS256(RSA-SHA256) 还有 ES256(ECDSA-SHA256)。 这三种算法都是一种消息签名算法，得到的都只是一段无法还原的签名。区别在于消息签名与签名验证需要的 「key」不同。 HS256 使用同一个「secret_key」进行签名与验证（对称加密）。一旦 secret_key 泄漏，就毫无安全性可言了。 因此 HS256 只适合集中式认证，签名和验证都必须由可信方进行。 传统的单体应用广泛使用这种算法，但是请不要在任何分布式的架构中使用它！ RS256 是使用 RSA 私钥进行签名，使用 RSA 公钥进行验证。公钥即使泄漏也毫无影响，只要确保私钥安全就行。 RS256 可以将验证委托给其他应用，只要将公钥给他们就行。 ES256 和 RS256 一样，都使用私钥签名，公钥验证。算法速度上差距也不大，但是它的签名长度相对短很多（省流量），并且算法强度和 RS256 差不多。 对于单体应用而言，HS256 和 RS256 的安全性没有多大差别。 而对于需要进行多方验证的微服务架构而言，显然只有 RS256/ES256 才能提供足够的安全性。 在使用 RS256 时，只有「身份认证的微服务(auth)」需要用 RSA 私钥生成 JWT，其他微服务使用公开的公钥即可进行签名验证，私钥得到了更好的保护。 更进一步，「JWT 生成」和「JWT 公钥分发」都可以直接委托给第三方的通用工具，比如 hydra。 甚至「JWT 验证」也可以委托给「API 网关」来处理，应用自身可以把认证鉴权完全委托给外部的平台，而应用自身只需要专注于业务。这也是目前的发展趋势。 RFC 7518 - JSON Web Algorithms (JWA) 中给出的 JWT 算法列表如下： +--------------+-------------------------------+--------------------+ | \"alg\" Param | Digital Signature or MAC | Implementation | | Value | Algorithm | Requirements | +--------------+-------------------------------+--------------------+ | HS256 | HMAC using SHA-256 | Required | | HS384 | HMAC using SHA-384 | Optional | | HS512 | HMAC using SHA-512 | Optional | | RS256 | RSASSA-PKCS1-v1_5 using | Recommended | | | SHA-256 | | | RS384 | RSASSA-PKCS1-v1_5 using | Optional | | | SHA-384 | | | RS512 | RSASSA-PKCS1-v1_5 using | Optional | | | SHA-512 | | | ES256 | ECDSA using P-256 and SHA-256 | Recommended+ | | ES384 | ECDSA using P-384 and SHA-384 | Optional | | ES512 | ECDSA using P-521 and SHA-512 | Optional | | PS256 | RSASSA-PSS using SHA-256 and | Optional | | | MGF1 with SHA-256 | | | PS384 | RSASSA-PSS using SHA-384 and | Optional | | | MGF1 with SHA-384 | | | PS512 | RSASSA-PSS using SHA-512 and | Optional | | | MGF1 with SHA-512 | | | none | No digital signature or MAC | Optional | | | performed | | +--------------+-------------------------------+--------------------+ The use of \"+\" in the Implementation Requirements column indicates that the requirement strength is likely to be increased in a future version of the specification. 目前应该所有 jwt 相关的库都支持 HS256/RS256/ES256 这三种算法。 ES256 使用 ECDSA 进行签名，它的安全性和运算速度目前和 RS256 差距不大，但是拥有更短的签名长度。 对于需要频繁发送的 JWT 而言，更短的长度长期下来可以节约大量流量。 因此更推荐使用 ES256 算法。 ","date":"2020-03-03","objectID":"/posts/jwt-algorithm-key-generation/:0:0","series":null,"tags":["JWT","算法","OpenSSL"],"title":"JWT 签名算法 HS256、RS256 及 ES256 及密钥生成","uri":"/posts/jwt-algorithm-key-generation/#前言"},{"categories":["tech"],"content":" 使用 OpenSSL 生成 RSA/ECC 公私钥RS256 使用 RSA 算法进行签名，可通过如下命令生成 RSA 密钥： # 1. 生成 2048 位（不是 256 位）的 RSA 密钥 openssl genrsa -out rsa-private-key.pem 2048 # 2. 通过密钥生成公钥 openssl rsa -in rsa-private-key.pem -pubout -out rsa-public-key.pem ES256 使用 ECDSA 算法进行签名，该算法使用 ECC 密钥，生成命令如下： # 1. 生成 ec 算法的私钥，使用 prime256v1 曲线（NIST P-256 标准），密钥长度 256 位。（强度大于 2048 位的 RSA 密钥） openssl ecparam -genkey -name prime256v1 -out ecc-private-key.pem # 2. 通过密钥生成公钥 openssl ec -in ecc-private-key.pem -pubout -out ecc-public-key.pem 密钥的使用应该就不需要介绍了，各类语言都有对应 JWT 库处理这些，请自行查看文档。 如果是调试/学习 JWT，需要手动签名与验证的话，推荐使用 jwt 工具网站 - jwt.io ","date":"2020-03-03","objectID":"/posts/jwt-algorithm-key-generation/:1:0","series":null,"tags":["JWT","算法","OpenSSL"],"title":"JWT 签名算法 HS256、RS256 及 ES256 及密钥生成","uri":"/posts/jwt-algorithm-key-generation/#使用-openssl-生成-rsaecc-公私钥"},{"categories":["tech"],"content":" 参考 RFC 7518 - JSON Web Algorithms (JWA) 什么是 JWT – JSON WEB TOKEN jwt 工具网站 - jwt.io JWT 算法比较 ","date":"2020-03-03","objectID":"/posts/jwt-algorithm-key-generation/:2:0","series":null,"tags":["JWT","算法","OpenSSL"],"title":"JWT 签名算法 HS256、RS256 及 ES256 及密钥生成","uri":"/posts/jwt-algorithm-key-generation/#参考"},{"categories":["life","tech"],"content":" 迟到的年终总结 ","date":"2020-01-31","objectID":"/posts/2019-summary/:0:0","series":null,"tags":["总结"],"title":"2019 年年终总结","uri":"/posts/2019-summary/#"},{"categories":["life","tech"],"content":" 闲言碎语我是今年六月底到的深圳，运气很好，第一面就面上了现在所在的公司，以下就叫它 W 公司吧。 公司的技术栈也很适合我，在入职到现在的这半年里，我学到了不少知识。 但是运气也差，只有这么一家公司约了我面试，投的其他简历都石沉大海… 总之，今年尝试参加过两次技术分享，Rancher 的技术沙龙，前几天又去听了 OSChina 的源创会。 ","date":"2020-01-31","objectID":"/posts/2019-summary/:1:0","series":null,"tags":["总结"],"title":"2019 年年终总结","uri":"/posts/2019-summary/#闲言碎语"},{"categories":["life","tech"],"content":" 技术能力总结我入职后做的是运维开发，主要负责通过 Jenkins Pipeline + Python 进行自动化的测试、构建和部署： 测试：指 UI 测试、API 测试、压力测试。单元测试算在构建流程中。 构建：更新依赖-\u003e单元测试-\u003e构建 Library 或镜像 公司的内部代码使用分层结构，底层封装了各种第三方包，并实现了一些通用的功能，形成了所谓的中台。目前是通过批量任务逐级自下向上构建。 部署：扫描镜像仓库中各镜像，生成最新的 k8s 部署文件，然后进行部署。 所以这半年中，我差不多熟悉了自动化运维的工作。主要包括 Jenkins Pipeline 的编写，我们基本都是使用 Jenkins 调用 Python 代码来进行具体的构建。 公司的构建有很多自己特殊的需求，Jenkins 自带的插件无法满足。 熟悉了 Python 的 subprocess 库，为了远程调用，又熟悉了 fabric（当作 library 用）。 做压测时，熟悉了 locust 因为基本都是通过命令行进行测试、构建，我现在比前后端组还熟悉 csharp/flutter/golang 的 cli… 学会了 Dockerfile 语法。我们的后端全部都是以容器方式部署的，这个是基本技能。 熟悉并且用上了 Kubernetes. 这东西基本上就是未来了，也将是我的主攻方向。 但是，也存在一些问题： 对 Linux/网络/vSphere 不够了解，导致每次处理这类问题只会排除法。 对监控/日志/告警不够了解，监控面板一堆参数却看不出问题，日志不知道怎么用 kibana 进行搜索，告警还没配过。。 解决问题的能力还有待提升，考虑总是不够全面，老是出问题。（不能让人放心） 总是想得太多，拖慢了解决问题的速度。（这倒也不能完全算是缺点。） ","date":"2020-01-31","objectID":"/posts/2019-summary/:2:0","series":null,"tags":["总结"],"title":"2019 年年终总结","uri":"/posts/2019-summary/#技术能力总结"},{"categories":["life","tech"],"content":" 今年在技术上的感受 Kubernetes 和云原生正在席卷整个互联网/物联网行业。 Kubernetes 目前主要用于 Stateless 应用，那后端的 数据库/缓存/消息服务 要如何做分布式呢？这也是大家关注的重点。 分布式、微服务模式下的监控(prometheus)、日志分析(elk)、安全、链路追踪(jaeger)，是运维关注的重点。 服务网格正在走向成熟，Istio 很值得学习和试用。 开源的分布式数据库/云数据库成为越来越多企业的选择，开源的 TiDB（HTAP）和阿里云的 PolarDB（计算存储分离）都应该了解了解。 Transaction Processing: 面向交易，数据的变动(增删改)多，涉及的数据量和计算量(查)少，实时性要求高。 Analytical Processing：面向分析，数据的变动少，但涉及的数据量和计算量很多！ HTAP（Hybrid transaction/analytical processing）：混合型数据库，可同时被用于上述两种场景。 Knative/Jenkins-X 这类 Serverless 的 CI/CD 也正在快速发展，需要深入调研。 ","date":"2020-01-31","objectID":"/posts/2019-summary/:3:0","series":null,"tags":["总结"],"title":"2019 年年终总结","uri":"/posts/2019-summary/#今年在技术上的感受"},{"categories":["life","tech"],"content":" 明年的展望作为一名萌新运维开发，明年显然还要继续在这条路上继续向前。 我明年的任务，第一件，就是优化掉部分自己目前存在的问题（前面有提到），第二呢，就是紧跟技术潮流。重点有下面几项： 充实自己网络部分欠缺的知识，尤其是 DNS 解析(CoreDNS)和 NAT(iptables)这俩玩意儿。 学习数据库组件的使用和性能调优：MySQL/Redis/ElasticSearch/MongoDB，另外熟悉 PostgreSQL 和分布式数据库 TiDB/Vitess Kubernetes/Istio Kubernetes 上的 CI/CD：Knative, Istio-GitOps 监控告警：Prometheus/Grafana 总结一套故障排除的方法论：网络故障、CPU/RAM/Disk 性能异常等、应用故障等。 最重要的任务，是维护公司这一套微服务在阿里云上的正常运行，积累经验。 关注 CNCF 蓝图 上的各项新技术。 另外呢，就是开发方面的任务： 设计模式应该要学学了！ Python 不能止步于此，要制定源码学习计划。 学习 C# 语言，阅读公司的源码，熟悉企业级的业务代码。 学习 go 语言，用于 DevOps。（其实还想学 rust，不过明年可能没时间） 要把 xhup 那个项目完成，也不知道能不能抽出时间。。 ","date":"2020-01-31","objectID":"/posts/2019-summary/:4:0","series":null,"tags":["总结"],"title":"2019 年年终总结","uri":"/posts/2019-summary/#明年的展望"},{"categories":["tech"],"content":" Pod 常见错误 OOMKilled: Pod 的内存使用超出了 resources.limits 中的限制，被强制杀死。 SandboxChanged: Pod sandbox changed, it will be killed and re-created: 很可能是由于内存限制导致容器被 OOMKilled，或者其他资源不足 如果是 OOM，容器通常会被重启，kubectl describe 能看到容器上次被重启的原因 State.Last State.Reason = OOMKilled, Exit Code=137. Pod 不断被重启，kubectl describe 显示重启原因 State.Last State.Reason = Error, Exit Code=137，137 对应 SIGKILL(kill -9) 信号，说明容器被强制重启。可能的原因： 最有可能的原因是，存活探针（livenessProbe）检查失败 节点资源不足，内核强制关闭了进程以释放资源，这种情况可以通过 journalctl -k 查看详细的系统日志。 CrashLoopBackoff: Pod 进入 崩溃-重启循环，重启间隔时间从 10 20 40 80 一直翻倍到上限 300 秒，然后以 300 秒为间隔无限重启。 Pod 一直 Pending: 这说明没有任何节点能满足 Pod 的要求，容器无法被调度。比如端口被别的容器用 hostPort 占用，节点有污点等。 FailedCreateSandBox: Failed create pod sandbox: rpc error: code = DeadlineExceeded desc = context deadline exceeded：很可能是 CNI 网络插件的问题（比如 ip 地址溢出）， FailedSync: error determining status: rpc error: code = DeadlineExceeded desc = context deadline exceeded: 常和前两个错误先后出现，很可能是 CNI 网络插件的问题。 开发集群，一次性部署所有服务时，各 Pod 互相争抢资源，导致 Pod 生存探针失败，不断重启，重启进一步加重资源使用。恶性循环。 需要给每个 Pod 加上 resources.requests，这样资源不足时，后续 Pod 会停止调度，直到资源恢复正常。 Pod 出现大量的 Failed 记录，Deployment 一直重复建立 Pod: 通过 kubectl describe/edit pod \u003cpod-name\u003e 查看 pod Events 和 Status，一般会看到失败信息，如节点异常导致 Pod 被驱逐。 Kubernetes 问题排查：Pod 状态一直 Terminating 创建了 Deployment 后，却没有自动创建 Pod: 缺少某些创建 Pod 必要的东西，比如设定的 ServiceAccount 不存在。 Pod 运行失败，状态为 MatchNodeSelector: 对主节点进行关机、迁移等操作，导致主调度器下线时，会在一段时间内导致 Pod 调度失败，调度失败会报这个错。 Pod 仍然存在，但是 Service 的 Endpoints 却为空，找不到对应的 Pod IPs: 遇到过一次，是因为时间跳变（从未来的时间改回了当前时间）导致的问题。 Pod 无法调度，报错 x node(s) had volume node affinity conflict: 说明该 pod 所绑定的 PV 有 nodeAffinity 无法满足，可以 check 对应的 PV yaml. 通常原因是 PV 所在的可用区，没有可用的节点，导致 Pod 无法调度。 最简单的解决方法是，在对应的可用区补充节点 如果数据可以丢，也可以考虑直接删除重建 PV/PVC ","date":"2019-11-24","objectID":"/posts/kubernetes-common-errors-and-solutions/:1:0","series":null,"tags":["Kubernetes"],"title":"Kubernetes 常见错误、原因及处理方法","uri":"/posts/kubernetes-common-errors-and-solutions/#pod-常见错误"},{"categories":["tech"],"content":" 控制面故障可能会导致各类奇怪的异常现象对于生产环境的集群，因为有高可用，通常我们比较少遇到控制面故障问题。但是一旦控制面发生故障，就可能会导致各类奇怪的异常现象。 如果能在排查问题时，把控制面异常考虑进来，在这种情况下，就能节约大量的排查时间，快速定位到问题。 其中比较隐晦的就是 controller-manager 故障导致的异常： 节点的服务器已经被终止，但是 Kuberntes 里还显示 node 为 Ready 状态，不会更新为 NotReady. 被删除的 Pods 可能会卡在 Terminating 状态，只有强制删除才能删除掉它们。并且确认 Pod 没有 metadata.finalizers 属性 HPA 的动态伸缩功能失效 … 如果这些现象同时发生，就要怀疑是否是 kube-controller-manager 出问题了. 其他控制面异常的详细分析，参见 kubernetes 控制面故障现象及分析 ","date":"2019-11-24","objectID":"/posts/kubernetes-common-errors-and-solutions/:1:1","series":null,"tags":["Kubernetes"],"title":"Kubernetes 常见错误、原因及处理方法","uri":"/posts/kubernetes-common-errors-and-solutions/#控制面故障可能会导致各类奇怪的异常现象"},{"categories":["tech"],"content":" Pod 无法删除可能是某些资源无法被GC，这会导致容器已经 Exited 了，但是 Pod 一直处于 Terminating 状态。 这个问题在网上能搜到很多案例,但大都只是提供了如下的强制清理命令，未分析具体原因： kubectl delete pods \u003cpod\u003e --grace-period=0 --force 最近找到几篇详细的原因分析文章，值得一看： 腾讯云原生 -【Pod Terminating原因追踪系列】之 containerd 中被漏掉的 runc 错误信息 腾讯云原生 -【Pod Terminating原因追踪系列之二】exec连接未关闭导致的事件阻塞 腾讯云原生 -【Pod Terminating原因追踪系列之三】让docker事件处理罢工的cancel状态码 Pod terminating - 问题排查 - KaKu Li 大致总结一下，主要原因来自 docker 18.06 以及 kubernetes 的 docker-shim 运行时的底层逻辑，已经在新版本被修复了。 ","date":"2019-11-24","objectID":"/posts/kubernetes-common-errors-and-solutions/:1:2","series":null,"tags":["Kubernetes"],"title":"Kubernetes 常见错误、原因及处理方法","uri":"/posts/kubernetes-common-errors-and-solutions/#pod-无法删除"},{"categories":["tech"],"content":" initContainers 不断 restart，但是 Containers 却都显示已 readyKubernetes 应该确保所有 initContainers 都 Completed，然后才能启动 Containers. 但是我们发现有一个节点上，所有包含 initContainers 的 Pod，状态全都是 Init:CrashLoopBackOff 或者 Init:Error. 而且进一步 kubectl describe po 查看细节，发现 initContainer 的状态为: ... State: Waiting Reason: CrashLoopBackOff Last State: Terminated Reason: Error Exit Code: 2 Started: Tue, 03 Aug 2021 06:02:42 +0000 Finished: Tue, 03 Aug 2021 06:02:42 +0000 Ready: False Restart Count: 67 ... 而 Containers 的状态居然是 ready: ... Host Port: 0/TCP State: Running Started: Tue, 03 Aug 2021 00:35:30 +0000 Ready: True Restart Count: 0 ... initContainers 还未运行成功，而 Containers 却 Ready 了，非常疑惑。 仔细想了下，早上因为磁盘余量告警，有手动运行过 docker system prune 命令，那么问题可能就是这条命令清理掉了已经 exited 的 initContainers 容器，导致 k8s 故障，不断尝试重启该容器。 网上一搜确实有相关的信息： https://stackoverflow.com/questions/62333064/cant-delete-exited-init-container https://github.com/kubernetes/kubernetes/issues/62362 结论：使用外部的垃圾清理命令可能导致 k8s 行为异常。 ","date":"2019-11-24","objectID":"/posts/kubernetes-common-errors-and-solutions/:1:3","series":null,"tags":["Kubernetes"],"title":"Kubernetes 常见错误、原因及处理方法","uri":"/posts/kubernetes-common-errors-and-solutions/#initcontainers-不断-restart但是-containers-却都显示已-ready"},{"categories":["tech"],"content":" 节点常见错误 DiskPressure：节点的可用空间不足。（通过df -h 查看，保证可用空间不小于 15%） The node was low on resource: ephemeral-storage: 同上，节点的存储空间不够了。 节点存储告警可能的原因： kubelet 的资源 GC 设置有问题，遗留的镜像等资源未及时 GC 导致告警 存在运行的 pod 使用了大量存储空间，在节点上通过 docker ps -a --size | grep G 可以查看到 如果使用的是 EKS，并且磁盘告警的挂载点为 /var/lib/kubelet/plugins/kubernetes.io/aws-ebs/mounts/aws/us-east-1b/vol-xxxxx 显然是 EBS 存储卷快满了导致的 可通过 kubectl get pv -A -o yaml | grep -C 30 vol-xxxxx 来定位到具体的存储卷 ","date":"2019-11-24","objectID":"/posts/kubernetes-common-errors-and-solutions/:2:0","series":null,"tags":["Kubernetes"],"title":"Kubernetes 常见错误、原因及处理方法","uri":"/posts/kubernetes-common-errors-and-solutions/#节点常见错误"},{"categories":["tech"],"content":" 网络常见错误","date":"2019-11-24","objectID":"/posts/kubernetes-common-errors-and-solutions/:3:0","series":null,"tags":["Kubernetes"],"title":"Kubernetes 常见错误、原因及处理方法","uri":"/posts/kubernetes-common-errors-and-solutions/#网络常见错误"},{"categories":["tech"],"content":" 1. Ingress/Istio Gateway 返回值 404：不存在该 Service/Istio Gateway，或者是服务自身返回 404 500：大概率是服务自身的错误导致 500，小概率是代理（Sidecar/Ingress 等）的错误 503：服务不可用，有如下几种可能的原因： Service 对应的 Pods 不存在，endpoints 为空 Service 对应的 Pods 全部都 NotReady，导致 endpoints 为空 也有可能是服务自身出错返回的 503 如果你使用了 envoy sidecar， 503 可能的原因就多了。基本上 sidecar 与主容器通信过程中的任何问题都会使 envoy 返回 503，使客户端重试。 详见 Istio：503、UC 和 TCP 502：Bad Gateway，通常是由于上游未返回正确的响应导致的，可能的根本原因： 应用程序未正确处理 SIGTERM 信号，在请求未处理完毕时直接终止了进程。详见 优雅停止（Gracful Shutdown）与 502/504 报错 - K8s 最佳实践 网络插件 bug 504：网关请求 upstream 超时，主要有两种可能 考虑是不是 Ingress Controller 的 IP 列表未更新，将请求代理到了不存在的 ip，导致得不到响应 Service Endpoints 移除不够及时，在 Pod 已经被终止后，仍然有个别请求被路由到了该 Pod，得不到响应导致 504。详见 优雅停止（Gracful Shutdown）与 502/504 报错 - K8s 最佳实践 Pod 响应太慢，代码问题 再总结一下常见的几种错误： 未设置优雅停止，导致 Pod 被重新终止时，有概率出现 502/504 服务的所有 Pods 的状态在「就绪」和「未就绪」之间摆动，导致间歇性地出现大量 503 错误 服务返回 5xx 错误导致客户端不断重试，请求流量被放大，导致服务一直起不来 解决办法：限流、熔断（网关层直接返回固定的相应内容） Ingress 相关网络问题的排查流程： Which ingress controller? Timeout between client and ingress controller, or between ingress controller and backend service/pod? HTTP/504 generated by the ingress controller, proven by logs from the ingress controller? If you port-forward to skip the internet between client and ingress controller, does the timeout still happen? ","date":"2019-11-24","objectID":"/posts/kubernetes-common-errors-and-solutions/:3:1","series":null,"tags":["Kubernetes"],"title":"Kubernetes 常见错误、原因及处理方法","uri":"/posts/kubernetes-common-errors-and-solutions/#1-ingressistio-gateway-返回值"},{"categories":["tech"],"content":" 2. 上了 istio sidecar 后，应用程序偶尔（间隔几天半个月）会 redis 连接相关的错误考虑是否和 tcp 长时间使用有关，比如连接长时间空闲的话，可能会被 istio sidecar 断开。 如果程序自身的重连机制有问题，就会导致这种现象。 确认方法： 检查 istio 的 idleTimeout 时长（默认 1h） 创建三五个没流量的 Pod 放置 1h（与 istio idleTimeout 时长一致），看看是否会准时开始报 redis 的错。 对照组：创建三五个同样没流量的 Pod，但是不注入 istio sidecar，应该一直很正常 这样就能确认问题，后续处理： 抓包观察程序在出错后的 tcp 层行为 查阅 redis sdk 的相关 issue、代码，通过升级 SDK 应该能解决问题。 ","date":"2019-11-24","objectID":"/posts/kubernetes-common-errors-and-solutions/:3:2","series":null,"tags":["Kubernetes"],"title":"Kubernetes 常见错误、原因及处理方法","uri":"/posts/kubernetes-common-errors-and-solutions/#2-上了-istio-sidecar-后应用程序偶尔间隔几天半个月会-redis-连接相关的错误"},{"categories":["tech"],"content":" 名字空间常见错误","date":"2019-11-24","objectID":"/posts/kubernetes-common-errors-and-solutions/:4:0","series":null,"tags":["Kubernetes"],"title":"Kubernetes 常见错误、原因及处理方法","uri":"/posts/kubernetes-common-errors-and-solutions/#名字空间常见错误"},{"categories":["tech"],"content":" 名字空间无法删除这通常是某些资源如 CR(custom resources)/存储等资源无法释放导致的。 比如常见的 monitoring 名字空间无法删除，应该就是 CR 无法 GC 导致的。 可手动删除 namespace 配置中的析构器（spec.finalizer，在名字空间生命周期结束前会生成的配置项），这样名字空间就会直接跳过 GC 步骤： # 编辑名字空间的配置 kubectl edit namespace \u003cns-name\u003e # 将 spec.finalizers 改成空列表 [] 如果上述方法也无法删除名字空间，也找不到具体的问题，就只能直接从 etcd 中删除掉它了(有风险，谨慎操作！)。方法如下： # 登录到 etcd 容器中，执行如下命令： export ETCDCTL_API=3 cd /etc/kubernetes/pki/etcd/ # 列出所有名字空间 etcdctl --cacert ca.crt --cert peer.crt --key peer.key get /registry/namespaces --prefix --keys-only # （谨慎操作！！！）强制删除名字空间 `monitoring`。这可能导致相关资源无法被 GC！ etcdctl --cacert ca.crt --cert peer.crt --key peer.key del /registry/namespaces/monitoring ","date":"2019-11-24","objectID":"/posts/kubernetes-common-errors-and-solutions/:4:1","series":null,"tags":["Kubernetes"],"title":"Kubernetes 常见错误、原因及处理方法","uri":"/posts/kubernetes-common-errors-and-solutions/#名字空间无法删除"},{"categories":["tech"],"content":" kubectl/istioctl 等客户端工具异常 socat not found: kubectl 使用 socat 进行端口转发，集群的所有节点，以及本机都必须安装有 socat 工具。 ","date":"2019-11-24","objectID":"/posts/kubernetes-common-errors-and-solutions/:5:0","series":null,"tags":["Kubernetes"],"title":"Kubernetes 常见错误、原因及处理方法","uri":"/posts/kubernetes-common-errors-and-solutions/#kubectlistioctl-等客户端工具异常"},{"categories":["tech"],"content":" 批量清理 Evicted 记录有时候 Pod 因为节点选择器的问题，被不断调度到有问题的 Node 上，就会不断被 Evicted，导致出现大量的 Evicted Pods。 排查完问题后，需要手动清理掉这些 Evicted Pods. 批量删除 Evicted 记录: kubectl get pods | grep Evicted | awk '{print $1}' | xargs kubectl delete pod ","date":"2019-11-24","objectID":"/posts/kubernetes-common-errors-and-solutions/:6:0","series":null,"tags":["Kubernetes"],"title":"Kubernetes 常见错误、原因及处理方法","uri":"/posts/kubernetes-common-errors-and-solutions/#批量清理-evicted-记录"},{"categories":["tech"],"content":" 容器镜像GC、Pod驱逐以及节点压力节点压力 DiskPressure 会导致 Pod 被驱逐，也会触发容器镜像的 GC。 根据官方文档 配置资源不足时的处理方式，Kubelet 提供如下用于配置容器 GC 及 Evicetion 的阈值： --eviction-hard 和 eviction-soft: 对应旧参数 --image-gc-high-threshold，这两个参数配置镜像 GC 及驱逐的触发阈值。磁盘使用率的阈值默认为 85% 区别在于 eviction-hard 是立即驱逐，而 eviction-soft 在超过 eviction-soft-grace-period 之后才驱逐。 --eviction-minimum-reclaim: 对应旧参数 --image-gc-low-threshold。这是进行资源回收（镜像GC、Pod驱逐等）后期望达到的磁盘使用率百分比。磁盘使用率的阈值默认值为 80%。 问：能否为 ImageGC 设置一个比 DiskPressure 更低的阈值？因为我们希望能自动进行镜像 GC，但是不想立即触发 Pod 驱逐。 答：这应该可以通过设置 eviction-soft 和长一点的 eviction-soft-grace-period 来实现。 另外 --eviction-minimum-reclaim 也可以设小一点，清理得更干净。示例如下： --eviction-soft=memory.available\u003c1Gi,nodefs.available\u003c2Gi,imagefs.available\u003c200Gi --eviction-soft-grace-period=3m --eviction-minimum-reclaim=memory.available=0Mi,nodefs.available=1Gi,imagefs.available=2Gi ","date":"2019-11-24","objectID":"/posts/kubernetes-common-errors-and-solutions/:7:0","series":null,"tags":["Kubernetes"],"title":"Kubernetes 常见错误、原因及处理方法","uri":"/posts/kubernetes-common-errors-and-solutions/#容器镜像gcpod驱逐以及节点压力"},{"categories":["tech"],"content":" 其他问题","date":"2019-11-24","objectID":"/posts/kubernetes-common-errors-and-solutions/:8:0","series":null,"tags":["Kubernetes"],"title":"Kubernetes 常见错误、原因及处理方法","uri":"/posts/kubernetes-common-errors-and-solutions/#其他问题"},{"categories":["tech"],"content":" 隔天 Istio 等工具的 sidecar 自动注入莫名其妙失效了如果服务器晚上会关机，可能导致第二天网络插件出问题，导致 sidecar 注入器无法观察到 pod 的创建，也就无法完成 sidecar 注入。 ","date":"2019-11-24","objectID":"/posts/kubernetes-common-errors-and-solutions/:9:0","series":null,"tags":["Kubernetes"],"title":"Kubernetes 常见错误、原因及处理方法","uri":"/posts/kubernetes-common-errors-and-solutions/#隔天-istio-等工具的-sidecar-自动注入莫名其妙失效了"},{"categories":["tech"],"content":" 如何重新运行一个 Job？我们有一个 Job 因为外部原因运行失败了，修复好后就需要重新运行它。 方法是：删除旧的 Job，再使用同一份配置重建 Job. 如果你使用的是 fluxcd 这类 GitOps 工具，就只需要手工删除旧 Pod，fluxcd 会定时自动 apply 所有配置，这就完成了 Job 的重建。 ","date":"2019-11-24","objectID":"/posts/kubernetes-common-errors-and-solutions/:9:1","series":null,"tags":["Kubernetes"],"title":"Kubernetes 常见错误、原因及处理方法","uri":"/posts/kubernetes-common-errors-and-solutions/#如何重新运行一个-job"},{"categories":["tech"],"content":" 参考 Kubernetes管理经验 504 Gateway Timeout when accessing workload via ingress Kubernetes Failure Stories Istio：503、UC 和 TCP istio 实践指南 - imroc.cc Kubernetes 实践指南 - imroc.cc ","date":"2019-11-24","objectID":"/posts/kubernetes-common-errors-and-solutions/:10:0","series":null,"tags":["Kubernetes"],"title":"Kubernetes 常见错误、原因及处理方法","uri":"/posts/kubernetes-common-errors-and-solutions/#参考"},{"categories":["tech"],"content":"Manjaro 是一个基于 Arch Linux 的 Linux 滚动发行版，用着挺舒服的。 最大的特点，是包仓库很丰富，而且都很新。代价是偶尔会出些小毛病。 2021-09-22 更新：今天被群友科普，可能我下面列举的几个滚挂事件，可能都和我使用了 archlinuxcn 这个源有关，这确实有可能。 我一年多的使用中，遇到过 qv2-ray 动态链接库炸掉的问题，没专门去找修复方法，好像是等了一两个月，升级了两个大版本才恢复。 另一个就是 VSCode - Incorrect locale ’en-US’ used everywhere 还遇到过 libguestfs 的一个问题：vrit-v2v/virt-p2v 两个工具被拆分出去，导致 manjaro 只能通过源码安装这俩货。这貌似目前仍旧没有解决。 总的来说体验很不错，能很及时地用上各种新版本的软件。 ","date":"2019-07-13","objectID":"/posts/manjaro-instruction/:0:0","series":null,"tags":["Manjaro","Linux","Arch Linux"],"title":"Manjaro 使用指南","uri":"/posts/manjaro-instruction/#"},{"categories":["tech"],"content":" 一、pacman/yay 的基础命令Manjaro 装好后，需要运行的第一条命令： sudo pacman -Syy ## 强制更新 package 目录 sudo pacman-mirrors --interactive --country China # 列出所有国内的镜像源，并提供交互式的界面手动选择镜像源 sudo pacman -Syyu # 强制更新 package 目录，并尝试更新已安装的所有 packages. sudo pacman -S yay # 安装 yay pacman 是 arch/manjaro 的官方包管理器，而刚刚安装的 yay，则是一个能查询 arch linux 的 aur 仓库的第三方包管理器，非常流行。 pacman 的常用命令语法： pacman -S package_name # 安装软件 pacman -S extra/package_name # 安装不同仓库中的版本 pacman -Syu # 升级整个系统，y是更新数据库，yy是强制更新，u是升级软件 pacman -Ss string # 在包数据库中查询软件 pacman -Si package_name # 显示软件的详细信息 pacman -Sc # 清除软件缓存，即/var/cache/pacman/pkg目录下的文件 pacman -R package_name # 删除单个软件 pacman -Rs package_name # 删除指定软件及其没有被其他已安装软件使用的依赖关系 pacman -Qs string # 查询已安装的软件包 pacman -Qi package_name # 查询本地安装包的详细信息 pacman -Ql package_name # 获取已安装软件所包含的文件的列表 pacman -U package.tar.zx # 从本地文件安装 pactree package_name # 显示软件的依赖树 yay 的用法和 pacman 完全类似，上述所有 pacman xxx 命令，均可替换成 yay xxx 执行。 此外，还有一条 yay 命令值得记一下： yay -c # 卸载所有无用的依赖。类比 apt-get autoremove ","date":"2019-07-13","objectID":"/posts/manjaro-instruction/:1:0","series":null,"tags":["Manjaro","Linux","Arch Linux"],"title":"Manjaro 使用指南","uri":"/posts/manjaro-instruction/#一pacmanyay-的基础命令"},{"categories":["tech"],"content":" 常用软件与配置","date":"2019-07-13","objectID":"/posts/manjaro-instruction/:2:0","series":null,"tags":["Manjaro","Linux","Arch Linux"],"title":"Manjaro 使用指南","uri":"/posts/manjaro-instruction/#常用软件与配置"},{"categories":["tech"],"content":" 1. 添加 archlinux 中文社区仓库Arch Linux 中文社区仓库 是由 Arch Linux 中文社区驱动的非官方用户仓库，包含一些额外的软件包以及已有软件的 git 版本等变种。部分软件包的打包脚本来源于 AUR。 一些国内软件，如果直接从 aur 安装，那就会有一个编译过程，有点慢。而 archlinuxcn 有已经编译好的包，可以直接安装。更新速度也很快，推荐使用。 配置方法见 Arch Linux Chinese Community Repository。 ","date":"2019-07-13","objectID":"/posts/manjaro-instruction/:2:1","series":null,"tags":["Manjaro","Linux","Arch Linux"],"title":"Manjaro 使用指南","uri":"/posts/manjaro-instruction/#1-添加-archlinux-中文社区仓库"},{"categories":["tech"],"content":" 2. 安装常用软件 sudo pacman -S google-chrome firefox # 浏览器 sudo pacman -S netease-cloud-music # 网易云音乐 sudo pacman -S noto-fonts-cjk wqy-bitmapfont wqy-microhei wqy-zenhei # 中文字体：思源系列、文泉系列 sudo pacman -S wps-office ttf-wps-fonts sudo pacman -S vim # 命令行编辑器 sudo pacman -S git # 版本管理工具 sudo pacman -S clang make cmake gdb # 编译调试环境 sudo pacman -S visual-studio-code-bin # 代码编辑器 sudo pacman -S wireshark-qt mitmproxy # 抓包工具 sudo pacman -S docker # docker 容器 其中 docker 和 wireshark 需要额外配置，否则会要求管理员权限： sudo groupadd wireshark sudo gpasswd --add $USER wireshark # 将你添加到 wireshark 用户组中 sudo groupadd docker sudo gpasswd --add $USER docker # 同上 ","date":"2019-07-13","objectID":"/posts/manjaro-instruction/:2:2","series":null,"tags":["Manjaro","Linux","Arch Linux"],"title":"Manjaro 使用指南","uri":"/posts/manjaro-instruction/#2-安装常用软件"},{"categories":["tech"],"content":" 3. 中文输入法有两个选择：中州韵（rime）和搜狗拼音（sogoupinyin）。 简单省事用搜狗，要用特殊的输入方案（五笔、音形、二笔等等）就只有 rime 可选了。 3.1 fcitx5-rime 配置小鹤音形首先安装 fcitx5-rime, 注意这些组件一个都不能省略： sudo pacman -S fcitx5 fcitx5-chinese-addons fcitx5-gtk fcitx5-qt kcm-fcitx5 fcitx5-rime 第二步是修改环境变量，将 fcitx5-rime 设为默认输入法并自动启动。 添加 ~/.pam_environment 文件，内容如下： INPUT_METHOD DEFAULT=fcitx5 GTK_IM_MODULE DEFAULT=fcitx5 QT_IM_MODULE DEFAULT=fcitx5 XMODIFIERS DEFAULT=@im=fcitx5 pam-env 模块会在所有登录会话中读取上面的配置文件，包括 X11 会话和 Wayland 会话。 添加自动启动： # ~/.xprofile 是 x11 GUI 的环境变量配置文件 echo 'fcitx5 \u0026' \u003e\u003e ~/.xprofile 然后，从 http://flypy.ys168.com/ 下载最新的鼠须管（MacOS）配置文件，将解压得到的 rime 文件夹拷贝到 ~/.local/share/fcitx5/ 下： mv rime ~/.local/share/fcitx5/ 现在重启系统，在 fcitx5 配置里面添加 rime，就可以正常使用小鹤音形了。 ","date":"2019-07-13","objectID":"/posts/manjaro-instruction/:2:3","series":null,"tags":["Manjaro","Linux","Arch Linux"],"title":"Manjaro 使用指南","uri":"/posts/manjaro-instruction/#3-中文输入法"},{"categories":["tech"],"content":" 3. 中文输入法有两个选择：中州韵（rime）和搜狗拼音（sogoupinyin）。 简单省事用搜狗，要用特殊的输入方案（五笔、音形、二笔等等）就只有 rime 可选了。 3.1 fcitx5-rime 配置小鹤音形首先安装 fcitx5-rime, 注意这些组件一个都不能省略： sudo pacman -S fcitx5 fcitx5-chinese-addons fcitx5-gtk fcitx5-qt kcm-fcitx5 fcitx5-rime 第二步是修改环境变量，将 fcitx5-rime 设为默认输入法并自动启动。 添加 ~/.pam_environment 文件，内容如下： INPUT_METHOD DEFAULT=fcitx5 GTK_IM_MODULE DEFAULT=fcitx5 QT_IM_MODULE DEFAULT=fcitx5 XMODIFIERS DEFAULT=@im=fcitx5 pam-env 模块会在所有登录会话中读取上面的配置文件，包括 X11 会话和 Wayland 会话。 添加自动启动： # ~/.xprofile 是 x11 GUI 的环境变量配置文件 echo 'fcitx5 \u0026' \u003e\u003e ~/.xprofile 然后，从 http://flypy.ys168.com/ 下载最新的鼠须管（MacOS）配置文件，将解压得到的 rime 文件夹拷贝到 ~/.local/share/fcitx5/ 下： mv rime ~/.local/share/fcitx5/ 现在重启系统，在 fcitx5 配置里面添加 rime，就可以正常使用小鹤音形了。 ","date":"2019-07-13","objectID":"/posts/manjaro-instruction/:2:3","series":null,"tags":["Manjaro","Linux","Arch Linux"],"title":"Manjaro 使用指南","uri":"/posts/manjaro-instruction/#31-fcitx5-rime-配置小鹤音形"},{"categories":["tech"],"content":" 坑使用过程中，我也遇到了一些坑： 安装软件包时，无法在线安装旧版本！除非你本地有旧版本的安装包没清除，才可以通过缓存安装旧版本。 这种问题没遇到时好说，但有时候新版本有问题，旧安装包也清理掉了无法回退，就非常麻烦。 而且就算你回退了版本，一升级它就又更新了。。 ","date":"2019-07-13","objectID":"/posts/manjaro-instruction/:3:0","series":null,"tags":["Manjaro","Linux","Arch Linux"],"title":"Manjaro 使用指南","uri":"/posts/manjaro-instruction/#坑"},{"categories":["tech"],"content":" 彻底删除 Manjaro 及其引导项最近(2021-01)切换到了 OpenSUSE，体验很好，于是决定删除掉 Manjaro。 一番操作，总结出的删除流程如下（以下命令均需要 root 权限）： # 1. 删除 EFI 引导项 ## 查看 efi 的所有启动项，找到 Manjaro 的编号 efibootmgr ## 删除掉 Manjaro 启动项 sudo efibootmgr --delete-bootnum -b 2 # 2. 删除 manjaro 的 bootloader ## 我使用了 manjaro 默认的安装策略，bootloader 被安装在了和 windows 相同的 EFI 分区下 ## 首先通过 opnsuse 的分区工具，找到 EFI 分区的设备号，然后挂载它 mkdir efi mount /dev/nvme0n1p1 efi # 删除 Manjaro bootloader rm -r EFI/Manjaro # 3. 重建 grub2 引导项 grub2-mkconfig \u003e /boot/grub2/grub.cfg # 4. 最后，通过分区工具删除 Manjaro 的所有分区，我是 SSD，只有一个分区 # 5. 重启系统，所有东西就全删除干净了。 ","date":"2019-07-13","objectID":"/posts/manjaro-instruction/:4:0","series":null,"tags":["Manjaro","Linux","Arch Linux"],"title":"Manjaro 使用指南","uri":"/posts/manjaro-instruction/#彻底删除-manjaro-及其引导项"},{"categories":["tech"],"content":" 参考 Arch Linux Wiki - 中文 AUR 仓库 Arch Linux 中文社区仓库 yay - Yet another Yogurt - An AUR Helper written in Go 安装Manjaro之后的配置 Arch Linux Wiki - Fcitx5 ","date":"2019-07-13","objectID":"/posts/manjaro-instruction/:5:0","series":null,"tags":["Manjaro","Linux","Arch Linux"],"title":"Manjaro 使用指南","uri":"/posts/manjaro-instruction/#参考"},{"categories":["life"],"content":"四年大学惨淡收场，坐在火车硬座上，心里有些忐忑。 对面坐着一个 16 岁的女孩子，从合肥去深圳当安检，隐约听到“700 块一个月”，“再怎么着十天也有一休吧”，说实话深感惭愧。 小时候我妈对我有点保护过度，从小到大几乎没下田干过活，在家帮忙干过的活屈指可数。 因为读书尚可，稀里糊涂读了十多年书，现在惨淡收场。 而对面女孩子 16 岁，已经要去离家这么远的地方实习了（当安检），而且还听到对面说“想和他分了，每次都是我给他打电话，他从来没主动过，现在还抱怨我电话打少了……” 感觉我 22 年，有点白活了。。 可即使这样，还是提不起多大动力去复习面试用的知识点。 心里慌的不行，可游戏照打不误。还瞒着家里，花白条分期买了个 6000 的小米游戏本，一月要还 500，万一工作找得不理想，我不知道这个钱窟窿到时候该怎么填上。。。 我的四年大学好像也有过些高光时刻，也有过许多值得铭记的欢乐时光，临到头来却是这么个惨淡的结尾。 也有想过努力努力，可一懒散起来，就有了借口——“当初差点高考都没参加，就直接退学了，现在起码还读了四年的三流一本，见了世面，赚到了。” 不知道十年后我再回首，会不会觉得现在内心的忐忑，不算什么。 总之呢，因为一句「搞计算机的话，深圳工作应该很多吧，不如过来找工作？」，我上了这趟火车。 学业啥的都随它去吧，是不是单车变摩托，就看这一把了… ","date":"2019-06-20","objectID":"/posts/escape-my-university/:0:0","series":null,"tags":[],"title":"逃离我的大学","uri":"/posts/escape-my-university/#"},{"categories":["tech"],"content":" 个人笔记，不保证正确。 ","date":"2019-05-21","objectID":"/posts/sqlalchemy-notes-3-relationship-and-foreignkey/:0:0","series":null,"tags":["SQLAlchemy","Python","ORM","后端","数据库","Database"],"title":"SQLAlchemy 学习笔记（三）：ORM 中的关系构建","uri":"/posts/sqlalchemy-notes-3-relationship-and-foreignkey/#"},{"categories":["tech"],"content":" 一、关系构建：ForeignKey 与 relationship关系构建的重点，在于搞清楚这两个函数的用法。ForeignKey 的用法已经在 SQL表达式语言 - 表定义中的约束 讲过了。主要是 ondelete 和 onupdate 两个参数的用法。 ","date":"2019-05-21","objectID":"/posts/sqlalchemy-notes-3-relationship-and-foreignkey/:1:0","series":null,"tags":["SQLAlchemy","Python","ORM","后端","数据库","Database"],"title":"SQLAlchemy 学习笔记（三）：ORM 中的关系构建","uri":"/posts/sqlalchemy-notes-3-relationship-and-foreignkey/#一关系构建foreignkey-与-relationship"},{"categories":["tech"],"content":" 二、relationshiprelationship 函数在 ORM 中用于构建表之间的关联关系。与 ForeignKey 不同的是，它定义的关系不属于表定义，而是动态计算的。 用它定义出来的属性，相当于 SQL 中的视图。 这个函数有点难用，一是因为它的有几个参数不太好理解，二是因为它的参数非常丰富，让人望而却步。下面通过一对多、多对一、多对多几个场景下 relationship 的使用，来一步步熟悉它的用法。 首先初始化： from sqlalchemy import Table, Column, Integer, ForeignKey from sqlalchemy.orm import relationship from sqlalchemy.ext.declarative import declarative_base Base = declarative_base() ","date":"2019-05-21","objectID":"/posts/sqlalchemy-notes-3-relationship-and-foreignkey/:2:0","series":null,"tags":["SQLAlchemy","Python","ORM","后端","数据库","Database"],"title":"SQLAlchemy 学习笔记（三）：ORM 中的关系构建","uri":"/posts/sqlalchemy-notes-3-relationship-and-foreignkey/#二relationship"},{"categories":["tech"],"content":" 1. 一对多 class Parent(Base): __tablename__ = 'parent' id = Column(Integer, primary_key=True) # 因为 Child 中有 Parent 的 ForeignKey，这边的声明不需要再额外指定什么。 children = relationship(\"Child\") # children 的集合，相当于一个视图。 class Child(Base): __tablename__ = 'child' id = Column(Integer, primary_key=True) parent_id = Column(Integer, ForeignKey('parent.id')) 一个 Parent 可以有多个 Children，通过 relationship，我们就能直接通过 parent.children 得到结果，免去繁琐的 query 语句。 1.1 反向引用 1.1.1 backref 与 back_populates那如果我们需要得知 child 的 parent 对象呢？能不能直接访问 child.parent？ 为了实现这个功能，SQLAlchemy 提供了 backref 和 back_populates 两个参数。 两个参数的效果完全一致，区别在于，backref 只需要在 Parent 类中声明 children，Child.parent 会被动态创建。 而 back_populates 必须在两个类中显式地使用 back_populates，更显繁琐。（但是也更清晰？） 先看 backref 版： class Parent(Base): __tablename__ = 'parent' id = Column(Integer, primary_key=True) children = relationship(\"Child\", backref=\"parent\") # backref 表示，在 Child 类中动态创建 parent 属性，指向当前类。 # Child 类不需要修改 再看 back_populates 版： class Parent(Base): __tablename__ = 'parent' id = Column(Integer, primary_key=True) children = relationship(\"Child\", back_populates=\"parent\") # back_populates class Child(Base): __tablename__ = 'child' id = Column(Integer, primary_key=True) parent_id = Column(Integer, ForeignKey('parent.id')) # 这边也必须声明，不能省略！ parent = relationship(\"Parent\", back_populates=\"children\") # parent 不是集合，是属性！ NOTE：声明的两个 relationship 不需要多余的说明，SQLAlchemy 能自动识别到 parent.children 是 collection，child.parent 是 attribute. 1.1.2. 反向引用的参数：sqlalchemy.orm.backref(name, **kwargs)使用 back_populates 时，我们可以很方便地在两个 relationship 函数中指定各种参数： class Parent(Base): __tablename__ = 'parent' id = Column(Integer, primary_key=True) children = relationship(\"Child\", back_populates=\"parent\", lazy='dynamic') # 指定 lazy 的值 class Child(Base): __tablename__ = 'child' id = Column(Integer, primary_key=True) parent_id = Column(Integer, ForeignKey('parent.id')) parent = relationship(\"Parent\", back_populates=\"children\", lazy='dynamic') # 指定 lazy 的值 但是如果使用 backref，因为我们只有一个 relationship 函数，Child.parent 是被隐式创建的，我们该如何指定这个属性的参数呢？ 答案就是 backref() 函数，使用它替代 backref 参数的值： from sqlalchemy.orm import backref class Parent(Base): __tablename__ = 'parent' id = Column(Integer, primary_key=True) children = relationship(\"Child\", backref=backref(\"parent\", lazy='dynamic')) # 使用 backref() 函数，指定 Child.parent 属性的参数 # Child 类不需要修改 backref() 的参数会被传递给 relationship()，因此它俩的参数也完全一致。 ","date":"2019-05-21","objectID":"/posts/sqlalchemy-notes-3-relationship-and-foreignkey/:2:1","series":null,"tags":["SQLAlchemy","Python","ORM","后端","数据库","Database"],"title":"SQLAlchemy 学习笔记（三）：ORM 中的关系构建","uri":"/posts/sqlalchemy-notes-3-relationship-and-foreignkey/#1-一对多"},{"categories":["tech"],"content":" 1. 一对多 class Parent(Base): __tablename__ = 'parent' id = Column(Integer, primary_key=True) # 因为 Child 中有 Parent 的 ForeignKey，这边的声明不需要再额外指定什么。 children = relationship(\"Child\") # children 的集合，相当于一个视图。 class Child(Base): __tablename__ = 'child' id = Column(Integer, primary_key=True) parent_id = Column(Integer, ForeignKey('parent.id')) 一个 Parent 可以有多个 Children，通过 relationship，我们就能直接通过 parent.children 得到结果，免去繁琐的 query 语句。 1.1 反向引用 1.1.1 backref 与 back_populates那如果我们需要得知 child 的 parent 对象呢？能不能直接访问 child.parent？ 为了实现这个功能，SQLAlchemy 提供了 backref 和 back_populates 两个参数。 两个参数的效果完全一致，区别在于，backref 只需要在 Parent 类中声明 children，Child.parent 会被动态创建。 而 back_populates 必须在两个类中显式地使用 back_populates，更显繁琐。（但是也更清晰？） 先看 backref 版： class Parent(Base): __tablename__ = 'parent' id = Column(Integer, primary_key=True) children = relationship(\"Child\", backref=\"parent\") # backref 表示，在 Child 类中动态创建 parent 属性，指向当前类。 # Child 类不需要修改 再看 back_populates 版： class Parent(Base): __tablename__ = 'parent' id = Column(Integer, primary_key=True) children = relationship(\"Child\", back_populates=\"parent\") # back_populates class Child(Base): __tablename__ = 'child' id = Column(Integer, primary_key=True) parent_id = Column(Integer, ForeignKey('parent.id')) # 这边也必须声明，不能省略！ parent = relationship(\"Parent\", back_populates=\"children\") # parent 不是集合，是属性！ NOTE：声明的两个 relationship 不需要多余的说明，SQLAlchemy 能自动识别到 parent.children 是 collection，child.parent 是 attribute. 1.1.2. 反向引用的参数：sqlalchemy.orm.backref(name, **kwargs)使用 back_populates 时，我们可以很方便地在两个 relationship 函数中指定各种参数： class Parent(Base): __tablename__ = 'parent' id = Column(Integer, primary_key=True) children = relationship(\"Child\", back_populates=\"parent\", lazy='dynamic') # 指定 lazy 的值 class Child(Base): __tablename__ = 'child' id = Column(Integer, primary_key=True) parent_id = Column(Integer, ForeignKey('parent.id')) parent = relationship(\"Parent\", back_populates=\"children\", lazy='dynamic') # 指定 lazy 的值 但是如果使用 backref，因为我们只有一个 relationship 函数，Child.parent 是被隐式创建的，我们该如何指定这个属性的参数呢？ 答案就是 backref() 函数，使用它替代 backref 参数的值： from sqlalchemy.orm import backref class Parent(Base): __tablename__ = 'parent' id = Column(Integer, primary_key=True) children = relationship(\"Child\", backref=backref(\"parent\", lazy='dynamic')) # 使用 backref() 函数，指定 Child.parent 属性的参数 # Child 类不需要修改 backref() 的参数会被传递给 relationship()，因此它俩的参数也完全一致。 ","date":"2019-05-21","objectID":"/posts/sqlalchemy-notes-3-relationship-and-foreignkey/:2:1","series":null,"tags":["SQLAlchemy","Python","ORM","后端","数据库","Database"],"title":"SQLAlchemy 学习笔记（三）：ORM 中的关系构建","uri":"/posts/sqlalchemy-notes-3-relationship-and-foreignkey/#11-反向引用"},{"categories":["tech"],"content":" 1. 一对多 class Parent(Base): __tablename__ = 'parent' id = Column(Integer, primary_key=True) # 因为 Child 中有 Parent 的 ForeignKey，这边的声明不需要再额外指定什么。 children = relationship(\"Child\") # children 的集合，相当于一个视图。 class Child(Base): __tablename__ = 'child' id = Column(Integer, primary_key=True) parent_id = Column(Integer, ForeignKey('parent.id')) 一个 Parent 可以有多个 Children，通过 relationship，我们就能直接通过 parent.children 得到结果，免去繁琐的 query 语句。 1.1 反向引用 1.1.1 backref 与 back_populates那如果我们需要得知 child 的 parent 对象呢？能不能直接访问 child.parent？ 为了实现这个功能，SQLAlchemy 提供了 backref 和 back_populates 两个参数。 两个参数的效果完全一致，区别在于，backref 只需要在 Parent 类中声明 children，Child.parent 会被动态创建。 而 back_populates 必须在两个类中显式地使用 back_populates，更显繁琐。（但是也更清晰？） 先看 backref 版： class Parent(Base): __tablename__ = 'parent' id = Column(Integer, primary_key=True) children = relationship(\"Child\", backref=\"parent\") # backref 表示，在 Child 类中动态创建 parent 属性，指向当前类。 # Child 类不需要修改 再看 back_populates 版： class Parent(Base): __tablename__ = 'parent' id = Column(Integer, primary_key=True) children = relationship(\"Child\", back_populates=\"parent\") # back_populates class Child(Base): __tablename__ = 'child' id = Column(Integer, primary_key=True) parent_id = Column(Integer, ForeignKey('parent.id')) # 这边也必须声明，不能省略！ parent = relationship(\"Parent\", back_populates=\"children\") # parent 不是集合，是属性！ NOTE：声明的两个 relationship 不需要多余的说明，SQLAlchemy 能自动识别到 parent.children 是 collection，child.parent 是 attribute. 1.1.2. 反向引用的参数：sqlalchemy.orm.backref(name, **kwargs)使用 back_populates 时，我们可以很方便地在两个 relationship 函数中指定各种参数： class Parent(Base): __tablename__ = 'parent' id = Column(Integer, primary_key=True) children = relationship(\"Child\", back_populates=\"parent\", lazy='dynamic') # 指定 lazy 的值 class Child(Base): __tablename__ = 'child' id = Column(Integer, primary_key=True) parent_id = Column(Integer, ForeignKey('parent.id')) parent = relationship(\"Parent\", back_populates=\"children\", lazy='dynamic') # 指定 lazy 的值 但是如果使用 backref，因为我们只有一个 relationship 函数，Child.parent 是被隐式创建的，我们该如何指定这个属性的参数呢？ 答案就是 backref() 函数，使用它替代 backref 参数的值： from sqlalchemy.orm import backref class Parent(Base): __tablename__ = 'parent' id = Column(Integer, primary_key=True) children = relationship(\"Child\", backref=backref(\"parent\", lazy='dynamic')) # 使用 backref() 函数，指定 Child.parent 属性的参数 # Child 类不需要修改 backref() 的参数会被传递给 relationship()，因此它俩的参数也完全一致。 ","date":"2019-05-21","objectID":"/posts/sqlalchemy-notes-3-relationship-and-foreignkey/:2:1","series":null,"tags":["SQLAlchemy","Python","ORM","后端","数据库","Database"],"title":"SQLAlchemy 学习笔记（三）：ORM 中的关系构建","uri":"/posts/sqlalchemy-notes-3-relationship-and-foreignkey/#111-backref-与-back_populates"},{"categories":["tech"],"content":" 1. 一对多 class Parent(Base): __tablename__ = 'parent' id = Column(Integer, primary_key=True) # 因为 Child 中有 Parent 的 ForeignKey，这边的声明不需要再额外指定什么。 children = relationship(\"Child\") # children 的集合，相当于一个视图。 class Child(Base): __tablename__ = 'child' id = Column(Integer, primary_key=True) parent_id = Column(Integer, ForeignKey('parent.id')) 一个 Parent 可以有多个 Children，通过 relationship，我们就能直接通过 parent.children 得到结果，免去繁琐的 query 语句。 1.1 反向引用 1.1.1 backref 与 back_populates那如果我们需要得知 child 的 parent 对象呢？能不能直接访问 child.parent？ 为了实现这个功能，SQLAlchemy 提供了 backref 和 back_populates 两个参数。 两个参数的效果完全一致，区别在于，backref 只需要在 Parent 类中声明 children，Child.parent 会被动态创建。 而 back_populates 必须在两个类中显式地使用 back_populates，更显繁琐。（但是也更清晰？） 先看 backref 版： class Parent(Base): __tablename__ = 'parent' id = Column(Integer, primary_key=True) children = relationship(\"Child\", backref=\"parent\") # backref 表示，在 Child 类中动态创建 parent 属性，指向当前类。 # Child 类不需要修改 再看 back_populates 版： class Parent(Base): __tablename__ = 'parent' id = Column(Integer, primary_key=True) children = relationship(\"Child\", back_populates=\"parent\") # back_populates class Child(Base): __tablename__ = 'child' id = Column(Integer, primary_key=True) parent_id = Column(Integer, ForeignKey('parent.id')) # 这边也必须声明，不能省略！ parent = relationship(\"Parent\", back_populates=\"children\") # parent 不是集合，是属性！ NOTE：声明的两个 relationship 不需要多余的说明，SQLAlchemy 能自动识别到 parent.children 是 collection，child.parent 是 attribute. 1.1.2. 反向引用的参数：sqlalchemy.orm.backref(name, **kwargs)使用 back_populates 时，我们可以很方便地在两个 relationship 函数中指定各种参数： class Parent(Base): __tablename__ = 'parent' id = Column(Integer, primary_key=True) children = relationship(\"Child\", back_populates=\"parent\", lazy='dynamic') # 指定 lazy 的值 class Child(Base): __tablename__ = 'child' id = Column(Integer, primary_key=True) parent_id = Column(Integer, ForeignKey('parent.id')) parent = relationship(\"Parent\", back_populates=\"children\", lazy='dynamic') # 指定 lazy 的值 但是如果使用 backref，因为我们只有一个 relationship 函数，Child.parent 是被隐式创建的，我们该如何指定这个属性的参数呢？ 答案就是 backref() 函数，使用它替代 backref 参数的值： from sqlalchemy.orm import backref class Parent(Base): __tablename__ = 'parent' id = Column(Integer, primary_key=True) children = relationship(\"Child\", backref=backref(\"parent\", lazy='dynamic')) # 使用 backref() 函数，指定 Child.parent 属性的参数 # Child 类不需要修改 backref() 的参数会被传递给 relationship()，因此它俩的参数也完全一致。 ","date":"2019-05-21","objectID":"/posts/sqlalchemy-notes-3-relationship-and-foreignkey/:2:1","series":null,"tags":["SQLAlchemy","Python","ORM","后端","数据库","Database"],"title":"SQLAlchemy 学习笔记（三）：ORM 中的关系构建","uri":"/posts/sqlalchemy-notes-3-relationship-and-foreignkey/#112-反向引用的参数sqlalchemyormbackrefname-kwargs"},{"categories":["tech"],"content":" 2. 多对一A many-to-one is similar to a one-to-many relationship. The difference is that this relationship is looked at from the “many” side. ","date":"2019-05-21","objectID":"/posts/sqlalchemy-notes-3-relationship-and-foreignkey/:2:2","series":null,"tags":["SQLAlchemy","Python","ORM","后端","数据库","Database"],"title":"SQLAlchemy 学习笔记（三）：ORM 中的关系构建","uri":"/posts/sqlalchemy-notes-3-relationship-and-foreignkey/#2-多对一"},{"categories":["tech"],"content":" 3. 一对一 class Parent(Base): __tablename__ = 'parent' id = Column(Integer, primary_key=True) child = relationship(\"Child\", uselist=False, # 不使用 collection！这是关键 back_populates=\"parent\") class Child(Base): __tablename__ = 'child' id = Column(Integer, primary_key=True) parent_id = Column(Integer, ForeignKey('parent.id')) # 包含 ForeignKey 的类，此属性默认为 attribute，因此不需要 uselist=False parent = relationship(\"Parent\", back_populates=\"child\") ","date":"2019-05-21","objectID":"/posts/sqlalchemy-notes-3-relationship-and-foreignkey/:2:3","series":null,"tags":["SQLAlchemy","Python","ORM","后端","数据库","Database"],"title":"SQLAlchemy 学习笔记（三）：ORM 中的关系构建","uri":"/posts/sqlalchemy-notes-3-relationship-and-foreignkey/#3-一对一"},{"categories":["tech"],"content":" 4. 多对多 # 多对多，必须要使用一个关联表！ association_table = Table('association', Base.metadata, Column('left_id', Integer, ForeignKey('left.id')), # 约定俗成的规矩，左边是 parent Column('right_id', Integer, ForeignKey('right.id')) # 右边是 child ) class Parent(Base): __tablename__ = 'left' id = Column(Integer, primary_key=True) children = relationship(\"Child\", secondary=association_table) # 专用参数 secondary，用于指定使用的关联表 class Child(Base): __tablename__ = 'right' id = Column(Integer, primary_key=True) 要添加反向引用时，同样可以使用 backref 或 back_populates. 4.1 user2user如果多对多关系中的两边都是 user，即都是同一个表时，该怎么声明？ 例如用户的「关注」与「粉丝」，你是 user，你的粉丝是 user，你关注的账号也是 user。 这个时候，关联表 association_table 的两个键都是 user，SQLAlchemy 无法区分主次，需要手动指定，为此需要使用 primaryjoin 和 secondaryjoin 两个参数。 # 关联表，左侧的 user 正在关注右侧的 user followers = db.Table('followers', db.Column('follower_id', db.Integer, db.ForeignKey('user.id')), # 左侧 db.Column('followed_id', db.Integer, db.ForeignKey('user.id')) # 右侧，被关注的 user ) class User(UserMixin, db.Model): id = db.Column(db.Integer, primary_key=True) username = db.Column(db.String(64), index=True, unique=True, nullable=False) email = db.Column(db.String(120), index=True, unique=True, nullable=False) password_hash = db.Column(db.String(128), nullable=False) # 我关注的 users followed = db.relationship( 'User', secondary=followers, # 指定多对多关联表 primaryjoin=(followers.c.follower_id == id), # 左侧，用于获取「我关注的 users」的 join 条件 secondaryjoin=(followers.c.followed_id == id), # 右侧，用于获取「我的粉丝」的 join 条件 lazy='dynamic', # 延迟求值，这样才能用 filter_by 等过滤函数 backref=db.backref('followers', lazy='dynamic')) # followers 也要延迟求值 这里比较绕的，就是容易搞混 primaryjoin 和 secondaryjoin 两个参数。 primaryjoin：（多对多中）用于从子对象查询其父对象的 condition（child.parents），默认只考虑外键。 secondaryjoin：（多对多中）用于从父对象查询其所有子对象的 condition（parent.children），同样的，默认情况下只考虑外键。 ","date":"2019-05-21","objectID":"/posts/sqlalchemy-notes-3-relationship-and-foreignkey/:2:4","series":null,"tags":["SQLAlchemy","Python","ORM","后端","数据库","Database"],"title":"SQLAlchemy 学习笔记（三）：ORM 中的关系构建","uri":"/posts/sqlalchemy-notes-3-relationship-and-foreignkey/#4-多对多"},{"categories":["tech"],"content":" 4. 多对多 # 多对多，必须要使用一个关联表！ association_table = Table('association', Base.metadata, Column('left_id', Integer, ForeignKey('left.id')), # 约定俗成的规矩，左边是 parent Column('right_id', Integer, ForeignKey('right.id')) # 右边是 child ) class Parent(Base): __tablename__ = 'left' id = Column(Integer, primary_key=True) children = relationship(\"Child\", secondary=association_table) # 专用参数 secondary，用于指定使用的关联表 class Child(Base): __tablename__ = 'right' id = Column(Integer, primary_key=True) 要添加反向引用时，同样可以使用 backref 或 back_populates. 4.1 user2user如果多对多关系中的两边都是 user，即都是同一个表时，该怎么声明？ 例如用户的「关注」与「粉丝」，你是 user，你的粉丝是 user，你关注的账号也是 user。 这个时候，关联表 association_table 的两个键都是 user，SQLAlchemy 无法区分主次，需要手动指定，为此需要使用 primaryjoin 和 secondaryjoin 两个参数。 # 关联表，左侧的 user 正在关注右侧的 user followers = db.Table('followers', db.Column('follower_id', db.Integer, db.ForeignKey('user.id')), # 左侧 db.Column('followed_id', db.Integer, db.ForeignKey('user.id')) # 右侧，被关注的 user ) class User(UserMixin, db.Model): id = db.Column(db.Integer, primary_key=True) username = db.Column(db.String(64), index=True, unique=True, nullable=False) email = db.Column(db.String(120), index=True, unique=True, nullable=False) password_hash = db.Column(db.String(128), nullable=False) # 我关注的 users followed = db.relationship( 'User', secondary=followers, # 指定多对多关联表 primaryjoin=(followers.c.follower_id == id), # 左侧，用于获取「我关注的 users」的 join 条件 secondaryjoin=(followers.c.followed_id == id), # 右侧，用于获取「我的粉丝」的 join 条件 lazy='dynamic', # 延迟求值，这样才能用 filter_by 等过滤函数 backref=db.backref('followers', lazy='dynamic')) # followers 也要延迟求值 这里比较绕的，就是容易搞混 primaryjoin 和 secondaryjoin 两个参数。 primaryjoin：（多对多中）用于从子对象查询其父对象的 condition（child.parents），默认只考虑外键。 secondaryjoin：（多对多中）用于从父对象查询其所有子对象的 condition（parent.children），同样的，默认情况下只考虑外键。 ","date":"2019-05-21","objectID":"/posts/sqlalchemy-notes-3-relationship-and-foreignkey/:2:4","series":null,"tags":["SQLAlchemy","Python","ORM","后端","数据库","Database"],"title":"SQLAlchemy 学习笔记（三）：ORM 中的关系构建","uri":"/posts/sqlalchemy-notes-3-relationship-and-foreignkey/#41-user2user"},{"categories":["tech"],"content":" 三、ORM 层 的 “delete” cascade vs. FOREIGN KEY 层的 “ON DELETE” cascade之前有讲过 Table 定义中的级联操作：ON DELETE 和 ON UPDATE，可以通过 ForeignKey 的参数指定为 CASCADE. 可 SQLAlchemy 还有一个 relationship 生成 SQL 语句时的配置参数 cascade，另外 passive_deletes 也可以指定为 cascade。 有这么多的 cascade，我真的是很懵。这三个 cascade 到底有何差别呢？ 外键约束中的 ON DELETE 和 ON UPDATE，与 ORM 层的 CASCADE 在功能上，确实有很多重叠的地方。 但是也有很多不同： 数据库层面的 ON DELETE 级联能高效地处理 many-to-one 的关联；我们在 many 方定义外键，也在这里添加 ON DELETE 约束。而在 ORM 层，就刚好相反。SQLAlchemy 在 one 方处理 many 方的删除操作，这意味着它更适合处理 one-to-many 的关联。 数据库层面上，不带 ON DELETE 的外键常用于防止父数据被删除，而导致子数据成为无法被索引到的垃圾数据。如果要在一个 one-to-many 映射上实现这个行为，SQLAlchemy 将外键设置为 NULL 的默认行为可以通过以下两种方式之一捕获： 最简单也最常用的方法，当然是将外键定义为 NOT NULL. 尝试将该列设为 NULL 会触发 NOT NULL constraint exception. 另一种更特殊的方法，是将 passive_deletes 标志设置为字 all. 这会完全禁用 SQLAlchemy 将外键列设置为 NULL 的行为，并且 DELETE 父数据而不会对子数据产生任何影响。这样才能触发数据库层面的 ON DELETE 约束，或者其他的触发器。 数据库层面的 ON DELETE 级联 比 ORM 层面的级联更高效。数据库可以同时在多个 relationship 中链接一系列级联操作。 SQLAlchemy 不需要这么复杂，因为我们通过将 passive_deletes 选项与正确配置的外键约束结合使用，提供与数据库的 ON DELETE 功能的平滑集成。 ","date":"2019-05-21","objectID":"/posts/sqlalchemy-notes-3-relationship-and-foreignkey/:3:0","series":null,"tags":["SQLAlchemy","Python","ORM","后端","数据库","Database"],"title":"SQLAlchemy 学习笔记（三）：ORM 中的关系构建","uri":"/posts/sqlalchemy-notes-3-relationship-and-foreignkey/#三orm-层-的-delete-cascade-vs-foreign-key-层的-on-delete-cascade"},{"categories":["tech"],"content":" 方法一：ORM 层的 cascade 实现relationship 的 cascade 参数决定了修改父表时，什么时候子表要进行级联操作。它的可选项有（str，选项之间用逗号分隔）： save-update：默认选项之一。在 add（对应 SQL 的 insert 或 update）一个对象的时候，会 add 所有它相关联的对象。 merge：默认选项之一。在 merge（相当字典的update操作，有就替换掉，没有就合并）一个对象的时候，会 merge 所有和它相关联的对象。 expunge ：移除操作的时候，会将相关联的对象也进行移除。这个操作只是从session中移除，并不会真正的从数据库中删除。 delete：删除父表数据时，同时删除与它关联的数据。 delete-orphan：当子对象与父对象解除关系时，删除掉此子对象（孤儿）。（其实还是没懂。。） refresh-expire：不常用。 all：表示选中除 delete-orphan 之外的所有选项。（因此 all, delete-orphan 很常用，它才是真正的 all） 默认属性是 “save-update, merge”. 这只是简略的说明，上述几个参数的详细文档见 SQLAlchemy - Cascades ","date":"2019-05-21","objectID":"/posts/sqlalchemy-notes-3-relationship-and-foreignkey/:3:1","series":null,"tags":["SQLAlchemy","Python","ORM","后端","数据库","Database"],"title":"SQLAlchemy 学习笔记（三）：ORM 中的关系构建","uri":"/posts/sqlalchemy-notes-3-relationship-and-foreignkey/#方法一orm-层的-cascade-实现"},{"categories":["tech"],"content":" 方法二：数据库层的 cascade 实现 将 ForeignKey 的 ondelete 和 onupdate 参数指定为 CASCADE，实现数据库层面的级联。 为 relationship 添加关键字参数 passive_deletes=\"all\"，这样就完全禁用 SQLAlchemy 将外键列设置为 NULL 的行为，并且 DELETE 父数据不会对子数据产生任何影响。 这样 DELETE 操作时，就会触发数据库的 ON DELETE 约束，从而级联删除子数据。 ","date":"2019-05-21","objectID":"/posts/sqlalchemy-notes-3-relationship-and-foreignkey/:3:2","series":null,"tags":["SQLAlchemy","Python","ORM","后端","数据库","Database"],"title":"SQLAlchemy 学习笔记（三）：ORM 中的关系构建","uri":"/posts/sqlalchemy-notes-3-relationship-and-foreignkey/#方法二数据库层的-cascade-实现"},{"categories":["tech"],"content":" 参考 SQLAlchemy - Relationship Configuration SQLAlchemy - Cascades SQLAlchemy 中的 backref 和 back_populates The Flask Mega-Tutorial Part VIII: Followers hackersandslackers/sqlalchemy-tutorial ","date":"2019-05-21","objectID":"/posts/sqlalchemy-notes-3-relationship-and-foreignkey/:4:0","series":null,"tags":["SQLAlchemy","Python","ORM","后端","数据库","Database"],"title":"SQLAlchemy 学习笔记（三）：ORM 中的关系构建","uri":"/posts/sqlalchemy-notes-3-relationship-and-foreignkey/#参考"},{"categories":["life"],"content":"这网络小说一起头，就停不下来。 开始还只是追更新，每天看看三四本书的更新（一更或二更）就行了，说不上心满意足，但也没多么欲求不满。 到了三月下旬，突然就想看本新书。念头一起就一发不可收拾，只挑二三十万字的连载书看——首要原因是怕收不住手，书太长的话，废寝忘食起来自己遭罪；其次是前二十多万字都可以免费看，后面才是收费章节。 感觉自己陆陆续续可能看了二三十本书，五六百万字就这么过了，二三十个故事就如过眼云烟，转眼就记不得几个书名了。 就像从小到大的同学老师一样，看小说看到讲高中室友，就尝试回想我的高中宿舍，一时竟连自己的床铺都搞错了，仔细回想下，也有好几个室友的名字长相都忘记了。这还算好了，毕竟高中三年，也过了这么长时间才淡忘。这网文的故事有的光怪陆离，有的异想天开，大部分都是专注一个「爽」字，无不是金手指打怪升级的套路。千篇一律下，能记住的没几个。 倒也有特立独行的，因此也就印象尤其深刻，《月明见君来/云胡不喜》是这样一本文笔优美的古言，为着“云胡不喜”一句，那时突然就喜欢上了《诗经》。 还有位作者的星际言情文：《星球上的完美家园》和《未来世界之我心安处》，写得极为细腻。尤其是前者，只是他写的研究生生活，有点刺激我这个曾经信誓旦旦要考研，现在却大学文凭都拿不到的学渣。。就看不下去。 有段时间对日系轻小说青睐有加，觉得里头的优秀作品就是比国内网文高个档次。现在一想，其实都一样，良莠不齐，都喜欢穿越重生异世界的套路。只是看多了动漫，才对轻小说多点好感而已。网文里写的好的书也不少。 这两天突然就有些厌倦了，字看多了开始头疼，厌烦。就像手冲后进入了贤者时间一样。。。这样一想，这书瘾到底还是没毒品那么可怕，再怎么凶猛，疯狂上一个月，也后继乏力了，不会越陷越深。 这一个月深居简出，连QQ都直接掐掉了好几天。我看起小说来就是这样，别的事完全没兴致理，疯狂起来就更是变本加厉。 忽地又想起了自己内心敏感、性格别扭、胆子小、受不得训斥。再加上这一个月的所作所为，真是凭实力啃老+单身。 颓废完了，自卑，就安慰自己全国大学生占比不过3%，又想想战争年代人们的悲惨，就觉得自己已经比很多人都活得好了。 生逢盛世，国泰民安，又能找到自己喜欢的东西，而且主动地追求了。 虽然没有尽全力，但是人生苦短嘛，要及时享乐。 酸甜苦辣混一起，我总归是知足的。 要真能穿越我肯定不会像过去那样干，但是不能穿越也不是太可惜。我经历过的糟心事和很多人有过的绝望比起来，根本不值一提，我还能有什么不满的呢？ 这个世界总归是不会停止转动，也不会随我心意。 我讨厌的繁文缛节在人际交往中是必不可少的，我的自卑是藏不住的，我知识上的短板也是实实在在的。 我还能怎么做呢？总之向前走吧，不论是被人流裹挟着，还是被自己的欲望拉扯着，都只能向前——人生是没有退路的。 ","date":"2019-04-14","objectID":"/posts/webnovel-addiction-recovery/:0:0","series":null,"tags":["Webnovel","网络小说"],"title":"瘾的退却","uri":"/posts/webnovel-addiction-recovery/#"},{"categories":["tech"],"content":" 个人笔记，如有疏漏，还请指正。 使用多线程（threading）和多进程（multiprocessing）完成常规的并发需求，在启动的时候 start、join 等步骤不能省，复杂的需要还要用 1-2 个队列。 随着需求越来越复杂，如果没有良好的设计和抽象这部分的功能层次，代码量越多调试的难度就越大。 对于需要并发执行、但是对实时性要求不高的任务，我们可以使用 concurrent.futures 包中的 PoolExecutor 类来实现。 这个包提供了两个执行器：线程池执行器 ThreadPoolExecutor 和进程池执行器 ProcessPoolExecutor，两个执行器提供同样的 API。 池的概念主要目的是为了重用：让线程或进程在生命周期内可以多次使用。它减少了创建创建线程和进程的开销，提高了程序性能。重用不是必须的规则，但它是程序员在应用中使用池的主要原因。 池，只有固定个数的线程/进程，通过 max_workers 指定。 任务通过 executor.submit 提交到 executor 的任务队列，返回一个 future 对象。 Future 是常见的一种并发设计模式。一个Future对象代表了一些尚未就绪（完成）的结果，在「将来」的某个时间就绪了之后就可以获取到这个结果。 任务被调度到各个 workers 中执行。但是要注意，一个任务一旦被执行，在执行完毕前，会一直占用该 worker！ **如果 workers 不够用，其他的任务会一直等待！**因此 PoolExecutor 不适合实时任务。 import concurrent.futures import time from itertools import count number_list = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10] def evaluate_item(x): for i in count(x): # count 是无限迭代器，会一直递增。 print(f\"{x} - {i}\") time.sleep(0.01) if __name__ == \"__main__\": # 进程池 start_time_2 = time.time() # 使用 with 在离开此代码块时，自动调用 executor.shutdown(wait=true) 释放 executor 资源 with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor: # 将 10 个任务提交给 executor，并收集 futures futures = [executor.submit(evaluate_item, item) for item in number_list] # as_completed 方法等待 futures 中的 future 完成 # 一旦某个 future 完成，as_completed 就立即返回该 future # 这个方法，使每次返回的 future，总是最先完成的 future # 而不是先等待任务 1，再等待任务 2... for future in concurrent.futures.as_completed(futures): # result 返回被调用函数的返回值 # 如果调用抛出了异常，result 会抛出同样的异常 # 如果调用被取消，result 抛出 CancelledError 异常 print(future.result()) print (\"Thread pool execution in \" + str(time.time() - start_time_2), \"seconds\") 上面的代码中，item 为 1 2 3 4 5 的五个任务会一直占用所有的 workers，而 6 7 8 9 10 这五个任务会永远等待！！！ ","date":"2019-03-15","objectID":"/posts/python-concurrency-pool-executor/:0:0","series":null,"tags":["Python","Concurrency","并发"],"title":"Python 并发编程：PoolExecutor 篇","uri":"/posts/python-concurrency-pool-executor/#"},{"categories":["tech"],"content":" API 详细说明concurrent.futures 包含三个部分的 API： PoolExecutor：也就是两个执行器的 API 构造器：主要的参数是 max_workers，用于指定线程池大小（或者说 workers 个数） submit(fn, *args, **kwargs)：将任务函数 fn 提交到执行器，args 和 kwargs 就是 fn 需要的参数。 返回一个 future，用于获取结果 map(func, *iterables, timeout=None, chunksize=1)：当任务是同一个，只有参数不同时，可以用这个方法代替 submit。iterables 的每个元素对应 func 的一组参数。 返回一个 futures 的迭代器 shutdown(wait=True)：关闭执行器，一般都使用 with 管理器自动关闭。 Future：任务被提交给执行器后，会返回一个 future future.result(timout=None)：最常用的方法，返回任务的结果。如果任务尚未结束，这个方法会一直等待！ timeout 指定超时时间，为 None 时没有超时限制。超时会抛出 concurrent.futures.TimeoutError 异常。 如果调用抛出了异常，result 会抛出同样的异常 如果调用被取消，result 抛出 CancelledError 异常 exception(timeout=None)：返回任务抛出的异常。和 result() 一样，也会等待任务结束。 timeout 参数跟 result 一致，超时会抛出 concurrent.futures.TimeoutError 异常。 如果调用抛出了异常，exception 会返回同样的异常，否则返回 None 如果调用被取消，result 抛出 CancelledError 异常 cancel()：取消此任务 add_done_callback(fn)：future 完成后，会执行 fn(future)。 running()：是否正在运行 done()：future 是否已经结束了，boolean …详见官方文档 模块带有的实用函数 concurrent.futures.as_completed(fs, timeout=None)：等待 fs （futures iterable）中的 future 完成 一旦 fs 中的某 future 完成了，这个函数就立即返回该 future。 这个方法，使每次返回的 future，总是最先完成的 future。而不是先等待任务 1，再等待任务 2… 常通过 for future in as_completed(fs): 使用此函数。 concurrent.futures.wait(fs, timeout=None, return_when=ALL_COMPLETED)：一直等待，直到 return_when 所指定的事发生，或者 timeout return_when 有三个选项：ALL_COMPLETED（fs 中的 futures 全部完成），FIRST__COMPLETED（fs 中任意一个 future 完成）还有 FIRST_EXCEPTION（某任务抛出异常） ","date":"2019-03-15","objectID":"/posts/python-concurrency-pool-executor/:1:0","series":null,"tags":["Python","Concurrency","并发"],"title":"Python 并发编程：PoolExecutor 篇","uri":"/posts/python-concurrency-pool-executor/#api-详细说明"},{"categories":["tech"],"content":" Future 设计模式这里的 PoolExecutor 的特点，在于它使用了 Future 设计模式，使任务的执行，与结果的获取，变成一个异步的流程。 **我们先通过 submit/map 将任务放入任务队列，这时任务就已经开始执行了！**然后我们在需要的时候，通过 future 获取结果，或者直接 add_done_callback(fn)。 这里任务的执行是在新的 workers 中的，主进程/线程不会阻塞，因此主线程可以干其他的事。这种方式被称作异步编程。 ","date":"2019-03-15","objectID":"/posts/python-concurrency-pool-executor/:2:0","series":null,"tags":["Python","Concurrency","并发"],"title":"Python 并发编程：PoolExecutor 篇","uri":"/posts/python-concurrency-pool-executor/#future-设计模式"},{"categories":["tech"],"content":" 画外concurrent.futures 基于 multiprocessing.pool 实现，因此实际上它比直接使用 线程/进程 的 Pool 要慢一点。但是它提供了更方便简洁的 API。 ","date":"2019-03-15","objectID":"/posts/python-concurrency-pool-executor/:3:0","series":null,"tags":["Python","Concurrency","并发"],"title":"Python 并发编程：PoolExecutor 篇","uri":"/posts/python-concurrency-pool-executor/#画外"},{"categories":["tech"],"content":" 参考 使用Python进行并发编程-PoolExecutor篇 Python Parallel Programming Cookbook concurrent.futures — Launching parallel tasks 进程线程协程与并发并行 并行设计模式（一）– Future模式 ","date":"2019-03-15","objectID":"/posts/python-concurrency-pool-executor/:4:0","series":null,"tags":["Python","Concurrency","并发"],"title":"Python 并发编程：PoolExecutor 篇","uri":"/posts/python-concurrency-pool-executor/#参考"},{"categories":["tech"],"content":" 个人笔记，不保证正确。 虽然说看到很多人不看好 asyncio，但是这个东西还是必须学的。。 基于协程的异步，在很多语言中都有，学会了 Python 的，就一通百通。 ","date":"2019-02-14","objectID":"/posts/python-asyncio/:0:0","series":null,"tags":["Python","asyncio"],"title":"Python 异步编程笔记：asyncio","uri":"/posts/python-asyncio/#"},{"categories":["tech"],"content":" 一、生成器 generatorPython 的 asyncio 是通过 generator 实现的，要学习 async，先得复习下 generator. ","date":"2019-02-14","objectID":"/posts/python-asyncio/:1:0","series":null,"tags":["Python","asyncio"],"title":"Python 异步编程笔记：asyncio","uri":"/posts/python-asyncio/#一生成器-generator"},{"categories":["tech"],"content":" 1. yield众所周知，yield 是用于定义 generator 函数的关键字，调用该函数，会返回一个 generator \u003e\u003e\u003e def f(): ... yield 1 ... yield 2 ... \u003e\u003e\u003e f() # 返回的是 generator \u003cgenerator object f at 0x7f672c460570\u003e \u003e\u003e\u003e g = f() \u003e\u003e\u003e next(g) # 通过 next 方法从 generator 获取值 1 \u003e\u003e\u003e g.__next__() # next 方法实际是调用了 generator 的 __next__ 方法 2 \u003e\u003e\u003e next(g) # 生成器运行结束，产生一个 StopIteration 的 exception Traceback (most recent call last): File \"\u003cstdin\u003e\", line 1, in \u003cmodule\u003e StopIteration 每次调用 next，generator 都只会运行到下一个 yield 关键字所在行，返回 yield 右侧的对象，然后暂停在该处，等待下一次 next 调用。 从上面的例子看，yield 就是延迟求值而已。**但是 yield 还有一个特性，就是它是一个 expression，有返回值！**看例子： \u003e\u003e\u003e def func(): ... r = yield 1 ... yield r ... \u003e\u003e\u003e g = func() \u003e\u003e\u003e next(g) 1 \u003e\u003e\u003e next(g) # 通过 next 调用，yield 的返回值为 None \u003e\u003e\u003e g2 = func() \u003e\u003e\u003e next(g2) # 首先需要通过 next 调用，运行到 yield 语句处 1 \u003e\u003e\u003e g2.send(419) # 现在用 send 方法，这会将当前所在的 yield 语句的值设置为你 send 的值，也就是 419 419 # 然后 generator 运行到下一个 yield，返回右边的值并暂停 generator 有四个实例函数：next、send 是刚刚已经介绍了的，此外还有 throw 用于从 yield 所在处抛出 Exception，和 close 用于关闭 Generator。详见 Generator-iterator methods ","date":"2019-02-14","objectID":"/posts/python-asyncio/:1:1","series":null,"tags":["Python","asyncio"],"title":"Python 异步编程笔记：asyncio","uri":"/posts/python-asyncio/#1-yield"},{"categories":["tech"],"content":" 2. yield from 可以理解成是 yield \u003cvalue\u003e from \u003citerable\u003e，每次调用时它都会从 \u003citerable\u003e 中取值，直到遇到 StopIteration。才会从下一个 yield 取值。 \u003e\u003e\u003e def f(): ... yield from [1, 2, 3, 4] # iterable ... yield 5 ... yield from range(4, 0, -1) # iterable ... \u003e\u003e\u003e list(f()) [1, 2, 3, 4, 5, 4, 3, 2, 1] 当然，yield from \u003citerable\u003e 也是一个 expression，也有值。它的值就是 StopIteration 异常的第一个参数，内置类型的这个值都是 None. \u003e\u003e\u003e def f(): ... r = yield from [1, 2] ... yield f\"value of yield from is {r}\" ... \u003e\u003e\u003e list(f()) [1, 2, 'value of yield from is None'] 当 \u003citerable\u003e 是 generator 时，yield from 会直接将函数调用委托给这个子 generator，这里的调用包括了前面说过的 next、send、throw、close 四个函数。 并直接将 sub generator yield 的值 yield 给 caller. ","date":"2019-02-14","objectID":"/posts/python-asyncio/:1:2","series":null,"tags":["Python","asyncio"],"title":"Python 异步编程笔记：asyncio","uri":"/posts/python-asyncio/#2-yield-from-iterable"},{"categories":["tech"],"content":" 3. yield 和 return 混用会发生什么？generator 中的 return value，语义上等同于 rasie StopIteration(value)： \u003e\u003e\u003e def f(): ... yield 1 ... return 2 ... yield 3 # 永远不会被执行 ... \u003e\u003e\u003e g = f() \u003e\u003e\u003e next(g) 1 \u003e\u003e\u003e next(g) # return 引发 StopIteration Traceback (most recent call last): File \"\u003cinput\u003e\", line 1, in \u003cmodule\u003e StopIteration: 2 \u003e\u003e\u003e next(g) # 再次调用，StopIteration 变成无参了。 Traceback (most recent call last): File \"\u003cinput\u003e\", line 1, in \u003cmodule\u003e StopIteration 可以看到 return 引发了 StopIteration 异常，而 return 的值则成了该异常的第一个参数。 之前说过 yield from \u003csub generator\u003e 表达式的值，就是该 \u003csub generator\u003e 的 StopIteration 异常的第一个参数，因此： \u003e\u003e\u003e def f2(): ... a = yield from f() ... yield a # a 是 f() 中 return 的值 ... \u003e\u003e\u003e list(f2()) [1, 2] PEP 479 – Change StopIteration handling inside generators 修改了StopIteration 的行为，该 PEP 使人为 raise 的 StopIteration 引发一个 RuntimeError。 该 PEP 在 Python 3.5 版本添加到 future 中，并在 Python 3.7 成为默认行为。 因此除非你确实想要引发异常，否则应该使用 return 来结束一个 generator 并返回值。 ","date":"2019-02-14","objectID":"/posts/python-asyncio/:1:3","series":null,"tags":["Python","asyncio"],"title":"Python 异步编程笔记：asyncio","uri":"/posts/python-asyncio/#3-yield-和-return-混用会发生什么"},{"categories":["tech"],"content":" 二、异步IO、协程与非阻塞 IO先了解一下 进程线程协程与并发并行 和 各种 IO 模型 ","date":"2019-02-14","objectID":"/posts/python-asyncio/:2:0","series":null,"tags":["Python","asyncio"],"title":"Python 异步编程笔记：asyncio","uri":"/posts/python-asyncio/#二异步io协程与非阻塞-io"},{"categories":["tech"],"content":" 三、asyncio 的简单使用asyncio 引入了两个新关键字：async 和 await，其中 async 能放在三个地方： async def：用于定义异步函数和异步生成器 不含有 yield 的是 async def 定义的是协程函数（coroutine function），调用该函数返回协程对象（coroutine object），协程对象需要通过 EventLoop 运行。 内部含有 yield 的 async def 定义的是异步生成器函数（asynchronous generator function），调用该函数返回异步生成器（async_generator） 异步生成器只能用在 Coroutine 中 async def 中不允许使用 yield from async for：表示 for 迭代的是一个异步生成器，该 for 循环的每一次迭代，都是异步的。 只能用在 async def 的内部 async with：表示 with 管理的是一个异步上下文管理器（asynchronous context manager） 该 context manager 的 enter 和 exit 两个步骤是异步的 只能用在 async def 的内部 注意异步 generator、context manager，它的 protocol 都和同步的不同，不能混为一谈。 具体而言，对同步 protocol xxx 函数，它的异步版本为 axxx，就是加个 a。 而 await，就相当于 yield from，差别在于 await 是异步的。还有我们关心的是 await 表达式的值，而 yield from 中我们更关心它向上层 yield 的值。 在 yield from 中，当前生成器调用另一个生成器，当前生成器会挂起，直到另一个生成器返回。 但是在 await 中，当前 Coroutine 挂起时， eventloop 会寻找其他 task 来跑，这就利用上了 IO 漫长的等待时间。 async for 是每次迭代都会 await 一次，如果迭代对象是 IO 操作，这个 IO 等待时间就会被利用上。 async with 也是同样，如果 context 的 enter 和 exit 是 IO 操作，这个 IO 时间就会被 eventloop 用于运行其他 task. 使用 asyncio 时，我们要用 async def 将所有的 IO 操作都定义成异步操作。然后在调用时，都使用 await/async for/async with 来调用。 ","date":"2019-02-14","objectID":"/posts/python-asyncio/:3:0","series":null,"tags":["Python","asyncio"],"title":"Python 异步编程笔记：asyncio","uri":"/posts/python-asyncio/#三asyncio-的简单使用"},{"categories":["tech"],"content":" 四、Coroutine、Task 和 Future首先，每个协程对象，都是一个独立的协程单元，协程对象之间可以异步运行。 协程需要放到 EventLoop 内运行，要运行一个协程 a，有三种方法： 通过 asyncio.run(coro) 运行一个协程。 该方法会新建一个 EventLoop 在另一个协程 b 中通过 await 调用 a。当 b 运行时， a 也会被 task 运行。 通过 asyncio.create_task(coro)，将需要运行的协程包装成 task，然后通过 task 相关的方法来异步运行它们。 asyncio.gather(*awaitable_objects): 并发执行所有的 task，阻塞到所有 task 结束。返回一个 result 列表。result 的列表顺序和 future 的顺序一致 asyncio.as_completed(aws, *, loop=None, timeout=None)，和 gather 的区别在于，它返回一个异步迭代器，每次迭代都返回最先完成的一个 future. concurrent.futures 是进程线程的异步执行，而 asyncio 是基于协程的单线程异步执行 ","date":"2019-02-14","objectID":"/posts/python-asyncio/:4:0","series":null,"tags":["Python","asyncio"],"title":"Python 异步编程笔记：asyncio","uri":"/posts/python-asyncio/#四coroutinetask-和-future"},{"categories":["tech"],"content":" 五、参考 从0到1，Python异步编程的演进之路 怎么掌握 asyncio Python Async/Await入门指南 谈谈Python协程技术的演进 Python Doc - Coroutines Python Doc - asyncio ","date":"2019-02-14","objectID":"/posts/python-asyncio/:5:0","series":null,"tags":["Python","asyncio"],"title":"Python 异步编程笔记：asyncio","uri":"/posts/python-asyncio/#五参考"},{"categories":["tech"],"content":"照例先看层次图 SQLAlchemy 层次结构 ","date":"2019-02-11","objectID":"/posts/sqlalchemy-notes-2-orm-basics/:0:0","series":null,"tags":["SQLAlchemy","Python","ORM","后端","数据库","Database"],"title":"SQLAlchemy 学习笔记（二）：ORM 基础","uri":"/posts/sqlalchemy-notes-2-orm-basics/#"},{"categories":["tech"],"content":" 一、声明映射关系使用 ORM 时，我们首先需要定义要操作的表（通过 Table），然后再定义该表对应的 Python class，并声明两者之间的映射关系（通过 Mapper）。 方便起见，SQLAlchemy 提供了 Declarative 系统来一次完成上述三个步骤，Declarative 系统提供 base class，这个 base class 会为继承了它的 Python class（可称作 model）创建 Table，并维护两者的映射关系。 from sqlalchemy.ext.declarative import declarative_base from SQLAlchemy import Column, Integer, String Base = declarative_base() # 拿到 Base 类 class User(Base): id = Column(Integer, primary_key=True) username = Column(String(32), nullable=False, index=True) # 添加 index 提升搜索效率 fullname = Column(String(64)) password = Column(String(32)) # 真实情况下一般只存 hash def __repr__(self): return f\"\u003cUser {self.username}\u003e\" 这样就声明好了一个对象-关系映射，上一篇文章说过所有的 Table 都在某个 MetaData 中，可以通过 Base.metadata 获取它。 Base.metadata.create_all(engine) # 通过 metadata 创建表（或者说生成模式 schema） engine 的创建请见上篇文档 SQLAlchemy 学习笔记（一）：Engine 与 SQL 表达式语言 约束条件 可参考 SQL 基础笔记（三）：约束 与 SQLAlchemy 学习笔记（一）：Engine 与 SQL 表达式语言 - 表定义中的约束 使用 ORM 来定义约束条件，与直接使用 SQL 表达式语言定义很类似，也有两种方法： 直接将约束条件作为 Column、ForeignKey 的参数传入。这种方式最简洁，也最常用。 使用 UniqueConstraint、CheckConstraint 等类构造约束，然后放入 __table_args__ 属性中。举例： class User(Base): id = Column(Integer, primary_key=True) username = Column(String(32), nullable=False, index=True) # 添加 index 提升搜索效率 fullname = Column(String(64)) password = Column(String(32)) # 真实情况下一般只存 hash # 顾名思义，这是 `Table` 类的参数的序列。里面的约束条件会被用于构建 __table__ __table_args__ = (UniqueConstraint('username', name='c_user'),) # username 的唯一性约束 def __repr__(self): return f\"\u003cUser {self.username}\u003e\" ","date":"2019-02-11","objectID":"/posts/sqlalchemy-notes-2-orm-basics/:0:1","series":null,"tags":["SQLAlchemy","Python","ORM","后端","数据库","Database"],"title":"SQLAlchemy 学习笔记（二）：ORM 基础","uri":"/posts/sqlalchemy-notes-2-orm-basics/#一声明映射关系"},{"categories":["tech"],"content":" 一、声明映射关系使用 ORM 时，我们首先需要定义要操作的表（通过 Table），然后再定义该表对应的 Python class，并声明两者之间的映射关系（通过 Mapper）。 方便起见，SQLAlchemy 提供了 Declarative 系统来一次完成上述三个步骤，Declarative 系统提供 base class，这个 base class 会为继承了它的 Python class（可称作 model）创建 Table，并维护两者的映射关系。 from sqlalchemy.ext.declarative import declarative_base from SQLAlchemy import Column, Integer, String Base = declarative_base() # 拿到 Base 类 class User(Base): id = Column(Integer, primary_key=True) username = Column(String(32), nullable=False, index=True) # 添加 index 提升搜索效率 fullname = Column(String(64)) password = Column(String(32)) # 真实情况下一般只存 hash def __repr__(self): return f\"\" 这样就声明好了一个对象-关系映射，上一篇文章说过所有的 Table 都在某个 MetaData 中，可以通过 Base.metadata 获取它。 Base.metadata.create_all(engine) # 通过 metadata 创建表（或者说生成模式 schema） engine 的创建请见上篇文档 SQLAlchemy 学习笔记（一）：Engine 与 SQL 表达式语言 约束条件 可参考 SQL 基础笔记（三）：约束 与 SQLAlchemy 学习笔记（一）：Engine 与 SQL 表达式语言 - 表定义中的约束 使用 ORM 来定义约束条件，与直接使用 SQL 表达式语言定义很类似，也有两种方法： 直接将约束条件作为 Column、ForeignKey 的参数传入。这种方式最简洁，也最常用。 使用 UniqueConstraint、CheckConstraint 等类构造约束，然后放入 __table_args__ 属性中。举例： class User(Base): id = Column(Integer, primary_key=True) username = Column(String(32), nullable=False, index=True) # 添加 index 提升搜索效率 fullname = Column(String(64)) password = Column(String(32)) # 真实情况下一般只存 hash # 顾名思义，这是 `Table` 类的参数的序列。里面的约束条件会被用于构建 __table__ __table_args__ = (UniqueConstraint('username', name='c_user'),) # username 的唯一性约束 def __repr__(self): return f\"\" ","date":"2019-02-11","objectID":"/posts/sqlalchemy-notes-2-orm-basics/:0:1","series":null,"tags":["SQLAlchemy","Python","ORM","后端","数据库","Database"],"title":"SQLAlchemy 学习笔记（二）：ORM 基础","uri":"/posts/sqlalchemy-notes-2-orm-basics/#约束条件"},{"categories":["tech"],"content":" 二、获取 session上一节讲 engine 时，我们是通过 connection 来与数据库交互，而在 ORM 中我们使用 Session 访问数据库。 from sqlalchemy.orm import sessionmaker Session = sessionmaker(bind=engine) # 获取 session ","date":"2019-02-11","objectID":"/posts/sqlalchemy-notes-2-orm-basics/:0:2","series":null,"tags":["SQLAlchemy","Python","ORM","后端","数据库","Database"],"title":"SQLAlchemy 学习笔记（二）：ORM 基础","uri":"/posts/sqlalchemy-notes-2-orm-basics/#二获取-session"},{"categories":["tech"],"content":" 三、增删改查直接使用 SQL 表达式语言时，我们使用 insert()、select()、update()、delete() 四个函数构造 SQL，使用 where() 添加条件，使用 model.join(another_model) 进行 join 操作。 而使用 ORM 时，数据库操作不再与 SQL 直接对应。我们现在是通过操作 Python 对象来操作数据库了。 现在，我们通过 db.session.add()、db.session.delete() 进行添加与删除，使用 db.session.query(Model) 进行查询，通过 filter 和 filter_by 添加过滤条件。 而修改，则是先查询出对应的 row 对象，直接修改这个对象，然后 commit 就行。 增添： ed_user = User(name='ed', fullname='Ed Jones', password='edspassword') # 用构造器构造对象 session.add(ed_user) # 添加，此外还有批量添加 add_all([user1, user2...]) session.commit() # 必须手动 commit 修改： ed_user = session.query(User).filter_by(name='ed').first() # 先获取到 User 对象 ed_user.password = 'f8s7ccs' # 改了密码 session.commit() # 提交 # 批量修改 session.query(User).filter(User.home=='shanghai') \\ .update({User.login_num:0}) # 将所有上海的用户的 login_num 设为 0 session.commit() 删除： ed_user = session.query(User).filter_by(name='ed').first() # 先获取到 User 对象 session.delete(ed_user) # 直接删除（session 知道 ed_user 属于哪个表） session.commit() # 提交 # 批量删除 session.query(User).filter(User.home=='shanghai') \\ .delete() # 删除所有上海的用户 session.commit() 同样的，也可以在外面检查异常，然后调用 session.rollback() 实现失败回滚。 ","date":"2019-02-11","objectID":"/posts/sqlalchemy-notes-2-orm-basics/:0:3","series":null,"tags":["SQLAlchemy","Python","ORM","后端","数据库","Database"],"title":"SQLAlchemy 学习笔记（二）：ORM 基础","uri":"/posts/sqlalchemy-notes-2-orm-basics/#三增删改查"},{"categories":["tech"],"content":" 四、进阶查询 filter_by：使用关键字参数进行过滤，前面的演示中已经用过多次了。 filter：它对应 SQL 表达式语言中的 where，支持各种复杂的 SQL 语法。 group_by: 通过指定 column 分组 distinct(): 去重 join(): 关联 query.filter(User.name == 'ed') # 这个等同于 filter_by，但是更繁琐 query.filter(User.name != 'ed') # 不等于，这个就是 filter_by 无法做到的了 query.filter(User.name.like('%ed%')) # SQL like 的 like 语法 query.filter(User.name.in_(['ed', 'wendy', 'jack'])) # 包含 # 查询还可以嵌套 query.filter(User.name.in_( session.query(User.name).filter(User.name.like('%ed%')) )) query.filter(~User.name.in_(['ed', 'wendy', 'jack'])) # 不包含 query.filter(User.name == None) # NULL 对应 Python 的 None from sqlalchemy import or_, and_, in_ query.filter(or_(User.name == 'ed', User.name == 'wendy')) # OR 语法 query.group_by(User.name) # 分组 query.distinct() # 去重 from sqlalchemy import func # SQL 函数包 session.query(func.count(User.name)).filter_by(xxx=xxx) # 使用 count 函数 # join 关联 # 默认使用内联（inner），即只取两表的交集 session.query(User, Address).filter(User.id==Address.user_id) # 方法一 session.query(User).join(Address).\\ # 方法二 filter(Address.email_address=='jack@google.com') # 外联 outer join，将另一表的列联结到主表，没有的行为 NULL session.query(User).outerjoin(User.addresses) \\ .filter(Address.email_address=='jack@google.com') 执行查询，获取数据查询返回 query 对象，但 SQL 还没有被执行，直到你调用下列几个方法： # 构造 query 对象 query = session.query(User).filter(User.name.like('%ed')).order_by(User.id) # 1. all 返回所有结果的列表 res_list = query.all() # 2. first 先在 SQL 中加入限制 `limit 1`，然后执行。 res = query.first() # 3. one 执行 sql 并获取所有结果 # 如果结果不止一行，抛出 MultipleResultsFound Error！！！ # 如果结果为空，抛出 NoResultFound Error ！！！ res = query.one() 4. one_or_none 差别在于结果为空，它不抛出异常，而是返回 None res = query.one_or_none() ","date":"2019-02-11","objectID":"/posts/sqlalchemy-notes-2-orm-basics/:0:4","series":null,"tags":["SQLAlchemy","Python","ORM","后端","数据库","Database"],"title":"SQLAlchemy 学习笔记（二）：ORM 基础","uri":"/posts/sqlalchemy-notes-2-orm-basics/#四进阶查询"},{"categories":["tech"],"content":" 四、进阶查询 filter_by：使用关键字参数进行过滤，前面的演示中已经用过多次了。 filter：它对应 SQL 表达式语言中的 where，支持各种复杂的 SQL 语法。 group_by: 通过指定 column 分组 distinct(): 去重 join(): 关联 query.filter(User.name == 'ed') # 这个等同于 filter_by，但是更繁琐 query.filter(User.name != 'ed') # 不等于，这个就是 filter_by 无法做到的了 query.filter(User.name.like('%ed%')) # SQL like 的 like 语法 query.filter(User.name.in_(['ed', 'wendy', 'jack'])) # 包含 # 查询还可以嵌套 query.filter(User.name.in_( session.query(User.name).filter(User.name.like('%ed%')) )) query.filter(~User.name.in_(['ed', 'wendy', 'jack'])) # 不包含 query.filter(User.name == None) # NULL 对应 Python 的 None from sqlalchemy import or_, and_, in_ query.filter(or_(User.name == 'ed', User.name == 'wendy')) # OR 语法 query.group_by(User.name) # 分组 query.distinct() # 去重 from sqlalchemy import func # SQL 函数包 session.query(func.count(User.name)).filter_by(xxx=xxx) # 使用 count 函数 # join 关联 # 默认使用内联（inner），即只取两表的交集 session.query(User, Address).filter(User.id==Address.user_id) # 方法一 session.query(User).join(Address).\\ # 方法二 filter(Address.email_address=='jack@google.com') # 外联 outer join，将另一表的列联结到主表，没有的行为 NULL session.query(User).outerjoin(User.addresses) \\ .filter(Address.email_address=='jack@google.com') 执行查询，获取数据查询返回 query 对象，但 SQL 还没有被执行，直到你调用下列几个方法： # 构造 query 对象 query = session.query(User).filter(User.name.like('%ed')).order_by(User.id) # 1. all 返回所有结果的列表 res_list = query.all() # 2. first 先在 SQL 中加入限制 `limit 1`，然后执行。 res = query.first() # 3. one 执行 sql 并获取所有结果 # 如果结果不止一行，抛出 MultipleResultsFound Error！！！ # 如果结果为空，抛出 NoResultFound Error ！！！ res = query.one() 4. one_or_none 差别在于结果为空，它不抛出异常，而是返回 None res = query.one_or_none() ","date":"2019-02-11","objectID":"/posts/sqlalchemy-notes-2-orm-basics/:0:4","series":null,"tags":["SQLAlchemy","Python","ORM","后端","数据库","Database"],"title":"SQLAlchemy 学习笔记（二）：ORM 基础","uri":"/posts/sqlalchemy-notes-2-orm-basics/#执行查询获取数据"},{"categories":["tech"],"content":" 参考 SQLAlchemy 对象关系入门 SQLAlchemy ORM 的关联映射定义：一对多、多对多 hackersandslackers/sqlalchemy-tutorial ","date":"2019-02-11","objectID":"/posts/sqlalchemy-notes-2-orm-basics/:0:5","series":null,"tags":["SQLAlchemy","Python","ORM","后端","数据库","Database"],"title":"SQLAlchemy 学习笔记（二）：ORM 基础","uri":"/posts/sqlalchemy-notes-2-orm-basics/#参考"},{"categories":["tech"],"content":" 一、WebSocketWebSocket 是一个双向通信协议，它在握手阶段采用 HTTP/1.1 协议（暂时不支持 HTTP/2）。 握手过程如下： 首先客户端向服务端发起一个特殊的 HTTP 请求，其消息头如下： GET /chat HTTP/1.1 // 请求行 Host: server.example.com Upgrade: websocket // required Connection: Upgrade // required Sec-WebSocket-Key: dGhlIHNhbXBsZSBub25jZQ== // required，一个 16bits 编码得到的 base64 串 Origin: http://example.com // 用于防止未认证的跨域脚本使用浏览器 websocket api 与服务端进行通信 Sec-WebSocket-Protocol: chat, superchat // optional, 子协议协商字段 Sec-WebSocket-Version: 13 如果服务端支持该版本的 WebSocket，会返回 101 响应，响应标头如下： HTTP/1.1 101 Switching Protocols // 状态行 Upgrade: websocket // required Connection: Upgrade // required Sec-WebSocket-Accept: s3pPLMBiTxaQ9kYGzzhZRbK+xOo= // required，加密后的 Sec-WebSocket-Key Sec-WebSocket-Protocol: chat // 表明选择的子协议 握手完成后，接下来的 TCP 数据包就都是 WebSocket 协议的帧了。 可以看到，这里的握手不是 TCP 的握手，而是在 TCP 连接内部，从 HTTP/1.1 upgrade 到 WebSocket 的握手。 WebSocket 提供两种协议：不加密的 ws:// 和 加密的 wss://. 因为是用 HTTP 握手，它和 HTTP 使用同样的端口：ws 是 80（HTTP），wss 是 443（HTTPS） 在 Python 编程中，可使用 websockets 实现的异步 WebSocket 客户端与服务端。此外 aiohttp 也提供了 WebSocket 支持。 Note：如果你搜索 Flask 的 WebScoket 插件，得到的第一个结果很可能是 Flask-SocketIO。但是 Flask-ScoektIO 使用的是它独有的 SocketIO 协议，并不是标准的 WebSocket。只是它刚好提供与 WebSocket 相同的功能而已。 SocketIO 的优势在于只要 Web 端使用了 SocketIO.js，就能支持该协议。而纯 WS 协议，只有较新的浏览器才支持。对于客户端非 Web 的情况，更好的选择可能是使用 Flask-Sockets。 ","date":"2019-02-11","objectID":"/posts/websocket-http2-and-grpc/:1:0","series":null,"tags":["WebSocket","gRPC","HTTP/2"],"title":"WebSocket、HTTP/2 与 gRPC","uri":"/posts/websocket-http2-and-grpc/#一websocket"},{"categories":["tech"],"content":" JS API // WebSocket API var socket = new WebSocket('ws://websocket.example.com'); // Show a connected message when the WebSocket is opened. socket.onopen = function(event) { console.log('WebSocket is connected.'); }; // Handle messages sent by the server. socket.onmessage = function(event) { var message = event.data; console.log(message); }; // Handle any error that occurs. socket.onerror = function(error) { console.log('WebSocket Error: ' + error); }; ","date":"2019-02-11","objectID":"/posts/websocket-http2-and-grpc/:1:1","series":null,"tags":["WebSocket","gRPC","HTTP/2"],"title":"WebSocket、HTTP/2 与 gRPC","uri":"/posts/websocket-http2-and-grpc/#js-api"},{"categories":["tech"],"content":" 二、HTTP/2HTTP/2 于 2015 年标准化，主要目的是优化性能。其特性如下： 二进制协议：HTTP/2 的消息头使用二进制格式，而非文本格式。并且使用专门设计的 HPack 算法压缩。 多路复用（Multiplexing）：就是说 HTTP/2 可以重复使用同一个 TCP 连接，并且连接是多路的，多个请求或响应可以同时传输。 对比之下，HTTP/1.1 的长连接也能复用 TCP 连接，但是只能串行，不能“多路”。 服务器推送：服务端能够直接把资源推送给客户端，当客户端需要这些文件的时候，它已经在客户端了。（该推送对 Web App 是隐藏的，由浏览器处理） HTTP/2 允许取消某个正在传输的数据流（通过发送 RST_STREAM 帧），而不关闭 TCP 连接。 这正是二进制协议的好处之一，可以定义多种功能的数据帧。 它允许服务端将资源推送到客户端缓存，我们访问淘宝等网站时，经常会发现很多请求的请求头部分会提示“provisional headers are shown”，这通常就是直接从缓存加载了资源，因此请求根本没有被发送。观察 Chrome Network 的 Size 列，这种请求的该字段一般都是 from disk cache 或者 from memroy cache. Chrome 可以通过如下方式查看请求使用的协议： 2019-02-10: 使用 Chrome 查看，目前主流网站基本都已经部分使用了 HTTP/2，知乎、bilibili、GIthub 使用了 wss 协议，也有很多网站使用了 SSE（格式如 data:image/png;base64,\u003cbase64 string\u003e） 而且很多网站都有使用 HTTP/2 + QUIC，该协议的新名称是 HTTP/3，它是基于 UDP 的 HTTP 协议。 ","date":"2019-02-11","objectID":"/posts/websocket-http2-and-grpc/:2:0","series":null,"tags":["WebSocket","gRPC","HTTP/2"],"title":"WebSocket、HTTP/2 与 gRPC","uri":"/posts/websocket-http2-and-grpc/#二http2"},{"categories":["tech"],"content":" SSE服务端推送事件，是通过 HTTP 长连接进行信息推送的一个功能。 它首先由浏览器向服务端建立一个 HTTP 长连接，然后服务端不断地通过这个长连接将消息推送给浏览器。JS API 如下： // create SSE connection var source = new EventSource('/dates'); // 连接建立时，这些 API 和 WebSocket 的很相似 source.onopen = function(event) { // handle open event }; // 收到消息时（它只捕获未命名 event） source.onmessage = function(event) { var data = event.data; // 发送过来的实际数据（string） var origin = event.origin; // 服务器端URL的域名部分，即协议、域名和端口。 var lastEventId = event.lastEventId; // 数据的编号，由服务器端发送。如果没有编号，这个属性为空。 // handle message }; source.onerror = function(event) { // handle error event }; ","date":"2019-02-11","objectID":"/posts/websocket-http2-and-grpc/:2:1","series":null,"tags":["WebSocket","gRPC","HTTP/2"],"title":"WebSocket、HTTP/2 与 gRPC","uri":"/posts/websocket-http2-and-grpc/#sse"},{"categories":["tech"],"content":" 具体的实现在收到客户端的 SSE 请求（HTTP 协议）时，服务端返回的响应首部如下： Content-Type: text/event-stream Cache-Control: no-cache Connection: keep-alive 而 body 部分，SSE 定义了四种信息： data：数据栏 event：自定义数据类型 id ：数据 id retry：最大间隔时间，超时则重新连接 body 举例说明： : 这种格式的消息是注释，会被忽略\\n\\n : 通常服务器每隔一段时间就会发送一个注释，防止超时 retry\\n\\n : 下面这个是一个单行数据\\n\\n data: some text\\n\\n : 下面这个是多行数据，在客户端会重组成一个 data\\n\\n data: {\\n data: \"foo\": \"bar\",\\n data: \"baz\", 555\\n data: }\\n\\n : 这是一个命名 event，只会被事件名与之相同的 listener 捕获\\n\\n event: foo\\n data: a foo event\\n\\n : 未命名事件，会被 onmessage 捕获\\n\\n data: an unnamed event\\n\\n event: bar\\n data: a bar event\\n\\n : 这个 id 对应 event.lastEventId\\n\\n id: msg1\\n data: message\\n\\n ","date":"2019-02-11","objectID":"/posts/websocket-http2-and-grpc/:2:2","series":null,"tags":["WebSocket","gRPC","HTTP/2"],"title":"WebSocket、HTTP/2 与 gRPC","uri":"/posts/websocket-http2-and-grpc/#具体的实现"},{"categories":["tech"],"content":" WebSocket、HTTP/2 与 SSE 的比较 加密与否： WebSocket 支持明文通信 ws:// 和加密 wss://， 而 HTTP/2 协议虽然没有规定必须加密，但是主流浏览器都只支持 HTTP/2 over TLS. SSE 是使用的 HTTP 协议通信，支持 http/https 消息推送： WebSocket是全双工通道，可以双向通信。而且消息是直接推送给 Web App. SSE 只能单向串行地从服务端将数据推送给 Web App. HTTP/2 虽然也支持 Server Push，但是服务器只能主动将资源推送到客户端缓存！并不允许将数据推送到客户端里跑的 Web App 本身。服务器推送只能由浏览器处理，不会在应用程序代码中弹出服务器数据，这意味着应用程序没有 API 来获取这些事件的通知。 为了接近实时地将数据推送给 Web App， HTTP/2 可以结合 SSE（Server-Sent Event）使用。 WebSocket 在需要接近实时双向通信的领域，很有用武之地。而 HTTP/2 + SSE 适合用于展示实时数据。 另外在客户端非浏览器的情况下，使用不加密的 HTTP/2 也是可能的。 ","date":"2019-02-11","objectID":"/posts/websocket-http2-and-grpc/:2:3","series":null,"tags":["WebSocket","gRPC","HTTP/2"],"title":"WebSocket、HTTP/2 与 gRPC","uri":"/posts/websocket-http2-and-grpc/#websockethttp2-与-sse-的比较"},{"categories":["tech"],"content":" requests 查看 HTTP 协议版本号可以通过 resp.raw.version 得到响应的 HTTP 版本号： \u003e\u003e\u003e import requests \u003e\u003e\u003e resp = requests.get(\"https://zhihu.com\") \u003e\u003e\u003e resp.raw.version 11 但是 requests 默认使用 HTTP/1.1，并且不支持 HTTP/2.（不过这也不是什么大问题，HTTP/2 只是做了性能优化，用 HTTP/1.1 也就是慢一点而已。） ","date":"2019-02-11","objectID":"/posts/websocket-http2-and-grpc/:2:4","series":null,"tags":["WebSocket","gRPC","HTTP/2"],"title":"WebSocket、HTTP/2 与 gRPC","uri":"/posts/websocket-http2-and-grpc/#requests-查看-http-协议版本号"},{"categories":["tech"],"content":" 三、gRPC 协议gRPC 是一个远程过程调用框架，默认使用 protobuf3 进行数据的高效序列化与 service 定义，使用 HTTP/2 进行数据传输。 这里讨论的是 gRPC over HTTP/2 协议。 目前 gRPC 主要被用在微服务通信中，但是因为其优越的性能，它也很契合游戏、loT 等需要高性能低延迟的场景。 其实光从协议先进程度上讲，gRPC 基本全面超越 REST: 使用二进制进行数据序列化，比 json 更节约流量、序列化与反序列化也更快。 protobuf3 要求 api 被完全清晰的定义好，而 REST api 只能靠程序员自觉定义。 gRPC 官方就支持从 api 定义生成代码，而 REST api 需要借助 openapi-codegen 等第三方工具。 支持 4 种通信模式：一对一(unary)、客户端流、服务端流、双端流。更灵活 只是目前 gRPC 对 broswer 的支持仍然不是很好，如果你需要通过浏览器访问 api，那 gRPC 可能不是你的菜。 如果你的产品只打算面向 App 等可控的客户端，可以考虑上 gRPC。 对同时需要为浏览器和 APP 提供服务应用而言，也可以考虑 APP 使用 gRPC 协议，而浏览器使用 API 网关提供的 HTTP 接口，在 API 网关上进行 HTTP - gRPC 协议转换。 ","date":"2019-02-11","objectID":"/posts/websocket-http2-and-grpc/:3:0","series":null,"tags":["WebSocket","gRPC","HTTP/2"],"title":"WebSocket、HTTP/2 与 gRPC","uri":"/posts/websocket-http2-and-grpc/#三grpc-协议"},{"categories":["tech"],"content":" gRPC over HTTP/2 定义详细的定义参见官方文档 gRPC over HTTP/2. 这里是简要说明几点： gRPC 完全隐藏了 HTTP/2 本身的 method、headers、path 等语义，这些信息对用户而言完全不可见了。 请求统一使用 POST，响应状态统一为 200。只要响应是标准的 gRPC 格式，响应中的 HTTP 状态码将被完全忽略。 gRPC 定义了自己的 status 状态码、格式固定的 path、还有它自己的 headers。 ","date":"2019-02-11","objectID":"/posts/websocket-http2-and-grpc/:3:1","series":null,"tags":["WebSocket","gRPC","HTTP/2"],"title":"WebSocket、HTTP/2 与 gRPC","uri":"/posts/websocket-http2-and-grpc/#grpc-over-http2-定义"},{"categories":["tech"],"content":" 参考 深入探索 WebSockets 和 HTTP/2 HTTP/2 特性与抓包分析 SSE：服务器发送事件,使用长链接进行通讯 Using server-sent events - MDN Can I Use HTTP/2 on Browsers Python 3.x how to get http version (using requests library) WebSocket 是什么原理？ 原生模块打造一个简单的 WebSocket 服务器 Google Cloud - API design: Understanding gRPC, OpenAPI and REST and when to use them ","date":"2019-02-11","objectID":"/posts/websocket-http2-and-grpc/:4:0","series":null,"tags":["WebSocket","gRPC","HTTP/2"],"title":"WebSocket、HTTP/2 与 gRPC","uri":"/posts/websocket-http2-and-grpc/#参考"},{"categories":["tech"],"content":"不管是做爬虫还是写 Web App，Chrome 和 Firefox 的 DevTools 都是超常用的，但是经常发现别人的截图有什么字段我找不到，别人的什么功能我的 Chrome 没有，仔细一搜索才知道，原来是我不会用。。 Ctrl + Shift + I：打开 DevTools Ctrl + Shift + J：打开控制台 ","date":"2019-02-11","objectID":"/posts/web-browser-dev-tools/:0:0","series":null,"tags":["Chrome","Firefox","DevTools","Browser"],"title":"Chrome 与 Firefox-Dev 的 DevTools","uri":"/posts/web-browser-dev-tools/#"},{"categories":["tech"],"content":" 搜索 Ctrl + F：在当前位置搜索关键字 在网页界面用这个快捷键，就是页内搜索 在 Network 的 Response 页面，就是在 Response 中搜索 Ctrl + Shift + F：全文搜索，在当前 Web App 的所有文件中搜索。 **爬虫必备！！！**要寻找一些特殊字符串的来源，用它搜索屡试不爽。 ","date":"2019-02-11","objectID":"/posts/web-browser-dev-tools/:0:1","series":null,"tags":["Chrome","Firefox","DevTools","Browser"],"title":"Chrome 与 Firefox-Dev 的 DevTools","uri":"/posts/web-browser-dev-tools/#搜索"},{"categories":["tech"],"content":" Command在 DevTools 里按快捷键 Ctrl + Shift + P 就能打开 Command 输入框，它和 vscode/sublime 的 Command 一样，用好了特别方便。 ","date":"2019-02-11","objectID":"/posts/web-browser-dev-tools/:0:2","series":null,"tags":["Chrome","Firefox","DevTools","Browser"],"title":"Chrome 与 Firefox-Dev 的 DevTools","uri":"/posts/web-browser-dev-tools/#command"},{"categories":["tech"],"content":" Network 1. 属性列Chrome 可以右键属性列名来增减属性列，Firefox-Dev 也是一样： 2. copy在 Chrome 中右键某个请求，选中 copy，会给出几个 options： 而 Firefox-Dev 的更强一点，可以复制消息头（请求头和响应头）： 3. response 的 pretty printChrome 的 Response 页面左下角，有{}按钮，可以 beautify 响应。 而 Firefox-Dev 只在 Debugger 页面提供该按钮，Response 中不支持。 Firefox 响应的 preview 和 payload 是放在 Response 页面下。而 Chrome 是分成了两个标签页 4. 导出 HAR右键任意一个请求，选择 save all as HAR，就可以导出 HAR 文件。（Chrome 有 with content，导出的 HAR 文件会含有所有请求与响应的 body） 该文件可用于存储当前监听到的所有浏览器请求信息。在以后需要分析这些请求时，将 HAR 文件拖到 Network 页面即可恢复所有请求。 5. Raw Headers（原始消息头）Chrome 只支持查看 HTTP/1.x 的 Raw Headers，对这种请求，会给出 view source 选项。 Chrome 不能查看 HTTP/2 的 Raw Headers。 而 Firefox 则支持查看 HTTP/2 的 Raw Headers。（是恢复后的，HTTP/2 的原始消息头是二进制压缩形式） 它还提供 Edit and Resend 请求的功能，这点要给个赞～ 6. 审查 WebSocket（Chrome only）在 NetWork 中点击对应的 WebScoket 请求，在右侧选择 Frames 标签，就可以看到所有的消息了 7. 跨页面加载时，保留网络请求记录当页面重载或者页面跳转时，默认情况下，Network面板下的网络请求记录表也是刷新的。如果想保留之前页面的网络请求数据，可以勾选Preserve log. 常用的一个应用场景：登录/注册时会调用登录/注册API，开发者想查看这个接口返回的情况，但是登录/注册成功后一般会跳转到新的页面，导致了Network面板的请求记录被刷新从而看不到登录/注册接口返回的情况。此时勾选上Preserve log，无论跳转到那个页面，都能在Network网络请求记录表中查看到之前接口返回的情况。 ","date":"2019-02-11","objectID":"/posts/web-browser-dev-tools/:0:3","series":null,"tags":["Chrome","Firefox","DevTools","Browser"],"title":"Chrome 与 Firefox-Dev 的 DevTools","uri":"/posts/web-browser-dev-tools/#network"},{"categories":["tech"],"content":" Network 1. 属性列Chrome 可以右键属性列名来增减属性列，Firefox-Dev 也是一样： 2. copy在 Chrome 中右键某个请求，选中 copy，会给出几个 options： 而 Firefox-Dev 的更强一点，可以复制消息头（请求头和响应头）： 3. response 的 pretty printChrome 的 Response 页面左下角，有{}按钮，可以 beautify 响应。 而 Firefox-Dev 只在 Debugger 页面提供该按钮，Response 中不支持。 Firefox 响应的 preview 和 payload 是放在 Response 页面下。而 Chrome 是分成了两个标签页 4. 导出 HAR右键任意一个请求，选择 save all as HAR，就可以导出 HAR 文件。（Chrome 有 with content，导出的 HAR 文件会含有所有请求与响应的 body） 该文件可用于存储当前监听到的所有浏览器请求信息。在以后需要分析这些请求时，将 HAR 文件拖到 Network 页面即可恢复所有请求。 5. Raw Headers（原始消息头）Chrome 只支持查看 HTTP/1.x 的 Raw Headers，对这种请求，会给出 view source 选项。 Chrome 不能查看 HTTP/2 的 Raw Headers。 而 Firefox 则支持查看 HTTP/2 的 Raw Headers。（是恢复后的，HTTP/2 的原始消息头是二进制压缩形式） 它还提供 Edit and Resend 请求的功能，这点要给个赞～ 6. 审查 WebSocket（Chrome only）在 NetWork 中点击对应的 WebScoket 请求，在右侧选择 Frames 标签，就可以看到所有的消息了 7. 跨页面加载时，保留网络请求记录当页面重载或者页面跳转时，默认情况下，Network面板下的网络请求记录表也是刷新的。如果想保留之前页面的网络请求数据，可以勾选Preserve log. 常用的一个应用场景：登录/注册时会调用登录/注册API，开发者想查看这个接口返回的情况，但是登录/注册成功后一般会跳转到新的页面，导致了Network面板的请求记录被刷新从而看不到登录/注册接口返回的情况。此时勾选上Preserve log，无论跳转到那个页面，都能在Network网络请求记录表中查看到之前接口返回的情况。 ","date":"2019-02-11","objectID":"/posts/web-browser-dev-tools/:0:3","series":null,"tags":["Chrome","Firefox","DevTools","Browser"],"title":"Chrome 与 Firefox-Dev 的 DevTools","uri":"/posts/web-browser-dev-tools/#1-属性列"},{"categories":["tech"],"content":" Network 1. 属性列Chrome 可以右键属性列名来增减属性列，Firefox-Dev 也是一样： 2. copy在 Chrome 中右键某个请求，选中 copy，会给出几个 options： 而 Firefox-Dev 的更强一点，可以复制消息头（请求头和响应头）： 3. response 的 pretty printChrome 的 Response 页面左下角，有{}按钮，可以 beautify 响应。 而 Firefox-Dev 只在 Debugger 页面提供该按钮，Response 中不支持。 Firefox 响应的 preview 和 payload 是放在 Response 页面下。而 Chrome 是分成了两个标签页 4. 导出 HAR右键任意一个请求，选择 save all as HAR，就可以导出 HAR 文件。（Chrome 有 with content，导出的 HAR 文件会含有所有请求与响应的 body） 该文件可用于存储当前监听到的所有浏览器请求信息。在以后需要分析这些请求时，将 HAR 文件拖到 Network 页面即可恢复所有请求。 5. Raw Headers（原始消息头）Chrome 只支持查看 HTTP/1.x 的 Raw Headers，对这种请求，会给出 view source 选项。 Chrome 不能查看 HTTP/2 的 Raw Headers。 而 Firefox 则支持查看 HTTP/2 的 Raw Headers。（是恢复后的，HTTP/2 的原始消息头是二进制压缩形式） 它还提供 Edit and Resend 请求的功能，这点要给个赞～ 6. 审查 WebSocket（Chrome only）在 NetWork 中点击对应的 WebScoket 请求，在右侧选择 Frames 标签，就可以看到所有的消息了 7. 跨页面加载时，保留网络请求记录当页面重载或者页面跳转时，默认情况下，Network面板下的网络请求记录表也是刷新的。如果想保留之前页面的网络请求数据，可以勾选Preserve log. 常用的一个应用场景：登录/注册时会调用登录/注册API，开发者想查看这个接口返回的情况，但是登录/注册成功后一般会跳转到新的页面，导致了Network面板的请求记录被刷新从而看不到登录/注册接口返回的情况。此时勾选上Preserve log，无论跳转到那个页面，都能在Network网络请求记录表中查看到之前接口返回的情况。 ","date":"2019-02-11","objectID":"/posts/web-browser-dev-tools/:0:3","series":null,"tags":["Chrome","Firefox","DevTools","Browser"],"title":"Chrome 与 Firefox-Dev 的 DevTools","uri":"/posts/web-browser-dev-tools/#2-copy"},{"categories":["tech"],"content":" Network 1. 属性列Chrome 可以右键属性列名来增减属性列，Firefox-Dev 也是一样： 2. copy在 Chrome 中右键某个请求，选中 copy，会给出几个 options： 而 Firefox-Dev 的更强一点，可以复制消息头（请求头和响应头）： 3. response 的 pretty printChrome 的 Response 页面左下角，有{}按钮，可以 beautify 响应。 而 Firefox-Dev 只在 Debugger 页面提供该按钮，Response 中不支持。 Firefox 响应的 preview 和 payload 是放在 Response 页面下。而 Chrome 是分成了两个标签页 4. 导出 HAR右键任意一个请求，选择 save all as HAR，就可以导出 HAR 文件。（Chrome 有 with content，导出的 HAR 文件会含有所有请求与响应的 body） 该文件可用于存储当前监听到的所有浏览器请求信息。在以后需要分析这些请求时，将 HAR 文件拖到 Network 页面即可恢复所有请求。 5. Raw Headers（原始消息头）Chrome 只支持查看 HTTP/1.x 的 Raw Headers，对这种请求，会给出 view source 选项。 Chrome 不能查看 HTTP/2 的 Raw Headers。 而 Firefox 则支持查看 HTTP/2 的 Raw Headers。（是恢复后的，HTTP/2 的原始消息头是二进制压缩形式） 它还提供 Edit and Resend 请求的功能，这点要给个赞～ 6. 审查 WebSocket（Chrome only）在 NetWork 中点击对应的 WebScoket 请求，在右侧选择 Frames 标签，就可以看到所有的消息了 7. 跨页面加载时，保留网络请求记录当页面重载或者页面跳转时，默认情况下，Network面板下的网络请求记录表也是刷新的。如果想保留之前页面的网络请求数据，可以勾选Preserve log. 常用的一个应用场景：登录/注册时会调用登录/注册API，开发者想查看这个接口返回的情况，但是登录/注册成功后一般会跳转到新的页面，导致了Network面板的请求记录被刷新从而看不到登录/注册接口返回的情况。此时勾选上Preserve log，无论跳转到那个页面，都能在Network网络请求记录表中查看到之前接口返回的情况。 ","date":"2019-02-11","objectID":"/posts/web-browser-dev-tools/:0:3","series":null,"tags":["Chrome","Firefox","DevTools","Browser"],"title":"Chrome 与 Firefox-Dev 的 DevTools","uri":"/posts/web-browser-dev-tools/#3-response-的-pretty-print"},{"categories":["tech"],"content":" Network 1. 属性列Chrome 可以右键属性列名来增减属性列，Firefox-Dev 也是一样： 2. copy在 Chrome 中右键某个请求，选中 copy，会给出几个 options： 而 Firefox-Dev 的更强一点，可以复制消息头（请求头和响应头）： 3. response 的 pretty printChrome 的 Response 页面左下角，有{}按钮，可以 beautify 响应。 而 Firefox-Dev 只在 Debugger 页面提供该按钮，Response 中不支持。 Firefox 响应的 preview 和 payload 是放在 Response 页面下。而 Chrome 是分成了两个标签页 4. 导出 HAR右键任意一个请求，选择 save all as HAR，就可以导出 HAR 文件。（Chrome 有 with content，导出的 HAR 文件会含有所有请求与响应的 body） 该文件可用于存储当前监听到的所有浏览器请求信息。在以后需要分析这些请求时，将 HAR 文件拖到 Network 页面即可恢复所有请求。 5. Raw Headers（原始消息头）Chrome 只支持查看 HTTP/1.x 的 Raw Headers，对这种请求，会给出 view source 选项。 Chrome 不能查看 HTTP/2 的 Raw Headers。 而 Firefox 则支持查看 HTTP/2 的 Raw Headers。（是恢复后的，HTTP/2 的原始消息头是二进制压缩形式） 它还提供 Edit and Resend 请求的功能，这点要给个赞～ 6. 审查 WebSocket（Chrome only）在 NetWork 中点击对应的 WebScoket 请求，在右侧选择 Frames 标签，就可以看到所有的消息了 7. 跨页面加载时，保留网络请求记录当页面重载或者页面跳转时，默认情况下，Network面板下的网络请求记录表也是刷新的。如果想保留之前页面的网络请求数据，可以勾选Preserve log. 常用的一个应用场景：登录/注册时会调用登录/注册API，开发者想查看这个接口返回的情况，但是登录/注册成功后一般会跳转到新的页面，导致了Network面板的请求记录被刷新从而看不到登录/注册接口返回的情况。此时勾选上Preserve log，无论跳转到那个页面，都能在Network网络请求记录表中查看到之前接口返回的情况。 ","date":"2019-02-11","objectID":"/posts/web-browser-dev-tools/:0:3","series":null,"tags":["Chrome","Firefox","DevTools","Browser"],"title":"Chrome 与 Firefox-Dev 的 DevTools","uri":"/posts/web-browser-dev-tools/#4-导出-har"},{"categories":["tech"],"content":" Network 1. 属性列Chrome 可以右键属性列名来增减属性列，Firefox-Dev 也是一样： 2. copy在 Chrome 中右键某个请求，选中 copy，会给出几个 options： 而 Firefox-Dev 的更强一点，可以复制消息头（请求头和响应头）： 3. response 的 pretty printChrome 的 Response 页面左下角，有{}按钮，可以 beautify 响应。 而 Firefox-Dev 只在 Debugger 页面提供该按钮，Response 中不支持。 Firefox 响应的 preview 和 payload 是放在 Response 页面下。而 Chrome 是分成了两个标签页 4. 导出 HAR右键任意一个请求，选择 save all as HAR，就可以导出 HAR 文件。（Chrome 有 with content，导出的 HAR 文件会含有所有请求与响应的 body） 该文件可用于存储当前监听到的所有浏览器请求信息。在以后需要分析这些请求时，将 HAR 文件拖到 Network 页面即可恢复所有请求。 5. Raw Headers（原始消息头）Chrome 只支持查看 HTTP/1.x 的 Raw Headers，对这种请求，会给出 view source 选项。 Chrome 不能查看 HTTP/2 的 Raw Headers。 而 Firefox 则支持查看 HTTP/2 的 Raw Headers。（是恢复后的，HTTP/2 的原始消息头是二进制压缩形式） 它还提供 Edit and Resend 请求的功能，这点要给个赞～ 6. 审查 WebSocket（Chrome only）在 NetWork 中点击对应的 WebScoket 请求，在右侧选择 Frames 标签，就可以看到所有的消息了 7. 跨页面加载时，保留网络请求记录当页面重载或者页面跳转时，默认情况下，Network面板下的网络请求记录表也是刷新的。如果想保留之前页面的网络请求数据，可以勾选Preserve log. 常用的一个应用场景：登录/注册时会调用登录/注册API，开发者想查看这个接口返回的情况，但是登录/注册成功后一般会跳转到新的页面，导致了Network面板的请求记录被刷新从而看不到登录/注册接口返回的情况。此时勾选上Preserve log，无论跳转到那个页面，都能在Network网络请求记录表中查看到之前接口返回的情况。 ","date":"2019-02-11","objectID":"/posts/web-browser-dev-tools/:0:3","series":null,"tags":["Chrome","Firefox","DevTools","Browser"],"title":"Chrome 与 Firefox-Dev 的 DevTools","uri":"/posts/web-browser-dev-tools/#5-raw-headers原始消息头"},{"categories":["tech"],"content":" Network 1. 属性列Chrome 可以右键属性列名来增减属性列，Firefox-Dev 也是一样： 2. copy在 Chrome 中右键某个请求，选中 copy，会给出几个 options： 而 Firefox-Dev 的更强一点，可以复制消息头（请求头和响应头）： 3. response 的 pretty printChrome 的 Response 页面左下角，有{}按钮，可以 beautify 响应。 而 Firefox-Dev 只在 Debugger 页面提供该按钮，Response 中不支持。 Firefox 响应的 preview 和 payload 是放在 Response 页面下。而 Chrome 是分成了两个标签页 4. 导出 HAR右键任意一个请求，选择 save all as HAR，就可以导出 HAR 文件。（Chrome 有 with content，导出的 HAR 文件会含有所有请求与响应的 body） 该文件可用于存储当前监听到的所有浏览器请求信息。在以后需要分析这些请求时，将 HAR 文件拖到 Network 页面即可恢复所有请求。 5. Raw Headers（原始消息头）Chrome 只支持查看 HTTP/1.x 的 Raw Headers，对这种请求，会给出 view source 选项。 Chrome 不能查看 HTTP/2 的 Raw Headers。 而 Firefox 则支持查看 HTTP/2 的 Raw Headers。（是恢复后的，HTTP/2 的原始消息头是二进制压缩形式） 它还提供 Edit and Resend 请求的功能，这点要给个赞～ 6. 审查 WebSocket（Chrome only）在 NetWork 中点击对应的 WebScoket 请求，在右侧选择 Frames 标签，就可以看到所有的消息了 7. 跨页面加载时，保留网络请求记录当页面重载或者页面跳转时，默认情况下，Network面板下的网络请求记录表也是刷新的。如果想保留之前页面的网络请求数据，可以勾选Preserve log. 常用的一个应用场景：登录/注册时会调用登录/注册API，开发者想查看这个接口返回的情况，但是登录/注册成功后一般会跳转到新的页面，导致了Network面板的请求记录被刷新从而看不到登录/注册接口返回的情况。此时勾选上Preserve log，无论跳转到那个页面，都能在Network网络请求记录表中查看到之前接口返回的情况。 ","date":"2019-02-11","objectID":"/posts/web-browser-dev-tools/:0:3","series":null,"tags":["Chrome","Firefox","DevTools","Browser"],"title":"Chrome 与 Firefox-Dev 的 DevTools","uri":"/posts/web-browser-dev-tools/#6-审查-websocketchrome-only"},{"categories":["tech"],"content":" Network 1. 属性列Chrome 可以右键属性列名来增减属性列，Firefox-Dev 也是一样： 2. copy在 Chrome 中右键某个请求，选中 copy，会给出几个 options： 而 Firefox-Dev 的更强一点，可以复制消息头（请求头和响应头）： 3. response 的 pretty printChrome 的 Response 页面左下角，有{}按钮，可以 beautify 响应。 而 Firefox-Dev 只在 Debugger 页面提供该按钮，Response 中不支持。 Firefox 响应的 preview 和 payload 是放在 Response 页面下。而 Chrome 是分成了两个标签页 4. 导出 HAR右键任意一个请求，选择 save all as HAR，就可以导出 HAR 文件。（Chrome 有 with content，导出的 HAR 文件会含有所有请求与响应的 body） 该文件可用于存储当前监听到的所有浏览器请求信息。在以后需要分析这些请求时，将 HAR 文件拖到 Network 页面即可恢复所有请求。 5. Raw Headers（原始消息头）Chrome 只支持查看 HTTP/1.x 的 Raw Headers，对这种请求，会给出 view source 选项。 Chrome 不能查看 HTTP/2 的 Raw Headers。 而 Firefox 则支持查看 HTTP/2 的 Raw Headers。（是恢复后的，HTTP/2 的原始消息头是二进制压缩形式） 它还提供 Edit and Resend 请求的功能，这点要给个赞～ 6. 审查 WebSocket（Chrome only）在 NetWork 中点击对应的 WebScoket 请求，在右侧选择 Frames 标签，就可以看到所有的消息了 7. 跨页面加载时，保留网络请求记录当页面重载或者页面跳转时，默认情况下，Network面板下的网络请求记录表也是刷新的。如果想保留之前页面的网络请求数据，可以勾选Preserve log. 常用的一个应用场景：登录/注册时会调用登录/注册API，开发者想查看这个接口返回的情况，但是登录/注册成功后一般会跳转到新的页面，导致了Network面板的请求记录被刷新从而看不到登录/注册接口返回的情况。此时勾选上Preserve log，无论跳转到那个页面，都能在Network网络请求记录表中查看到之前接口返回的情况。 ","date":"2019-02-11","objectID":"/posts/web-browser-dev-tools/:0:3","series":null,"tags":["Chrome","Firefox","DevTools","Browser"],"title":"Chrome 与 Firefox-Dev 的 DevTools","uri":"/posts/web-browser-dev-tools/#7-跨页面加载时保留网络请求记录"},{"categories":["tech"],"content":" JavaScript 控制台 可以通过 $x(\u003cxpath\u003e, \u003cDOM-element\u003e)，用 xpath 查询 DOM 元素。 通过控制台左上方的选单，可以切换 JS 的环境，它默认是当前页面（top）。 ","date":"2019-02-11","objectID":"/posts/web-browser-dev-tools/:0:4","series":null,"tags":["Chrome","Firefox","DevTools","Browser"],"title":"Chrome 与 Firefox-Dev 的 DevTools","uri":"/posts/web-browser-dev-tools/#javascript-控制台"},{"categories":["tech"],"content":" Elements 页面（Firefox-Dev 是 Inspector 页面） 1. DOM 元素断点（Chrome only）在 Elements 页面，右键一个元素，有一个 Break on 选项，可以控制在特定事件发生时 Break. - subtree modification: 子节点被修改 - attribute modification：当前节点的属性被修改。（inline style 被修改也会触发此事件） - node removal：节点被移除 2. 检索元素上注册的事件（Chrome only）在 Elements 页面选中一个元素（或者直接右键检查该元素），然后在右侧窗口，选择 Event Listeners 标签，就可以看到该元素上注册的所有事件。 3. 颜色选择器选中任一元素，在右侧选择 Styles 选单，会显示当前元素的 css 属性。 其中所有的颜色小方块都是可以点击的，点击颜色方块后 可以将颜色属性转换成多个格式（Chrome only） 默认格式：#207981 RGB格式：rgb(32, 121, 129) HSL格式：hsl(185, 60%, 32%) 提供 color picker，可用于在网页任意位置取色。（Firefox-Dev 也有） 提供复制按键，直接将该颜色当前格式的表达复制到剪切板 ","date":"2019-02-11","objectID":"/posts/web-browser-dev-tools/:0:5","series":null,"tags":["Chrome","Firefox","DevTools","Browser"],"title":"Chrome 与 Firefox-Dev 的 DevTools","uri":"/posts/web-browser-dev-tools/#elements-页面firefox-dev-是-inspector-页面"},{"categories":["tech"],"content":" Elements 页面（Firefox-Dev 是 Inspector 页面） 1. DOM 元素断点（Chrome only）在 Elements 页面，右键一个元素，有一个 Break on 选项，可以控制在特定事件发生时 Break. - subtree modification: 子节点被修改 - attribute modification：当前节点的属性被修改。（inline style 被修改也会触发此事件） - node removal：节点被移除 2. 检索元素上注册的事件（Chrome only）在 Elements 页面选中一个元素（或者直接右键检查该元素），然后在右侧窗口，选择 Event Listeners 标签，就可以看到该元素上注册的所有事件。 3. 颜色选择器选中任一元素，在右侧选择 Styles 选单，会显示当前元素的 css 属性。 其中所有的颜色小方块都是可以点击的，点击颜色方块后 可以将颜色属性转换成多个格式（Chrome only） 默认格式：#207981 RGB格式：rgb(32, 121, 129) HSL格式：hsl(185, 60%, 32%) 提供 color picker，可用于在网页任意位置取色。（Firefox-Dev 也有） 提供复制按键，直接将该颜色当前格式的表达复制到剪切板 ","date":"2019-02-11","objectID":"/posts/web-browser-dev-tools/:0:5","series":null,"tags":["Chrome","Firefox","DevTools","Browser"],"title":"Chrome 与 Firefox-Dev 的 DevTools","uri":"/posts/web-browser-dev-tools/#1-dom-元素断点chrome-only"},{"categories":["tech"],"content":" Elements 页面（Firefox-Dev 是 Inspector 页面） 1. DOM 元素断点（Chrome only）在 Elements 页面，右键一个元素，有一个 Break on 选项，可以控制在特定事件发生时 Break. - subtree modification: 子节点被修改 - attribute modification：当前节点的属性被修改。（inline style 被修改也会触发此事件） - node removal：节点被移除 2. 检索元素上注册的事件（Chrome only）在 Elements 页面选中一个元素（或者直接右键检查该元素），然后在右侧窗口，选择 Event Listeners 标签，就可以看到该元素上注册的所有事件。 3. 颜色选择器选中任一元素，在右侧选择 Styles 选单，会显示当前元素的 css 属性。 其中所有的颜色小方块都是可以点击的，点击颜色方块后 可以将颜色属性转换成多个格式（Chrome only） 默认格式：#207981 RGB格式：rgb(32, 121, 129) HSL格式：hsl(185, 60%, 32%) 提供 color picker，可用于在网页任意位置取色。（Firefox-Dev 也有） 提供复制按键，直接将该颜色当前格式的表达复制到剪切板 ","date":"2019-02-11","objectID":"/posts/web-browser-dev-tools/:0:5","series":null,"tags":["Chrome","Firefox","DevTools","Browser"],"title":"Chrome 与 Firefox-Dev 的 DevTools","uri":"/posts/web-browser-dev-tools/#2-检索元素上注册的事件chrome-only"},{"categories":["tech"],"content":" Elements 页面（Firefox-Dev 是 Inspector 页面） 1. DOM 元素断点（Chrome only）在 Elements 页面，右键一个元素，有一个 Break on 选项，可以控制在特定事件发生时 Break. - subtree modification: 子节点被修改 - attribute modification：当前节点的属性被修改。（inline style 被修改也会触发此事件） - node removal：节点被移除 2. 检索元素上注册的事件（Chrome only）在 Elements 页面选中一个元素（或者直接右键检查该元素），然后在右侧窗口，选择 Event Listeners 标签，就可以看到该元素上注册的所有事件。 3. 颜色选择器选中任一元素，在右侧选择 Styles 选单，会显示当前元素的 css 属性。 其中所有的颜色小方块都是可以点击的，点击颜色方块后 可以将颜色属性转换成多个格式（Chrome only） 默认格式：#207981 RGB格式：rgb(32, 121, 129) HSL格式：hsl(185, 60%, 32%) 提供 color picker，可用于在网页任意位置取色。（Firefox-Dev 也有） 提供复制按键，直接将该颜色当前格式的表达复制到剪切板 ","date":"2019-02-11","objectID":"/posts/web-browser-dev-tools/:0:5","series":null,"tags":["Chrome","Firefox","DevTools","Browser"],"title":"Chrome 与 Firefox-Dev 的 DevTools","uri":"/posts/web-browser-dev-tools/#3-颜色选择器"},{"categories":["tech"],"content":" 元素审查Ctrl + Shift + C 或者点击 DevTools 最左上角的小箭头按钮，就能进入元素审查模式。 该模式下会自动审查鼠标所指的元素，Elements 页面也会自动定位到该元素。 ","date":"2019-02-11","objectID":"/posts/web-browser-dev-tools/:0:6","series":null,"tags":["Chrome","Firefox","DevTools","Browser"],"title":"Chrome 与 Firefox-Dev 的 DevTools","uri":"/posts/web-browser-dev-tools/#元素审查"},{"categories":["tech"],"content":" Sources 页面（Firefox-Dev 是 Debuuger 页面）Sources 右侧的 Debugger 支持各种断点调试。 条件断点 Sources 中，在任意 JS 代码的行号上单击鼠标左键，就能在该行设置一个普通断点（在 Response 中可不行）。在行号上右键，能直接设置条件断点。 XHR 断点：在右侧 Debugger 中，可以添加 XHR 断点。 如果条件留空，一旦有 XHR 发起，就会无条件进入调试。 条件是 “Break When URL Contaions ” Watch Expressions：表达式审查 双击选中 JS 代码中的任意变量，然后右键它，可以将该变量添加到 Watch 中，这样就可以随时观察它的值。 也可以在右侧 Watch 中手动输入 JS 表达式。 DOM 元素断点（Chrome only）：在 Elements 部分讲过了。 Chrome 的断点功能比 Firefox-Dev 的更丰富。 ","date":"2019-02-11","objectID":"/posts/web-browser-dev-tools/:0:7","series":null,"tags":["Chrome","Firefox","DevTools","Browser"],"title":"Chrome 与 Firefox-Dev 的 DevTools","uri":"/posts/web-browser-dev-tools/#sources-页面firefox-dev-是-debuuger-页面"},{"categories":["tech"],"content":" Screenshot 1. For Chrome方法一：在 DevTools 界面，按快捷键 Ctrl + Shift + P 打开 Command 窗口，然后输入 screenshot，在下拉栏里选择你需要的截图命令就行。 方法二： 先进 dev tools，点击 左上角的设备图标（toggle device toolbar），然后页面顶部就会出现一个导航栏，在这里好选择设备或者自定义图像尺寸，然后点击该导航栏右侧（不是 dev tools 右侧）的 options 图标，会有两个选项：“截图（capture screenshot）”和“截网页全图（capture full size screenshot）”，如下： 2. For Firefox 只截显示出来的部分：和 Chrome 一样点击设备图标，然后在页面上面的 toolbar 就有截图按钮 截网页全图：在 DevTools 右边的 options 中进入 Settings，勾选 take a screenshot of the entire page，DevTools 右上角就会出现截图按钮了。 ","date":"2019-02-11","objectID":"/posts/web-browser-dev-tools/:0:8","series":null,"tags":["Chrome","Firefox","DevTools","Browser"],"title":"Chrome 与 Firefox-Dev 的 DevTools","uri":"/posts/web-browser-dev-tools/#screenshot"},{"categories":["tech"],"content":" Screenshot 1. For Chrome方法一：在 DevTools 界面，按快捷键 Ctrl + Shift + P 打开 Command 窗口，然后输入 screenshot，在下拉栏里选择你需要的截图命令就行。 方法二： 先进 dev tools，点击 左上角的设备图标（toggle device toolbar），然后页面顶部就会出现一个导航栏，在这里好选择设备或者自定义图像尺寸，然后点击该导航栏右侧（不是 dev tools 右侧）的 options 图标，会有两个选项：“截图（capture screenshot）”和“截网页全图（capture full size screenshot）”，如下： 2. For Firefox 只截显示出来的部分：和 Chrome 一样点击设备图标，然后在页面上面的 toolbar 就有截图按钮 截网页全图：在 DevTools 右边的 options 中进入 Settings，勾选 take a screenshot of the entire page，DevTools 右上角就会出现截图按钮了。 ","date":"2019-02-11","objectID":"/posts/web-browser-dev-tools/:0:8","series":null,"tags":["Chrome","Firefox","DevTools","Browser"],"title":"Chrome 与 Firefox-Dev 的 DevTools","uri":"/posts/web-browser-dev-tools/#1-for-chrome"},{"categories":["tech"],"content":" Screenshot 1. For Chrome方法一：在 DevTools 界面，按快捷键 Ctrl + Shift + P 打开 Command 窗口，然后输入 screenshot，在下拉栏里选择你需要的截图命令就行。 方法二： 先进 dev tools，点击 左上角的设备图标（toggle device toolbar），然后页面顶部就会出现一个导航栏，在这里好选择设备或者自定义图像尺寸，然后点击该导航栏右侧（不是 dev tools 右侧）的 options 图标，会有两个选项：“截图（capture screenshot）”和“截网页全图（capture full size screenshot）”，如下： 2. For Firefox 只截显示出来的部分：和 Chrome 一样点击设备图标，然后在页面上面的 toolbar 就有截图按钮 截网页全图：在 DevTools 右边的 options 中进入 Settings，勾选 take a screenshot of the entire page，DevTools 右上角就会出现截图按钮了。 ","date":"2019-02-11","objectID":"/posts/web-browser-dev-tools/:0:8","series":null,"tags":["Chrome","Firefox","DevTools","Browser"],"title":"Chrome 与 Firefox-Dev 的 DevTools","uri":"/posts/web-browser-dev-tools/#2-for-firefox"},{"categories":["tech"],"content":" 其他 1. Fake Geolocation（Chrome only）在 Chrome 中进入 DevTools，点击右上角的 options 按钮，选择 More tools -\u003e Sensors，在 Geolocation 处选择 Custom location，就可以修改地理位置了。 2. 自定义请求头 For Chrome和 上一小节一样，先进 More tools，选择 Network conditions，取消勾选 Select atuomatically，就可以修改请求头了。 上面的演示中，使用 python-requests/2.21.0 做 user agent，知乎返回 404. For Firefox打开设备模拟，在 toolbar 的右上角勾选 show user agent，然后就可以在 toolbar 修改 user agent 了： ","date":"2019-02-11","objectID":"/posts/web-browser-dev-tools/:0:9","series":null,"tags":["Chrome","Firefox","DevTools","Browser"],"title":"Chrome 与 Firefox-Dev 的 DevTools","uri":"/posts/web-browser-dev-tools/#其他"},{"categories":["tech"],"content":" 其他 1. Fake Geolocation（Chrome only）在 Chrome 中进入 DevTools，点击右上角的 options 按钮，选择 More tools -\u003e Sensors，在 Geolocation 处选择 Custom location，就可以修改地理位置了。 2. 自定义请求头 For Chrome和 上一小节一样，先进 More tools，选择 Network conditions，取消勾选 Select atuomatically，就可以修改请求头了。 上面的演示中，使用 python-requests/2.21.0 做 user agent，知乎返回 404. For Firefox打开设备模拟，在 toolbar 的右上角勾选 show user agent，然后就可以在 toolbar 修改 user agent 了： ","date":"2019-02-11","objectID":"/posts/web-browser-dev-tools/:0:9","series":null,"tags":["Chrome","Firefox","DevTools","Browser"],"title":"Chrome 与 Firefox-Dev 的 DevTools","uri":"/posts/web-browser-dev-tools/#1-fake-geolocationchrome-only"},{"categories":["tech"],"content":" 其他 1. Fake Geolocation（Chrome only）在 Chrome 中进入 DevTools，点击右上角的 options 按钮，选择 More tools -\u003e Sensors，在 Geolocation 处选择 Custom location，就可以修改地理位置了。 2. 自定义请求头 For Chrome和 上一小节一样，先进 More tools，选择 Network conditions，取消勾选 Select atuomatically，就可以修改请求头了。 上面的演示中，使用 python-requests/2.21.0 做 user agent，知乎返回 404. For Firefox打开设备模拟，在 toolbar 的右上角勾选 show user agent，然后就可以在 toolbar 修改 user agent 了： ","date":"2019-02-11","objectID":"/posts/web-browser-dev-tools/:0:9","series":null,"tags":["Chrome","Firefox","DevTools","Browser"],"title":"Chrome 与 Firefox-Dev 的 DevTools","uri":"/posts/web-browser-dev-tools/#2-自定义请求头"},{"categories":["tech"],"content":" 其他 1. Fake Geolocation（Chrome only）在 Chrome 中进入 DevTools，点击右上角的 options 按钮，选择 More tools -\u003e Sensors，在 Geolocation 处选择 Custom location，就可以修改地理位置了。 2. 自定义请求头 For Chrome和 上一小节一样，先进 More tools，选择 Network conditions，取消勾选 Select atuomatically，就可以修改请求头了。 上面的演示中，使用 python-requests/2.21.0 做 user agent，知乎返回 404. For Firefox打开设备模拟，在 toolbar 的右上角勾选 show user agent，然后就可以在 toolbar 修改 user agent 了： ","date":"2019-02-11","objectID":"/posts/web-browser-dev-tools/:0:9","series":null,"tags":["Chrome","Firefox","DevTools","Browser"],"title":"Chrome 与 Firefox-Dev 的 DevTools","uri":"/posts/web-browser-dev-tools/#for-chrome"},{"categories":["tech"],"content":" 其他 1. Fake Geolocation（Chrome only）在 Chrome 中进入 DevTools，点击右上角的 options 按钮，选择 More tools -\u003e Sensors，在 Geolocation 处选择 Custom location，就可以修改地理位置了。 2. 自定义请求头 For Chrome和 上一小节一样，先进 More tools，选择 Network conditions，取消勾选 Select atuomatically，就可以修改请求头了。 上面的演示中，使用 python-requests/2.21.0 做 user agent，知乎返回 404. For Firefox打开设备模拟，在 toolbar 的右上角勾选 show user agent，然后就可以在 toolbar 修改 user agent 了： ","date":"2019-02-11","objectID":"/posts/web-browser-dev-tools/:0:9","series":null,"tags":["Chrome","Firefox","DevTools","Browser"],"title":"Chrome 与 Firefox-Dev 的 DevTools","uri":"/posts/web-browser-dev-tools/#for-firefox"},{"categories":["tech"],"content":" 3. Request Blocking（Chrome only）在 Network 的任意请求上右键，菜单中就有 Block request URL（阻塞该 URL）和 Block request domain（阻塞请求所在的整个域） 然后就可以在 More tools -\u003e Request blocking 中看到你设置的阻塞条件。 ","date":"2019-02-11","objectID":"/posts/web-browser-dev-tools/:0:10","series":null,"tags":["Chrome","Firefox","DevTools","Browser"],"title":"Chrome 与 Firefox-Dev 的 DevTools","uri":"/posts/web-browser-dev-tools/#3-request-blockingchrome-only"},{"categories":["tech"],"content":" 参考 你不知道的 Chrome 调试技巧 Chrome Dev Tools 中文手册 Firefox Quantum：开发者版本 How to Capture Screenshots in Google Chrome without using Extensions Chrome DevTools - Network ","date":"2019-02-11","objectID":"/posts/web-browser-dev-tools/:0:11","series":null,"tags":["Chrome","Firefox","DevTools","Browser"],"title":"Chrome 与 Firefox-Dev 的 DevTools","uri":"/posts/web-browser-dev-tools/#参考"},{"categories":["tech"],"content":" 前言如果要在Linux 上设置一个开机自启，出现问题自动重启，并且有良好日志的程序，比较流行的方法有 supervisord、systemd，除此之外，还有 upstart、runit 等类似的工具。 但是自从 systemd 被 ubuntu、centos 等主流 Linux 发行版应用以来，systemd 渐渐成为主流方案。 如果你需要跨平台(Linux/MacOSX/FreeBSD)的方案，那么建议使用 supervisord，如果只需要支持 Linux 则建议选用 systemd. ","date":"2019-01-28","objectID":"/posts/systemd-basics/:1:0","series":null,"tags":["Linux","Systemd","Init System"],"title":"通过 systemctl 设置自定义 Service","uri":"/posts/systemd-basics/#前言"},{"categories":["tech"],"content":" 配置说明要自定义一个服务，需要在 /usr/lib/systemd/system/ 下添加一个配置文件：\u003csoftware-name\u003e.service 如果 /usr/lib/systemd/system/ 不存在，可考虑使用 /lib/systemd/system/ 或 /etc/systemd/system/ ExecXXX 中的命令，均可以正常使用转义字符以及环境变量插值语法，比如用 \\ 结尾表示换行，用 $Xxx 获取环境变量。 配置文件的内容说明： [Unit]: 服务的启动顺序与依赖关系 Description: 当前服务的简单描述 After: 当前服务（\u003csoftware-name\u003e.service）需要在这些服务启动后，才启动 Before: 和 After 相反，当前服务需要在这些服务启动前，先启动 Wants：表示当前服务\"弱依赖\"于这些服务。即当前服务依赖于它们，但是没有它们，当前服务也能正常运行。 Requires: 表示\"强依赖\"关系，即如果该服务启动失败或异常退出，那么当前服务也必须退出。 [Service] 服务运行参数的设置 Type=forking 后台运行的形式 PIDFile=/software-name/pid pid文件路径 EnvironmentFile=/xxx/prod.env 通过文件设定环境变量，注意这东西不支持环境变量的插值语法 ${xxx} WorkingDirectory=/xxx/xxx 工作目录 ExecStartPre 为启动做准备的命令 ExecStart 服务的具体运行命令（对非 workingdirectory 的文件，必须用绝对路径！ ExecReload 重载命令，如果程序支持 HUP 信号的话，通常将此项设为 `/bin/kill -HUP $MAINPID` ExecStop 停止命令 ExecStartPre：启动服务之前执行的命令 ExecStartPost：启动服务之后执行的命令 ExecStopPost：停止服务之后执行的命令 RuntimeDirectory=xxxx RuntimeDirectoryMode=0775 PrivateTmp=True 表示给服务分配独立的临时空间 RestartSec 自动重启当前服务间隔的秒数 Restart 定义何种情况 Systemd 会自动重启当前服务，可能的值包括always（总是重启）、on-success、on-failure 等 # 程序的 user 和 group User=ryan Group=ryan 注意：启动、重载、停止命令全部要求使用绝对路径 [Install] 定义如何安装这个配置文件，即怎样做到开机启动。 # Target的含义是服务组，表示一组服务。 WantedBy=multi-user.target 注意，service 文件不支持行内注释！！！注释必须单独一行 ","date":"2019-01-28","objectID":"/posts/systemd-basics/:1:1","series":null,"tags":["Linux","Systemd","Init System"],"title":"通过 systemctl 设置自定义 Service","uri":"/posts/systemd-basics/#配置说明"},{"categories":["tech"],"content":" Type 说明Type 感觉是整个配置文件里面最不好理解的一个配置项，它的实际作用就是：告诉 systemd 你的 Service 是如何启动的 Type=simple（默认值）：ExecStart 命令会立即启动你的服务，并且持续运行，不会退出。 Type=forking：ExecStart 命令会 fork 出你的服务主进程，然后正常退出。使用此 Type 时应同时指定 PIDFile=，systemd 使用它跟踪服务的主进程。 Type=oneshot：ExecStart 命令。可能需要同时设置 RemainAfterExit=yes 使得 systemd 在服务进程退出之后仍然认为服务处于激活状态 Type=notify：与 Type=simple 相同，但约定服务会在就绪后向 systemd 发送一个信号，以表明自己已经启动成功。 细节：systemd 会创建一个 unix socket，并将地址通过 $NOTIFY_SOCKET 环境变量提供给服务，同时监听该 socket 上的信号。服务可以使用 systemd 提供的 C 函数 sd_notify() 或者命令行工具 systemd-notify 发送信号给 systemd. 因为多了个 notify 信号，所以这一 Type 要比 simple 更精确一点。但是需要服务的配合， Type=dbus：若以此方式启动，当指定的 BusName 出现在 DBus 系统总线上时，systemd 认为服务就绪。 Type=idle：没搞明白，不过通常也用不到。 更详细的见 Systemd 入门教程：命令篇 - 阮一峰。 ","date":"2019-01-28","objectID":"/posts/systemd-basics/:1:2","series":null,"tags":["Linux","Systemd","Init System"],"title":"通过 systemctl 设置自定义 Service","uri":"/posts/systemd-basics/#type-说明"},{"categories":["tech"],"content":" 配置举例比如 shadsocks Server Service，的配置文件 ss-server.service 的内容为： [Unit] Description=shadsocks server After=network.target auditd.service [Service] Type=forking ExecStart=/usr/local/bin/ssserver -c /etc/shadsocks.json --user shadsocks --pid-file /var/run/shadsocks.pid -d start ExecStop=/usr/local/bin/ssserver -c /etc/shadsocks.json --user shadsocks --pid-file /var/run/shadsocks.pid -d stop PIDFile=/var/run/shadsocks.pid Restart=always RestartSec=4 [Install] WantedBy=multi-user.target 而 enginx 的配置文件 nginx.service 的内容是： [Unit] Description=The NGINX HTTP and reverse proxy server After=syslog.target network-online.target remote-fs.target nss-lookup.target Wants=network-online.target [Service] Type=forking PIDFile=/run/nginx.pid ExecStartPre=/usr/sbin/nginx -t ExecStart=/usr/sbin/nginx ExecReload=/usr/sbin/nginx -s reload ExecStop=/bin/kill -s QUIT $MAINPID PrivateTmp=true [Install] WantedBy=multi-user.target 为了使用环境变量插值，而使用 sh 启动的 etcd 服务，它的 etcd.service 配置如下: [Unit] Description=etcd key-value store Documentation=https://github.com/etcd-io/etcd After=network.target [Service] Type=simple # EnvironmentFile 不支持使用 ${xxx} 变量插值，这里不适合使用 # EnvironmentFile=/data/etcd.env # -a 表示传递环境变量 ExecStart=/bin/bash -ac '. /data/etcd.env; /data/bin/etcd' Restart=always RestartSec=5s LimitNOFILE=40000 [Install] WantedBy=multi-user.target 如果你不需要在 /data/etcd.env 中使用环境变量的插值语法，那可以这样写: [Unit] Description=etcd key-value store Documentation=https://github.com/etcd-io/etcd After=network.target [Service] Type=notify EnvironmentFile=/data/etcd.env # ExecXXX 的命令中是可以使用 ${Xxx} 插值语法的 ExecStart=/data/bin/etcd \\ --initial-advertise-peer-urls http://${THIS_IP}:2380 \\ --listen-peer-urls http://${THIS_IP}:2380 \\ --advertise-client-urls http://${THIS_IP}:2379 \\ --listen-client-urls http://${THIS_IP}:2379 \\ --initial-cluster \"${NAME_1}=http://${HOST_1}:2380,${NAME_2}=http://${HOST_2}:2380,${NAME_3}=http://${HOST_3}:2380\" Restart=always RestartSec=5s LimitNOFILE=40000 [Install] WantedBy=multi-user.target ","date":"2019-01-28","objectID":"/posts/systemd-basics/:1:3","series":null,"tags":["Linux","Systemd","Init System"],"title":"通过 systemctl 设置自定义 Service","uri":"/posts/systemd-basics/#配置举例"},{"categories":["tech"],"content":" 服务的启动、关闭 systemctl enable ss-server.service # 启用服务，即开机自动启动 systemctl disable ss-server.service # 取消服务，取消开机启动 systemctl start ss-server.service # 启动服务 systemctl stop ss-server.service # 停止服务 systemctl restart ss-server.service # 重启服务(stop + start) systemctl reload ss-server.service # 服务不 stop，直接加载配置更新等（对应 ExecReload） # 检查状态 systemctl status ss-server.service -l systemctl list-units --type=service # 查看所有服务 ","date":"2019-01-28","objectID":"/posts/systemd-basics/:1:4","series":null,"tags":["Linux","Systemd","Init System"],"title":"通过 systemctl 设置自定义 Service","uri":"/posts/systemd-basics/#服务的启动关闭"},{"categories":["tech"],"content":" 参考 Systemd 入门教程：命令篇 - 阮一峰 systemd.exec 中文手册 ","date":"2019-01-28","objectID":"/posts/systemd-basics/:1:5","series":null,"tags":["Linux","Systemd","Init System"],"title":"通过 systemctl 设置自定义 Service","uri":"/posts/systemd-basics/#参考"},{"categories":["tech"],"content":" 个人笔记，如有错误烦请指正。 SQLAlchemy 是一个用 Python 实现的 ORM （Object Relational Mapping）框架，它由多个组件构成，这些组件可以单独使用，也能独立使用。它的组件层次结构如下： SQLAlchemy 层次结构 其中最常用的组件，应该是 ORM 和 SQL 表达式语言，这两者既可以独立使用，也能结合使用。 ORM 的好处在于它 自动处理了数据库和 Python 对象之间的映射关系，屏蔽了两套系统之间的差异。程序员不需要再编写复杂的 SQL 语句，直接操作 Python 对象就行。 屏蔽了各数据库之间的差异，更换底层数据库不需要修改 SQL 语句，改下配置就行。 使数据库结构文档化，models 定义很清晰地描述了数据库的结构。 避免了不规范、冗余、风格不统一的 SQL 语句，可以避免很多人为 Bug，也方便维护。 但是 ORM 需要消耗额外的性能来处理对象关系映射，此外用 ORM 做多表关联查询或复杂 SQL 查询时，效率低下。因此它适用于场景不太复杂，性能要求不太苛刻的场景。 都说 ORM 学习成本高，我自己也更倾向于直接使用 SQL 语句（毕竟更熟悉），因此这一篇笔记不涉及 ORM 部分，只记录 SQLAlchemy 的 Engine 与 SQL 表达式语言。 ","date":"2019-01-21","objectID":"/posts/sqlalchemy-notes-1-engine-and-sql-expression-language/:0:0","series":null,"tags":["SQLAlchemy","Python","ORM","后端","数据库","Database"],"title":"SQLAlchemy 学习笔记（一）：Engine 与 SQL 表达式语言","uri":"/posts/sqlalchemy-notes-1-engine-and-sql-expression-language/#"},{"categories":["tech"],"content":" 一、直接使用 Engine 和 Connections第一步是创建数据库引擎实例： from sqlalchemy import create_engine engine = create_engine('sqlite:///:memory:', echo=True, # echo=True 表示打印出自动生成的 SQL 语句（通过 logging） pool_size=5, # 连接池容量，默认为 5，生产环境下太小，需要修改。 # 下面是 connection 回收的时间限制，默认 -1 不回收 pool_recycle=7200) # 超过 2 小时就重新连接（MySQL 默认的连接最大闲置时间为 8 小时） create_engine 接受的第一个参数是数据库 URI，格式为 dialect+driver://username:password@host:port/database，dialect 是具体的数据库名称，driver 是驱动名称。key-value 是可选的参数。举例： # PostgreSQL postgresql+psycopg2://scott:tiger@localhost/dbtest # MySQL + PyMySQL（或者用更快的 mysqlclient） mysql+pymysql://scott:tiger@localhost/dbtest # sqlite 内存数据库 # 注意 sqlite 要用三个斜杠，表示不存在 hostname，sqlite://\u003cnohostname\u003e/\u003cpath\u003e sqlite:///:memory: # sqlite 文件数据库 # 四个斜杠是因为文件的绝对路径以 / 开头：/home/ryan/Codes/Python/dbtest.db sqlite:////home/ryan/Codes/Python/dbtest.db # SQL Server + pyodbc # 首选基于 dsn 的连接，dsn 的配置请搜索hhh mssql+pyodbc://scott:tiger@some_dsn 如果你的密码中含有 ‘@’ 等特殊字符，就不能直接放入 URI 中，必须使用 urllib.parse.quote_plus 编码，然后再插入 URI. 引擎创建后，我们就可以直接获取 connection，然后执行 SQL 语句了。这种用法相当于把 SQLAlchemy 当成带 log 的数据库连接池使用： with engine.connect() as conn: res = conn.execute(\"select username from users\") # 无参直接使用 # 使用问号作占位符，前提是下层的 DBAPI 支持。更好的方式是使用 text()，这个后面说 conn.execute(\"INSERT INTO table (id, value) VALUES (?, ?)\", 1, \"v1\") # 参数不需要包装成元组 # 查询返回的是 ResultProxy 对象，有和 dbapi 相同的 fetchone()、fetchall()、first() 等方法，还有一些拓展方法 for row in result: print(\"username:\", row['username']) 但是要注意的是，connection 的 execute 是自动提交的（autocommit），这就像在 shell 里打开一个数据库客户端一样，分号结尾的 SQL 会被自动提交。 只有在 BEGIN TRANSACTION 内部，AUTOCOMMIT 会被临时设置为 FALSE，可以通过如下方法开始一个内部事务： def transaction_a(connection): trans = connection.begin() # 开启一个 transaction try: # do sthings trans.commit() # 这里需要手动提交 except: trans.rollback() # 出现异常则 rollback raise # do other things with engine.connect() as conn: transaction_a(conn) ","date":"2019-01-21","objectID":"/posts/sqlalchemy-notes-1-engine-and-sql-expression-language/:1:0","series":null,"tags":["SQLAlchemy","Python","ORM","后端","数据库","Database"],"title":"SQLAlchemy 学习笔记（一）：Engine 与 SQL 表达式语言","uri":"/posts/sqlalchemy-notes-1-engine-and-sql-expression-language/#一直接使用-enginehttpsdocssqlalchemyorgenlatestcoreengineshtmlsqlalchemycreate_engine-和-connections"},{"categories":["tech"],"content":" 1. 使用 text() 构建 SQL相比直接使用 string，text() 的优势在于它： 提供了统一的参数绑定语法，与具体的 DBAPI 无关。 # 1. 参数绑定语法 from sqlalchemy import text result = connection.execute( # 使用 :key 做占位符 text('select * from table where id \u003c :id and typeName=:type'), {'id': 2,'type':'USER_TABLE'}) # 用 dict 传参数，更易读 # 2. 参数类型指定 from sqlalchemy import DateTime date_param=datetime.today()+timedelta(days=-1*10) sql=\"delete from caw_job_alarm_log where alarm_time \u003c :alarm_time_param\" # bindparams 是 bindparam 的列表，bindparam 则提供参数的一些额外信息（类型、值、限制等） t=text(sql, bindparams=[bindparam('alarm_time_param', type_=DateTime, required=True)]) connection.execute(t, {\"alarm_time_param\": date_param}) 可以很方便地转换 Result 中列的类型 stmt = text(\"SELECT * FROM table\", # 使用 typemap 指定将 id 列映射为 Integer 类型，name 映射为 String 类型 typemap={'id': Integer, 'name': String}, ) result = connection.execute(stmt) # 对多个查询结果，可以用 for obj in result 遍历 # 也可用 fetchone() 只获取一个 ","date":"2019-01-21","objectID":"/posts/sqlalchemy-notes-1-engine-and-sql-expression-language/:1:1","series":null,"tags":["SQLAlchemy","Python","ORM","后端","数据库","Database"],"title":"SQLAlchemy 学习笔记（一）：Engine 与 SQL 表达式语言","uri":"/posts/sqlalchemy-notes-1-engine-and-sql-expression-language/#1-使用-texthttpsdocssqlalchemyorgenlatestcoresqlelementhtmlsqlalchemysqlexpressiontext-构建-sql"},{"categories":["tech"],"content":" 二、SQL 表达式语言 复杂的 SQL 查询可以直接用 raw sql 写，而增删改一般都是单表操作，用 SQL 表达式语言最方便。 SQLAlchemy 表达式语言是一个使用 Python 结构表示关系数据库结构和表达式的系统。 ","date":"2019-01-21","objectID":"/posts/sqlalchemy-notes-1-engine-and-sql-expression-language/:2:0","series":null,"tags":["SQLAlchemy","Python","ORM","后端","数据库","Database"],"title":"SQLAlchemy 学习笔记（一）：Engine 与 SQL 表达式语言","uri":"/posts/sqlalchemy-notes-1-engine-and-sql-expression-language/#二sql-表达式语言"},{"categories":["tech"],"content":" 1. 定义并创建表SQL 表达式语言使用 Table 来定义表，而表的列则用 Column 定义。Column 总是关联到一个 Table 对象上。 一组 Table 对象以及它们的子对象的集合就被称作「数据库元数据（database metadata）」。metadata 就像你的网页分类收藏夹，相关的 Table 放在一个 metadata 中。 下面是创建元数据（一组相关联的表）的例子，： from sqlalchemy import Table, Column, Integer, String, MetaData, ForeignKey metadata = MetaData() # 先创建元数据（收藏夹） users = Table('user', metadata, # 创建 user 表，并放到 metadata 中 Column('id', Integer, primary_key=True), Column('name', String), Column('fullname', String) ) addresses = Table('address', metadata, Column('id', Integer, primary_key=True), Column('user_id', None, ForeignKey('user.id')), # 外键约束，引用 user 表的 id 列 Column('email_address', String, nullable=False) ) metadata.create_all(engine) # 使用 engine 创建 metadata 内的所有 Tables（会检测表是否已经存在，所以可以重复调用） 表定义中的约束 应该给所有的约束命名，即为 name 参数指定一个不冲突的列名。详见 The Importance of Naming Constraints 表还有一个属性：约束条件。下面一一进行说明。 外键约束：用于在删除或更新某个值或行时，对主键/外键关系中一组数据列强制进行的操作限制。 用法一：Column('user_id', None, ForeignKey('user.id'))，直接在 Column 中指定。这也是最常用的方法 用法二：通过 ForeignKeyConstraint(columns, refcolumns) 构建约束，作为参数传给 Table. item = Table('item', metadata, # 商品 table Column('id', Integer, primary_key=True), Column('name', String(60), nullable=False), Column('invoice_id', Integer, nullable=False), # 发票 id，是外键 Column('ref_num', Integer, nullable=False), ForeignKeyConstraint(['invoice_id', 'ref_num'], # 当前表中的外键名称 ['invoice.id', 'invoice.ref_num']) # 被引用的外键名称的序列（被引用的表） ) on delete 与 on update：外键约束的两个约束条件，通过 ForeignKey() 或 ForeignKeyConstraint() 的关键字参数 ondelete/onupdate 传入。 可选值有： 默认行为 NO ACTION：什么都不做，直接报错。 CASCADE：删除/更新 父表数据时，从表数据会同时被 删除/更新。（无报错） RESTRICT：不允许直接 删除/更新 父表数据，直接报错。（和默认行为基本一致） SET NULL or SET DEFAULT：删除/更新 父表数据时，将对应的从表数据重置为 NULL 或者默认值。 唯一性约束：UniqueConstraint('col2', 'col3', name='uix_1')，作为参数传给 Table. CHECK 约束：CheckConstraint('col2 \u003e col3 + 5', name='check1')， 作为参数传给 Table. 主键约束：不解释 方法一：通过 Column('id', Integer, primary_key=True) 指定主键。（参数 primary_key 可在多个 Column 上使用） 方法二：使用 PrimaryKeyConstraint from sqlalchemy import PrimaryKeyConstraint my_table = Table('mytable', metadata, Column('id', Integer), Column('version_id', Integer), Column('data', String(50)), PrimaryKeyConstraint('id', 'version_id', name='mytable_pk') ) ","date":"2019-01-21","objectID":"/posts/sqlalchemy-notes-1-engine-and-sql-expression-language/:2:1","series":null,"tags":["SQLAlchemy","Python","ORM","后端","数据库","Database"],"title":"SQLAlchemy 学习笔记（一）：Engine 与 SQL 表达式语言","uri":"/posts/sqlalchemy-notes-1-engine-and-sql-expression-language/#1-定义并创建表"},{"categories":["tech"],"content":" 1. 定义并创建表SQL 表达式语言使用 Table 来定义表，而表的列则用 Column 定义。Column 总是关联到一个 Table 对象上。 一组 Table 对象以及它们的子对象的集合就被称作「数据库元数据（database metadata）」。metadata 就像你的网页分类收藏夹，相关的 Table 放在一个 metadata 中。 下面是创建元数据（一组相关联的表）的例子，： from sqlalchemy import Table, Column, Integer, String, MetaData, ForeignKey metadata = MetaData() # 先创建元数据（收藏夹） users = Table('user', metadata, # 创建 user 表，并放到 metadata 中 Column('id', Integer, primary_key=True), Column('name', String), Column('fullname', String) ) addresses = Table('address', metadata, Column('id', Integer, primary_key=True), Column('user_id', None, ForeignKey('user.id')), # 外键约束，引用 user 表的 id 列 Column('email_address', String, nullable=False) ) metadata.create_all(engine) # 使用 engine 创建 metadata 内的所有 Tables（会检测表是否已经存在，所以可以重复调用） 表定义中的约束 应该给所有的约束命名，即为 name 参数指定一个不冲突的列名。详见 The Importance of Naming Constraints 表还有一个属性：约束条件。下面一一进行说明。 外键约束：用于在删除或更新某个值或行时，对主键/外键关系中一组数据列强制进行的操作限制。 用法一：Column('user_id', None, ForeignKey('user.id'))，直接在 Column 中指定。这也是最常用的方法 用法二：通过 ForeignKeyConstraint(columns, refcolumns) 构建约束，作为参数传给 Table. item = Table('item', metadata, # 商品 table Column('id', Integer, primary_key=True), Column('name', String(60), nullable=False), Column('invoice_id', Integer, nullable=False), # 发票 id，是外键 Column('ref_num', Integer, nullable=False), ForeignKeyConstraint(['invoice_id', 'ref_num'], # 当前表中的外键名称 ['invoice.id', 'invoice.ref_num']) # 被引用的外键名称的序列（被引用的表） ) on delete 与 on update：外键约束的两个约束条件，通过 ForeignKey() 或 ForeignKeyConstraint() 的关键字参数 ondelete/onupdate 传入。 可选值有： 默认行为 NO ACTION：什么都不做，直接报错。 CASCADE：删除/更新 父表数据时，从表数据会同时被 删除/更新。（无报错） RESTRICT：不允许直接 删除/更新 父表数据，直接报错。（和默认行为基本一致） SET NULL or SET DEFAULT：删除/更新 父表数据时，将对应的从表数据重置为 NULL 或者默认值。 唯一性约束：UniqueConstraint('col2', 'col3', name='uix_1')，作为参数传给 Table. CHECK 约束：CheckConstraint('col2 \u003e col3 + 5', name='check1')， 作为参数传给 Table. 主键约束：不解释 方法一：通过 Column('id', Integer, primary_key=True) 指定主键。（参数 primary_key 可在多个 Column 上使用） 方法二：使用 PrimaryKeyConstraint from sqlalchemy import PrimaryKeyConstraint my_table = Table('mytable', metadata, Column('id', Integer), Column('version_id', Integer), Column('data', String(50)), PrimaryKeyConstraint('id', 'version_id', name='mytable_pk') ) ","date":"2019-01-21","objectID":"/posts/sqlalchemy-notes-1-engine-and-sql-expression-language/:2:1","series":null,"tags":["SQLAlchemy","Python","ORM","后端","数据库","Database"],"title":"SQLAlchemy 学习笔记（一）：Engine 与 SQL 表达式语言","uri":"/posts/sqlalchemy-notes-1-engine-and-sql-expression-language/#表定义中的约束"},{"categories":["tech"],"content":" 2. 增删改查语句 增: # 方法一，使用 values 传参 ins = users.insert().values(name=\"Jack\", fullname=\"Jack Jones\") # 可以通过 str(ins) 查看自动生成的 sql connection.execute(ins) # 方法二，参数传递给 execute() conn.execute(users.insert(), id=2, name='wendy', fullname='Wendy Williams') # 方法三，批量 INSERT，相当于 executemany conn.execute(addresses.insert(), [ # 插入到 addresses 表 {'user_id': 1, 'email_address': 'jack@yahoo.com'}, # 传入 dict 列表 {'user_id': 1, 'email_address': 'jack@msn.com'}, {'user_id': 2, 'email_address': 'www@www.org'}, {'user_id': 2, 'email_address': 'wendy@aol.com'} ]) # 此外，通过使用 bindparam，INSERT 还可以执行更复杂的操作 stmt = users.insert() \\ .values(name=bindparam('_name') + \" .. name\") # string 拼接 conn.execute(stmt, [ {'id':4, '_name':'name1'}, {'id':5, '_name':'name2'}, {'id':6, '_name':'name3'}, ]) 删： _table.delete() \\ .where(_table.c.f1==value1) \\ .where(_table.c.f2==value2) # where 指定条件 改： # 举例 stmt = users.update() \\ .where(users.c.name == 'jack') \\ .values(name='tom') conn.execute(stmt) # 批量更新 stmt = users.update() \\ .where(users.c.name == bindparam('oldname')) \\ .values(name=bindparam('newname')) conn.execute(stmt, [ {'oldname':'jack', 'newname':'ed'}, {'oldname':'wendy', 'newname':'mary'}, {'oldname':'jim', 'newname':'jake'}, ]) 可以看到，所有的条件都是通过 where 指定的，它和后面 ORM 的 filter 接受的参数是一样的。（详细的会在第二篇文章里讲） 查 from sqlalchemy.sql import select # 1. 字段选择 s1 = select([users]) # 相当于 select * from users s2 = select([users.c.name, users.c.fullname]) # 这个就是只 select 一部分 # 2. 添加过滤条件 s3 = select([users]) \\ .where(users.c.id == addresses.c.user_id) res = conn.execute(s1) # 可用 for row in res 遍历结果集，也可用 fetchone() 只获取一行 查询返回的是 ResultProxy 对象，这是 SQLAlchemy 对 Python DB-API 的 cursor 的一个封装类，要从中获取结果行，主要有下列几个方法： row1 = result.fetchone() # 对应 cursor.fetchone() row2 = result.fetchall() # 对应 cursor.fetchall() row3 = result.fetchmany(size=3) # 对应 cursor.fetchmany(size=3) row4 = result.first() # 获取一行，然后立即调用 result 的 close() 方法 col = row[mytable.c.mycol] # 获取 mycol 这一列 result.rowcount # 结果集的行数 同时，result 也实现了 next protocol，因此可以直接用 for 循环遍历 ","date":"2019-01-21","objectID":"/posts/sqlalchemy-notes-1-engine-and-sql-expression-language/:2:2","series":null,"tags":["SQLAlchemy","Python","ORM","后端","数据库","Database"],"title":"SQLAlchemy 学习笔记（一）：Engine 与 SQL 表达式语言","uri":"/posts/sqlalchemy-notes-1-engine-and-sql-expression-language/#2-增删改查语句"},{"categories":["tech"],"content":" where 进阶通过使用 or_、and_、in_ model.join 等方法，where 可以构建更复杂的 SQL 语句。 from sqlalchemy.sql import and_, or_, not_ s = select([(users.c.fullname + \", \" + addresses.c.email_address). label('title')]).\\ where(users.c.id == addresses.c.user_id).\\ where(users.c.name.between('m', 'z')).\\ where( or_( addresses.c.email_address.like('%@aol.com'), addresses.c.email_address.like('%@msn.com') ) ) ","date":"2019-01-21","objectID":"/posts/sqlalchemy-notes-1-engine-and-sql-expression-language/:2:3","series":null,"tags":["SQLAlchemy","Python","ORM","后端","数据库","Database"],"title":"SQLAlchemy 学习笔记（一）：Engine 与 SQL 表达式语言","uri":"/posts/sqlalchemy-notes-1-engine-and-sql-expression-language/#where-进阶"},{"categories":["tech"],"content":" 链接 使用 Engines 和 Connections SQL 表达式语言入门 SQLAlchemy - 定义约束 SQLAlchemy个人学习笔记完整汇总 hackersandslackers/sqlalchemy-tutorial ","date":"2019-01-21","objectID":"/posts/sqlalchemy-notes-1-engine-and-sql-expression-language/:3:0","series":null,"tags":["SQLAlchemy","Python","ORM","后端","数据库","Database"],"title":"SQLAlchemy 学习笔记（一）：Engine 与 SQL 表达式语言","uri":"/posts/sqlalchemy-notes-1-engine-and-sql-expression-language/#链接"},{"categories":["tech"],"content":" 个人笔记不保证正确。 数据类型是限制我们可以在表里存储什么数据的一种方法。不过，对于许多应用来说， 这种限制实在是太粗糙了。比如，一个包含产品价格的字段应该只接受正数。 但是没有哪种标准数据类型只接受正数。 另外一个问题是你可能需要根据其它字段或者其它行的数据来约束字段数据。比如， 在一个包含产品信息的表中，每个产品编号都应该只有一行。 对于这些问题，SQL 允许你在字段和表上定义约束。约束允许你对数据施加任意控制。 如果用户企图在字段里存储违反约束的数据，那么就会抛出一个错误。 这种情况同时也适用于数值来自默认值的情况。 ","date":"2019-01-20","objectID":"/posts/sql-basics-3-restrict/:0:0","series":null,"tags":["SQL","数据库","Database"],"title":"SQL 基础笔记（三）约束","uri":"/posts/sql-basics-3-restrict/#"},{"categories":["tech"],"content":" 1. 外键 FOREIGN KEY外键约束声明一个字段(或者一组字段)的数值必须匹配另外一个表中出现的数值。 创建外键约束的前提是，该外键所在的表已经存在，并且外键必须是 UNIQUE 的。（主键默认 UNIQUE 且 NOT NULL） CREATE TABLE \u003c表名\u003e ( \u003c字段名\u003e \u003c类型\u003e PRIMARY KEY, \u003c字段名\u003e \u003c类型\u003e REFERENCES \u003c外键所在的表名\u003e (\u003c字段名\u003e), -- 这创建了一个外键 ... ); 还有另一种语法，它支持以多个字段为外键（字段约束也可以写成表约束，也就是放在一个独立的行中。而反过来很可能不行）： CREATE TABLE \u003c表名\u003e ( \u003c字段名1\u003e \u003c类型\u003e PRIMARY KEY, \u003c字段名2\u003e \u003c类型\u003e \u003c字段名3\u003e \u003c类型\u003e ... FOREIGN KEY (\u003c字段名2\u003e, \u003c字段名3\u003e) REFERENCES \u003c外键所在的表名\u003e (\u003c字段名4\u003e, \u003c字段名5\u003e) ); 一个表也可以包含多个外键约束。这个特性用于实现表之间的多对多关系。 比如你有关于产品和订单的表，但现在你想允许一个订单可以包含多种产品 (上面那个结构是不允许这么做的)，你可以使用这样的结构： CREATE TABLE products ( product_no integer PRIMARY KEY, name text, price numeric ); CREATE TABLE orders ( order_id integer PRIMARY KEY, shipping_address text, ... ); CREATE TABLE order_items ( product_no integer REFERENCES products, order_id integer REFERENCES orders, quantity integer, PRIMARY KEY (product_no, order_id) ); 外键能通过 ALTER 语句添加或删除 ","date":"2019-01-20","objectID":"/posts/sql-basics-3-restrict/:0:1","series":null,"tags":["SQL","数据库","Database"],"title":"SQL 基础笔记（三）约束","uri":"/posts/sql-basics-3-restrict/#1-外键-foreign-key"},{"categories":["tech"],"content":" 2. 级联操作 ON DELETE 与 ON UPDATE上面说过：外键约束声明一个字段(或者一组字段)的数值必须匹配另外一个表中出现的数值。 但是以 1. 中最后一个 sql 为例，如果一个订单（order）在创建之后，该订单包含的某个产品（product）被删除了，会发生什么？ 这个例子中，订单包含的产品通过外键被记录在 order_items 表中。现在如果你要删除 product 中某个被 order_items 引用了的行，默认情况为 NO ACTION，就是直接报错。 这个行为也可以手动指定： CREATE TABLE products ( product_no integer PRIMARY KEY, name text, price numeric ); CREATE TABLE orders ( order_id integer PRIMARY KEY, shipping_address text, ... ); CREATE TABLE order_items ( product_no integer REFERENCES products ON DELETE RESTRICT, -- 限制，也就是禁止删除被它引用的行 order_id integer REFERENCES orders ON DELETE CASCADE, -- 级联，在删除被它引用的行的时候，这一行本身也会被自动删除掉 quantity integer, PRIMARY KEY (product_no, order_id) ); 除了 RESTRICT 和 CASCADE 外，在外键上的动作还有两个选项：SET NULL 和 SET DEFAULT，顾名思义，就是在被引用的行删除后将外键设置为 NULL 或默认值。 ON UPDATE 与 ON DELETE 的动作是一样的，只是 CASCADE 表示同步更新。 ","date":"2019-01-20","objectID":"/posts/sql-basics-3-restrict/:0:2","series":null,"tags":["SQL","数据库","Database"],"title":"SQL 基础笔记（三）约束","uri":"/posts/sql-basics-3-restrict/#2-级联操作-on-delete-与-on-update"},{"categories":["tech"],"content":" 3. CHECK 约束 CREATE TABLE products ( product_no integer, name text, price numeric CHECK (price \u003e 0) ); 你还可以给这个约束取一个独立的名字。这样就可以令错误消息更清晰， 并且在你需要修改它的时候引用这个名字。语法是： CREATE TABLE products ( product_no integer, name text, price numeric CONSTRAINT positive_price CHECK (price \u003e 0) ); 稍微复杂一点的例子： CREATE TABLE products ( product_no integer, name text, price numeric CHECK (price \u003e 0), discounted_price numeric, CHECK (discounted_price \u003e 0 AND price \u003e discounted_price) ); 同样的，可以为 CHECK 命名，令错误信息更清晰： CREATE TABLE products ( product_no integer, name text, price numeric, CHECK (price \u003e 0), discounted_price numeric, CHECK (discounted_price \u003e 0), CONSTRAINT valid_discount CHECK (price \u003e discounted_price) ); 要注意的是，当约束表达式计算结果为真或 NULL 的时候，检查约束会被认为是满足条件的。 因为大多数表达式在含有 NULL 操作数的时候结果都是 NULL ，所以这些约束不能阻止字段值为 NULL 。要排除掉 NULL，只能使用 NOT NULL 约束。（所以就说 NULL 是万恶之源hhh） ","date":"2019-01-20","objectID":"/posts/sql-basics-3-restrict/:0:3","series":null,"tags":["SQL","数据库","Database"],"title":"SQL 基础笔记（三）约束","uri":"/posts/sql-basics-3-restrict/#3-check-约束"},{"categories":["tech"],"content":" 参考 约束 ","date":"2019-01-20","objectID":"/posts/sql-basics-3-restrict/:0:4","series":null,"tags":["SQL","数据库","Database"],"title":"SQL 基础笔记（三）约束","uri":"/posts/sql-basics-3-restrict/#参考"},{"categories":["tech"],"content":" 个人向，只会记录一些需要注意的点。 ","date":"2019-01-14","objectID":"/posts/julia-notes-1-array/:0:0","series":null,"tags":["JuliaLang"],"title":"Julia 学习笔记（一）：数组","uri":"/posts/julia-notes-1-array/#"},{"categories":["tech"],"content":" 前言学习 Julia 已经有一段时间了，但是进步缓慢。这一方面是最近代码写得少，一方面是 Julia 学习资料少、中文资料更少，但也有我没做笔记的缘故导致学习效率不佳。 最近发现一份很不错的入门教程：Introducing_Julia，但是它的中文版本仍然有很多不足，就打算给它添加翻译和润色（zxj5470 完成了绝大部分翻译工作），顺便总结一份自己的笔记。 NOTE：Julia 的主要语言特征在于类型系统和多重派发，而主要的科学计算特征则是矩阵和整个标准库及生态圈。 ","date":"2019-01-14","objectID":"/posts/julia-notes-1-array/:1:0","series":null,"tags":["JuliaLang"],"title":"Julia 学习笔记（一）：数组","uri":"/posts/julia-notes-1-array/#前言"},{"categories":["tech"],"content":" 一、数组在 Julia 中，数组被用作列表（lists）、向量（vectors）、表（tables）和矩阵（matrices）。 ","date":"2019-01-14","objectID":"/posts/julia-notes-1-array/:2:0","series":null,"tags":["JuliaLang"],"title":"Julia 学习笔记（一）：数组","uri":"/posts/julia-notes-1-array/#一数组"},{"categories":["tech"],"content":" 1. 数组的创建这里尤其需要注意的是数组构造的几种方法，以及它们的区别。 1.1 一维数组（vector/list） julia\u003e v = [1, 2, 3, 4] # 逗号分隔的语法用于创建一维数组 4-element Array{Int64,1}: 1 2 3 4 向量，指列向量，Julia 使用的是 Fortran Order，各种操作都是列优先于行的。（和 numpy 相反，numpy 是 C Order 的，行优先于列） 1.2. 二维数组（table/matrix） julia\u003e mat = [1 2 3 4] # 空格分隔的语法，用于创建二维数组（或称行向量） 1×4 Array{Int64,2}: 1 2 3 4 julia\u003e [1 2; 3 4] # 分号和换行符(\\n)，用于分隔数组中不同的行 2×2 Array{Int64,2}: 1 2 3 4 空格对应函数 hcat，表示横向拼接各个矩阵/元素。 分号和换行对应函数 vcat，表示垂直拼接各个矩阵/元素。 下面的例子演示了拼接（空格）和单纯分隔各个元素（逗号）的区别： julia\u003e [1 2 [3 4] 5] # 用空格做横向拼接（或称水平拼接） 1×5 Array{Int64,2}: 1 2 3 4 5 julia\u003e [1, 2, [3, 4], 5] # 用逗号分隔 4-element Array{Any,1}: 1 2 [3, 4] 5 能看到在拼接操作中，[3 4] 被“解开”了，而用逗号时，它的行为和 Python 的 list 一样（区别只是 Julia 的 list 列优先）。 使用拼接需要注意的情况举例： julia\u003e [1 2 [3, 4] 5] # 横向拼接要求 items 的行数相同！ ERROR: DimensionMismatch(\"mismatch in dimension 1 (expected 1 got 2)\") 因为 [3, 4] 有两行，而 数组中的其他项是数值，显然行数不同，所以抛出了 Error. 可以想见，垂直拼接则要求 items 的列数相同。 另外当垂直拼接用于基本元素时，效果等同于逗号。（结果都是单列数组） julia\u003e v = [1, 2, 3, 4] 4-element Array{Int64,1}: 1 2 3 4 julia\u003e h = [1; 2; 3; 4] 4-element Array{Int64,1}: 1 2 3 4 julia\u003e [[1; 2]; [3, 4]] # 等价于 [[1, 2]; [3, 4]] 4-element Array{Int64,1}: 1 2 3 4 ","date":"2019-01-14","objectID":"/posts/julia-notes-1-array/:2:1","series":null,"tags":["JuliaLang"],"title":"Julia 学习笔记（一）：数组","uri":"/posts/julia-notes-1-array/#1-数组的创建"},{"categories":["tech"],"content":" 1. 数组的创建这里尤其需要注意的是数组构造的几种方法，以及它们的区别。 1.1 一维数组（vector/list） julia\u003e v = [1, 2, 3, 4] # 逗号分隔的语法用于创建一维数组 4-element Array{Int64,1}: 1 2 3 4 向量，指列向量，Julia 使用的是 Fortran Order，各种操作都是列优先于行的。（和 numpy 相反，numpy 是 C Order 的，行优先于列） 1.2. 二维数组（table/matrix） julia\u003e mat = [1 2 3 4] # 空格分隔的语法，用于创建二维数组（或称行向量） 1×4 Array{Int64,2}: 1 2 3 4 julia\u003e [1 2; 3 4] # 分号和换行符(\\n)，用于分隔数组中不同的行 2×2 Array{Int64,2}: 1 2 3 4 空格对应函数 hcat，表示横向拼接各个矩阵/元素。 分号和换行对应函数 vcat，表示垂直拼接各个矩阵/元素。 下面的例子演示了拼接（空格）和单纯分隔各个元素（逗号）的区别： julia\u003e [1 2 [3 4] 5] # 用空格做横向拼接（或称水平拼接） 1×5 Array{Int64,2}: 1 2 3 4 5 julia\u003e [1, 2, [3, 4], 5] # 用逗号分隔 4-element Array{Any,1}: 1 2 [3, 4] 5 能看到在拼接操作中，[3 4] 被“解开”了，而用逗号时，它的行为和 Python 的 list 一样（区别只是 Julia 的 list 列优先）。 使用拼接需要注意的情况举例： julia\u003e [1 2 [3, 4] 5] # 横向拼接要求 items 的行数相同！ ERROR: DimensionMismatch(\"mismatch in dimension 1 (expected 1 got 2)\") 因为 [3, 4] 有两行，而 数组中的其他项是数值，显然行数不同，所以抛出了 Error. 可以想见，垂直拼接则要求 items 的列数相同。 另外当垂直拼接用于基本元素时，效果等同于逗号。（结果都是单列数组） julia\u003e v = [1, 2, 3, 4] 4-element Array{Int64,1}: 1 2 3 4 julia\u003e h = [1; 2; 3; 4] 4-element Array{Int64,1}: 1 2 3 4 julia\u003e [[1; 2]; [3, 4]] # 等价于 [[1, 2]; [3, 4]] 4-element Array{Int64,1}: 1 2 3 4 ","date":"2019-01-14","objectID":"/posts/julia-notes-1-array/:2:1","series":null,"tags":["JuliaLang"],"title":"Julia 学习笔记（一）：数组","uri":"/posts/julia-notes-1-array/#11-一维数组vectorlist"},{"categories":["tech"],"content":" 1. 数组的创建这里尤其需要注意的是数组构造的几种方法，以及它们的区别。 1.1 一维数组（vector/list） julia\u003e v = [1, 2, 3, 4] # 逗号分隔的语法用于创建一维数组 4-element Array{Int64,1}: 1 2 3 4 向量，指列向量，Julia 使用的是 Fortran Order，各种操作都是列优先于行的。（和 numpy 相反，numpy 是 C Order 的，行优先于列） 1.2. 二维数组（table/matrix） julia\u003e mat = [1 2 3 4] # 空格分隔的语法，用于创建二维数组（或称行向量） 1×4 Array{Int64,2}: 1 2 3 4 julia\u003e [1 2; 3 4] # 分号和换行符(\\n)，用于分隔数组中不同的行 2×2 Array{Int64,2}: 1 2 3 4 空格对应函数 hcat，表示横向拼接各个矩阵/元素。 分号和换行对应函数 vcat，表示垂直拼接各个矩阵/元素。 下面的例子演示了拼接（空格）和单纯分隔各个元素（逗号）的区别： julia\u003e [1 2 [3 4] 5] # 用空格做横向拼接（或称水平拼接） 1×5 Array{Int64,2}: 1 2 3 4 5 julia\u003e [1, 2, [3, 4], 5] # 用逗号分隔 4-element Array{Any,1}: 1 2 [3, 4] 5 能看到在拼接操作中，[3 4] 被“解开”了，而用逗号时，它的行为和 Python 的 list 一样（区别只是 Julia 的 list 列优先）。 使用拼接需要注意的情况举例： julia\u003e [1 2 [3, 4] 5] # 横向拼接要求 items 的行数相同！ ERROR: DimensionMismatch(\"mismatch in dimension 1 (expected 1 got 2)\") 因为 [3, 4] 有两行，而 数组中的其他项是数值，显然行数不同，所以抛出了 Error. 可以想见，垂直拼接则要求 items 的列数相同。 另外当垂直拼接用于基本元素时，效果等同于逗号。（结果都是单列数组） julia\u003e v = [1, 2, 3, 4] 4-element Array{Int64,1}: 1 2 3 4 julia\u003e h = [1; 2; 3; 4] 4-element Array{Int64,1}: 1 2 3 4 julia\u003e [[1; 2]; [3, 4]] # 等价于 [[1, 2]; [3, 4]] 4-element Array{Int64,1}: 1 2 3 4 ","date":"2019-01-14","objectID":"/posts/julia-notes-1-array/:2:1","series":null,"tags":["JuliaLang"],"title":"Julia 学习笔记（一）：数组","uri":"/posts/julia-notes-1-array/#12-二维数组tablematrix"},{"categories":["tech"],"content":" 2. 数组的索引数组的索引方式和 numpy 很类似。有很多高级索引方式。 这里我想说的是类似“齐次坐标”的索引特性。 首先，单个元素可以看作是零维的向量，数学上零维也可以看作是任意维，因此可以这样玩： julia\u003e 2[1] 2 julia\u003e 2[1, 1] # 被当成二维 2 julia\u003e 2[1][1] # 2[1] 仍然是整数 2 2 julia\u003e 2[1, 1, 1] # 三维 2 julia\u003e 3.14[1] 3.14 julia\u003e π[1, 1] π = 3.1415926535897... julia\u003e '1'[1] '1': ASCII/Unicode U+0031 (category Nd: Number, decimal digit) julia\u003e '1'[1, 1] '1': ASCII/Unicode U+0031 (category Nd: Number, decimal digit) 多维数组也能使用类似“齐次坐标”的索引方式： julia\u003e m = [1 2; 3 4] 2×2 Array{Int64,2}: 1 2 3 4 julia\u003e m[1][1] # m[1] 是整数 1，这相当于 1[1] 1 julia\u003e m[1, 1, 1] 1 julia\u003e m[1, 1, 1, 1] 1 多维矩阵，在更高的维度上，也能被当成“零维”来看待，前面说过了“零维”也相当于“无限维”，所以多维数组也能用这么索引。 但是拓展的维度索引只能是 1！既然被看作“零维”，就只相当于一个点，自然不可能有更高的索引： julia\u003e 1[1, 2] ERROR: BoundsError julia\u003e m[1, 1, 2] ERROR: BoundsError: attempt to access 2×2 Array{Int64,2} at index [1, 1, 2] ... julia\u003e m[1, 1, 1, 2] ERROR: BoundsError: attempt to access 2×2 Array{Int64,2} at index [1, 1, 1, 2] ... ","date":"2019-01-14","objectID":"/posts/julia-notes-1-array/:2:2","series":null,"tags":["JuliaLang"],"title":"Julia 学习笔记（一）：数组","uri":"/posts/julia-notes-1-array/#2-数组的索引"},{"categories":["tech"],"content":" 3. 推导式（comprehension）与生成器表达式（generator expression）和 Python 的列表推导式与生成器表达式很像，但是更强大——Julia 是面向矩阵的。 julia\u003e [i+j for i in 1:3 for j in 1:3] # 这个语法和 Python 一致 9-element Array{Int64,1}: 2 3 4 3 4 5 4 5 6 julia\u003e [i+j for i in 1:3, j in 1:3] # 这个是多维的语法 3×3 Array{Int64,2}: 2 3 4 3 4 5 4 5 6 # 在后面加 guard 的情况下，结果坍缩成一维（这时两种语法结果没有差别） julia\u003e [i+j for i in 1:3, j in 1:3 if iseven(i+j)] 5-element Array{Int64,1}: 2 4 4 4 6 # 在前面做判断，因为没有过滤元素，所以仍然保持了原有结构。 julia\u003e [(iseven(i+j) ? 1 : 2) for i in 1:3, j in 1:3] 3×3 Array{Int64,2}: 1 2 1 2 1 2 1 2 1 ","date":"2019-01-14","objectID":"/posts/julia-notes-1-array/:2:3","series":null,"tags":["JuliaLang"],"title":"Julia 学习笔记（一）：数组","uri":"/posts/julia-notes-1-array/#3-推导式comprehensionhttpsdocsjulialangorgenv1manualarrayscomprehensions-1与生成器表达式generator-expressionhttpsdocsjulialangorgenv1manualarraysgenerator-expressions-1"},{"categories":["tech"],"content":" 可先浏览加粗部分 ","date":"2019-01-14","objectID":"/posts/compression-related-instructions-under-linux/:0:0","series":null,"tags":["Linux","Compression","压缩","tar"],"title":"常见压缩格式的区别，及 Linux 下的压缩相关指令","uri":"/posts/compression-related-instructions-under-linux/#"},{"categories":["tech"],"content":" 一、常见压缩档*.zip | zip 程序压缩打包的档案； （很常见，但是因为不包含文档名编码信息，跨平台可能会乱码） *.rar | rar 程序压缩打包的档案；（在windows上很常见，但是是商业软件。） *.gz | gzip 程序压缩的档案； （linux目前使用最广泛的压缩格式） *.bz2 | bzip2 程序压缩的档案； *.xz | xz 程序压缩的档案； *.tar | tar 程序打包的资料，并没有压缩过； *.tar.gz | tar 程序打包的档案，其中并且经过 gzip 的压缩 （最常见） *.tar.bz2 | tar 程序打包的档案，其中并且经过 bzip2 的压缩 *.tar.xz | tar 程序打包的档案，其中并且经过 xz 的压缩 （新一代压缩选择） *.7z | 7zip 程序压缩打包的档案。 目前最常见的是 tar.gz tar.xz tar.7z 这三种格式。 ","date":"2019-01-14","objectID":"/posts/compression-related-instructions-under-linux/:1:0","series":null,"tags":["Linux","Compression","压缩","tar"],"title":"常见压缩格式的区别，及 Linux 下的压缩相关指令","uri":"/posts/compression-related-instructions-under-linux/#一常见压缩档"},{"categories":["tech"],"content":" 二、以能否压缩多文档分类 gzip bzip2 xz 这三个压缩格式都只能压缩单个文档。（换而言之，该进程的输入输出都是流，不包含文档树信息。） 因此如果要用它们压缩多个文档或目录，需要使用另一个软件来先将要压缩的文档打包成一个文档（包含文档树信息），这个命令就是 tar. 先使用 tar 归档要压缩的多文档，再对生成的 *.tar 使用 上述压缩指令（或者直接使用管道重定向），Linux 下是这样实现多文档压缩的。 而 7z 和 zip，以及 rar 格式，都同时具有了 归档(tar) 和 压缩 两个功能，（也就是该格式包含了文档树信息咯）也就是说它们可以直接压缩多个文档。 ","date":"2019-01-14","objectID":"/posts/compression-related-instructions-under-linux/:2:0","series":null,"tags":["Linux","Compression","压缩","tar"],"title":"常见压缩格式的区别，及 Linux 下的压缩相关指令","uri":"/posts/compression-related-instructions-under-linux/#二以能否压缩多文档分类"},{"categories":["tech"],"content":" 三、各格式使用的算法差别 gzip 成熟的格式，使用的算法基于 DEFLATE。（压缩比适中） 7z 新一代格式，使用的压缩算法可替换，默认是使用的 lzma/lzma2 算法，使用 AES-256 作为加密算法。 xz 同样使用的 lzma/lzma2 算法，不过只能压缩一个文档。（压缩比很高，相对的用时也更多） zip 同样是支持多种算法的压缩格式，默认应该是使用的 DEFLATE 算法。诞生较早，有很多缺陷。（跨平台乱码、容易被破解等） rar 使用 类DEFLATE 的专有算法，使用 AES 加密。(rar5.0 以后使用 AES-256CBC) 不过 zip 被广泛应用在安卓的 apk 格式、java 的 jar、电子书的 epub，还有 github、云硬盘的多文档下载中，原因嘛大概是 zip 很流行，所以不用担心目标平台没解压软件吧。 ","date":"2019-01-14","objectID":"/posts/compression-related-instructions-under-linux/:3:0","series":null,"tags":["Linux","Compression","压缩","tar"],"title":"常见压缩格式的区别，及 Linux 下的压缩相关指令","uri":"/posts/compression-related-instructions-under-linux/#三各格式使用的算法差别"},{"categories":["tech"],"content":" 四、如何选用压缩方案 tar.gz 在 linux 上最常见，在压缩率和压缩时间上拥有良好的平衡。如果有任何疑惑，就选用它吧，不会错。 tar.xz 是新一代的压缩格式，虽然拥有更好的压缩率，压缩/解压速度相对要慢很多倍。一般在电脑性能足够好的时候，可选用它。 7z 和 xz 同为新一代压缩格式，它更复杂，支持多文档压缩。而且更适合跨平台，推荐使用。 zip 因为跨平台容易导致文档名乱码，不建议使用。（虽然有这样的缺陷，但是却意外的用得很广泛，在前一节有说过） rar 性能不差，但是是商业格式，不开源，不建议使用。（做得比较好的是它的 recovery records，在网络环境不好，容易导致包损坏时，这个功能就特别棒） tar.bz2 算是 linux 压缩历史上，过渡时期的产物，性能也介于 gz 和 xz 之间，一般来说不需要考虑它。 总的来说，就是 Windows 上推荐使用 7z，而 Linux 上 推荐使用 tar.gz tar.xz 7z 之一。此外 rar 的损坏很容易修复，zip 受众多（要注意乱码问题），也可以考虑。 ","date":"2019-01-14","objectID":"/posts/compression-related-instructions-under-linux/:4:0","series":null,"tags":["Linux","Compression","压缩","tar"],"title":"常见压缩格式的区别，及 Linux 下的压缩相关指令","uri":"/posts/compression-related-instructions-under-linux/#四如何选用压缩方案"},{"categories":["tech"],"content":" 五、Linux 上的压缩相关指令","date":"2019-01-14","objectID":"/posts/compression-related-instructions-under-linux/:5:0","series":null,"tags":["Linux","Compression","压缩","tar"],"title":"常见压缩格式的区别，及 Linux 下的压缩相关指令","uri":"/posts/compression-related-instructions-under-linux/#五linux-上的压缩相关指令"},{"categories":["tech"],"content":" 1. tar 指令通过之前的介绍，可以看出常用的就是 tar gzip xz 等，如果要压缩多个文档，需要先使用tar，再用管道重定向到 gzip 或 xz，比较麻烦，而这几个指令又很常用。于是后来对tar做了增强。 tar 最初只是一个归档进程，而压缩则由其他的压缩软件来完成（一个进程只干一件事）。后来为了方便，丧心病狂地集成了各种压缩指令。因此这里就只介绍这一个命令了（它囊括了所有）。 tar 的选项与参数非常的多！我们只讲几个常用的选项，更多选项您可以自行 man tar 查询啰！ [dmtsai@study ~]$ tar [-z|-j|-J] [cv] [-f 待创建的新档名] filename... \u003c==打包与压缩 [dmtsai@study ~]$ tar [-z|-j|-J] [tv] [-f 既有的 tar档名] \u003c==察看档名 [dmtsai@study ~]$ tar [-z|-j|-J] [xv] [-f 既有的 tar档名] [-C 目录] \u003c==解压缩 选项与参数-c ：创建打包档案，可搭配 -v 来察看过程中被打包的档名(filename) -t ：察看打包档案的内容含有哪些档名，重点在察看『档名』就是了； -x ：解打包或解压缩的功能，可以搭配 -C (大写) 在特定目录解开 特别留意的是， -c, -t, -x 不可同时出现在一串指令列中。 -z ：透过 gzip 的支持进行压缩/解压缩：此时档名最好为 *.tar.gz -j ：透过 bzip2 的支持进行压缩/解压缩：此时档名最好为 *.tar.bz2 -J ：透过 xz 的支持进行压缩/解压缩：此时档名最好为 *.tar.xz 特别留意， -z, -j, -J 不可以同时出现在一串指令列中 -v ：在压缩/解压缩的过程中，将正在处理的档名显示出来！ -f filename：-f 后面要立刻接要被处理的档名！建议 -f 单独写一个选项啰！(比较不会忘记) -C 目录 ：这个选项用在解压缩，若要在特定目录解压缩，可以使用这个选项。 其他后续练习会使用到的选项介绍： -p(小写) ：保留备份资料的原本权限与属性，常用于备份(-c)重要的设定档 -P(大写) ：保留绝对路径，亦即允许备份资料中含有根目录存在之意； –exclude=FILE：在压缩的过程中，不要将 FILE 打包！ 其实最简单的使用 tar 就只要记忆底下的方式即可 压　缩：tar -zcv -f filename.tar.gz \u003c要被压缩的档案或目录名称\u003e 查看文档树：tar -ztv -f filename.tar.gz 解压缩：tar -zxv -f filename.tar.gz -C \u003c欲解压缩的目录\u003e` 上面的命令需要根据压缩格式的不同，选用 -z -j -J 选项，而实际上文档的后缀就已经表明了它的压缩格式，不免让人觉得多余。 因此就有这幺一条通用的压缩/解压 option -a, --auto-compress Use archive suffix to determine the compression program. 使用这个，便有了通用的解压指令： tar -axv -f file.tar.* （它适用于上述三种压缩格式） 仅解压指定的文档 先查看文档树找到需要解压的文档的文档名 tar -zxv -f 打包档.tar.gz 待解开档名 打包某目录，但不含该目录下的某些档案之作法使用 –exclude=FILE 选项（支持文档名的模式匹配，而且可重复） tar -zcv -f filename.tar.gz directory --exclude=FILE1 --exclude=func* 只打包目录中比指定时刻更新的文档使用 --newer-mtime=\"2015/06/17\" 选项。 tarfile, tarballtarfile | 纯打包、未压缩的 tar 文档 tarball | 压缩了的 tar 文档 2. zip格式（linux 一般也会自带，详细的请man） 压缩命令：zip 压缩目录：zip -r filename.zip directory r 表示递归压缩，压缩包会包含这个目录 解压命令：unzip 解压到某目录：unzip -d directory filename.zip (-d dir 表示将内容解压到dir目录内) -t 测试压缩档的完整性 -x filename 排除某文档 3. 7z格式（需要p7zip，deepin自带，更多的请man） 查看目录树：7z l file.7z (List contents of archive) 压缩：7z a file.7z file1 directory1 (a 为创建压缩档或向压缩档中添加文档/目录，一次可指定多个要压缩的文档或目录) 解压：7z x file.7z -o directory (解压到指定目录) 测试完整性： 7z t file.7z p7zip 安装好后，会提供 7z、7za、7zr 三个指令，一般来说直接用 7z 就行。 P.S. 7z 不会保存 Linux 文档的用户、用户组信息，因此不能直接用于 Linux 系统备份，此时建议用 tar.xz 或 tar.7z（也就是先用tar打包） 4. rar格式（还是那句话，更多的请man）rar 是非开源的格式，Linux 默认是不会包含 rar 压缩软件的，但是它的解压软件是开源的，deepin 自带 unrar，顺便 7zip 也可解压 rar 文档。 若想用linux创建rar压缩档，需要从rarlab上下载 Linux 版，（deepin源自带）不过要注意的是该 linux 版是 40 天试用版，要长期使用的话，可能需要破解。（rar 的 key 网上一搜一大把） 压缩：rar a file.rar file （这个是试用的） 解压：unrar x file.rar （这个开源免费） 其实我挺中意 rar 的修复功能的，不知道为啥 7z xz 这样的新格式没有添加类似的 recorvery records。上次下个 idea 的 tarball，下了四五次才下到一个完整的，要是用 rar 的话，大概一键修复就好了，可 tar.gz 我不知道怎幺修复，只好一遍遍重复下载。。 ","date":"2019-01-14","objectID":"/posts/compression-related-instructions-under-linux/:5:1","series":null,"tags":["Linux","Compression","压缩","tar"],"title":"常见压缩格式的区别，及 Linux 下的压缩相关指令","uri":"/posts/compression-related-instructions-under-linux/#1-tar-指令"},{"categories":["tech"],"content":" 1. tar 指令通过之前的介绍，可以看出常用的就是 tar gzip xz 等，如果要压缩多个文档，需要先使用tar，再用管道重定向到 gzip 或 xz，比较麻烦，而这几个指令又很常用。于是后来对tar做了增强。 tar 最初只是一个归档进程，而压缩则由其他的压缩软件来完成（一个进程只干一件事）。后来为了方便，丧心病狂地集成了各种压缩指令。因此这里就只介绍这一个命令了（它囊括了所有）。 tar 的选项与参数非常的多！我们只讲几个常用的选项，更多选项您可以自行 man tar 查询啰！ [dmtsai@study ~]$ tar [-z|-j|-J] [cv] [-f 待创建的新档名] filename... \u003c==打包与压缩 [dmtsai@study ~]$ tar [-z|-j|-J] [tv] [-f 既有的 tar档名] \u003c==察看档名 [dmtsai@study ~]$ tar [-z|-j|-J] [xv] [-f 既有的 tar档名] [-C 目录] \u003c==解压缩 选项与参数-c ：创建打包档案，可搭配 -v 来察看过程中被打包的档名(filename) -t ：察看打包档案的内容含有哪些档名，重点在察看『档名』就是了； -x ：解打包或解压缩的功能，可以搭配 -C (大写) 在特定目录解开 特别留意的是， -c, -t, -x 不可同时出现在一串指令列中。 -z ：透过 gzip 的支持进行压缩/解压缩：此时档名最好为 *.tar.gz -j ：透过 bzip2 的支持进行压缩/解压缩：此时档名最好为 *.tar.bz2 -J ：透过 xz 的支持进行压缩/解压缩：此时档名最好为 *.tar.xz 特别留意， -z, -j, -J 不可以同时出现在一串指令列中 -v ：在压缩/解压缩的过程中，将正在处理的档名显示出来！ -f filename：-f 后面要立刻接要被处理的档名！建议 -f 单独写一个选项啰！(比较不会忘记) -C 目录 ：这个选项用在解压缩，若要在特定目录解压缩，可以使用这个选项。 其他后续练习会使用到的选项介绍： -p(小写) ：保留备份资料的原本权限与属性，常用于备份(-c)重要的设定档 -P(大写) ：保留绝对路径，亦即允许备份资料中含有根目录存在之意； –exclude=FILE：在压缩的过程中，不要将 FILE 打包！ 其实最简单的使用 tar 就只要记忆底下的方式即可 压　缩：tar -zcv -f filename.tar.gz \u003c要被压缩的档案或目录名称\u003e 查看文档树：tar -ztv -f filename.tar.gz 解压缩：tar -zxv -f filename.tar.gz -C \u003c欲解压缩的目录\u003e` 上面的命令需要根据压缩格式的不同，选用 -z -j -J 选项，而实际上文档的后缀就已经表明了它的压缩格式，不免让人觉得多余。 因此就有这幺一条通用的压缩/解压 option -a, --auto-compress Use archive suffix to determine the compression program. 使用这个，便有了通用的解压指令： tar -axv -f file.tar.* （它适用于上述三种压缩格式） 仅解压指定的文档 先查看文档树找到需要解压的文档的文档名 tar -zxv -f 打包档.tar.gz 待解开档名 打包某目录，但不含该目录下的某些档案之作法使用 –exclude=FILE 选项（支持文档名的模式匹配，而且可重复） tar -zcv -f filename.tar.gz directory --exclude=FILE1 --exclude=func* 只打包目录中比指定时刻更新的文档使用 --newer-mtime=\"2015/06/17\" 选项。 tarfile, tarballtarfile | 纯打包、未压缩的 tar 文档 tarball | 压缩了的 tar 文档 2. zip格式（linux 一般也会自带，详细的请man） 压缩命令：zip 压缩目录：zip -r filename.zip directory r 表示递归压缩，压缩包会包含这个目录 解压命令：unzip 解压到某目录：unzip -d directory filename.zip (-d dir 表示将内容解压到dir目录内) -t 测试压缩档的完整性 -x filename 排除某文档 3. 7z格式（需要p7zip，deepin自带，更多的请man） 查看目录树：7z l file.7z (List contents of archive) 压缩：7z a file.7z file1 directory1 (a 为创建压缩档或向压缩档中添加文档/目录，一次可指定多个要压缩的文档或目录) 解压：7z x file.7z -o directory (解压到指定目录) 测试完整性： 7z t file.7z p7zip 安装好后，会提供 7z、7za、7zr 三个指令，一般来说直接用 7z 就行。 P.S. 7z 不会保存 Linux 文档的用户、用户组信息，因此不能直接用于 Linux 系统备份，此时建议用 tar.xz 或 tar.7z（也就是先用tar打包） 4. rar格式（还是那句话，更多的请man）rar 是非开源的格式，Linux 默认是不会包含 rar 压缩软件的，但是它的解压软件是开源的，deepin 自带 unrar，顺便 7zip 也可解压 rar 文档。 若想用linux创建rar压缩档，需要从rarlab上下载 Linux 版，（deepin源自带）不过要注意的是该 linux 版是 40 天试用版，要长期使用的话，可能需要破解。（rar 的 key 网上一搜一大把） 压缩：rar a file.rar file （这个是试用的） 解压：unrar x file.rar （这个开源免费） 其实我挺中意 rar 的修复功能的，不知道为啥 7z xz 这样的新格式没有添加类似的 recorvery records。上次下个 idea 的 tarball，下了四五次才下到一个完整的，要是用 rar 的话，大概一键修复就好了，可 tar.gz 我不知道怎幺修复，只好一遍遍重复下载。。 ","date":"2019-01-14","objectID":"/posts/compression-related-instructions-under-linux/:5:1","series":null,"tags":["Linux","Compression","压缩","tar"],"title":"常见压缩格式的区别，及 Linux 下的压缩相关指令","uri":"/posts/compression-related-instructions-under-linux/#选项与参数"},{"categories":["tech"],"content":" 1. tar 指令通过之前的介绍，可以看出常用的就是 tar gzip xz 等，如果要压缩多个文档，需要先使用tar，再用管道重定向到 gzip 或 xz，比较麻烦，而这几个指令又很常用。于是后来对tar做了增强。 tar 最初只是一个归档进程，而压缩则由其他的压缩软件来完成（一个进程只干一件事）。后来为了方便，丧心病狂地集成了各种压缩指令。因此这里就只介绍这一个命令了（它囊括了所有）。 tar 的选项与参数非常的多！我们只讲几个常用的选项，更多选项您可以自行 man tar 查询啰！ [dmtsai@study ~]$ tar [-z|-j|-J] [cv] [-f 待创建的新档名] filename... \u003c==打包与压缩 [dmtsai@study ~]$ tar [-z|-j|-J] [tv] [-f 既有的 tar档名] \u003c==察看档名 [dmtsai@study ~]$ tar [-z|-j|-J] [xv] [-f 既有的 tar档名] [-C 目录] \u003c==解压缩 选项与参数-c ：创建打包档案，可搭配 -v 来察看过程中被打包的档名(filename) -t ：察看打包档案的内容含有哪些档名，重点在察看『档名』就是了； -x ：解打包或解压缩的功能，可以搭配 -C (大写) 在特定目录解开 特别留意的是， -c, -t, -x 不可同时出现在一串指令列中。 -z ：透过 gzip 的支持进行压缩/解压缩：此时档名最好为 *.tar.gz -j ：透过 bzip2 的支持进行压缩/解压缩：此时档名最好为 *.tar.bz2 -J ：透过 xz 的支持进行压缩/解压缩：此时档名最好为 *.tar.xz 特别留意， -z, -j, -J 不可以同时出现在一串指令列中 -v ：在压缩/解压缩的过程中，将正在处理的档名显示出来！ -f filename：-f 后面要立刻接要被处理的档名！建议 -f 单独写一个选项啰！(比较不会忘记) -C 目录 ：这个选项用在解压缩，若要在特定目录解压缩，可以使用这个选项。 其他后续练习会使用到的选项介绍： -p(小写) ：保留备份资料的原本权限与属性，常用于备份(-c)重要的设定档 -P(大写) ：保留绝对路径，亦即允许备份资料中含有根目录存在之意； –exclude=FILE：在压缩的过程中，不要将 FILE 打包！ 其实最简单的使用 tar 就只要记忆底下的方式即可 压　缩：tar -zcv -f filename.tar.gz \u003c要被压缩的档案或目录名称\u003e 查看文档树：tar -ztv -f filename.tar.gz 解压缩：tar -zxv -f filename.tar.gz -C \u003c欲解压缩的目录\u003e` 上面的命令需要根据压缩格式的不同，选用 -z -j -J 选项，而实际上文档的后缀就已经表明了它的压缩格式，不免让人觉得多余。 因此就有这幺一条通用的压缩/解压 option -a, --auto-compress Use archive suffix to determine the compression program. 使用这个，便有了通用的解压指令： tar -axv -f file.tar.* （它适用于上述三种压缩格式） 仅解压指定的文档 先查看文档树找到需要解压的文档的文档名 tar -zxv -f 打包档.tar.gz 待解开档名 打包某目录，但不含该目录下的某些档案之作法使用 –exclude=FILE 选项（支持文档名的模式匹配，而且可重复） tar -zcv -f filename.tar.gz directory --exclude=FILE1 --exclude=func* 只打包目录中比指定时刻更新的文档使用 --newer-mtime=\"2015/06/17\" 选项。 tarfile, tarballtarfile | 纯打包、未压缩的 tar 文档 tarball | 压缩了的 tar 文档 2. zip格式（linux 一般也会自带，详细的请man） 压缩命令：zip 压缩目录：zip -r filename.zip directory r 表示递归压缩，压缩包会包含这个目录 解压命令：unzip 解压到某目录：unzip -d directory filename.zip (-d dir 表示将内容解压到dir目录内) -t 测试压缩档的完整性 -x filename 排除某文档 3. 7z格式（需要p7zip，deepin自带，更多的请man） 查看目录树：7z l file.7z (List contents of archive) 压缩：7z a file.7z file1 directory1 (a 为创建压缩档或向压缩档中添加文档/目录，一次可指定多个要压缩的文档或目录) 解压：7z x file.7z -o directory (解压到指定目录) 测试完整性： 7z t file.7z p7zip 安装好后，会提供 7z、7za、7zr 三个指令，一般来说直接用 7z 就行。 P.S. 7z 不会保存 Linux 文档的用户、用户组信息，因此不能直接用于 Linux 系统备份，此时建议用 tar.xz 或 tar.7z（也就是先用tar打包） 4. rar格式（还是那句话，更多的请man）rar 是非开源的格式，Linux 默认是不会包含 rar 压缩软件的，但是它的解压软件是开源的，deepin 自带 unrar，顺便 7zip 也可解压 rar 文档。 若想用linux创建rar压缩档，需要从rarlab上下载 Linux 版，（deepin源自带）不过要注意的是该 linux 版是 40 天试用版，要长期使用的话，可能需要破解。（rar 的 key 网上一搜一大把） 压缩：rar a file.rar file （这个是试用的） 解压：unrar x file.rar （这个开源免费） 其实我挺中意 rar 的修复功能的，不知道为啥 7z xz 这样的新格式没有添加类似的 recorvery records。上次下个 idea 的 tarball，下了四五次才下到一个完整的，要是用 rar 的话，大概一键修复就好了，可 tar.gz 我不知道怎幺修复，只好一遍遍重复下载。。 ","date":"2019-01-14","objectID":"/posts/compression-related-instructions-under-linux/:5:1","series":null,"tags":["Linux","Compression","压缩","tar"],"title":"常见压缩格式的区别，及 Linux 下的压缩相关指令","uri":"/posts/compression-related-instructions-under-linux/#其实最简单的使用-tar-就只要记忆底下的方式即可"},{"categories":["tech"],"content":" 1. tar 指令通过之前的介绍，可以看出常用的就是 tar gzip xz 等，如果要压缩多个文档，需要先使用tar，再用管道重定向到 gzip 或 xz，比较麻烦，而这几个指令又很常用。于是后来对tar做了增强。 tar 最初只是一个归档进程，而压缩则由其他的压缩软件来完成（一个进程只干一件事）。后来为了方便，丧心病狂地集成了各种压缩指令。因此这里就只介绍这一个命令了（它囊括了所有）。 tar 的选项与参数非常的多！我们只讲几个常用的选项，更多选项您可以自行 man tar 查询啰！ [dmtsai@study ~]$ tar [-z|-j|-J] [cv] [-f 待创建的新档名] filename... \u003c==打包与压缩 [dmtsai@study ~]$ tar [-z|-j|-J] [tv] [-f 既有的 tar档名] \u003c==察看档名 [dmtsai@study ~]$ tar [-z|-j|-J] [xv] [-f 既有的 tar档名] [-C 目录] \u003c==解压缩 选项与参数-c ：创建打包档案，可搭配 -v 来察看过程中被打包的档名(filename) -t ：察看打包档案的内容含有哪些档名，重点在察看『档名』就是了； -x ：解打包或解压缩的功能，可以搭配 -C (大写) 在特定目录解开 特别留意的是， -c, -t, -x 不可同时出现在一串指令列中。 -z ：透过 gzip 的支持进行压缩/解压缩：此时档名最好为 *.tar.gz -j ：透过 bzip2 的支持进行压缩/解压缩：此时档名最好为 *.tar.bz2 -J ：透过 xz 的支持进行压缩/解压缩：此时档名最好为 *.tar.xz 特别留意， -z, -j, -J 不可以同时出现在一串指令列中 -v ：在压缩/解压缩的过程中，将正在处理的档名显示出来！ -f filename：-f 后面要立刻接要被处理的档名！建议 -f 单独写一个选项啰！(比较不会忘记) -C 目录 ：这个选项用在解压缩，若要在特定目录解压缩，可以使用这个选项。 其他后续练习会使用到的选项介绍： -p(小写) ：保留备份资料的原本权限与属性，常用于备份(-c)重要的设定档 -P(大写) ：保留绝对路径，亦即允许备份资料中含有根目录存在之意； –exclude=FILE：在压缩的过程中，不要将 FILE 打包！ 其实最简单的使用 tar 就只要记忆底下的方式即可 压　缩：tar -zcv -f filename.tar.gz \u003c要被压缩的档案或目录名称\u003e 查看文档树：tar -ztv -f filename.tar.gz 解压缩：tar -zxv -f filename.tar.gz -C \u003c欲解压缩的目录\u003e` 上面的命令需要根据压缩格式的不同，选用 -z -j -J 选项，而实际上文档的后缀就已经表明了它的压缩格式，不免让人觉得多余。 因此就有这幺一条通用的压缩/解压 option -a, --auto-compress Use archive suffix to determine the compression program. 使用这个，便有了通用的解压指令： tar -axv -f file.tar.* （它适用于上述三种压缩格式） 仅解压指定的文档 先查看文档树找到需要解压的文档的文档名 tar -zxv -f 打包档.tar.gz 待解开档名 打包某目录，但不含该目录下的某些档案之作法使用 –exclude=FILE 选项（支持文档名的模式匹配，而且可重复） tar -zcv -f filename.tar.gz directory --exclude=FILE1 --exclude=func* 只打包目录中比指定时刻更新的文档使用 --newer-mtime=\"2015/06/17\" 选项。 tarfile, tarballtarfile | 纯打包、未压缩的 tar 文档 tarball | 压缩了的 tar 文档 2. zip格式（linux 一般也会自带，详细的请man） 压缩命令：zip 压缩目录：zip -r filename.zip directory r 表示递归压缩，压缩包会包含这个目录 解压命令：unzip 解压到某目录：unzip -d directory filename.zip (-d dir 表示将内容解压到dir目录内) -t 测试压缩档的完整性 -x filename 排除某文档 3. 7z格式（需要p7zip，deepin自带，更多的请man） 查看目录树：7z l file.7z (List contents of archive) 压缩：7z a file.7z file1 directory1 (a 为创建压缩档或向压缩档中添加文档/目录，一次可指定多个要压缩的文档或目录) 解压：7z x file.7z -o directory (解压到指定目录) 测试完整性： 7z t file.7z p7zip 安装好后，会提供 7z、7za、7zr 三个指令，一般来说直接用 7z 就行。 P.S. 7z 不会保存 Linux 文档的用户、用户组信息，因此不能直接用于 Linux 系统备份，此时建议用 tar.xz 或 tar.7z（也就是先用tar打包） 4. rar格式（还是那句话，更多的请man）rar 是非开源的格式，Linux 默认是不会包含 rar 压缩软件的，但是它的解压软件是开源的，deepin 自带 unrar，顺便 7zip 也可解压 rar 文档。 若想用linux创建rar压缩档，需要从rarlab上下载 Linux 版，（deepin源自带）不过要注意的是该 linux 版是 40 天试用版，要长期使用的话，可能需要破解。（rar 的 key 网上一搜一大把） 压缩：rar a file.rar file （这个是试用的） 解压：unrar x file.rar （这个开源免费） 其实我挺中意 rar 的修复功能的，不知道为啥 7z xz 这样的新格式没有添加类似的 recorvery records。上次下个 idea 的 tarball，下了四五次才下到一个完整的，要是用 rar 的话，大概一键修复就好了，可 tar.gz 我不知道怎幺修复，只好一遍遍重复下载。。 ","date":"2019-01-14","objectID":"/posts/compression-related-instructions-under-linux/:5:1","series":null,"tags":["Linux","Compression","压缩","tar"],"title":"常见压缩格式的区别，及 Linux 下的压缩相关指令","uri":"/posts/compression-related-instructions-under-linux/#仅解压指定的文档"},{"categories":["tech"],"content":" 1. tar 指令通过之前的介绍，可以看出常用的就是 tar gzip xz 等，如果要压缩多个文档，需要先使用tar，再用管道重定向到 gzip 或 xz，比较麻烦，而这几个指令又很常用。于是后来对tar做了增强。 tar 最初只是一个归档进程，而压缩则由其他的压缩软件来完成（一个进程只干一件事）。后来为了方便，丧心病狂地集成了各种压缩指令。因此这里就只介绍这一个命令了（它囊括了所有）。 tar 的选项与参数非常的多！我们只讲几个常用的选项，更多选项您可以自行 man tar 查询啰！ [dmtsai@study ~]$ tar [-z|-j|-J] [cv] [-f 待创建的新档名] filename... \u003c==打包与压缩 [dmtsai@study ~]$ tar [-z|-j|-J] [tv] [-f 既有的 tar档名] \u003c==察看档名 [dmtsai@study ~]$ tar [-z|-j|-J] [xv] [-f 既有的 tar档名] [-C 目录] \u003c==解压缩 选项与参数-c ：创建打包档案，可搭配 -v 来察看过程中被打包的档名(filename) -t ：察看打包档案的内容含有哪些档名，重点在察看『档名』就是了； -x ：解打包或解压缩的功能，可以搭配 -C (大写) 在特定目录解开 特别留意的是， -c, -t, -x 不可同时出现在一串指令列中。 -z ：透过 gzip 的支持进行压缩/解压缩：此时档名最好为 *.tar.gz -j ：透过 bzip2 的支持进行压缩/解压缩：此时档名最好为 *.tar.bz2 -J ：透过 xz 的支持进行压缩/解压缩：此时档名最好为 *.tar.xz 特别留意， -z, -j, -J 不可以同时出现在一串指令列中 -v ：在压缩/解压缩的过程中，将正在处理的档名显示出来！ -f filename：-f 后面要立刻接要被处理的档名！建议 -f 单独写一个选项啰！(比较不会忘记) -C 目录 ：这个选项用在解压缩，若要在特定目录解压缩，可以使用这个选项。 其他后续练习会使用到的选项介绍： -p(小写) ：保留备份资料的原本权限与属性，常用于备份(-c)重要的设定档 -P(大写) ：保留绝对路径，亦即允许备份资料中含有根目录存在之意； –exclude=FILE：在压缩的过程中，不要将 FILE 打包！ 其实最简单的使用 tar 就只要记忆底下的方式即可 压　缩：tar -zcv -f filename.tar.gz \u003c要被压缩的档案或目录名称\u003e 查看文档树：tar -ztv -f filename.tar.gz 解压缩：tar -zxv -f filename.tar.gz -C \u003c欲解压缩的目录\u003e` 上面的命令需要根据压缩格式的不同，选用 -z -j -J 选项，而实际上文档的后缀就已经表明了它的压缩格式，不免让人觉得多余。 因此就有这幺一条通用的压缩/解压 option -a, --auto-compress Use archive suffix to determine the compression program. 使用这个，便有了通用的解压指令： tar -axv -f file.tar.* （它适用于上述三种压缩格式） 仅解压指定的文档 先查看文档树找到需要解压的文档的文档名 tar -zxv -f 打包档.tar.gz 待解开档名 打包某目录，但不含该目录下的某些档案之作法使用 –exclude=FILE 选项（支持文档名的模式匹配，而且可重复） tar -zcv -f filename.tar.gz directory --exclude=FILE1 --exclude=func* 只打包目录中比指定时刻更新的文档使用 --newer-mtime=\"2015/06/17\" 选项。 tarfile, tarballtarfile | 纯打包、未压缩的 tar 文档 tarball | 压缩了的 tar 文档 2. zip格式（linux 一般也会自带，详细的请man） 压缩命令：zip 压缩目录：zip -r filename.zip directory r 表示递归压缩，压缩包会包含这个目录 解压命令：unzip 解压到某目录：unzip -d directory filename.zip (-d dir 表示将内容解压到dir目录内) -t 测试压缩档的完整性 -x filename 排除某文档 3. 7z格式（需要p7zip，deepin自带，更多的请man） 查看目录树：7z l file.7z (List contents of archive) 压缩：7z a file.7z file1 directory1 (a 为创建压缩档或向压缩档中添加文档/目录，一次可指定多个要压缩的文档或目录) 解压：7z x file.7z -o directory (解压到指定目录) 测试完整性： 7z t file.7z p7zip 安装好后，会提供 7z、7za、7zr 三个指令，一般来说直接用 7z 就行。 P.S. 7z 不会保存 Linux 文档的用户、用户组信息，因此不能直接用于 Linux 系统备份，此时建议用 tar.xz 或 tar.7z（也就是先用tar打包） 4. rar格式（还是那句话，更多的请man）rar 是非开源的格式，Linux 默认是不会包含 rar 压缩软件的，但是它的解压软件是开源的，deepin 自带 unrar，顺便 7zip 也可解压 rar 文档。 若想用linux创建rar压缩档，需要从rarlab上下载 Linux 版，（deepin源自带）不过要注意的是该 linux 版是 40 天试用版，要长期使用的话，可能需要破解。（rar 的 key 网上一搜一大把） 压缩：rar a file.rar file （这个是试用的） 解压：unrar x file.rar （这个开源免费） 其实我挺中意 rar 的修复功能的，不知道为啥 7z xz 这样的新格式没有添加类似的 recorvery records。上次下个 idea 的 tarball，下了四五次才下到一个完整的，要是用 rar 的话，大概一键修复就好了，可 tar.gz 我不知道怎幺修复，只好一遍遍重复下载。。 ","date":"2019-01-14","objectID":"/posts/compression-related-instructions-under-linux/:5:1","series":null,"tags":["Linux","Compression","压缩","tar"],"title":"常见压缩格式的区别，及 Linux 下的压缩相关指令","uri":"/posts/compression-related-instructions-under-linux/#打包某目录但不含该目录下的某些档案之作法"},{"categories":["tech"],"content":" 1. tar 指令通过之前的介绍，可以看出常用的就是 tar gzip xz 等，如果要压缩多个文档，需要先使用tar，再用管道重定向到 gzip 或 xz，比较麻烦，而这几个指令又很常用。于是后来对tar做了增强。 tar 最初只是一个归档进程，而压缩则由其他的压缩软件来完成（一个进程只干一件事）。后来为了方便，丧心病狂地集成了各种压缩指令。因此这里就只介绍这一个命令了（它囊括了所有）。 tar 的选项与参数非常的多！我们只讲几个常用的选项，更多选项您可以自行 man tar 查询啰！ [dmtsai@study ~]$ tar [-z|-j|-J] [cv] [-f 待创建的新档名] filename... \u003c==打包与压缩 [dmtsai@study ~]$ tar [-z|-j|-J] [tv] [-f 既有的 tar档名] \u003c==察看档名 [dmtsai@study ~]$ tar [-z|-j|-J] [xv] [-f 既有的 tar档名] [-C 目录] \u003c==解压缩 选项与参数-c ：创建打包档案，可搭配 -v 来察看过程中被打包的档名(filename) -t ：察看打包档案的内容含有哪些档名，重点在察看『档名』就是了； -x ：解打包或解压缩的功能，可以搭配 -C (大写) 在特定目录解开 特别留意的是， -c, -t, -x 不可同时出现在一串指令列中。 -z ：透过 gzip 的支持进行压缩/解压缩：此时档名最好为 *.tar.gz -j ：透过 bzip2 的支持进行压缩/解压缩：此时档名最好为 *.tar.bz2 -J ：透过 xz 的支持进行压缩/解压缩：此时档名最好为 *.tar.xz 特别留意， -z, -j, -J 不可以同时出现在一串指令列中 -v ：在压缩/解压缩的过程中，将正在处理的档名显示出来！ -f filename：-f 后面要立刻接要被处理的档名！建议 -f 单独写一个选项啰！(比较不会忘记) -C 目录 ：这个选项用在解压缩，若要在特定目录解压缩，可以使用这个选项。 其他后续练习会使用到的选项介绍： -p(小写) ：保留备份资料的原本权限与属性，常用于备份(-c)重要的设定档 -P(大写) ：保留绝对路径，亦即允许备份资料中含有根目录存在之意； –exclude=FILE：在压缩的过程中，不要将 FILE 打包！ 其实最简单的使用 tar 就只要记忆底下的方式即可 压　缩：tar -zcv -f filename.tar.gz \u003c要被压缩的档案或目录名称\u003e 查看文档树：tar -ztv -f filename.tar.gz 解压缩：tar -zxv -f filename.tar.gz -C \u003c欲解压缩的目录\u003e` 上面的命令需要根据压缩格式的不同，选用 -z -j -J 选项，而实际上文档的后缀就已经表明了它的压缩格式，不免让人觉得多余。 因此就有这幺一条通用的压缩/解压 option -a, --auto-compress Use archive suffix to determine the compression program. 使用这个，便有了通用的解压指令： tar -axv -f file.tar.* （它适用于上述三种压缩格式） 仅解压指定的文档 先查看文档树找到需要解压的文档的文档名 tar -zxv -f 打包档.tar.gz 待解开档名 打包某目录，但不含该目录下的某些档案之作法使用 –exclude=FILE 选项（支持文档名的模式匹配，而且可重复） tar -zcv -f filename.tar.gz directory --exclude=FILE1 --exclude=func* 只打包目录中比指定时刻更新的文档使用 --newer-mtime=\"2015/06/17\" 选项。 tarfile, tarballtarfile | 纯打包、未压缩的 tar 文档 tarball | 压缩了的 tar 文档 2. zip格式（linux 一般也会自带，详细的请man） 压缩命令：zip 压缩目录：zip -r filename.zip directory r 表示递归压缩，压缩包会包含这个目录 解压命令：unzip 解压到某目录：unzip -d directory filename.zip (-d dir 表示将内容解压到dir目录内) -t 测试压缩档的完整性 -x filename 排除某文档 3. 7z格式（需要p7zip，deepin自带，更多的请man） 查看目录树：7z l file.7z (List contents of archive) 压缩：7z a file.7z file1 directory1 (a 为创建压缩档或向压缩档中添加文档/目录，一次可指定多个要压缩的文档或目录) 解压：7z x file.7z -o directory (解压到指定目录) 测试完整性： 7z t file.7z p7zip 安装好后，会提供 7z、7za、7zr 三个指令，一般来说直接用 7z 就行。 P.S. 7z 不会保存 Linux 文档的用户、用户组信息，因此不能直接用于 Linux 系统备份，此时建议用 tar.xz 或 tar.7z（也就是先用tar打包） 4. rar格式（还是那句话，更多的请man）rar 是非开源的格式，Linux 默认是不会包含 rar 压缩软件的，但是它的解压软件是开源的，deepin 自带 unrar，顺便 7zip 也可解压 rar 文档。 若想用linux创建rar压缩档，需要从rarlab上下载 Linux 版，（deepin源自带）不过要注意的是该 linux 版是 40 天试用版，要长期使用的话，可能需要破解。（rar 的 key 网上一搜一大把） 压缩：rar a file.rar file （这个是试用的） 解压：unrar x file.rar （这个开源免费） 其实我挺中意 rar 的修复功能的，不知道为啥 7z xz 这样的新格式没有添加类似的 recorvery records。上次下个 idea 的 tarball，下了四五次才下到一个完整的，要是用 rar 的话，大概一键修复就好了，可 tar.gz 我不知道怎幺修复，只好一遍遍重复下载。。 ","date":"2019-01-14","objectID":"/posts/compression-related-instructions-under-linux/:5:1","series":null,"tags":["Linux","Compression","压缩","tar"],"title":"常见压缩格式的区别，及 Linux 下的压缩相关指令","uri":"/posts/compression-related-instructions-under-linux/#只打包目录中比指定时刻更新的文档"},{"categories":["tech"],"content":" 1. tar 指令通过之前的介绍，可以看出常用的就是 tar gzip xz 等，如果要压缩多个文档，需要先使用tar，再用管道重定向到 gzip 或 xz，比较麻烦，而这几个指令又很常用。于是后来对tar做了增强。 tar 最初只是一个归档进程，而压缩则由其他的压缩软件来完成（一个进程只干一件事）。后来为了方便，丧心病狂地集成了各种压缩指令。因此这里就只介绍这一个命令了（它囊括了所有）。 tar 的选项与参数非常的多！我们只讲几个常用的选项，更多选项您可以自行 man tar 查询啰！ [dmtsai@study ~]$ tar [-z|-j|-J] [cv] [-f 待创建的新档名] filename... \u003c==打包与压缩 [dmtsai@study ~]$ tar [-z|-j|-J] [tv] [-f 既有的 tar档名] \u003c==察看档名 [dmtsai@study ~]$ tar [-z|-j|-J] [xv] [-f 既有的 tar档名] [-C 目录] \u003c==解压缩 选项与参数-c ：创建打包档案，可搭配 -v 来察看过程中被打包的档名(filename) -t ：察看打包档案的内容含有哪些档名，重点在察看『档名』就是了； -x ：解打包或解压缩的功能，可以搭配 -C (大写) 在特定目录解开 特别留意的是， -c, -t, -x 不可同时出现在一串指令列中。 -z ：透过 gzip 的支持进行压缩/解压缩：此时档名最好为 *.tar.gz -j ：透过 bzip2 的支持进行压缩/解压缩：此时档名最好为 *.tar.bz2 -J ：透过 xz 的支持进行压缩/解压缩：此时档名最好为 *.tar.xz 特别留意， -z, -j, -J 不可以同时出现在一串指令列中 -v ：在压缩/解压缩的过程中，将正在处理的档名显示出来！ -f filename：-f 后面要立刻接要被处理的档名！建议 -f 单独写一个选项啰！(比较不会忘记) -C 目录 ：这个选项用在解压缩，若要在特定目录解压缩，可以使用这个选项。 其他后续练习会使用到的选项介绍： -p(小写) ：保留备份资料的原本权限与属性，常用于备份(-c)重要的设定档 -P(大写) ：保留绝对路径，亦即允许备份资料中含有根目录存在之意； –exclude=FILE：在压缩的过程中，不要将 FILE 打包！ 其实最简单的使用 tar 就只要记忆底下的方式即可 压　缩：tar -zcv -f filename.tar.gz \u003c要被压缩的档案或目录名称\u003e 查看文档树：tar -ztv -f filename.tar.gz 解压缩：tar -zxv -f filename.tar.gz -C \u003c欲解压缩的目录\u003e` 上面的命令需要根据压缩格式的不同，选用 -z -j -J 选项，而实际上文档的后缀就已经表明了它的压缩格式，不免让人觉得多余。 因此就有这幺一条通用的压缩/解压 option -a, --auto-compress Use archive suffix to determine the compression program. 使用这个，便有了通用的解压指令： tar -axv -f file.tar.* （它适用于上述三种压缩格式） 仅解压指定的文档 先查看文档树找到需要解压的文档的文档名 tar -zxv -f 打包档.tar.gz 待解开档名 打包某目录，但不含该目录下的某些档案之作法使用 –exclude=FILE 选项（支持文档名的模式匹配，而且可重复） tar -zcv -f filename.tar.gz directory --exclude=FILE1 --exclude=func* 只打包目录中比指定时刻更新的文档使用 --newer-mtime=\"2015/06/17\" 选项。 tarfile, tarballtarfile | 纯打包、未压缩的 tar 文档 tarball | 压缩了的 tar 文档 2. zip格式（linux 一般也会自带，详细的请man） 压缩命令：zip 压缩目录：zip -r filename.zip directory r 表示递归压缩，压缩包会包含这个目录 解压命令：unzip 解压到某目录：unzip -d directory filename.zip (-d dir 表示将内容解压到dir目录内) -t 测试压缩档的完整性 -x filename 排除某文档 3. 7z格式（需要p7zip，deepin自带，更多的请man） 查看目录树：7z l file.7z (List contents of archive) 压缩：7z a file.7z file1 directory1 (a 为创建压缩档或向压缩档中添加文档/目录，一次可指定多个要压缩的文档或目录) 解压：7z x file.7z -o directory (解压到指定目录) 测试完整性： 7z t file.7z p7zip 安装好后，会提供 7z、7za、7zr 三个指令，一般来说直接用 7z 就行。 P.S. 7z 不会保存 Linux 文档的用户、用户组信息，因此不能直接用于 Linux 系统备份，此时建议用 tar.xz 或 tar.7z（也就是先用tar打包） 4. rar格式（还是那句话，更多的请man）rar 是非开源的格式，Linux 默认是不会包含 rar 压缩软件的，但是它的解压软件是开源的，deepin 自带 unrar，顺便 7zip 也可解压 rar 文档。 若想用linux创建rar压缩档，需要从rarlab上下载 Linux 版，（deepin源自带）不过要注意的是该 linux 版是 40 天试用版，要长期使用的话，可能需要破解。（rar 的 key 网上一搜一大把） 压缩：rar a file.rar file （这个是试用的） 解压：unrar x file.rar （这个开源免费） 其实我挺中意 rar 的修复功能的，不知道为啥 7z xz 这样的新格式没有添加类似的 recorvery records。上次下个 idea 的 tarball，下了四五次才下到一个完整的，要是用 rar 的话，大概一键修复就好了，可 tar.gz 我不知道怎幺修复，只好一遍遍重复下载。。 ","date":"2019-01-14","objectID":"/posts/compression-related-instructions-under-linux/:5:1","series":null,"tags":["Linux","Compression","压缩","tar"],"title":"常见压缩格式的区别，及 Linux 下的压缩相关指令","uri":"/posts/compression-related-instructions-under-linux/#tarfile-tarball"},{"categories":["tech"],"content":" 1. tar 指令通过之前的介绍，可以看出常用的就是 tar gzip xz 等，如果要压缩多个文档，需要先使用tar，再用管道重定向到 gzip 或 xz，比较麻烦，而这几个指令又很常用。于是后来对tar做了增强。 tar 最初只是一个归档进程，而压缩则由其他的压缩软件来完成（一个进程只干一件事）。后来为了方便，丧心病狂地集成了各种压缩指令。因此这里就只介绍这一个命令了（它囊括了所有）。 tar 的选项与参数非常的多！我们只讲几个常用的选项，更多选项您可以自行 man tar 查询啰！ [dmtsai@study ~]$ tar [-z|-j|-J] [cv] [-f 待创建的新档名] filename... \u003c==打包与压缩 [dmtsai@study ~]$ tar [-z|-j|-J] [tv] [-f 既有的 tar档名] \u003c==察看档名 [dmtsai@study ~]$ tar [-z|-j|-J] [xv] [-f 既有的 tar档名] [-C 目录] \u003c==解压缩 选项与参数-c ：创建打包档案，可搭配 -v 来察看过程中被打包的档名(filename) -t ：察看打包档案的内容含有哪些档名，重点在察看『档名』就是了； -x ：解打包或解压缩的功能，可以搭配 -C (大写) 在特定目录解开 特别留意的是， -c, -t, -x 不可同时出现在一串指令列中。 -z ：透过 gzip 的支持进行压缩/解压缩：此时档名最好为 *.tar.gz -j ：透过 bzip2 的支持进行压缩/解压缩：此时档名最好为 *.tar.bz2 -J ：透过 xz 的支持进行压缩/解压缩：此时档名最好为 *.tar.xz 特别留意， -z, -j, -J 不可以同时出现在一串指令列中 -v ：在压缩/解压缩的过程中，将正在处理的档名显示出来！ -f filename：-f 后面要立刻接要被处理的档名！建议 -f 单独写一个选项啰！(比较不会忘记) -C 目录 ：这个选项用在解压缩，若要在特定目录解压缩，可以使用这个选项。 其他后续练习会使用到的选项介绍： -p(小写) ：保留备份资料的原本权限与属性，常用于备份(-c)重要的设定档 -P(大写) ：保留绝对路径，亦即允许备份资料中含有根目录存在之意； –exclude=FILE：在压缩的过程中，不要将 FILE 打包！ 其实最简单的使用 tar 就只要记忆底下的方式即可 压　缩：tar -zcv -f filename.tar.gz \u003c要被压缩的档案或目录名称\u003e 查看文档树：tar -ztv -f filename.tar.gz 解压缩：tar -zxv -f filename.tar.gz -C \u003c欲解压缩的目录\u003e` 上面的命令需要根据压缩格式的不同，选用 -z -j -J 选项，而实际上文档的后缀就已经表明了它的压缩格式，不免让人觉得多余。 因此就有这幺一条通用的压缩/解压 option -a, --auto-compress Use archive suffix to determine the compression program. 使用这个，便有了通用的解压指令： tar -axv -f file.tar.* （它适用于上述三种压缩格式） 仅解压指定的文档 先查看文档树找到需要解压的文档的文档名 tar -zxv -f 打包档.tar.gz 待解开档名 打包某目录，但不含该目录下的某些档案之作法使用 –exclude=FILE 选项（支持文档名的模式匹配，而且可重复） tar -zcv -f filename.tar.gz directory --exclude=FILE1 --exclude=func* 只打包目录中比指定时刻更新的文档使用 --newer-mtime=\"2015/06/17\" 选项。 tarfile, tarballtarfile | 纯打包、未压缩的 tar 文档 tarball | 压缩了的 tar 文档 2. zip格式（linux 一般也会自带，详细的请man） 压缩命令：zip 压缩目录：zip -r filename.zip directory r 表示递归压缩，压缩包会包含这个目录 解压命令：unzip 解压到某目录：unzip -d directory filename.zip (-d dir 表示将内容解压到dir目录内) -t 测试压缩档的完整性 -x filename 排除某文档 3. 7z格式（需要p7zip，deepin自带，更多的请man） 查看目录树：7z l file.7z (List contents of archive) 压缩：7z a file.7z file1 directory1 (a 为创建压缩档或向压缩档中添加文档/目录，一次可指定多个要压缩的文档或目录) 解压：7z x file.7z -o directory (解压到指定目录) 测试完整性： 7z t file.7z p7zip 安装好后，会提供 7z、7za、7zr 三个指令，一般来说直接用 7z 就行。 P.S. 7z 不会保存 Linux 文档的用户、用户组信息，因此不能直接用于 Linux 系统备份，此时建议用 tar.xz 或 tar.7z（也就是先用tar打包） 4. rar格式（还是那句话，更多的请man）rar 是非开源的格式，Linux 默认是不会包含 rar 压缩软件的，但是它的解压软件是开源的，deepin 自带 unrar，顺便 7zip 也可解压 rar 文档。 若想用linux创建rar压缩档，需要从rarlab上下载 Linux 版，（deepin源自带）不过要注意的是该 linux 版是 40 天试用版，要长期使用的话，可能需要破解。（rar 的 key 网上一搜一大把） 压缩：rar a file.rar file （这个是试用的） 解压：unrar x file.rar （这个开源免费） 其实我挺中意 rar 的修复功能的，不知道为啥 7z xz 这样的新格式没有添加类似的 recorvery records。上次下个 idea 的 tarball，下了四五次才下到一个完整的，要是用 rar 的话，大概一键修复就好了，可 tar.gz 我不知道怎幺修复，只好一遍遍重复下载。。 ","date":"2019-01-14","objectID":"/posts/compression-related-instructions-under-linux/:5:1","series":null,"tags":["Linux","Compression","压缩","tar"],"title":"常见压缩格式的区别，及 Linux 下的压缩相关指令","uri":"/posts/compression-related-instructions-under-linux/#2-zip格式linux-一般也会自带详细的请man"},{"categories":["tech"],"content":" 1. tar 指令通过之前的介绍，可以看出常用的就是 tar gzip xz 等，如果要压缩多个文档，需要先使用tar，再用管道重定向到 gzip 或 xz，比较麻烦，而这几个指令又很常用。于是后来对tar做了增强。 tar 最初只是一个归档进程，而压缩则由其他的压缩软件来完成（一个进程只干一件事）。后来为了方便，丧心病狂地集成了各种压缩指令。因此这里就只介绍这一个命令了（它囊括了所有）。 tar 的选项与参数非常的多！我们只讲几个常用的选项，更多选项您可以自行 man tar 查询啰！ [dmtsai@study ~]$ tar [-z|-j|-J] [cv] [-f 待创建的新档名] filename... \u003c==打包与压缩 [dmtsai@study ~]$ tar [-z|-j|-J] [tv] [-f 既有的 tar档名] \u003c==察看档名 [dmtsai@study ~]$ tar [-z|-j|-J] [xv] [-f 既有的 tar档名] [-C 目录] \u003c==解压缩 选项与参数-c ：创建打包档案，可搭配 -v 来察看过程中被打包的档名(filename) -t ：察看打包档案的内容含有哪些档名，重点在察看『档名』就是了； -x ：解打包或解压缩的功能，可以搭配 -C (大写) 在特定目录解开 特别留意的是， -c, -t, -x 不可同时出现在一串指令列中。 -z ：透过 gzip 的支持进行压缩/解压缩：此时档名最好为 *.tar.gz -j ：透过 bzip2 的支持进行压缩/解压缩：此时档名最好为 *.tar.bz2 -J ：透过 xz 的支持进行压缩/解压缩：此时档名最好为 *.tar.xz 特别留意， -z, -j, -J 不可以同时出现在一串指令列中 -v ：在压缩/解压缩的过程中，将正在处理的档名显示出来！ -f filename：-f 后面要立刻接要被处理的档名！建议 -f 单独写一个选项啰！(比较不会忘记) -C 目录 ：这个选项用在解压缩，若要在特定目录解压缩，可以使用这个选项。 其他后续练习会使用到的选项介绍： -p(小写) ：保留备份资料的原本权限与属性，常用于备份(-c)重要的设定档 -P(大写) ：保留绝对路径，亦即允许备份资料中含有根目录存在之意； –exclude=FILE：在压缩的过程中，不要将 FILE 打包！ 其实最简单的使用 tar 就只要记忆底下的方式即可 压　缩：tar -zcv -f filename.tar.gz \u003c要被压缩的档案或目录名称\u003e 查看文档树：tar -ztv -f filename.tar.gz 解压缩：tar -zxv -f filename.tar.gz -C \u003c欲解压缩的目录\u003e` 上面的命令需要根据压缩格式的不同，选用 -z -j -J 选项，而实际上文档的后缀就已经表明了它的压缩格式，不免让人觉得多余。 因此就有这幺一条通用的压缩/解压 option -a, --auto-compress Use archive suffix to determine the compression program. 使用这个，便有了通用的解压指令： tar -axv -f file.tar.* （它适用于上述三种压缩格式） 仅解压指定的文档 先查看文档树找到需要解压的文档的文档名 tar -zxv -f 打包档.tar.gz 待解开档名 打包某目录，但不含该目录下的某些档案之作法使用 –exclude=FILE 选项（支持文档名的模式匹配，而且可重复） tar -zcv -f filename.tar.gz directory --exclude=FILE1 --exclude=func* 只打包目录中比指定时刻更新的文档使用 --newer-mtime=\"2015/06/17\" 选项。 tarfile, tarballtarfile | 纯打包、未压缩的 tar 文档 tarball | 压缩了的 tar 文档 2. zip格式（linux 一般也会自带，详细的请man） 压缩命令：zip 压缩目录：zip -r filename.zip directory r 表示递归压缩，压缩包会包含这个目录 解压命令：unzip 解压到某目录：unzip -d directory filename.zip (-d dir 表示将内容解压到dir目录内) -t 测试压缩档的完整性 -x filename 排除某文档 3. 7z格式（需要p7zip，deepin自带，更多的请man） 查看目录树：7z l file.7z (List contents of archive) 压缩：7z a file.7z file1 directory1 (a 为创建压缩档或向压缩档中添加文档/目录，一次可指定多个要压缩的文档或目录) 解压：7z x file.7z -o directory (解压到指定目录) 测试完整性： 7z t file.7z p7zip 安装好后，会提供 7z、7za、7zr 三个指令，一般来说直接用 7z 就行。 P.S. 7z 不会保存 Linux 文档的用户、用户组信息，因此不能直接用于 Linux 系统备份，此时建议用 tar.xz 或 tar.7z（也就是先用tar打包） 4. rar格式（还是那句话，更多的请man）rar 是非开源的格式，Linux 默认是不会包含 rar 压缩软件的，但是它的解压软件是开源的，deepin 自带 unrar，顺便 7zip 也可解压 rar 文档。 若想用linux创建rar压缩档，需要从rarlab上下载 Linux 版，（deepin源自带）不过要注意的是该 linux 版是 40 天试用版，要长期使用的话，可能需要破解。（rar 的 key 网上一搜一大把） 压缩：rar a file.rar file （这个是试用的） 解压：unrar x file.rar （这个开源免费） 其实我挺中意 rar 的修复功能的，不知道为啥 7z xz 这样的新格式没有添加类似的 recorvery records。上次下个 idea 的 tarball，下了四五次才下到一个完整的，要是用 rar 的话，大概一键修复就好了，可 tar.gz 我不知道怎幺修复，只好一遍遍重复下载。。 ","date":"2019-01-14","objectID":"/posts/compression-related-instructions-under-linux/:5:1","series":null,"tags":["Linux","Compression","压缩","tar"],"title":"常见压缩格式的区别，及 Linux 下的压缩相关指令","uri":"/posts/compression-related-instructions-under-linux/#3-7z格式需要p7zipdeepin自带更多的请man"},{"categories":["tech"],"content":" 1. tar 指令通过之前的介绍，可以看出常用的就是 tar gzip xz 等，如果要压缩多个文档，需要先使用tar，再用管道重定向到 gzip 或 xz，比较麻烦，而这几个指令又很常用。于是后来对tar做了增强。 tar 最初只是一个归档进程，而压缩则由其他的压缩软件来完成（一个进程只干一件事）。后来为了方便，丧心病狂地集成了各种压缩指令。因此这里就只介绍这一个命令了（它囊括了所有）。 tar 的选项与参数非常的多！我们只讲几个常用的选项，更多选项您可以自行 man tar 查询啰！ [dmtsai@study ~]$ tar [-z|-j|-J] [cv] [-f 待创建的新档名] filename... \u003c==打包与压缩 [dmtsai@study ~]$ tar [-z|-j|-J] [tv] [-f 既有的 tar档名] \u003c==察看档名 [dmtsai@study ~]$ tar [-z|-j|-J] [xv] [-f 既有的 tar档名] [-C 目录] \u003c==解压缩 选项与参数-c ：创建打包档案，可搭配 -v 来察看过程中被打包的档名(filename) -t ：察看打包档案的内容含有哪些档名，重点在察看『档名』就是了； -x ：解打包或解压缩的功能，可以搭配 -C (大写) 在特定目录解开 特别留意的是， -c, -t, -x 不可同时出现在一串指令列中。 -z ：透过 gzip 的支持进行压缩/解压缩：此时档名最好为 *.tar.gz -j ：透过 bzip2 的支持进行压缩/解压缩：此时档名最好为 *.tar.bz2 -J ：透过 xz 的支持进行压缩/解压缩：此时档名最好为 *.tar.xz 特别留意， -z, -j, -J 不可以同时出现在一串指令列中 -v ：在压缩/解压缩的过程中，将正在处理的档名显示出来！ -f filename：-f 后面要立刻接要被处理的档名！建议 -f 单独写一个选项啰！(比较不会忘记) -C 目录 ：这个选项用在解压缩，若要在特定目录解压缩，可以使用这个选项。 其他后续练习会使用到的选项介绍： -p(小写) ：保留备份资料的原本权限与属性，常用于备份(-c)重要的设定档 -P(大写) ：保留绝对路径，亦即允许备份资料中含有根目录存在之意； –exclude=FILE：在压缩的过程中，不要将 FILE 打包！ 其实最简单的使用 tar 就只要记忆底下的方式即可 压　缩：tar -zcv -f filename.tar.gz \u003c要被压缩的档案或目录名称\u003e 查看文档树：tar -ztv -f filename.tar.gz 解压缩：tar -zxv -f filename.tar.gz -C \u003c欲解压缩的目录\u003e` 上面的命令需要根据压缩格式的不同，选用 -z -j -J 选项，而实际上文档的后缀就已经表明了它的压缩格式，不免让人觉得多余。 因此就有这幺一条通用的压缩/解压 option -a, --auto-compress Use archive suffix to determine the compression program. 使用这个，便有了通用的解压指令： tar -axv -f file.tar.* （它适用于上述三种压缩格式） 仅解压指定的文档 先查看文档树找到需要解压的文档的文档名 tar -zxv -f 打包档.tar.gz 待解开档名 打包某目录，但不含该目录下的某些档案之作法使用 –exclude=FILE 选项（支持文档名的模式匹配，而且可重复） tar -zcv -f filename.tar.gz directory --exclude=FILE1 --exclude=func* 只打包目录中比指定时刻更新的文档使用 --newer-mtime=\"2015/06/17\" 选项。 tarfile, tarballtarfile | 纯打包、未压缩的 tar 文档 tarball | 压缩了的 tar 文档 2. zip格式（linux 一般也会自带，详细的请man） 压缩命令：zip 压缩目录：zip -r filename.zip directory r 表示递归压缩，压缩包会包含这个目录 解压命令：unzip 解压到某目录：unzip -d directory filename.zip (-d dir 表示将内容解压到dir目录内) -t 测试压缩档的完整性 -x filename 排除某文档 3. 7z格式（需要p7zip，deepin自带，更多的请man） 查看目录树：7z l file.7z (List contents of archive) 压缩：7z a file.7z file1 directory1 (a 为创建压缩档或向压缩档中添加文档/目录，一次可指定多个要压缩的文档或目录) 解压：7z x file.7z -o directory (解压到指定目录) 测试完整性： 7z t file.7z p7zip 安装好后，会提供 7z、7za、7zr 三个指令，一般来说直接用 7z 就行。 P.S. 7z 不会保存 Linux 文档的用户、用户组信息，因此不能直接用于 Linux 系统备份，此时建议用 tar.xz 或 tar.7z（也就是先用tar打包） 4. rar格式（还是那句话，更多的请man）rar 是非开源的格式，Linux 默认是不会包含 rar 压缩软件的，但是它的解压软件是开源的，deepin 自带 unrar，顺便 7zip 也可解压 rar 文档。 若想用linux创建rar压缩档，需要从rarlab上下载 Linux 版，（deepin源自带）不过要注意的是该 linux 版是 40 天试用版，要长期使用的话，可能需要破解。（rar 的 key 网上一搜一大把） 压缩：rar a file.rar file （这个是试用的） 解压：unrar x file.rar （这个开源免费） 其实我挺中意 rar 的修复功能的，不知道为啥 7z xz 这样的新格式没有添加类似的 recorvery records。上次下个 idea 的 tarball，下了四五次才下到一个完整的，要是用 rar 的话，大概一键修复就好了，可 tar.gz 我不知道怎幺修复，只好一遍遍重复下载。。 ","date":"2019-01-14","objectID":"/posts/compression-related-instructions-under-linux/:5:1","series":null,"tags":["Linux","Compression","压缩","tar"],"title":"常见压缩格式的区别，及 Linux 下的压缩相关指令","uri":"/posts/compression-related-instructions-under-linux/#4-rar格式还是那句话更多的请man"},{"categories":["tech"],"content":" 六、参考 档案与档案系统的压缩,打包与备份 维基百科 rar tar gz zip 7z 有什幺区别? - 知乎 为什幺linux的包都是.tar.gz？要解压两次 - 知乎 ","date":"2019-01-14","objectID":"/posts/compression-related-instructions-under-linux/:5:2","series":null,"tags":["Linux","Compression","压缩","tar"],"title":"常见压缩格式的区别，及 Linux 下的压缩相关指令","uri":"/posts/compression-related-instructions-under-linux/#六参考"},{"categories":["life"],"content":" 高中考虑志向的时候，我最开始想选择电子信息工程，因为小时候就喜欢摆弄各种电子器件，这个方向硬件软件都能玩，就感觉很有趣，只是担心自己高三太放浪形骸考不上。 偶然想起在学校阅览室读杂志时，曾被科幻世界2013年12期里沖氏武彦的《在回声中重历》给打动——用耳朵“看见”世界实在是太奇妙了，我当时痴痴地幻想了好几天。 这样我开始考虑选择声学。 我从同桌推荐的《刀剑神域》开始接触日本的 ACG 文化，后来接触到初音未来和洛天依，就对歌声合成(singing synthesis)产生了很大的兴趣，仔细一想发现这也应该是声学的范畴，这使我坚定了我选择声学的想法。 从那时到现在的各种破事，实在不想多说，就略过不提了。总之我选择了声学，然后干得很失败。。。 为啥想写这篇文章呢？ 话还得从去年十一月份说起，当时要学 Matlab，就在 bilibili 上找了个教程：自制合成器演奏鸟之诗，讲用 Matlab 做吉他音合成，讲得特别棒，我跟着做出了 guitar-synthesizer 这个小玩意儿。 然后今天发现那个教程的作者就是做歌声合成的，而且从 2011 年 13 岁开始，因为想让初音唱中文，就开始写代码，一写就写到现在，从初中写到留学美国常春藤，从简单的时域拼接到现在的深度神经网络、隐马尔可夫模型。他在上个暑假到雅马哈（vocaloid 的制作公司）实习，并推出了自己第五次重制的歌声合成引擎 Synthesizer V。 这一系列的事迹，七年五次重制，从初中生到现在双修计算机科学和数学，简直让人叹为观止。 这位作者的名字叫华侃如（Kanru Hua），网络常用昵称 sleepwalking，他的个人网站 Kanru Hua’s Website - About Me. 通过他的博客，我了解到歌声合成需要两个方向的知识：信号处理和机器学习。 于是我想起了我的初心。 我小的时候特别喜欢拆各种电子设备，曾经用手机电池和坏手电做过手电筒，再加上个手机振动器用来吓人，还喜欢在家做各种实验。 可整个大学，我的各种手工课实验课弄得一塌糊涂，垫底的存在。这样说来，如果当初选了电子信息工程，可能会混得更差。 信号处理学得一团糟，一直想努力可在这方面总是半途而废。 说到底还是自控力太差，为何我就不能按部就班一回呢？到底是怎么搞的会养成这样的性格，这样的时候会很恨自己窝囊的性格。 如果我也能像室友一样按部就班的完成学业，只在空闲时间做自己的事…可惜没有如果。 经常会告诉自己做过的事已经不可逆转了，不需要后悔，所以也不会后悔。大概潜意识里还是会有怨念，所以才会在这样的时刻爆发出满满的恶意。 只是后悔却不做出改变是没有用的，但我完全不相信自己能改掉这样的性格，自暴自弃。纵欲一时爽，一直纵欲一直爽hhh… 说虽这样说，还是想继续挣扎下去，信号处理就像一道槛，不跨过去我无法面对自己。 “希望明年年底的总结中，我不要再这么丧”，这样大声说出来的话，言就能“灵”吧？我记得我运气一直挺好的（又窝囊到要靠运气。。） 更新： 今天二书介绍给我一篇文章我的编程经历，作者和我一样“不能兼顾兴趣与学业”，大一结束就退学了，现在在阿里巴巴工作。 他博客的观点很中肯： 我家人都是很传统的一代，不理解我学习编程最终能做出什么，他们主张先完成学业，再做这件事。但是我很清楚我自己的智商，不足以多线程处理不同的大领域，所以我顶着压力，努力地做出来他们能看出来的「成绩」，才能换来他们的理解。所以，如果你通过你的理性分析，坚持认为某件事情是对的，就努力的去做，不要放弃了以后看到另一个人做了你曾经想做的东西然后感慨当初应该怎么样怎么样。 –我的编程经历 只有在已经拥有解决问题的能力的时候，才有资格考虑退学这件事。 –你根本用不着退学 我仔细想了想，我不需要太在意自己“不能兼顾”、“自控力太弱”。如果真考虑清楚了得失，我只需要“拥有解决问题的能力”，然后去做我想做的就行了。 我自控力很弱，而且这么多年了一直改不了，即使再延期一年，也有可能还是拿不到毕业证，折磨自己而已。 而上班的话，就暑假实习的经验来看，工作都有进度条催着，反而活的更有朝气。 ","date":"2019-01-08","objectID":"/posts/relive-in-the-echo/:0:0","series":null,"tags":["闲言碎语","初心","歌声合成"],"title":"在回声中重历","uri":"/posts/relive-in-the-echo/#"},{"categories":["tech"],"content":" 本笔记整理自《SQL 基础教程》、《MySQL 必知必会》和网上资料。个人笔记不保证正确。 ","date":"2018-06-17","objectID":"/posts/sql-basics-2-queries/:0:0","series":null,"tags":["SQL","数据库","Database"],"title":"SQL 基础笔记（二）进阶查询","uri":"/posts/sql-basics-2-queries/#"},{"categories":["tech"],"content":" 一、复杂查询","date":"2018-06-17","objectID":"/posts/sql-basics-2-queries/:1:0","series":null,"tags":["SQL","数据库","Database"],"title":"SQL 基础笔记（二）进阶查询","uri":"/posts/sql-basics-2-queries/#一复杂查询"},{"categories":["tech"],"content":" 视图将 SELECT 查询包装成一个虚拟表，该虚拟表就被称为视图。（因为只是一个包装，因此视图的数据也会随着原表的更新而更新） 用途： 简化复杂的SQL查询，用它替换子查询，能降低查询的嵌套深度。 SELECT 查询的重用，减少重复查询。 … 创建视图： CREATE VIEW \u003c视图名称\u003e (\u003c视图列名1\u003e, \u003c视图列名2\u003e... ) AS \u003cSELECT 语句\u003e; 其中 SELECT 的结果列和视图列名一一对应。 3. 视图的限制 1. 视图的 SELECT 子句，不能包含 ORDER BY 子句。因为视图也是表，而表是集合，它没有顺序。（也有些DB支持该用法，但不通用） 1. 视图的更新：只在很有限的条件下，才能在视图上使用 INSERT/DELETE/UPDATE 这样的变更数据的语句。（视图应该只用于检索，能不更新就不要更新它） 4. 删除视图：DROP VIEW \u003c视图名称\u003e; ","date":"2018-06-17","objectID":"/posts/sql-basics-2-queries/:1:1","series":null,"tags":["SQL","数据库","Database"],"title":"SQL 基础笔记（二）进阶查询","uri":"/posts/sql-basics-2-queries/#视图"},{"categories":["tech"],"content":" 子查询子查询，其实就是一次性的视图: SELECT ... FROM ( SELECT ... -- 这就是一个子查询：嵌套的 select 语句 ) AS \u003c别名\u003e ... 上面的查询的 FROM 子句中，给另一 SELECT 子句定义了一个别名，并将它作为了查询对象。这就是一个子查询。 子查询不仅能用于 FROM，还能用在 WHERE 子句等很多地方。 关联子查询即用到了外部数据的子查询语句： SELECT ... FROM product AS p1 WHERE ( SELECT ... FROM duck AS p2 WHERE p1.price \u003e p2.price -- 这里，内部子查询访问了外部查询的表p1，因此是一个关联子查询。 ); ","date":"2018-06-17","objectID":"/posts/sql-basics-2-queries/:1:2","series":null,"tags":["SQL","数据库","Database"],"title":"SQL 基础笔记（二）进阶查询","uri":"/posts/sql-basics-2-queries/#子查询"},{"categories":["tech"],"content":" 子查询子查询，其实就是一次性的视图: SELECT ... FROM ( SELECT ... -- 这就是一个子查询：嵌套的 select 语句 ) AS \u003c别名\u003e ... 上面的查询的 FROM 子句中，给另一 SELECT 子句定义了一个别名，并将它作为了查询对象。这就是一个子查询。 子查询不仅能用于 FROM，还能用在 WHERE 子句等很多地方。 关联子查询即用到了外部数据的子查询语句： SELECT ... FROM product AS p1 WHERE ( SELECT ... FROM duck AS p2 WHERE p1.price \u003e p2.price -- 这里，内部子查询访问了外部查询的表p1，因此是一个关联子查询。 ); ","date":"2018-06-17","objectID":"/posts/sql-basics-2-queries/:1:2","series":null,"tags":["SQL","数据库","Database"],"title":"SQL 基础笔记（二）进阶查询","uri":"/posts/sql-basics-2-queries/#关联子查询"},{"categories":["tech"],"content":" 二、函数、谓词、CASE 表达式","date":"2018-06-17","objectID":"/posts/sql-basics-2-queries/:2:0","series":null,"tags":["SQL","数据库","Database"],"title":"SQL 基础笔记（二）进阶查询","uri":"/posts/sql-basics-2-queries/#二函数谓词case-表达式"},{"categories":["tech"],"content":" 函数 给出的链接都是 MySQL 的 算术函数 加减乘除：+ - * / ABS 绝对值 MOD 求余 ROUND 四舍五入 字符串函数 CONCAT(str1,str2,…) 拼接 LENGTH(str) 字符串的 bytes 长度 CHAR_LENGTH(str) LOWER/UPPER 大小写转换 REPLACE(str,from_str,to_str) 替换 SUBSTRING(str FROM pos FOR len) 截取 时间和日期函数 CURRENT_DATE 当前日期 CURRENT_TIME 当前时间 CURRENT_TIMESTAMP 当前的日期和时间 EXTRACT(unit FROM date) 截取日期元素，unit 可为 YEAR MONTH HOUR 等等 转换函数 CAST(expr AS type) 将 expr 的结果转换成 type 类型 COALESCE(value,…) 从左往右扫描，返回第一个非 NULL 的值。常用于将 NULL 转换为其他值。eg. COALESCE(sth, 1) 如果 sth 为 NULL 就会返回1. 聚合函数：基本就五个，已经学过了。 ","date":"2018-06-17","objectID":"/posts/sql-basics-2-queries/:2:1","series":null,"tags":["SQL","数据库","Database"],"title":"SQL 基础笔记（二）进阶查询","uri":"/posts/sql-basics-2-queries/#函数httpsdevmysqlcomdocrefman57enfunctionshtml"},{"categories":["tech"],"content":" 谓词即返回布尔值的表达式 LIKE谓词——简单字符串匹配（慢） 匹配整个列 %：任意字符出现任意次 _：匹配任意一个字符 举例： SELECT name FROM list WHERE name LIKE '%Ryan%'; -- 匹配任意包含 'Ryan' 的字符串 REGEXP谓词——正则字符串匹配 MySQL 只实现了通用正则的一个子集，而且是search模式。（非match） 其他 BETWEEN：范围匹配，eg. BETWEEN 1 AND 10 IS NULL、IS NOT NULL IN、NOT IN：是否在某集合内 EXISTS、NOT EXISTS（比较难的一个，入门阶段不要求）：该谓词比较特殊，只需要右侧一个参数，**而且该参数绝大多数情况下，都是一个关联子查询。**而且该子查询的SELECT子句的参数基本可以随意，通常使用SELECT *. 对于子查询有返回值的列，它返回True，否则返回False. 但要注意为 NULL 时返回 UNKNOW.（而 WHERE 只认 True） ","date":"2018-06-17","objectID":"/posts/sql-basics-2-queries/:2:2","series":null,"tags":["SQL","数据库","Database"],"title":"SQL 基础笔记（二）进阶查询","uri":"/posts/sql-basics-2-queries/#谓词"},{"categories":["tech"],"content":" 谓词即返回布尔值的表达式 LIKE谓词——简单字符串匹配（慢） 匹配整个列 %：任意字符出现任意次 _：匹配任意一个字符 举例： SELECT name FROM list WHERE name LIKE '%Ryan%'; -- 匹配任意包含 'Ryan' 的字符串 REGEXP谓词——正则字符串匹配 MySQL 只实现了通用正则的一个子集，而且是search模式。（非match） 其他 BETWEEN：范围匹配，eg. BETWEEN 1 AND 10 IS NULL、IS NOT NULL IN、NOT IN：是否在某集合内 EXISTS、NOT EXISTS（比较难的一个，入门阶段不要求）：该谓词比较特殊，只需要右侧一个参数，**而且该参数绝大多数情况下，都是一个关联子查询。**而且该子查询的SELECT子句的参数基本可以随意，通常使用SELECT *. 对于子查询有返回值的列，它返回True，否则返回False. 但要注意为 NULL 时返回 UNKNOW.（而 WHERE 只认 True） ","date":"2018-06-17","objectID":"/posts/sql-basics-2-queries/:2:2","series":null,"tags":["SQL","数据库","Database"],"title":"SQL 基础笔记（二）进阶查询","uri":"/posts/sql-basics-2-queries/#like谓词简单字符串匹配慢"},{"categories":["tech"],"content":" 谓词即返回布尔值的表达式 LIKE谓词——简单字符串匹配（慢） 匹配整个列 %：任意字符出现任意次 _：匹配任意一个字符 举例： SELECT name FROM list WHERE name LIKE '%Ryan%'; -- 匹配任意包含 'Ryan' 的字符串 REGEXP谓词——正则字符串匹配 MySQL 只实现了通用正则的一个子集，而且是search模式。（非match） 其他 BETWEEN：范围匹配，eg. BETWEEN 1 AND 10 IS NULL、IS NOT NULL IN、NOT IN：是否在某集合内 EXISTS、NOT EXISTS（比较难的一个，入门阶段不要求）：该谓词比较特殊，只需要右侧一个参数，**而且该参数绝大多数情况下，都是一个关联子查询。**而且该子查询的SELECT子句的参数基本可以随意，通常使用SELECT *. 对于子查询有返回值的列，它返回True，否则返回False. 但要注意为 NULL 时返回 UNKNOW.（而 WHERE 只认 True） ","date":"2018-06-17","objectID":"/posts/sql-basics-2-queries/:2:2","series":null,"tags":["SQL","数据库","Database"],"title":"SQL 基础笔记（二）进阶查询","uri":"/posts/sql-basics-2-queries/#regexp谓词正则字符串匹配"},{"categories":["tech"],"content":" 谓词即返回布尔值的表达式 LIKE谓词——简单字符串匹配（慢） 匹配整个列 %：任意字符出现任意次 _：匹配任意一个字符 举例： SELECT name FROM list WHERE name LIKE '%Ryan%'; -- 匹配任意包含 'Ryan' 的字符串 REGEXP谓词——正则字符串匹配 MySQL 只实现了通用正则的一个子集，而且是search模式。（非match） 其他 BETWEEN：范围匹配，eg. BETWEEN 1 AND 10 IS NULL、IS NOT NULL IN、NOT IN：是否在某集合内 EXISTS、NOT EXISTS（比较难的一个，入门阶段不要求）：该谓词比较特殊，只需要右侧一个参数，**而且该参数绝大多数情况下，都是一个关联子查询。**而且该子查询的SELECT子句的参数基本可以随意，通常使用SELECT *. 对于子查询有返回值的列，它返回True，否则返回False. 但要注意为 NULL 时返回 UNKNOW.（而 WHERE 只认 True） ","date":"2018-06-17","objectID":"/posts/sql-basics-2-queries/:2:2","series":null,"tags":["SQL","数据库","Database"],"title":"SQL 基础笔记（二）进阶查询","uri":"/posts/sql-basics-2-queries/#其他"},{"categories":["tech"],"content":" CASE 表达式 if - else if - else 形式： CASE WHEN \u003c求值表达式\u003e THEN \u003c表达式\u003e WHEN \u003c求值表达式\u003e THEN \u003c表达式\u003e WHEN \u003c求值表达式\u003e THEN \u003c表达式\u003e ... ELSE \u003c表达式\u003e END switch 模式（但不需要break） CASE \u003c表达式\u003e WHEN \u003c表达式\u003e THEN \u003c表达式\u003e WHEN \u003c表达式\u003e THEN \u003c表达式\u003e ... ELSE \u003c表达式\u003e END 这是对 CASE 后的元素做switch比较。 ","date":"2018-06-17","objectID":"/posts/sql-basics-2-queries/:2:3","series":null,"tags":["SQL","数据库","Database"],"title":"SQL 基础笔记（二）进阶查询","uri":"/posts/sql-basics-2-queries/#case-表达式"},{"categories":["tech"],"content":" 三、集合运算","date":"2018-06-17","objectID":"/posts/sql-basics-2-queries/:3:0","series":null,"tags":["SQL","数据库","Database"],"title":"SQL 基础笔记（二）进阶查询","uri":"/posts/sql-basics-2-queries/#三集合运算"},{"categories":["tech"],"content":" 注意事项 作为运算对象的结果集，列字段必须一一对应，而且对应列的类型必须相同。 ","date":"2018-06-17","objectID":"/posts/sql-basics-2-queries/:3:1","series":null,"tags":["SQL","数据库","Database"],"title":"SQL 基础笔记（二）进阶查询","uri":"/posts/sql-basics-2-queries/#注意事项"},{"categories":["tech"],"content":" 结果集的交并差 \u003c查询1\u003e UNION \u003c查询2\u003e：对两个结果集求并 UNION ALL：添加 ALL 能使结果集包含重复行。 \u003c查询1\u003e INTERSECT \u003c查询2\u003e：两结果集的交集 \u003c查询1\u003e EXCEPT \u003c查询2\u003e：两结果集的差集 ","date":"2018-06-17","objectID":"/posts/sql-basics-2-queries/:3:2","series":null,"tags":["SQL","数据库","Database"],"title":"SQL 基础笔记（二）进阶查询","uri":"/posts/sql-basics-2-queries/#结果集的交并差"},{"categories":["tech"],"content":" 以列为单位，对表进行联结(JOIN) 最强大的功能之一 交并差是以行为单位的操作，是竖向的运算。而联结是以列为单位的操作，是横向的拼接。 内联(INNER JOIN) 内联结果只包含两表的交集 语法： SELECT ... FROM (product INNER JOIN shop ON product.p_id = shop.p_id) WHERE filter_condition; 使用 跟在 INNER JOIN 子句后的 ON 子句指定联结条件。（这里我特意用了括号，表示 JOIN 和 ON 两个子句是配套的） 也有另一个很常用的语法（但是现在已经不推荐使用）： SELECT ... FROM product, shop WHERE product.p_id = shop.p_id AND filter_condition; 对于 shop 表中有多行对应同一个 product 的情况（有多人购买了同一款商品），结果中该 product 会被复制给 shop 中的多个购买记录。（也就是说该 product 会变成多行） INNER 可以省略，也就是说只写 JOIN，就默认是 INNER JOIN 外联(OUTER JOIN) **外联以某表为主表，将另一表的列联结到该表。**另一表没有值的列，就用 NULL 代替。使用LEFT 或 RIGHT指定主表。（两个关键字都能实现同样的效果，不过用 LEFT 的多一些） 语法： SELECT ... FROM product LEFT OUTER JOIN shop ON product.p_id = shop.p_id; 这和内联很相似，差别只是联结关键词改成了LEFT OUTER JOIN。这表示以左边的表为主表，把右边的表的内容联结上去。因此左表的所有列都会出现在结果集中。 多表联查举例： -- 登录异常的账号及密码 select distinct batches.identity_number as '登录失败账号', accounts.password from ((batches left outer join tasks on batches.id = tasks.batch_id) -- 批次表联结具体的任务表 left outer join `status` on tasks.id = status.task_id) -- 再联结上状态表 left outer join accounts on batches.identity_number = accounts.identity_number -- 再联结上账号表 where `status`.step_type = 'check_login' -- 只提取 \"check_login\" 步骤的记录 and status.status != 'info' -- 状态不为 info，说明登录异常 此外还有 FULL OUTER JOIN 表示返回左右两表的所有行！所有没有匹配的行都给出 NULL P.S. 其中的关键字 OUTER 通常可省略。但是 LEFT、RIGHT、FULL 不可以省略。 ","date":"2018-06-17","objectID":"/posts/sql-basics-2-queries/:3:3","series":null,"tags":["SQL","数据库","Database"],"title":"SQL 基础笔记（二）进阶查询","uri":"/posts/sql-basics-2-queries/#以列为单位对表进行联结join"},{"categories":["tech"],"content":" 以列为单位，对表进行联结(JOIN) 最强大的功能之一 交并差是以行为单位的操作，是竖向的运算。而联结是以列为单位的操作，是横向的拼接。 内联(INNER JOIN) 内联结果只包含两表的交集 语法： SELECT ... FROM (product INNER JOIN shop ON product.p_id = shop.p_id) WHERE filter_condition; 使用 跟在 INNER JOIN 子句后的 ON 子句指定联结条件。（这里我特意用了括号，表示 JOIN 和 ON 两个子句是配套的） 也有另一个很常用的语法（但是现在已经不推荐使用）： SELECT ... FROM product, shop WHERE product.p_id = shop.p_id AND filter_condition; 对于 shop 表中有多行对应同一个 product 的情况（有多人购买了同一款商品），结果中该 product 会被复制给 shop 中的多个购买记录。（也就是说该 product 会变成多行） INNER 可以省略，也就是说只写 JOIN，就默认是 INNER JOIN 外联(OUTER JOIN) **外联以某表为主表，将另一表的列联结到该表。**另一表没有值的列，就用 NULL 代替。使用LEFT 或 RIGHT指定主表。（两个关键字都能实现同样的效果，不过用 LEFT 的多一些） 语法： SELECT ... FROM product LEFT OUTER JOIN shop ON product.p_id = shop.p_id; 这和内联很相似，差别只是联结关键词改成了LEFT OUTER JOIN。这表示以左边的表为主表，把右边的表的内容联结上去。因此左表的所有列都会出现在结果集中。 多表联查举例： -- 登录异常的账号及密码 select distinct batches.identity_number as '登录失败账号', accounts.password from ((batches left outer join tasks on batches.id = tasks.batch_id) -- 批次表联结具体的任务表 left outer join `status` on tasks.id = status.task_id) -- 再联结上状态表 left outer join accounts on batches.identity_number = accounts.identity_number -- 再联结上账号表 where `status`.step_type = 'check_login' -- 只提取 \"check_login\" 步骤的记录 and status.status != 'info' -- 状态不为 info，说明登录异常 此外还有 FULL OUTER JOIN 表示返回左右两表的所有行！所有没有匹配的行都给出 NULL P.S. 其中的关键字 OUTER 通常可省略。但是 LEFT、RIGHT、FULL 不可以省略。 ","date":"2018-06-17","objectID":"/posts/sql-basics-2-queries/:3:3","series":null,"tags":["SQL","数据库","Database"],"title":"SQL 基础笔记（二）进阶查询","uri":"/posts/sql-basics-2-queries/#内联inner-join"},{"categories":["tech"],"content":" 以列为单位，对表进行联结(JOIN) 最强大的功能之一 交并差是以行为单位的操作，是竖向的运算。而联结是以列为单位的操作，是横向的拼接。 内联(INNER JOIN) 内联结果只包含两表的交集 语法： SELECT ... FROM (product INNER JOIN shop ON product.p_id = shop.p_id) WHERE filter_condition; 使用 跟在 INNER JOIN 子句后的 ON 子句指定联结条件。（这里我特意用了括号，表示 JOIN 和 ON 两个子句是配套的） 也有另一个很常用的语法（但是现在已经不推荐使用）： SELECT ... FROM product, shop WHERE product.p_id = shop.p_id AND filter_condition; 对于 shop 表中有多行对应同一个 product 的情况（有多人购买了同一款商品），结果中该 product 会被复制给 shop 中的多个购买记录。（也就是说该 product 会变成多行） INNER 可以省略，也就是说只写 JOIN，就默认是 INNER JOIN 外联(OUTER JOIN) **外联以某表为主表，将另一表的列联结到该表。**另一表没有值的列，就用 NULL 代替。使用LEFT 或 RIGHT指定主表。（两个关键字都能实现同样的效果，不过用 LEFT 的多一些） 语法： SELECT ... FROM product LEFT OUTER JOIN shop ON product.p_id = shop.p_id; 这和内联很相似，差别只是联结关键词改成了LEFT OUTER JOIN。这表示以左边的表为主表，把右边的表的内容联结上去。因此左表的所有列都会出现在结果集中。 多表联查举例： -- 登录异常的账号及密码 select distinct batches.identity_number as '登录失败账号', accounts.password from ((batches left outer join tasks on batches.id = tasks.batch_id) -- 批次表联结具体的任务表 left outer join `status` on tasks.id = status.task_id) -- 再联结上状态表 left outer join accounts on batches.identity_number = accounts.identity_number -- 再联结上账号表 where `status`.step_type = 'check_login' -- 只提取 \"check_login\" 步骤的记录 and status.status != 'info' -- 状态不为 info，说明登录异常 此外还有 FULL OUTER JOIN 表示返回左右两表的所有行！所有没有匹配的行都给出 NULL P.S. 其中的关键字 OUTER 通常可省略。但是 LEFT、RIGHT、FULL 不可以省略。 ","date":"2018-06-17","objectID":"/posts/sql-basics-2-queries/:3:3","series":null,"tags":["SQL","数据库","Database"],"title":"SQL 基础笔记（二）进阶查询","uri":"/posts/sql-basics-2-queries/#外联outer-join"},{"categories":["tech"],"content":" 画外：字段引用符号如果数据库的字段名/数据库名/表名可能和数据库关键字重复，就需要用引用符号将他们引用起来，消除歧义。 MySQL 中经常用反引号干这个事。 而 SQL Server 则使用方括号。 标准 SQL 使用双引号。在看到这些符号时要知道这些差别。 ","date":"2018-06-17","objectID":"/posts/sql-basics-2-queries/:4:0","series":null,"tags":["SQL","数据库","Database"],"title":"SQL 基础笔记（二）进阶查询","uri":"/posts/sql-basics-2-queries/#画外字段引用符号"},{"categories":["tech"],"content":" 查询语句分析 MySQL Explain详解 ","date":"2018-06-17","objectID":"/posts/sql-basics-2-queries/:5:0","series":null,"tags":["SQL","数据库","Database"],"title":"SQL 基础笔记（二）进阶查询","uri":"/posts/sql-basics-2-queries/#查询语句分析"},{"categories":["tech"],"content":" 常见问题","date":"2018-06-17","objectID":"/posts/sql-basics-2-queries/:6:0","series":null,"tags":["SQL","数据库","Database"],"title":"SQL 基础笔记（二）进阶查询","uri":"/posts/sql-basics-2-queries/#常见问题"},{"categories":["tech"],"content":" 隐式类型转换在MySQL中，当操作符与不同类型的操作数一起使用时，会发生类型转换以使操作数兼容。则会发生隐式类型转换。 隐式类型转换会导致查询不会走索引！！！可能会严重拖累性能。另外还可能会导致各种奇怪的问题。 详见 MYSQL隐式类型转换 完。（接下来就是用 Python/Java 连接 MySQL 了） ","date":"2018-06-17","objectID":"/posts/sql-basics-2-queries/:6:1","series":null,"tags":["SQL","数据库","Database"],"title":"SQL 基础笔记（二）进阶查询","uri":"/posts/sql-basics-2-queries/#隐式类型转换"},{"categories":["tech"],"content":" 本笔记整理自《SQL 基础教程》、《MySQL 必知必会》和网上资料。个人笔记不保证正确。 ","date":"2018-06-15","objectID":"/posts/sql-basics-1/:0:0","series":null,"tags":["SQL","数据库","Database"],"title":"SQL 基础笔记（一）","uri":"/posts/sql-basics-1/#"},{"categories":["tech"],"content":" 一、基础SQL，即结构化查询语言，是为访问与操作关系数据库中的数据而设计的语言。 关系数据库以行(row)为单位读写数据 SQL 根据功能的不同，可分为三类（其中DML用得最多，增删查改嘛） DDL(Data Definition Language, 数据定义语言): CREATE/DROP/ALTER DML(Data Manipulation Language, 数据操作语言): SELECT/INSERT/UPDATE/DELETE DCL(Data Control Language, 数据控制语言): COMMIT/ROLLBACK/GRANT/REVOKE SQL 语句要以分号结尾。换行在 SQL 中不表示结束，而等同于空格。 SQL 不区分**关键字(Keyword)**的大小写，但是描述符就不一定了。 这里有个坑：MySQL 中，数据库和表其实就是数据目录下的目录和文件，因而，操作系统的敏感性决定数据库名和表名 是否大小写敏感。这就意味着数据库名和表名在 Windows 中是大小写不敏感的，而在大多数类型的 Unix/Linux 系统中是大小写敏感的。（注意仅指数据库名和表名）可通过修改配置文件的lower_case_table_names属性来统一这一行为。 而字段名、字段内容都是内部数据，是操作系统无关的。它们的大小写敏感性，由 MySQL 的的校对（COLLATE）规则来控制。该规则体现在 MySQL 的 校对字符集（COLLATION）的后缀上：比如 utf8字符集，utf8_general_ci表示不区分大小写，这个是 utf8 字符集默认的校对规则；utf8_general_cs 表示区分大小写，utf8_bin 表示二进制比较，同样也区分大小写 。 SQL 中的字符串和日期需要用单引号引用起来，日期有特定格式年-月-日 修改字符集：set names \u003c字符集名\u003e 记住在 MySQL 中，utf-8mb4 才是完全的 utf-8字符集。 ","date":"2018-06-15","objectID":"/posts/sql-basics-1/:1:0","series":null,"tags":["SQL","数据库","Database"],"title":"SQL 基础笔记（一）","uri":"/posts/sql-basics-1/#一基础"},{"categories":["tech"],"content":" 二、DDL","date":"2018-06-15","objectID":"/posts/sql-basics-1/:2:0","series":null,"tags":["SQL","数据库","Database"],"title":"SQL 基础笔记（一）","uri":"/posts/sql-basics-1/#二ddl"},{"categories":["tech"],"content":" 1. 数据库的创建和删除 创建数据库 CREATE DATABASE \u003c数据库名称\u003e; DROP DATABASE \u003c数据库名称\u003e; ","date":"2018-06-15","objectID":"/posts/sql-basics-1/:2:1","series":null,"tags":["SQL","数据库","Database"],"title":"SQL 基础笔记（一）","uri":"/posts/sql-basics-1/#1-数据库的创建和删除"},{"categories":["tech"],"content":" 2. 创建表： 关系表的设计关系表的设计，要确保把信息分解成多个表，一类信息一个表，各表通过某些常用的，基本不会改变的值（即关系表设计中的关系，也常称为外键）互相关联。尽量不要有冗余数据。 语句： CREATE TABLE \u003c表名\u003e ( \u003c列名1\u003e \u003c数据类型\u003e \u003c该列所需约束\u003e, \u003c列名2\u003e \u003c数据类型\u003e \u003c该列所需约束\u003e, \u003c列名3\u003e \u003c数据类型\u003e \u003c该列所需约束\u003e, ... \u003c该表的约束1\u003e, \u003c该表的约束2\u003e... ); 举例： CREATE TABLE `persons` ( `id` INT UNSIGNED NOT NULL AUTO_INCREMENT, `name` CHAR(20) NOT NULL, PRIMARY KEY (`id`) )ENGINE=InnoDB DEFAULT CHARSET=utf8mb4; 后面的是设置引擎和默认字符集。工作上，表的设计一定要深思熟虑，因为改起来很困难。 字段类型（MySQL）有四类数据类型：字符串、数字、日期、二进制。它们又根据数据长度的区别，下分为多个类型。 字符串： 数字 日期 二进制 约束SQL 约束是除了数据类型之外，对列中数据追加的限定条件。 类型约束：NOT NULL、AUTO_INCREMENT、UNSIGNED（这个只 MySQL 支持） 默认值：DEFAULT，举例 \u003c列名3\u003e VARCHAR(32) NOT NULL DEFAULT \"los angeles\" 表约束：PRIMARY KEY 主键约束（主键默认 UNIQUE 且 NOT NULL） 此外还有 FOREIGN KEY 和 CHECK 两个约束语句，在进阶笔记中介绍。 P.S. 字段约束也可以写成表约束（比如主键约束），而反过来很可能不行。 ","date":"2018-06-15","objectID":"/posts/sql-basics-1/:2:2","series":null,"tags":["SQL","数据库","Database"],"title":"SQL 基础笔记（一）","uri":"/posts/sql-basics-1/#2-创建表"},{"categories":["tech"],"content":" 2. 创建表： 关系表的设计关系表的设计，要确保把信息分解成多个表，一类信息一个表，各表通过某些常用的，基本不会改变的值（即关系表设计中的关系，也常称为外键）互相关联。尽量不要有冗余数据。 语句： CREATE TABLE \u003c表名\u003e ( \u003c列名1\u003e \u003c数据类型\u003e \u003c该列所需约束\u003e, \u003c列名2\u003e \u003c数据类型\u003e \u003c该列所需约束\u003e, \u003c列名3\u003e \u003c数据类型\u003e \u003c该列所需约束\u003e, ... \u003c该表的约束1\u003e, \u003c该表的约束2\u003e... ); 举例： CREATE TABLE `persons` ( `id` INT UNSIGNED NOT NULL AUTO_INCREMENT, `name` CHAR(20) NOT NULL, PRIMARY KEY (`id`) )ENGINE=InnoDB DEFAULT CHARSET=utf8mb4; 后面的是设置引擎和默认字符集。工作上，表的设计一定要深思熟虑，因为改起来很困难。 字段类型（MySQL）有四类数据类型：字符串、数字、日期、二进制。它们又根据数据长度的区别，下分为多个类型。 字符串： 数字 日期 二进制 约束SQL 约束是除了数据类型之外，对列中数据追加的限定条件。 类型约束：NOT NULL、AUTO_INCREMENT、UNSIGNED（这个只 MySQL 支持） 默认值：DEFAULT，举例 \u003c列名3\u003e VARCHAR(32) NOT NULL DEFAULT \"los angeles\" 表约束：PRIMARY KEY 主键约束（主键默认 UNIQUE 且 NOT NULL） 此外还有 FOREIGN KEY 和 CHECK 两个约束语句，在进阶笔记中介绍。 P.S. 字段约束也可以写成表约束（比如主键约束），而反过来很可能不行。 ","date":"2018-06-15","objectID":"/posts/sql-basics-1/:2:2","series":null,"tags":["SQL","数据库","Database"],"title":"SQL 基础笔记（一）","uri":"/posts/sql-basics-1/#关系表的设计"},{"categories":["tech"],"content":" 2. 创建表： 关系表的设计关系表的设计，要确保把信息分解成多个表，一类信息一个表，各表通过某些常用的，基本不会改变的值（即关系表设计中的关系，也常称为外键）互相关联。尽量不要有冗余数据。 语句： CREATE TABLE \u003c表名\u003e ( \u003c列名1\u003e \u003c数据类型\u003e \u003c该列所需约束\u003e, \u003c列名2\u003e \u003c数据类型\u003e \u003c该列所需约束\u003e, \u003c列名3\u003e \u003c数据类型\u003e \u003c该列所需约束\u003e, ... \u003c该表的约束1\u003e, \u003c该表的约束2\u003e... ); 举例： CREATE TABLE `persons` ( `id` INT UNSIGNED NOT NULL AUTO_INCREMENT, `name` CHAR(20) NOT NULL, PRIMARY KEY (`id`) )ENGINE=InnoDB DEFAULT CHARSET=utf8mb4; 后面的是设置引擎和默认字符集。工作上，表的设计一定要深思熟虑，因为改起来很困难。 字段类型（MySQL）有四类数据类型：字符串、数字、日期、二进制。它们又根据数据长度的区别，下分为多个类型。 字符串： 数字 日期 二进制 约束SQL 约束是除了数据类型之外，对列中数据追加的限定条件。 类型约束：NOT NULL、AUTO_INCREMENT、UNSIGNED（这个只 MySQL 支持） 默认值：DEFAULT，举例 \u003c列名3\u003e VARCHAR(32) NOT NULL DEFAULT \"los angeles\" 表约束：PRIMARY KEY 主键约束（主键默认 UNIQUE 且 NOT NULL） 此外还有 FOREIGN KEY 和 CHECK 两个约束语句，在进阶笔记中介绍。 P.S. 字段约束也可以写成表约束（比如主键约束），而反过来很可能不行。 ","date":"2018-06-15","objectID":"/posts/sql-basics-1/:2:2","series":null,"tags":["SQL","数据库","Database"],"title":"SQL 基础笔记（一）","uri":"/posts/sql-basics-1/#语句"},{"categories":["tech"],"content":" 2. 创建表： 关系表的设计关系表的设计，要确保把信息分解成多个表，一类信息一个表，各表通过某些常用的，基本不会改变的值（即关系表设计中的关系，也常称为外键）互相关联。尽量不要有冗余数据。 语句： CREATE TABLE \u003c表名\u003e ( \u003c列名1\u003e \u003c数据类型\u003e \u003c该列所需约束\u003e, \u003c列名2\u003e \u003c数据类型\u003e \u003c该列所需约束\u003e, \u003c列名3\u003e \u003c数据类型\u003e \u003c该列所需约束\u003e, ... \u003c该表的约束1\u003e, \u003c该表的约束2\u003e... ); 举例： CREATE TABLE `persons` ( `id` INT UNSIGNED NOT NULL AUTO_INCREMENT, `name` CHAR(20) NOT NULL, PRIMARY KEY (`id`) )ENGINE=InnoDB DEFAULT CHARSET=utf8mb4; 后面的是设置引擎和默认字符集。工作上，表的设计一定要深思熟虑，因为改起来很困难。 字段类型（MySQL）有四类数据类型：字符串、数字、日期、二进制。它们又根据数据长度的区别，下分为多个类型。 字符串： 数字 日期 二进制 约束SQL 约束是除了数据类型之外，对列中数据追加的限定条件。 类型约束：NOT NULL、AUTO_INCREMENT、UNSIGNED（这个只 MySQL 支持） 默认值：DEFAULT，举例 \u003c列名3\u003e VARCHAR(32) NOT NULL DEFAULT \"los angeles\" 表约束：PRIMARY KEY 主键约束（主键默认 UNIQUE 且 NOT NULL） 此外还有 FOREIGN KEY 和 CHECK 两个约束语句，在进阶笔记中介绍。 P.S. 字段约束也可以写成表约束（比如主键约束），而反过来很可能不行。 ","date":"2018-06-15","objectID":"/posts/sql-basics-1/:2:2","series":null,"tags":["SQL","数据库","Database"],"title":"SQL 基础笔记（一）","uri":"/posts/sql-basics-1/#字段类型mysql"},{"categories":["tech"],"content":" 2. 创建表： 关系表的设计关系表的设计，要确保把信息分解成多个表，一类信息一个表，各表通过某些常用的，基本不会改变的值（即关系表设计中的关系，也常称为外键）互相关联。尽量不要有冗余数据。 语句： CREATE TABLE \u003c表名\u003e ( \u003c列名1\u003e \u003c数据类型\u003e \u003c该列所需约束\u003e, \u003c列名2\u003e \u003c数据类型\u003e \u003c该列所需约束\u003e, \u003c列名3\u003e \u003c数据类型\u003e \u003c该列所需约束\u003e, ... \u003c该表的约束1\u003e, \u003c该表的约束2\u003e... ); 举例： CREATE TABLE `persons` ( `id` INT UNSIGNED NOT NULL AUTO_INCREMENT, `name` CHAR(20) NOT NULL, PRIMARY KEY (`id`) )ENGINE=InnoDB DEFAULT CHARSET=utf8mb4; 后面的是设置引擎和默认字符集。工作上，表的设计一定要深思熟虑，因为改起来很困难。 字段类型（MySQL）有四类数据类型：字符串、数字、日期、二进制。它们又根据数据长度的区别，下分为多个类型。 字符串： 数字 日期 二进制 约束SQL 约束是除了数据类型之外，对列中数据追加的限定条件。 类型约束：NOT NULL、AUTO_INCREMENT、UNSIGNED（这个只 MySQL 支持） 默认值：DEFAULT，举例 \u003c列名3\u003e VARCHAR(32) NOT NULL DEFAULT \"los angeles\" 表约束：PRIMARY KEY 主键约束（主键默认 UNIQUE 且 NOT NULL） 此外还有 FOREIGN KEY 和 CHECK 两个约束语句，在进阶笔记中介绍。 P.S. 字段约束也可以写成表约束（比如主键约束），而反过来很可能不行。 ","date":"2018-06-15","objectID":"/posts/sql-basics-1/:2:2","series":null,"tags":["SQL","数据库","Database"],"title":"SQL 基础笔记（一）","uri":"/posts/sql-basics-1/#约束"},{"categories":["tech"],"content":" 3. 删除表和更新表定义 删除表（危险操作） 删除整个表： DROP TABLE \u003c表名\u003e; - 只清空表内数据，但留下表： TRUNCATE \u003c表名\u003e; -- 非标准SQL语句，但是大部分DB都支持。（可能不能ROLLBACK） 更新表定义（麻烦的操作） 所以所创建表前要仔细想好格式了，更新表定义是不得已才能为之。 添加列定义： ALTER TABLE \u003c表名\u003e ADD COLUMN \u003c列名\u003e \u003c数据类型\u003e \u003c该列的约束\u003e; 删除列定义： ALTER TABLE \u003c表名\u003e DROP COLUMN \u003c列名\u003e; ","date":"2018-06-15","objectID":"/posts/sql-basics-1/:2:3","series":null,"tags":["SQL","数据库","Database"],"title":"SQL 基础笔记（一）","uri":"/posts/sql-basics-1/#3-删除表和更新表定义"},{"categories":["tech"],"content":" 三、DML 万恶之源 NULL ","date":"2018-06-15","objectID":"/posts/sql-basics-1/:3:0","series":null,"tags":["SQL","数据库","Database"],"title":"SQL 基础笔记（一）","uri":"/posts/sql-basics-1/#三dml"},{"categories":["tech"],"content":" 1. 查询（重点） 基本语句： SELECT \u003c字段1\u003e AS \u003c别名1\u003e, \u003c字段2\u003e AS \u003c别名2\u003e, ... FROM \u003c表名\u003e WHERE \u003c过滤条件\u003e; 可用 DISTINCT 修饰列名，使查询结果无重。例：SELECT DISTINCT \u003c列名\u003e FROM \u003c表名\u003e 过滤条件可使用比较运算(\u003c\u003e、=等)和逻辑运算(AND OR NOT). 过滤条件中，比较运算会永远忽略 NULL 值，如果需要对 NULL 值做操作，需要使用 IS NULL 或 IS NOT NULL（说忽略也许不太准确，NULL 既不为真也不为假，反正少用NULL。。） 包含NULL的四则运算，得到的结果总为NULL ","date":"2018-06-15","objectID":"/posts/sql-basics-1/:3:1","series":null,"tags":["SQL","数据库","Database"],"title":"SQL 基础笔记（一）","uri":"/posts/sql-basics-1/#1-查询重点"},{"categories":["tech"],"content":" 2. 聚合与排序（重点） 聚合函数即对列进行统计分析的函数，主要有五个： COUNT：计算列的行数。（只有COUNT(*)会计算NULL行） SUM：求该列之和。 AVG：求该列的平均值。 MAX/MIN：求该列的 最大/最小 值 NOTE： 聚合函数计算时会排除所有NULL行。只有COUNT(*)例外，NULL行也会被它计数。 MAX/MIN 几乎适用于所有数据类型的列（对非数值型，以其二进制值来排序），而SUM/AVG只能用于数值类型的列。 聚合函数操作的列，也能用DISTINCT修饰。例：SELECT COUNT(DISTINCT \u003c列名\u003e) FROM \u003c表名\u003e 聚合函数只能用于SELECT子句和还没讲的HAVING子句（以及 ORDER BY 子句）中。 分组(GROUP BY)分组以聚合键为分类标准，将数据分为多个逻辑组，从而能分别对每个组进行聚合运算。（分组是为了分类聚合） 若出现了 GROUP BY 子句，查询结果一定是每一组一行。 GROUP BY 会将 NULL 作为一组特定数据，显示为空。 聚合对SELECT子句的限制首先要理解的是： 聚合函数的操作对象为某一列，而产生的结果只有一个值。 GROUP BY 的操作对象为一列或者多列，产生的结果呢，是每一组一个值。 因此为了避免歧义，只要使用了聚合函数或 GROUP BY 二者之一，SELECT 子句就只能包含： 常数 其他聚合函数（该聚合函数的操作对象可以为其他列） 如果使用了 GROUP BY 子句，还能包括该子句所指定的列名。（也就是聚合键）但是绝不能包含其他的列名，因为这会有歧义。 此外，还有一个问题是由 SQL 的执行顺序引起的。应该能很容易猜到，SELECT 语句的执行顺序和书写顺序是不一致的。 查询应该是从表开始，所以 FROM 语句一定先执行。然后应该要过滤(WHERE)，再是分组(GROUP BY)，最后才是 SELECT 语句。（就已经学到的子句而言，顺序是这样） 因此按理说，SELECT 语句 定义的别名，是不能在 GROUP BY 里使用的。（也有些DB支持该用法，但不通用） 对聚合结果进行过滤(HAVING)从刚刚说过的SQL执行顺序可见，WHERE要比GROUP BY先执行，因此如果想过滤分组后的结果，不能用它。而应该使用 HAVING 子句。 HAVING 子句和 WHERE 子句都是用来过滤的，但是执行顺序的不同也就决定了它们的用途不同。 NOTE： 有时候，会发现某个过滤条件，不论是先执行（就是写在WHERE子句中）还是后执行（写在HAVING中）都没问题，这时候应该将它写在WHERE子句中，这样GROUP BY操作的数据会更少，处理更快。 HAVING 子句的元素，也存在和 SELECT 子句同样的限制。不能使用聚合键以外的列名。 排序(ORDER BY)ORDER BY 子句在 SELECT 子句之后执行，因此它能使用 SELECT 子句中定义的别名。（而 GROUP BY 之前已经说过不能用别名了） 格式： SELECT \u003c字段1\u003e AS \u003c别名1\u003e, \u003c字段2\u003e AS \u003c别名2\u003e, ... FROM \u003c表名\u003e WHERE \u003c过滤条件1\u003e GROUP BY \u003c列名1\u003e, \u003c列名2\u003e... HAVING \u003c过滤条件2\u003e ORDER BY \u003c列名/别名1\u003e, \u003c列名/别名2\u003e... ; 多排序键/列：指定多排序键时的排序规则为：优先使用左侧的列，如果该列存在相同值，再接着参考右侧的键，依此类推。（如果左侧键值不同，右侧的键就不会被使用了） NULL 值的顺序：排序键中出现了 NULL 值时，这类值会在结果的开头或结尾汇总，究竟是排在开头还是结尾，并没有特殊规定。 ORDER BY 子句只影响结果的先后顺序，因此排序键可以是结果集以外的东西，比如其他的列，或者使用了 GROUP BY 时，还能用聚合函数。 ","date":"2018-06-15","objectID":"/posts/sql-basics-1/:3:2","series":null,"tags":["SQL","数据库","Database"],"title":"SQL 基础笔记（一）","uri":"/posts/sql-basics-1/#2-聚合与排序重点"},{"categories":["tech"],"content":" 2. 聚合与排序（重点） 聚合函数即对列进行统计分析的函数，主要有五个： COUNT：计算列的行数。（只有COUNT(*)会计算NULL行） SUM：求该列之和。 AVG：求该列的平均值。 MAX/MIN：求该列的 最大/最小 值 NOTE： 聚合函数计算时会排除所有NULL行。只有COUNT(*)例外，NULL行也会被它计数。 MAX/MIN 几乎适用于所有数据类型的列（对非数值型，以其二进制值来排序），而SUM/AVG只能用于数值类型的列。 聚合函数操作的列，也能用DISTINCT修饰。例：SELECT COUNT(DISTINCT \u003c列名\u003e) FROM \u003c表名\u003e 聚合函数只能用于SELECT子句和还没讲的HAVING子句（以及 ORDER BY 子句）中。 分组(GROUP BY)分组以聚合键为分类标准，将数据分为多个逻辑组，从而能分别对每个组进行聚合运算。（分组是为了分类聚合） 若出现了 GROUP BY 子句，查询结果一定是每一组一行。 GROUP BY 会将 NULL 作为一组特定数据，显示为空。 聚合对SELECT子句的限制首先要理解的是： 聚合函数的操作对象为某一列，而产生的结果只有一个值。 GROUP BY 的操作对象为一列或者多列，产生的结果呢，是每一组一个值。 因此为了避免歧义，只要使用了聚合函数或 GROUP BY 二者之一，SELECT 子句就只能包含： 常数 其他聚合函数（该聚合函数的操作对象可以为其他列） 如果使用了 GROUP BY 子句，还能包括该子句所指定的列名。（也就是聚合键）但是绝不能包含其他的列名，因为这会有歧义。 此外，还有一个问题是由 SQL 的执行顺序引起的。应该能很容易猜到，SELECT 语句的执行顺序和书写顺序是不一致的。 查询应该是从表开始，所以 FROM 语句一定先执行。然后应该要过滤(WHERE)，再是分组(GROUP BY)，最后才是 SELECT 语句。（就已经学到的子句而言，顺序是这样） 因此按理说，SELECT 语句 定义的别名，是不能在 GROUP BY 里使用的。（也有些DB支持该用法，但不通用） 对聚合结果进行过滤(HAVING)从刚刚说过的SQL执行顺序可见，WHERE要比GROUP BY先执行，因此如果想过滤分组后的结果，不能用它。而应该使用 HAVING 子句。 HAVING 子句和 WHERE 子句都是用来过滤的，但是执行顺序的不同也就决定了它们的用途不同。 NOTE： 有时候，会发现某个过滤条件，不论是先执行（就是写在WHERE子句中）还是后执行（写在HAVING中）都没问题，这时候应该将它写在WHERE子句中，这样GROUP BY操作的数据会更少，处理更快。 HAVING 子句的元素，也存在和 SELECT 子句同样的限制。不能使用聚合键以外的列名。 排序(ORDER BY)ORDER BY 子句在 SELECT 子句之后执行，因此它能使用 SELECT 子句中定义的别名。（而 GROUP BY 之前已经说过不能用别名了） 格式： SELECT \u003c字段1\u003e AS \u003c别名1\u003e, \u003c字段2\u003e AS \u003c别名2\u003e, ... FROM \u003c表名\u003e WHERE \u003c过滤条件1\u003e GROUP BY \u003c列名1\u003e, \u003c列名2\u003e... HAVING \u003c过滤条件2\u003e ORDER BY \u003c列名/别名1\u003e, \u003c列名/别名2\u003e... ; 多排序键/列：指定多排序键时的排序规则为：优先使用左侧的列，如果该列存在相同值，再接着参考右侧的键，依此类推。（如果左侧键值不同，右侧的键就不会被使用了） NULL 值的顺序：排序键中出现了 NULL 值时，这类值会在结果的开头或结尾汇总，究竟是排在开头还是结尾，并没有特殊规定。 ORDER BY 子句只影响结果的先后顺序，因此排序键可以是结果集以外的东西，比如其他的列，或者使用了 GROUP BY 时，还能用聚合函数。 ","date":"2018-06-15","objectID":"/posts/sql-basics-1/:3:2","series":null,"tags":["SQL","数据库","Database"],"title":"SQL 基础笔记（一）","uri":"/posts/sql-basics-1/#聚合函数"},{"categories":["tech"],"content":" 2. 聚合与排序（重点） 聚合函数即对列进行统计分析的函数，主要有五个： COUNT：计算列的行数。（只有COUNT(*)会计算NULL行） SUM：求该列之和。 AVG：求该列的平均值。 MAX/MIN：求该列的 最大/最小 值 NOTE： 聚合函数计算时会排除所有NULL行。只有COUNT(*)例外，NULL行也会被它计数。 MAX/MIN 几乎适用于所有数据类型的列（对非数值型，以其二进制值来排序），而SUM/AVG只能用于数值类型的列。 聚合函数操作的列，也能用DISTINCT修饰。例：SELECT COUNT(DISTINCT \u003c列名\u003e) FROM \u003c表名\u003e 聚合函数只能用于SELECT子句和还没讲的HAVING子句（以及 ORDER BY 子句）中。 分组(GROUP BY)分组以聚合键为分类标准，将数据分为多个逻辑组，从而能分别对每个组进行聚合运算。（分组是为了分类聚合） 若出现了 GROUP BY 子句，查询结果一定是每一组一行。 GROUP BY 会将 NULL 作为一组特定数据，显示为空。 聚合对SELECT子句的限制首先要理解的是： 聚合函数的操作对象为某一列，而产生的结果只有一个值。 GROUP BY 的操作对象为一列或者多列，产生的结果呢，是每一组一个值。 因此为了避免歧义，只要使用了聚合函数或 GROUP BY 二者之一，SELECT 子句就只能包含： 常数 其他聚合函数（该聚合函数的操作对象可以为其他列） 如果使用了 GROUP BY 子句，还能包括该子句所指定的列名。（也就是聚合键）但是绝不能包含其他的列名，因为这会有歧义。 此外，还有一个问题是由 SQL 的执行顺序引起的。应该能很容易猜到，SELECT 语句的执行顺序和书写顺序是不一致的。 查询应该是从表开始，所以 FROM 语句一定先执行。然后应该要过滤(WHERE)，再是分组(GROUP BY)，最后才是 SELECT 语句。（就已经学到的子句而言，顺序是这样） 因此按理说，SELECT 语句 定义的别名，是不能在 GROUP BY 里使用的。（也有些DB支持该用法，但不通用） 对聚合结果进行过滤(HAVING)从刚刚说过的SQL执行顺序可见，WHERE要比GROUP BY先执行，因此如果想过滤分组后的结果，不能用它。而应该使用 HAVING 子句。 HAVING 子句和 WHERE 子句都是用来过滤的，但是执行顺序的不同也就决定了它们的用途不同。 NOTE： 有时候，会发现某个过滤条件，不论是先执行（就是写在WHERE子句中）还是后执行（写在HAVING中）都没问题，这时候应该将它写在WHERE子句中，这样GROUP BY操作的数据会更少，处理更快。 HAVING 子句的元素，也存在和 SELECT 子句同样的限制。不能使用聚合键以外的列名。 排序(ORDER BY)ORDER BY 子句在 SELECT 子句之后执行，因此它能使用 SELECT 子句中定义的别名。（而 GROUP BY 之前已经说过不能用别名了） 格式： SELECT \u003c字段1\u003e AS \u003c别名1\u003e, \u003c字段2\u003e AS \u003c别名2\u003e, ... FROM \u003c表名\u003e WHERE \u003c过滤条件1\u003e GROUP BY \u003c列名1\u003e, \u003c列名2\u003e... HAVING \u003c过滤条件2\u003e ORDER BY \u003c列名/别名1\u003e, \u003c列名/别名2\u003e... ; 多排序键/列：指定多排序键时的排序规则为：优先使用左侧的列，如果该列存在相同值，再接着参考右侧的键，依此类推。（如果左侧键值不同，右侧的键就不会被使用了） NULL 值的顺序：排序键中出现了 NULL 值时，这类值会在结果的开头或结尾汇总，究竟是排在开头还是结尾，并没有特殊规定。 ORDER BY 子句只影响结果的先后顺序，因此排序键可以是结果集以外的东西，比如其他的列，或者使用了 GROUP BY 时，还能用聚合函数。 ","date":"2018-06-15","objectID":"/posts/sql-basics-1/:3:2","series":null,"tags":["SQL","数据库","Database"],"title":"SQL 基础笔记（一）","uri":"/posts/sql-basics-1/#分组group-by"},{"categories":["tech"],"content":" 2. 聚合与排序（重点） 聚合函数即对列进行统计分析的函数，主要有五个： COUNT：计算列的行数。（只有COUNT(*)会计算NULL行） SUM：求该列之和。 AVG：求该列的平均值。 MAX/MIN：求该列的 最大/最小 值 NOTE： 聚合函数计算时会排除所有NULL行。只有COUNT(*)例外，NULL行也会被它计数。 MAX/MIN 几乎适用于所有数据类型的列（对非数值型，以其二进制值来排序），而SUM/AVG只能用于数值类型的列。 聚合函数操作的列，也能用DISTINCT修饰。例：SELECT COUNT(DISTINCT \u003c列名\u003e) FROM \u003c表名\u003e 聚合函数只能用于SELECT子句和还没讲的HAVING子句（以及 ORDER BY 子句）中。 分组(GROUP BY)分组以聚合键为分类标准，将数据分为多个逻辑组，从而能分别对每个组进行聚合运算。（分组是为了分类聚合） 若出现了 GROUP BY 子句，查询结果一定是每一组一行。 GROUP BY 会将 NULL 作为一组特定数据，显示为空。 聚合对SELECT子句的限制首先要理解的是： 聚合函数的操作对象为某一列，而产生的结果只有一个值。 GROUP BY 的操作对象为一列或者多列，产生的结果呢，是每一组一个值。 因此为了避免歧义，只要使用了聚合函数或 GROUP BY 二者之一，SELECT 子句就只能包含： 常数 其他聚合函数（该聚合函数的操作对象可以为其他列） 如果使用了 GROUP BY 子句，还能包括该子句所指定的列名。（也就是聚合键）但是绝不能包含其他的列名，因为这会有歧义。 此外，还有一个问题是由 SQL 的执行顺序引起的。应该能很容易猜到，SELECT 语句的执行顺序和书写顺序是不一致的。 查询应该是从表开始，所以 FROM 语句一定先执行。然后应该要过滤(WHERE)，再是分组(GROUP BY)，最后才是 SELECT 语句。（就已经学到的子句而言，顺序是这样） 因此按理说，SELECT 语句 定义的别名，是不能在 GROUP BY 里使用的。（也有些DB支持该用法，但不通用） 对聚合结果进行过滤(HAVING)从刚刚说过的SQL执行顺序可见，WHERE要比GROUP BY先执行，因此如果想过滤分组后的结果，不能用它。而应该使用 HAVING 子句。 HAVING 子句和 WHERE 子句都是用来过滤的，但是执行顺序的不同也就决定了它们的用途不同。 NOTE： 有时候，会发现某个过滤条件，不论是先执行（就是写在WHERE子句中）还是后执行（写在HAVING中）都没问题，这时候应该将它写在WHERE子句中，这样GROUP BY操作的数据会更少，处理更快。 HAVING 子句的元素，也存在和 SELECT 子句同样的限制。不能使用聚合键以外的列名。 排序(ORDER BY)ORDER BY 子句在 SELECT 子句之后执行，因此它能使用 SELECT 子句中定义的别名。（而 GROUP BY 之前已经说过不能用别名了） 格式： SELECT \u003c字段1\u003e AS \u003c别名1\u003e, \u003c字段2\u003e AS \u003c别名2\u003e, ... FROM \u003c表名\u003e WHERE \u003c过滤条件1\u003e GROUP BY \u003c列名1\u003e, \u003c列名2\u003e... HAVING \u003c过滤条件2\u003e ORDER BY \u003c列名/别名1\u003e, \u003c列名/别名2\u003e... ; 多排序键/列：指定多排序键时的排序规则为：优先使用左侧的列，如果该列存在相同值，再接着参考右侧的键，依此类推。（如果左侧键值不同，右侧的键就不会被使用了） NULL 值的顺序：排序键中出现了 NULL 值时，这类值会在结果的开头或结尾汇总，究竟是排在开头还是结尾，并没有特殊规定。 ORDER BY 子句只影响结果的先后顺序，因此排序键可以是结果集以外的东西，比如其他的列，或者使用了 GROUP BY 时，还能用聚合函数。 ","date":"2018-06-15","objectID":"/posts/sql-basics-1/:3:2","series":null,"tags":["SQL","数据库","Database"],"title":"SQL 基础笔记（一）","uri":"/posts/sql-basics-1/#聚合对select子句的限制"},{"categories":["tech"],"content":" 2. 聚合与排序（重点） 聚合函数即对列进行统计分析的函数，主要有五个： COUNT：计算列的行数。（只有COUNT(*)会计算NULL行） SUM：求该列之和。 AVG：求该列的平均值。 MAX/MIN：求该列的 最大/最小 值 NOTE： 聚合函数计算时会排除所有NULL行。只有COUNT(*)例外，NULL行也会被它计数。 MAX/MIN 几乎适用于所有数据类型的列（对非数值型，以其二进制值来排序），而SUM/AVG只能用于数值类型的列。 聚合函数操作的列，也能用DISTINCT修饰。例：SELECT COUNT(DISTINCT \u003c列名\u003e) FROM \u003c表名\u003e 聚合函数只能用于SELECT子句和还没讲的HAVING子句（以及 ORDER BY 子句）中。 分组(GROUP BY)分组以聚合键为分类标准，将数据分为多个逻辑组，从而能分别对每个组进行聚合运算。（分组是为了分类聚合） 若出现了 GROUP BY 子句，查询结果一定是每一组一行。 GROUP BY 会将 NULL 作为一组特定数据，显示为空。 聚合对SELECT子句的限制首先要理解的是： 聚合函数的操作对象为某一列，而产生的结果只有一个值。 GROUP BY 的操作对象为一列或者多列，产生的结果呢，是每一组一个值。 因此为了避免歧义，只要使用了聚合函数或 GROUP BY 二者之一，SELECT 子句就只能包含： 常数 其他聚合函数（该聚合函数的操作对象可以为其他列） 如果使用了 GROUP BY 子句，还能包括该子句所指定的列名。（也就是聚合键）但是绝不能包含其他的列名，因为这会有歧义。 此外，还有一个问题是由 SQL 的执行顺序引起的。应该能很容易猜到，SELECT 语句的执行顺序和书写顺序是不一致的。 查询应该是从表开始，所以 FROM 语句一定先执行。然后应该要过滤(WHERE)，再是分组(GROUP BY)，最后才是 SELECT 语句。（就已经学到的子句而言，顺序是这样） 因此按理说，SELECT 语句 定义的别名，是不能在 GROUP BY 里使用的。（也有些DB支持该用法，但不通用） 对聚合结果进行过滤(HAVING)从刚刚说过的SQL执行顺序可见，WHERE要比GROUP BY先执行，因此如果想过滤分组后的结果，不能用它。而应该使用 HAVING 子句。 HAVING 子句和 WHERE 子句都是用来过滤的，但是执行顺序的不同也就决定了它们的用途不同。 NOTE： 有时候，会发现某个过滤条件，不论是先执行（就是写在WHERE子句中）还是后执行（写在HAVING中）都没问题，这时候应该将它写在WHERE子句中，这样GROUP BY操作的数据会更少，处理更快。 HAVING 子句的元素，也存在和 SELECT 子句同样的限制。不能使用聚合键以外的列名。 排序(ORDER BY)ORDER BY 子句在 SELECT 子句之后执行，因此它能使用 SELECT 子句中定义的别名。（而 GROUP BY 之前已经说过不能用别名了） 格式： SELECT \u003c字段1\u003e AS \u003c别名1\u003e, \u003c字段2\u003e AS \u003c别名2\u003e, ... FROM \u003c表名\u003e WHERE \u003c过滤条件1\u003e GROUP BY \u003c列名1\u003e, \u003c列名2\u003e... HAVING \u003c过滤条件2\u003e ORDER BY \u003c列名/别名1\u003e, \u003c列名/别名2\u003e... ; 多排序键/列：指定多排序键时的排序规则为：优先使用左侧的列，如果该列存在相同值，再接着参考右侧的键，依此类推。（如果左侧键值不同，右侧的键就不会被使用了） NULL 值的顺序：排序键中出现了 NULL 值时，这类值会在结果的开头或结尾汇总，究竟是排在开头还是结尾，并没有特殊规定。 ORDER BY 子句只影响结果的先后顺序，因此排序键可以是结果集以外的东西，比如其他的列，或者使用了 GROUP BY 时，还能用聚合函数。 ","date":"2018-06-15","objectID":"/posts/sql-basics-1/:3:2","series":null,"tags":["SQL","数据库","Database"],"title":"SQL 基础笔记（一）","uri":"/posts/sql-basics-1/#对聚合结果进行过滤having"},{"categories":["tech"],"content":" 2. 聚合与排序（重点） 聚合函数即对列进行统计分析的函数，主要有五个： COUNT：计算列的行数。（只有COUNT(*)会计算NULL行） SUM：求该列之和。 AVG：求该列的平均值。 MAX/MIN：求该列的 最大/最小 值 NOTE： 聚合函数计算时会排除所有NULL行。只有COUNT(*)例外，NULL行也会被它计数。 MAX/MIN 几乎适用于所有数据类型的列（对非数值型，以其二进制值来排序），而SUM/AVG只能用于数值类型的列。 聚合函数操作的列，也能用DISTINCT修饰。例：SELECT COUNT(DISTINCT \u003c列名\u003e) FROM \u003c表名\u003e 聚合函数只能用于SELECT子句和还没讲的HAVING子句（以及 ORDER BY 子句）中。 分组(GROUP BY)分组以聚合键为分类标准，将数据分为多个逻辑组，从而能分别对每个组进行聚合运算。（分组是为了分类聚合） 若出现了 GROUP BY 子句，查询结果一定是每一组一行。 GROUP BY 会将 NULL 作为一组特定数据，显示为空。 聚合对SELECT子句的限制首先要理解的是： 聚合函数的操作对象为某一列，而产生的结果只有一个值。 GROUP BY 的操作对象为一列或者多列，产生的结果呢，是每一组一个值。 因此为了避免歧义，只要使用了聚合函数或 GROUP BY 二者之一，SELECT 子句就只能包含： 常数 其他聚合函数（该聚合函数的操作对象可以为其他列） 如果使用了 GROUP BY 子句，还能包括该子句所指定的列名。（也就是聚合键）但是绝不能包含其他的列名，因为这会有歧义。 此外，还有一个问题是由 SQL 的执行顺序引起的。应该能很容易猜到，SELECT 语句的执行顺序和书写顺序是不一致的。 查询应该是从表开始，所以 FROM 语句一定先执行。然后应该要过滤(WHERE)，再是分组(GROUP BY)，最后才是 SELECT 语句。（就已经学到的子句而言，顺序是这样） 因此按理说，SELECT 语句 定义的别名，是不能在 GROUP BY 里使用的。（也有些DB支持该用法，但不通用） 对聚合结果进行过滤(HAVING)从刚刚说过的SQL执行顺序可见，WHERE要比GROUP BY先执行，因此如果想过滤分组后的结果，不能用它。而应该使用 HAVING 子句。 HAVING 子句和 WHERE 子句都是用来过滤的，但是执行顺序的不同也就决定了它们的用途不同。 NOTE： 有时候，会发现某个过滤条件，不论是先执行（就是写在WHERE子句中）还是后执行（写在HAVING中）都没问题，这时候应该将它写在WHERE子句中，这样GROUP BY操作的数据会更少，处理更快。 HAVING 子句的元素，也存在和 SELECT 子句同样的限制。不能使用聚合键以外的列名。 排序(ORDER BY)ORDER BY 子句在 SELECT 子句之后执行，因此它能使用 SELECT 子句中定义的别名。（而 GROUP BY 之前已经说过不能用别名了） 格式： SELECT \u003c字段1\u003e AS \u003c别名1\u003e, \u003c字段2\u003e AS \u003c别名2\u003e, ... FROM \u003c表名\u003e WHERE \u003c过滤条件1\u003e GROUP BY \u003c列名1\u003e, \u003c列名2\u003e... HAVING \u003c过滤条件2\u003e ORDER BY \u003c列名/别名1\u003e, \u003c列名/别名2\u003e... ; 多排序键/列：指定多排序键时的排序规则为：优先使用左侧的列，如果该列存在相同值，再接着参考右侧的键，依此类推。（如果左侧键值不同，右侧的键就不会被使用了） NULL 值的顺序：排序键中出现了 NULL 值时，这类值会在结果的开头或结尾汇总，究竟是排在开头还是结尾，并没有特殊规定。 ORDER BY 子句只影响结果的先后顺序，因此排序键可以是结果集以外的东西，比如其他的列，或者使用了 GROUP BY 时，还能用聚合函数。 ","date":"2018-06-15","objectID":"/posts/sql-basics-1/:3:2","series":null,"tags":["SQL","数据库","Database"],"title":"SQL 基础笔记（一）","uri":"/posts/sql-basics-1/#排序order-by"},{"categories":["tech"],"content":" 3. 数据的增、删、改 插入(INSERT INTO) 也算用的多了语法： INSERT INTO \u003c表名\u003e (列名1, 列名2...) VALUES (值1, 值2...); 或者也可以使用 SELECT 语句来替代 VALUES 子句，达到将 SELECT 到的结果集插入某表的效果。（但是不要用ORDER BY，因为结果是集，没有顺序，排序是徒劳无功的） 插入时主键不能重复，否则会报错。（因此如果需要包含重复数据，一般都会定义一个自增的id字段） 删除(DELETE)清空表（危险操作，而且效率不如 TRUNCATE）： DELETE FROM \u003c表名\u003e; 条件删除： DELETE FROM \u003c表名\u003e WHERE \u003c条件\u003e; 因此使用DELETE时，一定要记得带WHERE，不然就好玩了。。 更新(UPDATE) UPDATE \u003c表名\u003e SET \u003c列名1\u003e = \u003c算术表达式1\u003e, \u003c列名2\u003e = \u003c表达式2\u003e, ... WHERE \u003c条件\u003e; 同 DELETE 一样，不带 WHERE 子句的 UPDATE 是很危险的。 ","date":"2018-06-15","objectID":"/posts/sql-basics-1/:3:3","series":null,"tags":["SQL","数据库","Database"],"title":"SQL 基础笔记（一）","uri":"/posts/sql-basics-1/#3-数据的增删改"},{"categories":["tech"],"content":" 3. 数据的增、删、改 插入(INSERT INTO) 也算用的多了语法： INSERT INTO \u003c表名\u003e (列名1, 列名2...) VALUES (值1, 值2...); 或者也可以使用 SELECT 语句来替代 VALUES 子句，达到将 SELECT 到的结果集插入某表的效果。（但是不要用ORDER BY，因为结果是集，没有顺序，排序是徒劳无功的） 插入时主键不能重复，否则会报错。（因此如果需要包含重复数据，一般都会定义一个自增的id字段） 删除(DELETE)清空表（危险操作，而且效率不如 TRUNCATE）： DELETE FROM \u003c表名\u003e; 条件删除： DELETE FROM \u003c表名\u003e WHERE \u003c条件\u003e; 因此使用DELETE时，一定要记得带WHERE，不然就好玩了。。 更新(UPDATE) UPDATE \u003c表名\u003e SET \u003c列名1\u003e = \u003c算术表达式1\u003e, \u003c列名2\u003e = \u003c表达式2\u003e, ... WHERE \u003c条件\u003e; 同 DELETE 一样，不带 WHERE 子句的 UPDATE 是很危险的。 ","date":"2018-06-15","objectID":"/posts/sql-basics-1/:3:3","series":null,"tags":["SQL","数据库","Database"],"title":"SQL 基础笔记（一）","uri":"/posts/sql-basics-1/#插入insert-into-也算用的多了"},{"categories":["tech"],"content":" 3. 数据的增、删、改 插入(INSERT INTO) 也算用的多了语法： INSERT INTO \u003c表名\u003e (列名1, 列名2...) VALUES (值1, 值2...); 或者也可以使用 SELECT 语句来替代 VALUES 子句，达到将 SELECT 到的结果集插入某表的效果。（但是不要用ORDER BY，因为结果是集，没有顺序，排序是徒劳无功的） 插入时主键不能重复，否则会报错。（因此如果需要包含重复数据，一般都会定义一个自增的id字段） 删除(DELETE)清空表（危险操作，而且效率不如 TRUNCATE）： DELETE FROM \u003c表名\u003e; 条件删除： DELETE FROM \u003c表名\u003e WHERE \u003c条件\u003e; 因此使用DELETE时，一定要记得带WHERE，不然就好玩了。。 更新(UPDATE) UPDATE \u003c表名\u003e SET \u003c列名1\u003e = \u003c算术表达式1\u003e, \u003c列名2\u003e = \u003c表达式2\u003e, ... WHERE \u003c条件\u003e; 同 DELETE 一样，不带 WHERE 子句的 UPDATE 是很危险的。 ","date":"2018-06-15","objectID":"/posts/sql-basics-1/:3:3","series":null,"tags":["SQL","数据库","Database"],"title":"SQL 基础笔记（一）","uri":"/posts/sql-basics-1/#删除delete"},{"categories":["tech"],"content":" 3. 数据的增、删、改 插入(INSERT INTO) 也算用的多了语法： INSERT INTO \u003c表名\u003e (列名1, 列名2...) VALUES (值1, 值2...); 或者也可以使用 SELECT 语句来替代 VALUES 子句，达到将 SELECT 到的结果集插入某表的效果。（但是不要用ORDER BY，因为结果是集，没有顺序，排序是徒劳无功的） 插入时主键不能重复，否则会报错。（因此如果需要包含重复数据，一般都会定义一个自增的id字段） 删除(DELETE)清空表（危险操作，而且效率不如 TRUNCATE）： DELETE FROM \u003c表名\u003e; 条件删除： DELETE FROM \u003c表名\u003e WHERE \u003c条件\u003e; 因此使用DELETE时，一定要记得带WHERE，不然就好玩了。。 更新(UPDATE) UPDATE \u003c表名\u003e SET \u003c列名1\u003e = \u003c算术表达式1\u003e, \u003c列名2\u003e = \u003c表达式2\u003e, ... WHERE \u003c条件\u003e; 同 DELETE 一样，不带 WHERE 子句的 UPDATE 是很危险的。 ","date":"2018-06-15","objectID":"/posts/sql-basics-1/:3:3","series":null,"tags":["SQL","数据库","Database"],"title":"SQL 基础笔记（一）","uri":"/posts/sql-basics-1/#更新update"},{"categories":["tech"],"content":" 四、DCL - 事务处理(MySQL)事务是一系列不可分割的数据库操作，也就是说，这一系列操作要么全部执行，要么全部不执行。如果执行过程中发生了问题（检查执行状态），可以通过执行 ROLLBACK 回滚到该事务执行前的状态。（注意并不会自动回滚） START TRANSACTION; -- do somthing COMMIT; START TRANSACTION: 标识事务的开始 COMMIT：提交事务。一旦提交，所执行过的操作就已成定论，恢复不了了。 ROLLBACK：事务回滚，**只能回滚未 COMMIT 的 DML 操作！**也就是说只能用在 START TRANSACTION 和 COMMIT 之间，并且只能回滚 INSERT/UPDATE/DELETE。（回滚 SELECT 没啥意义） SAVEPOINT \u003c保留点\u003e 和 ROLLBACK TO \u003c保留点\u003e：同样只能用在 START TRANSACTION 和 COMMIT 之间，其优势在于，ROLLBACK TO 可以指定回滚到某特定保留点，更灵活，而 ROLLBACK 只能回滚到事务开始前。 需要注意的有： COMMIT 和 ROLLBACK 语句也是事务的结束，因此如果执行了 ROLLBACK，那它与 COMMIT 之间的内容会被跳过。（在这一点上，它相当于大多数 PL 的 return） 如果事务执行出现问题，问题行后面的所有语句都不会被执行！包括 COMMIT 和 ROLLBACK！ 如果想用纯 SQL 实现事务原子性，必须使用存储过程检查执行状态！举例如下： CREATE PROCEDURE my_test() BEGIN DECLARE EXIT HANDLER FOR SQLEXCEPTION ROLLBACK -- 检测到 SQLEXCEPTION 则 rollback，然后 exit START TRANSACTION INSERT INTO table_test VALUES(1, 'A') INSERT INTO table_test VALUES(1, 'B') -- 这里主键冲突，会触发 SQLEXCEPTION COMMIT END CALL my_test() 或者在 PL 中通过异常处理执行 ROLLBACK。（事务虽然中止了，但并未结束！所以仍然可以 ROLLBACK 或者 COMMIT） ","date":"2018-06-15","objectID":"/posts/sql-basics-1/:4:0","series":null,"tags":["SQL","数据库","Database"],"title":"SQL 基础笔记（一）","uri":"/posts/sql-basics-1/#四dcl---事务处理mysql"},{"categories":["tech"],"content":" 数据何时被提交到数据库 显式提交：在事务中使用 COMMIT 提交数据操作被称为显式提交 隐式提交：非 DML 操作会被立即提交，也就是说这些语句本身就隐含了提交语义 自动提交： 如果 AUTOCOMMIT 被设置为 ON，当前 session 中的 DML 语句会在执行后被自动提交（START TRANSACTION 内部的 DML 除外，在它内部必须显式 COMMIT） 所有的 DML 语句都是要显式提交的，MySQL session 的 AUTOCOMMIT 默认为 ON，所以 DML 会被自动提交。 P.S. 许多语言的数据库 API 会定义自己的事务操作，不一定与这里一致。 ","date":"2018-06-15","objectID":"/posts/sql-basics-1/:4:1","series":null,"tags":["SQL","数据库","Database"],"title":"SQL 基础笔记（一）","uri":"/posts/sql-basics-1/#数据何时被提交到数据库"},{"categories":["tech"],"content":" 本文最初于 2018-05-25 发表在博客园，2022-08-13 搬迁至 https://thiscute.world ","date":"2018-05-25","objectID":"/posts/video2chars-2/:0:0","series":null,"tags":[],"title":"Python 视频转字符动画（二）进阶","uri":"/posts/video2chars-2/#"},{"categories":["tech"],"content":" 0. 话说在前头最新版使用了画布方式实现，和本文相比改动非常大，如果对旧版本的实现没啥兴趣，可以直接移步 video2chars，它的效果动画见 极乐净土。新版本的核心代码不算注释70行不到，功能更强大。 下面的效果动画是使用 html 实现的字符动画效果（上一篇的效果动画是 shell 版的）： There should have been a video here but your browser does not seem to support it. 本文的优化仍然是针对 shell 版本的，html 版由于缺陷太大就不写文章介绍了。 ","date":"2018-05-25","objectID":"/posts/video2chars-2/:0:1","series":null,"tags":[],"title":"Python 视频转字符动画（二）进阶","uri":"/posts/video2chars-2/#0-话说在前头"},{"categories":["tech"],"content":" 1. 速度优化要是每次播放都要等个一分钟，也太痛苦了一点。 所以可以用 pickle 模块把 video_chars 保存下来，下次播放时，如果发现当前目录下有这个保存下来的数据，就跳过转换，直接播放了。这样就快多了。 只需要改一下测试代码， 先在开头添加两个依赖 import os import pickle 然后在文件结尾添加代码： def dump(obj, file_name): \"\"\" 将指定对象，以file_nam为名，保存到本地 \"\"\" with open(file_name, 'wb') as f: pickle.dump(obj, f) return def load(filename): \"\"\" 从当前文件夹的指定文件中load对象 \"\"\" with open(filename, 'rb') as f: return pickle.load(f) def get_file_name(file_path): \"\"\" 从文件路径中提取出不带拓展名的文件名 \"\"\" # 从文件路径获取文件名 _name path, file_name_with_extension = os.path.split(file_path) # 拿到文件名前缀 file_name, file_extension = os.path.splitext(file_name_with_extension) return file_name def has_file(path, file_name): \"\"\" 判断指定目录下，是否存在某文件 \"\"\" return file_name in os.listdir(path) def get_video_chars(video_path, size): \"\"\" 返回视频对应的字符视频 \"\"\" video_dump = get_file_name(video_path) + \".pickle\" # 如果 video_dump 已经存在于当前文件夹，就可以直接读取进来了 if has_file(\".\", video_dump): print(\"发现该视频的转换缓存，直接读取\") video_chars = load(video_dump) else: print(\"未发现缓存，开始字符视频转换\") print(\"开始逐帧读取\") # 视频转字符动画 imgs = video2imgs(video_path, size) print(\"视频已全部转换到图像， 开始逐帧转换为字符画\") video_chars = imgs2chars(imgs) print(\"转换完成，开始缓存结果\") # 把转换结果保存下来 dump(video_chars, video_dump) print(\"缓存完毕\") return video_chars if __name__ == \"__main__\": # 宽，高 size = (64, 48) # 视频路径，换成你自己的 video_path = \"BadApple.mp4\" video_chars = get_video_chars(video_path, size) play_video(video_chars) 另一个优化方法就是边转换边播放，就是同时执行上述三个步骤。学会了的话，可以自己实现一下试试。 ","date":"2018-05-25","objectID":"/posts/video2chars-2/:0:2","series":null,"tags":[],"title":"Python 视频转字符动画（二）进阶","uri":"/posts/video2chars-2/#1-速度优化"},{"categories":["tech"],"content":" 2. 字符视频和音乐同时播放没有配乐的动画，虽然做出来了是很有成就感，但是你可能看上两遍就厌倦了。 所以让我们来给它加上配乐。（不要担心，其实就只需要添加几行代码而已） 首先我们需要找个方法来播放视频的配乐，怎么做呢？ 先介绍一下一个跨平台视频播放器：mpv，它有很棒的命令行支持，请先安装好它。 要让 mpv 只播放视频的音乐部分，只需要命令： mpv --no-video video_path 好了，现在有了音乐，可总不能还让人开俩shell，先放音乐，再放字符画吧。 这时候，我们需要的功能是：使用 Python 调用外部应用. 但是 mpv 使用了类似 curses 的功能，标准库的 os.system 不能隐藏掉这个部分，播放效果不尽如人意。 因此我使用了 pyinvoke 模块，只要给它指定参数hide=True，就可以完美隐藏掉被调用程序的输出（指 stdout，其实 subprocess 也可以的）。运行下面代码前，请先用pip安装好 invoke.（能够看到这里的，安装个模块还不是小菜一碟） 好了废话说这么多，上代码： import invoke video_path = \"BadApple.mp4\" invoke.run(f\"mpv --no-video {video_path}\", hide=True, warn=True) 运行上面的测试代码，如果听到了音乐，而shell啥都没输出，但是能听到音乐的话，就正常了。我们继续。（这里使用了python3.6的f字符串） 音乐已经有了，那就好办了。 添加一个播放音乐的函数 import invoke def play_audio(video_path): invoke.run(f\"mpv --no-video {video_path}\", hide=True, warn=True) 然后修改main()方法： def main(): # 宽，高 size = (64, 48) # 视频路径，换成你自己的 video_path = \"BadApple.mp4\" # 只转换三十秒，这个属性是才添加的，但是上一篇的代码没有更新。你可能需要先上github看看最新的代码。其实就稍微改了一点。 seconds = 30 # 这里的fps是帧率，也就是每秒钟播放的的字符画数。用于和音乐同步。这个更新也没写进上一篇，请上github看看新代码。 video_chars, fps = get_video_chars(video_path, size, seconds) # 播放音轨 play_audio(video_path) # 播放视频 play_video(video_chars, fps) if __name__ == \"__main__\": main() 然后运行。。并不是我坑你，你只听到了声音，却没看到字符画。。原因是： invoke.run()函数是阻塞的，音乐没放完，代码就到不了play_video(video_chars, fps)这一行。 所以 play_audio 还要改一下，改成这样： import invoke from threading import Thread def play_audio(video_path): def call(): invoke.run(f\"mpv --no-video {video_path}\", hide=True, warn=True) # 这里创建子线程来执行音乐播放指令，因为 invoke.run() 是一个阻塞的方法，要同时播放字符画和音乐的话，就要用多线程/进程。 # P.S. 更新：现在发现可以用 subprocess.Popen 实现异步调用 mpv，不需要开新线程。有兴趣的同学可以自己试试。 p = Thread(target=call) p.setDaemon(True) p.start() 这里使用标准库的 threading.Thread 类来创建子线程，让音乐的播放在子线程里执行，然后字符动画还是主线程执行，Ok，这就可以看到最终效果了。实际上只添加了十多行代码而已。 ","date":"2018-05-25","objectID":"/posts/video2chars-2/:0:3","series":null,"tags":[],"title":"Python 视频转字符动画（二）进阶","uri":"/posts/video2chars-2/#2-字符视频和音乐同时播放"},{"categories":["tech"],"content":" 3. 彩色字符动画 html+javascript 方式：核心都是一样的内容，只是需要点 html 和 javascript 的知识。代码见 video2chars-html 画布方式：直接把画在图片上，然后自动合成为 mp4 文件。这种方式要优于 html 方式，而且有个很方便的库能用，核心代码就 70 行的样子。代码见 video2chars ","date":"2018-05-25","objectID":"/posts/video2chars-2/:0:4","series":null,"tags":[],"title":"Python 视频转字符动画（二）进阶","uri":"/posts/video2chars-2/#3-彩色字符动画"},{"categories":["tech"],"content":" 参考 Python将视频转换为全字符视频（含音频） ","date":"2018-05-25","objectID":"/posts/video2chars-2/:0:5","series":null,"tags":[],"title":"Python 视频转字符动画（二）进阶","uri":"/posts/video2chars-2/#参考"},{"categories":["life"],"content":"对我而言，学英语是一件挺痛苦的事。从初中开始学英文，可从来不觉得它有趣，主动性也就不强。 直到我开始学计算机，我开始认识到英文是不可避免的。于是尝试了很多方法。 最普遍的方式：背单词，可我从初中背到现在，背单词的计划从没哪次坚持超过一个月的。 后来听说看英文原版书有效，信心满满，结果也是看了一星期 Harry Potter，稍微看了点 Animal Farm，就没后续了。 又想练听力，开始听 ESLPod、EnglishPod，也就听了一个月的时间。。 练口语，听完了《赖世雄美语音标》，又看了点美国的口语纠正视频。这件事干得倒还算可以，口语的确标准了不少。不过也就花了十多天，就没然后了。 我学计算机的过程，和我的英文学习过程也有不少重合的地方。 印象中第一次读英文资料，是想学计算机图形学，被知乎上的高手们推荐看一个英文教程。死嗑了三天，坚持不下去放弃了。。 之后听了季索清学长的推荐，又见知乎上也都说 Python 好，就开始学 Python。看了 A Byte of Python, 印象中花了一天看完的，但是没在脑子里留下啥印象。 后来慢慢的开始熟悉 Python，在图书馆借了 Head First Python 英文版，可能是被厚度吓到了，看了几页就不了了之了。。 再后来用 Github，Pycharm IDEA 也是英文的，Python doc 和 Java doc 也全是英文的，标准库里的注释是英文的，Error 信息是英文的…… 虽然学啥都半途而废，但英文水平的确是慢慢地提升着。 慢慢地，能够不怎么吃力地看懂 Python 标准库了，有问题也可以看英文博客解决了。 最近看一个动漫看完不过瘾，转去看这部动漫的轻小说。轻小说中文翻译的太呆板，发现居然有英文的，直接啃起了英文。 换了好几部小说，现在在看 Grimgar of Fantasy and Ash、Tasogare-iro no Uta Tsukai(黄昏色の詠使い)，还找了个英文动漫网站，颇有以后看动漫也只看英文字幕的打算。希望能持续下去吧。 不过也有点苦恼，因为暑假就要找工作，现在却沉迷看英文小说。。本来这个月该学算法的，可半个月都过了，我的进度大概才认真的时候的五天的样子。。真不知道未来会是啥样。 ","date":"2018-05-16","objectID":"/posts/learning-english/:0:0","series":null,"tags":["英语","语言学习"],"title":"学英语啊学英语","uri":"/posts/learning-english/#"},{"categories":["书藉","影视","life"],"content":"看完了动画，也看了点小说。最敬佩、最喜欢、最向往的人物是珠晶，也就是供王。能感觉得到她是所有角色中，最自信、方向最明确的，而且她思考一直比较理性。身为富商之女，年仅十二，却能拥有超出所有国民的觉悟，“既然大人们没有勇气，那就由我去当王！”，并最终称王，不得不敬佩。她有句让人难以忘却的台词，“我之所以能过着比别人更好的生活，是因为我担负了相比更沉重的责任。如果没能完成相应的使命，我就会像峰王一样被砍掉脑袋。而祥琼没有认识到这一点，她不想担负责任，却觉得自己应该享受荣华富贵。” 其次就是“专职心理治疗”的乐俊小老鼠了，我简直有点怀疑存不存在这样可敬的老鼠(废话)。乐俊成熟得不适合当主角，几乎无可挑剔，大概也因此而戏份不多。 而花了大篇幅描写的庆东国的景王，还有一路走来的祥琼和铃、更夜还有泰麒，他们一度迷失掉了自我，虽然作为结果的他们实现了自我救赎，但是这个过程我喜欢不起来。大概因为我也是个偏激的人吧…… 珠晶遇到了顽丘，景王和祥琼被乐俊救赎，铃也有自己的贵人，更夜在斡由被杀时终于承认了自己的错误，泰麒也是被麒麟们合力救回来的。 谁都不可能只活在自己的世界，就能得到救赎。(这样就又得到了一个和刺猬的优雅类似的结论…) ","date":"2018-04-27","objectID":"/posts/the-twelve-kingdoms/:0:0","series":null,"tags":["读后感","观后感","动漫"],"title":"《十二国记》","uri":"/posts/the-twelve-kingdoms/#"},{"categories":["tech"],"content":" 一、charAt 与 codePonitAt我们知道 Java 内部使用的是 utf-16 作为它的 char、String 的字符编码方式，这里我们叫它内部字符集。而 utf-16 是变长编码，一个字符的编码被称为一个 code point，它可能是 16 位 —— 一个 code unit，也可能是 32 位 —— 两个 code unit。 Java 的 char 类型长度为二字节，它对应的是 code unit。换句话说，一个字符的编码，可能需要用两个 char 来存储。 作为一个输入法爱好者，我偶尔会编程处理一些生僻字。其中有些生僻字大概是后来才加入 unicode 字符集里的，直接用 charAt 方法读取它们，会得到一堆问号。原因很清楚 —— 因为这些字符（eg. “𫖯”）是用两个 code unit，也就是两个 char 表示的。charAt 找不到对应的编码，就会将这些 char 输出成「?」。 //示例 public class Test { public static void main(String[] args){ String s = \"𫖯\"; System.out.println(s.length()); //输出：2 System.out.println(s.charAt(0)); //输出：? System.out.println(s.charAt(1)); //输出：? } } 因此，涉及到中文，一定要使用 String 而不是 char，并且使用 codePoint 相关方法来处理它。否则的话，如果用户使用了生僻字，很可能就会得到不想要的结果。 下面是一个使用 codePoint 遍历一个字符串的示例，需要注意的是，codePoint 是 int 类型的（因为 char 不足以保存一个 codepoint），因此需要做些额外的转换： public class Test { public static void main(String[] args){ String s = \"赵孟𫖯孟\"; for (int i = 0; i \u003c s.codePointCount(0,s.length()); i++) { System.out.println( new String(Character.toChars(s.codePointAt(i)))); // 这里的轨迹是：类型为 int 的 codepoint -\u003e char数组 -\u003e String } } } /* 结果： 赵 孟 𫖯 ? */ 问题来了，「𫖯」这个字是正常地输出了，可最后的「孟」却变成了黑人问号。。 原因就在于 codepointAt(i) 是以 char 偏移量索引的。。所以只是这样输出也是不行的。。 正确的遍历姿势是这样的 final int length = s.length(); for (int offset = 0; offset \u003c length; ) { final int codepoint = s.codePointAt(offset); System.out.println(new String(Character.toChars(codepoint))); offset += Character.charCount(codepoint); } 这个代码保持了一个变量offset, 来指示下一个 codepoint 的偏移量。最后那一句在处理完毕后，更新这个偏移量 而 Java 8 添加了 CharSequence#codePoints， 该方法返回一个 IntStream，该流包含所有的 codepoint。可以直接通过 forEach 方法来遍历他。 string.codePoints().forEach( c -\u003e System.out.println(new String(Character.toChars(c))); ); 或者用循环 for(int c : string.codePoints().toArray()){ System.out.println(new String(Character.toChars(c))); } ","date":"2018-03-11","objectID":"/posts/how-java-handles-chinese/:0:1","series":null,"tags":["Java","字符集","字符编码"],"title":"Java 中文编码分析","uri":"/posts/how-java-handles-chinese/#一charat-与-codeponitat"},{"categories":["tech"],"content":" 二、内部字符集与输出字符集（内码与外码）现在我们知道了中文字符在 java 内部可能会保存成两个 char，可还有个问题：如果我把一个字符输出到某个流，它还会是两个 char，也就是 4 字节么？ 回想一下，Java io 有字符流，字符流使用 jvm 默认的字符集输出，而若要指定字符集，可使用转换流。 因此，一个中文字符，在内部是使用 utf-16 表示，可输出就不一定了。 来看个示例： import java.io.UnsupportedEncodingException; public class Test { public static void main(String[] args) throws UnsupportedEncodingException { String s = \"中\"; //𫖯 System.out.println(s + \": chars: \" + s.length()); System.out.println(s + \": utf-8 bytes:\" + s.getBytes(\"utf-8\").length); System.out.println(s + \": unicode bytes: \" + s.getBytes(\"unicode\").length); System.out.println(s + \": utf-16 bytes: \" + s.getBytes(\"utf-16\").length); } } 输出为： 中: chars: 1 // 2 bytes 中: utf-8 bytes:3 中: unicode bytes: 4 中: utf-16 bytes: 4 𫖯: chars: 2 // 4 bytes 𫖯: utf-8 bytes:4 𫖯: unicode bytes: 6 𫖯: utf-16 bytes: 6 一个「中」字，内部存储只用了一个 char，也就是 2 个字节。可转换成 utf-8 编码后，却用了 3 个字节。怎么会不一样呢，是不是程序出了问题？ 当然不是程序的问题，这是内码(utf-16)转换成外码(utf-8)，字符集发生了改变，所使用的字节数自然也可能会改变。（尤其这俩字符集还都是变长编码） ","date":"2018-03-11","objectID":"/posts/how-java-handles-chinese/:0:2","series":null,"tags":["Java","字符集","字符编码"],"title":"Java 中文编码分析","uri":"/posts/how-java-handles-chinese/#二内部字符集与输出字符集内码与外码"},{"categories":["tech"],"content":" 三、utf-16、utf-16le、utf-16be、bom不知道在刚刚的示例中，你有没有发现问题：同是 utf-16，为何「中」和「𫖯」的 s.getBytes(\"utf-16\").length 比 s.length 要多个 2？开头就说了 String 也是 utf-16 编码的，这两个数应该相等才对不是吗？ 原因在于，utf-16 以 16 位为单位表示数据，而计算机是以字节为基本单位来存储/读取数据的。因此一个 utf-16 的 code unit 会被存储为两个字节，需要明确指明这两个字节的先后顺序，计算机才能正确地找出它对应的字符。而 utf-16 本身并没有指定这些，所以它会在字符串开头插入一个两字节的数据，来存储这些信息（大端还是小端）。这两个字节被称为BOM（Byte Order Mark）。刚刚发现的多出的两字节就是这么来的。 如果你指定编码为 utf-16le 或 utf-16be，就不会有这个 BOM 的存在了。这时就需要你自己记住该文件的大小端。。 ","date":"2018-03-11","objectID":"/posts/how-java-handles-chinese/:0:3","series":null,"tags":["Java","字符集","字符编码"],"title":"Java 中文编码分析","uri":"/posts/how-java-handles-chinese/#三utf-16utf-16leutf-16bebom"},{"categories":["tech"],"content":" 四、更多：utf-8 unicode 在 windows 中，utf-8 格式的文件也可能会带有 BOM，但 utf-8 的基本单位本来就是一个字节，因此它不需要 BOM 来表示 所谓大小端。这个 BOM 一般是用来表示该文件是一个 utf-8 文件。不过 linux 系统则对这种带 BOM 的文件不太友好。不般不建议加。。（虽如此说，上面的测试中，utf-8 的数据应该是没加 bom 的结果） unicode字符集UCS（Unicode Character Set） 就是一张包含全世界所有文字的一个编码表，但是 UCS 太占内存了，所以实际使用基本都是使用它的其他变体。一般来说，指定字符集时使用的 unicode 基本等同于 utf-16.（所以你会发现第二节演示的小程序里，utf-16 和 unicode 得出的结果是一样的。） ","date":"2018-03-11","objectID":"/posts/how-java-handles-chinese/:0:4","series":null,"tags":["Java","字符集","字符编码"],"title":"Java 中文编码分析","uri":"/posts/how-java-handles-chinese/#四更多utf-8-unicode"},{"categories":["tech"],"content":" 四、与 Python3 对比python3 在字符串表示上，做了大刀阔斧的改革，python3 的 len(str) 得到的就是 unicode 字符数，因此程序员完全不需要去考虑字符的底层表示的问题。（实际上其内部表示也可能随着更新而变化）带 BOM 的 utf-8 也可通过指定字符集为 utf-8-sig 解决。若需要做字符集层面处理，需要 encode 为特定字符集的 byte 类型。 Encoding pertains mostly to files and transfers. Once loaded into a Python string, text in memory has no notion of an “encoding,” and is simply a sequence of Unicode characters (a.k.a. code points) stored generically. – Learning Python 5th P.S. Python2 存在和 Java 相同的问题 ","date":"2018-03-11","objectID":"/posts/how-java-handles-chinese/:0:5","series":null,"tags":["Java","字符集","字符编码"],"title":"Java 中文编码分析","uri":"/posts/how-java-handles-chinese/#四与-python3-对比"},{"categories":["tech"],"content":" 参考 java 语言中的一个字符占几个字节？ - RednaxelaFX - 知乎 How can I iterate through the unicode codepoints of a Java String? 彻底搞懂字符编码(unicode,mbcs,utf-8,utf-16,utf-32,big endian,little endian…) Java_字符编码 本文允许转载，但要求附上源链接：Java 中文编码分析 ","date":"2018-03-11","objectID":"/posts/how-java-handles-chinese/:0:6","series":null,"tags":["Java","字符集","字符编码"],"title":"Java 中文编码分析","uri":"/posts/how-java-handles-chinese/#参考"},{"categories":["tech"],"content":" 个人笔记，不保证正确。 ","date":"2018-01-23","objectID":"/posts/process-thread-coroutines-concurrency-parallelism/:0:0","series":null,"tags":["进程","线程","协程","并发","并行","Coroutines","Concurrency"],"title":"进程线程协程与并发并行","uri":"/posts/process-thread-coroutines-concurrency-parallelism/#"},{"categories":["tech"],"content":" 一、进程 Process：（并行运算，分布式）每一个进程，都可以看作是一个完整的 Program，它有自己完全独立的内容。不与其他进程直接共享数据。（一个工作(job)可以由多个 process 完成，例如电脑上的qq/360就会有好几个进程，这种程序可能会有一个守护进程，当主进程挂掉，它会自动重启主进程。） 每个进程可以由多个线程组成。进程抽象由操作系统提供，Linux 是使用 fork 函数，Windows 是用 CreateProccess。 ","date":"2018-01-23","objectID":"/posts/process-thread-coroutines-concurrency-parallelism/:0:1","series":null,"tags":["进程","线程","协程","并发","并行","Coroutines","Concurrency"],"title":"进程线程协程与并发并行","uri":"/posts/process-thread-coroutines-concurrency-parallelism/#一进程-process并行运算分布式"},{"categories":["tech"],"content":" 二、线程 Thread：（并发执行）属于同一个进程的线程之间，是共享一套工作内容的。这使得线程的创建和移除开销很小，但同时也使编程变得复杂。 关于线程，分用户级线程和内核级线程。不同的语言中，这两种线程的对应关系也不尽相同。 多对一模型 将多个用户级线程映射到一个内核级线程，线程管理在用户空间完成，这种模型下操作系统并不知道多线程的存在。Python 就是这种模型。 优点：线程管理是在用户空间进行的，切换上下文开销比较小，性能较高。 缺点：当一个线程在使用内核服务时被阻塞，那么整个进程都会被阻塞；多个线程不能并行地运行在多处理机上。 一对一模型 将每个用户级线程映射到一个内核级线程。Java的线程就属于这种模型。 优点：当一个线程被阻塞后，允许另一个线程继续执行，所以并发能力较强；能很好的利用到CPU的多核心。 缺点：每创建一个用户级线程都需要创建一个内核级线程与其对应，这样创建线程的开销比较大，会影响到应用程序的性能。并且切换线程要进出内核，代价比较大。 多对多模型 将n个用户级线程映射到m个内核级线程上，要求 m \u003c= n。GO（1.5之后）的协程就属于这种线程模型。 特点：既克服了多对一模型的并发度不高的缺点，又克服了一对一模型的一个用户进程占用太多内核级线程，开销太大的缺点。又拥有多对一模型和一对一模型各自的优点。 ","date":"2018-01-23","objectID":"/posts/process-thread-coroutines-concurrency-parallelism/:0:2","series":null,"tags":["进程","线程","协程","并发","并行","Coroutines","Concurrency"],"title":"进程线程协程与并发并行","uri":"/posts/process-thread-coroutines-concurrency-parallelism/#二线程-thread并发执行"},{"categories":["tech"],"content":" 三、协程 Coroutine（并发执行）如果说线程是轻量级的进程，那么协程就是轻量级的线程。线程跑在进程里，协程就跑在线程里。 优点： 协程是跑在同一个线程里，并且是由程序本身来调度的。协程间的切换就是函数的调用，完全没有线程切换那么大的开销。 线程的数量越多，协程的优势越大 因为协程是程序调度的，它实际上是串行运行的，因此不需要复杂的锁机制来保证线程安全。 在协程中控制共享资源不加锁，只需要判断状态就好了。这免去了锁机制带来的开销。 因为协程跑在单个线程内，所占用的 CPU 资源有限，所以多协程并不能提升计算性能。不仅如此，因为多了程序本身的调度开销，计算密集型程序的性能反而会下降。 此外，协程代码中决不能出现阻塞，否则整个线程都会停下来等待该操作完成，这就麻烦了。 协程适合用于 IO 密集型任务，可用于简化异步 IO 的 callback hell。例如 Python 的 asyncio 就是用协程实现的。 ","date":"2018-01-23","objectID":"/posts/process-thread-coroutines-concurrency-parallelism/:0:3","series":null,"tags":["进程","线程","协程","并发","并行","Coroutines","Concurrency"],"title":"进程线程协程与并发并行","uri":"/posts/process-thread-coroutines-concurrency-parallelism/#三协程-coroutine并发执行"},{"categories":["tech"],"content":" 并发并行由此，又引出两个名词： 并发（Concurrent）：多个任务交替进行。 并行（Parallel）：多个任务同时进行。 一张图说明两者的差别 Note：进程 和 线程 都可能是 并发 或 并行 的。关键看你程序的运行状态。多核是并行的前提。并发则只要求交替执行，因此单核也没问题。 ","date":"2018-01-23","objectID":"/posts/process-thread-coroutines-concurrency-parallelism/:0:4","series":null,"tags":["进程","线程","协程","并发","并行","Coroutines","Concurrency"],"title":"进程线程协程与并发并行","uri":"/posts/process-thread-coroutines-concurrency-parallelism/#并发并行"},{"categories":["tech"],"content":" 同步异步 同步：不同程序单元为了完成某个任务，在执行过程中需靠某种通信方式以协调一致，称这些程序单元是同步执行的。 多线程编程中，所有修改共享变量的行为，都必须加锁，保证顺序执行，保证同步。或者加原子锁，保证该修改操作是原子的。 同步意味着有序 异步：为完成某个任务，不同程序单元之间过程中无需通信协调，也能完成任务的方式。 不相关的程序单元之间可以是异步的。比如爬虫下载网页 异步意味着无序 进程、线程和协程 ","date":"2018-01-23","objectID":"/posts/process-thread-coroutines-concurrency-parallelism/:0:5","series":null,"tags":["进程","线程","协程","并发","并行","Coroutines","Concurrency"],"title":"进程线程协程与并发并行","uri":"/posts/process-thread-coroutines-concurrency-parallelism/#同步异步"},{"categories":["书藉","life"],"content":"店小二杀了巡界使，然后离了客栈，入了道德宗，应了一错缘，又给道德宗干了一堆破事，活的浑浑噩噩。 受了一剑斩缘后，本以为终于能求得解脱，谁知造化弄人，自己竟又从地府回得人界。 本一心复仇，临到头还是没对吟风下得杀手。 欲与顾清再续缘，却被说一句看不穿。 再次醒来，本欲杀上天界，可看到青衣时，猛然醒悟。 什么王图霸业，什么诸界称雄，什么夙世情仇，在这一刻，皆化浮云。眼中便只有这一世尘缘了。 青石与巡界使，被命运捉弄，被贬下凡间受百世轮回，九十九世相濡以沫，最后一世却横生波折。 终于回得仙界，相对大笑三声，相忘于江湖。 ","date":"2017-11-18","objectID":"/posts/fate-of-mortals/:0:0","series":null,"tags":["读后感","小说"],"title":"《尘缘》","uri":"/posts/fate-of-mortals/#"},{"categories":["life"],"content":" 啊啊，还有十天就可以摆脱这个城市，回到那个令人安心的山林里了，一边期待着，一边焦躁着，想着为什么剩下的十天这么难熬这样的问题。 复习又是一塌糊涂，我也太懒了点。 这样懒散的我还做着码完几千行代码这样的春秋大梦，太不现实了。有点想认命了。 半夜一点多，寝室空调还是不习惯，过道阳台上的凉风倒是很舒服，这座城市此刻的静谧倒也有几分韵味。 不过不管怎么说，好想回家… ","date":"2017-06-27","objectID":"/posts/the-end-of-another-semester/:0:0","series":null,"tags":[],"title":"又一个期末","uri":"/posts/the-end-of-another-semester/#"},{"categories":["影视","life"],"content":"即使大人的世界也千疮百孔，但是要是让我们自己来做的话，只能做的更差？ 所以你看船里的小社会是什么样子的吧，嫉妒、欺骗、恐惧、自暴自弃、愤怒、自私……各种欲望。 于是阶级分化、工作效率低下，有人开始求助于神，暴动也时不时的发生。 舰长害怕有人死去，害怕承担责任，不敢冒险，喜欢上强势的阿岚，心心念念那个人，把事情搞的一团糟。祐希心里有话不讲，一味的揍人泄愤。伊克米恐惧死亡，恐惧得不像样。梢有点自我中心，觉得伊克米是她一个人的，只为了她而努力。阿岚很有当舰长的能力，但是一心复仇，除了这个其他的都不怎么管。 男主忍受着各种非议和殴打，但是坚守本心。不管局势如何变化，他一直在做着正确的事，虽然有时候正确的事未必是最好的…… 人在坏掉。 大家都在自欺欺人吗？ 真的可以抛弃过去活下去吗？ 如果不可以，那为了活着，背负过去是必要的？ 最后反转剧情，靠的还是一个「情」字。也对，科技发展的动力也是人的欲望。 欲望有好有坏，理智的把握它。无欲无求，就是死。 ","date":"2017-06-14","objectID":"/posts/infinite-ryvius/:0:0","series":null,"tags":["观后感","动漫"],"title":"《无限的未知》","uri":"/posts/infinite-ryvius/#"},{"categories":["数学","tech"],"content":"很早就学过欧几里得算法，但是一直不知道它的原理。几乎每本算法书都会提到它，但是貌似只有数学书上才会见到它的原理。。。 前段时间粗粗看了点数论（《什么是数学》），惊讶于这个原理的奇妙。现在把它通俗地写下来，以免自己忘记。 欧几里得算法是求两个数的最大公约数(Greatest Common Divisor (GCD))的算法，我们首先假设有两个数 $a$ 和 $b$，其中 $a$ 是不小于 $b$ 的数， 记 $a$ 被 $b$ 除的余数为 $r$，那么 $a$ 可以写成这样的形式： $$a = bq + r$$ 其中 $q$ 是整数（我们不需要去管 $q$ 到底是多少，这和我们的目标无关）。 现在假设 $a$ 和 $b$ 的一个约数为 $u$，那么 $a$ 和 $b$ 都能被 $u$ 整除，即 $$a = su$$ $$b = tu$$ $s$ 和 $t$ 都是整数（同样的，我们只需要知道存在这样的整数 $s$ 和 $t$ 就行）。 这样可以得出 $$r = a - bq = su - (tu)q = (s - tq)u$$ 所以 $r$ 也能被 $u$ 整除，一般规律如下 $a$ 和 $b$ 的约数也整除它们的余数 $r$，所以 $a$ 和 $b$ 的任一约数同时也是 $b$ 和 $r$ 的约数。 —— 条件一 反过来可以得出 $b$ 和 $r$ 的任一约数同时也是 $a$ 和 $b$ 的约数。 ——条件二 这是因为对 $b$ 和 $r$ 每一个约数 $v$，有 $$b = kv$$ $$r = cv$$ 于是有 $$a = bq + r = (kv)q + cv = (kq + c)v$$ 由条件一和条件二可知 $a$ 和 $b$ 的约数的集合，全等于 $b$ 和 $r$ 的约数的集合。 于是 $a$ 和 $b$ 的最大公约数，就是 $b$ 和 $r$ 的最大公约数。 接下来用递推法， $a \\div b$ 余 $r$，现在设 $b \\div r$ 余 $r_1$ $r \\div r_1$ 余 $r_2$ …… $r_{n-3} \\div r_{n-2}$ 余 $r_{n-1}$ $r_{n-2} \\div r_{n-1}$ 余 $r_n=0$ 因为 $a \\ge b$，可以看出余数 $r_n$ 会越来越小，最终变成 $0$. 当 $r_{n-1} \\neq 0$ 且 $r_n = 0$ 时，可知 $r_{n-2}$ 可被 $r_{n-1}$ 整除（余数为 $0$ 嘛） 此时 $r_{n-2}$ 和 $r_{n-1}$ 的约数就只有：$r_{n-1}$ 和 $r_{n-1}$ 的因数，所以他们的最大公约数就是 $r_{n-1}$！ 所以 $r_{n-1}$ 就是 $a$ 和 $b$ 的最大公约数。（若 $r = 0$，则 $b$ 为最大公约数） 这个递推法写成c语言函数是这样的（比推导更简洁…）: unsigned int Gcd(unsigned int M,unsigned int N){ unsigned int Rem; while(N){ Rem = M % N; M = N; N = Rem; } return Rem; } 可以发现这里没有要求 M\u003e=N，这是因为如果那样，循环会自动交换它们的值。 P.S. 此外，还有最小公倍数(Least Common Multiple (LCM))算法，详见 GCD and LCM calculator ","date":"2017-05-26","objectID":"/posts/mathematics-in-euclidean-gcd/:0:0","series":null,"tags":["算法"],"title":"欧几里得算法求最大公约数(GCD)的数学原理","uri":"/posts/mathematics-in-euclidean-gcd/#"},{"categories":["life"],"content":"生活总是在给你希望之时，再埋点伏笔。本来我以为进了大学，就是一个全新的世界了，我可以重新开始，只要我很努力很努力，一切困难都将不堪一击。 显然那个时候，我还不知道，现实不同于想象。 高三在高压下全线崩溃，因此对大学寄予了过多期望。但这期望同时也带来了更大的压力。 我患上了阅读焦虑症。 从进入大学的那一刻起，就开始疯狂地制定阅读计划，泡图书馆，看各种学习方法、读书方法、记忆方法、速读术之类的书籍，恨不得一目十行。 但是很快的，我就发现自己出了问题：我太想提升自己了，因此翻开书的第一页，就期盼着翻到最后一页，读书的愉悦，被对看完一本书的渴望冲淡了。更多的时候，感觉到的是还没把这本书看完的焦虑。 而且因为长时间全神贯注，一本书看不到一半，耐心也渐渐失去，于是翻页速度越来越快，这个时候所谓的“阅读”已经名存实亡了。 这样的阅读的结果，只是在读书量上徒然添加几个数字，于自我提升而言，却是收效甚微。我很明白这一点，但是明白和作出改变之间，隔着一道鸿沟，我怎么也跨不过去。明明知道松弛有度效率会更高，但是心理上的焦虑让我无法说服自己放下书本哪怕一分钟，直到自己的耐心消耗殆尽…… 买了一大堆文学书放在柜头。可笑的是，大一整整一年，除了韩寒，我没看任何一本文学书超过半小时。“快速浏览”完十几本方法类书籍后，我开始阅读技术书籍。但是除了韩寒的书和几本技术书籍，阅读过程中的焦虑感从未远离我，这不仅降低了我的学习效率，更让我的倦怠期长了数倍(过度消耗精力)。其结果是，往往一本厚一点的书读上两三天，就有半个月会厌倦到不想碰它。 我能感觉到如果按着计划读书，我的成果绝不会差到现在这样。也想着有计划性一点，可是一看QQ，人家初三的小男孩已经学遍了高中数学、算法、初等数论、自然数学……网上认识的同龄人已经开始做神经网络了，知乎上一大群自学者也在努力攻克python/c/算法，我就停不下来，甚至平静下来做个计划都觉得浪费时间(实际上很明显这样带着焦虑阅读才是浪费时间)。 迫切的想要成为那个“自己想要成为的人”，因此连基本的理性都无法保持。 我想要的是从容、带着脑子的阅读，而不是这样走马观花，盲目追求量的阅读。 我又焦虑地打开知乎，不断搜索，然后写下这篇文章。 ","date":"2017-03-07","objectID":"/posts/reading-anxiety/:0:0","series":null,"tags":["阅读","焦虑"],"title":"我患上了阅读焦虑症","uri":"/posts/reading-anxiety/#"},{"categories":["life"],"content":" 2017年2月的18号，清晨6点。天还只是朦朦亮，当空挂着半边弯月，一颗不知名的星星(大约是大角星)缀在月的旁边。 还没开学，学校几乎看不到人。 南食堂的一楼已亮起了灯，鸟儿们开始鸣叫个不停，可以听出有好几种鸟叫声。 易海仍是风平浪静。 我背着书包，拖着皮箱，耳边最清晰的声音便是皮箱轮胎与地面的摩擦声。 手机随便放起一首歌，恰好是《遥远的歌》。这首歌真是应景呢，逝去的时光遥远得无法触及，自己也离家千里，未来更是难以捉摸。 我还会记得吗？记得这个我印象中，最宁静安详的，安徽建筑大学。 ","date":"2017-02-18","objectID":"/posts/quiet-and-peaceful-campus/:0:0","series":null,"tags":[],"title":"少有人迹的校园","uri":"/posts/quiet-and-peaceful-campus/#"},{"categories":["life"],"content":" 上个暑假，刚刚从低谷爬出来，那时候整个人散发着一股子向上的气息，豪情万丈，甚至感染了周围的亲朋好友。那个时候，满以为以后的挫折都不能阻挡我的脚步。 可是，到底为什么，现在又变成了这个样子了呢？人生这样的东西，总是出人意料，以至于怎么也猜不透。 十二月十四，我度过了我的十九岁生日，现在应该正是那所谓的青春将逝未逝之时。而我的青春应该最辉煌的时候，我在干什么呢？ 我在翻山越岭。 上山时一路坎坷，累的要死不活的。陡然间萌生退意，心就在不断挣扎。就在却意战胜壮志之时，忽然间天地开阔，才发觉自己已然站在了大山之巅，于是一切痛苦尽皆远去，心也变得如这天地一般开阔。这个时候自然豪情万丈，看山山美，看水水秀。想当然的就觉得后面的山岭有再多的阻碍，也不能阻挡这个见过如此美景的登山人了。 可是事与愿违，山岭就像时间一样看不到边，翻过了一座又是一座，这又是一种更大的痛苦。这个登山人身心俱疲，只好万事随缘，继续一脚深，一脚浅的往那无尽山岭行去。 最近看了是枝裕和的电影《比海更深》，“我的人生到底出了什么差错？” 这样一个问句，道出了多少辛酸苦辣……想起了以前写过一篇文章，标题是《对不起，我没有成为你想成为的那个人》。 理想与现实之间仿佛总隔着一道鸿沟。 现在没有了万丈豪情，不再敢说“未来将是一片坦途”；也没有绝望到要写“我的人生到底出了什么差错？”这样的句子，那还是用我最喜欢的那个模棱两可的四字词作结吧。 且行且寻 ","date":"2017-02-06","objectID":"/posts/the-holiday-is-coming-to-an-end/:0:0","series":null,"tags":[],"title":"忽而假末","uri":"/posts/the-holiday-is-coming-to-an-end/#"},{"categories":["tech"],"content":" 本文最初于 2016-10-18 发表在博客园，2022-08-13 搬迁至 https://thiscute.world 昨晚我朋友 @三十六咲 跟我说在网上看到了别人做的视频转字符动画，觉得很厉害，我于是也打算玩玩。今天中午花时间实现了这样一个小玩意。 顺便把过程记录在这里。 注：最新版使用了画布方式实现，和本文相比改动非常大，如果对旧版本的实现没啥兴趣，可以直接移步 video2chars，它的效果动画见 极乐净土。新版本的核心代码不算注释70行不到，功能更强大。 ","date":"2016-10-18","objectID":"/posts/video2chars-1-basics/:0:0","series":null,"tags":[],"title":"Python 视频转字符动画（一）60 行代码","uri":"/posts/video2chars-1-basics/#"},{"categories":["tech"],"content":" 效果先上效果，来点动力。 源视频 【東方】Bad Apple!! ＰＶ【影絵】 转换后的效果如下： There should have been a video here but your browser does not seem to support it. ","date":"2016-10-18","objectID":"/posts/video2chars-1-basics/:0:1","series":null,"tags":[],"title":"Python 视频转字符动画（一）60 行代码","uri":"/posts/video2chars-1-basics/#效果"},{"categories":["tech"],"content":" 步骤 将视频转化为一帧一帧的图片 把图片转化为字符画 按顺序播放字符画 ","date":"2016-10-18","objectID":"/posts/video2chars-1-basics/:0:2","series":null,"tags":[],"title":"Python 视频转字符动画（一）60 行代码","uri":"/posts/video2chars-1-basics/#步骤"},{"categories":["tech"],"content":" 一、准备 1. 模块这个程序需要用到这样几个模块: opencv-python # 用来读取视频和图片 numpy # opencv-python 依赖于它 准备阶段，首先安装依赖： pip3 install numpy opencv-python 然后新建python代码文档，在开头添加上下面的导入语句 #-*- coding:utf-8 -*- # numpy 是一个矩阵运算库，图像处理需要用到。 import numpy as np 2. 材料材料就是需要转换的视频文件了，我这里用的是BadApple.mp4，下载下来和代码放到同一目录下 你也可以换成自己的，建议是学习时尽量选个短一点的视频，几十秒就行了，不然调试起来很痛苦。（或者自己稍微修改一下函数，只转换一定范围、一定数量的帧。） 此外，要选择对比度高的视频。否则的话，就需要彩色字符才能有足够好的表现，有时间我试试。 ","date":"2016-10-18","objectID":"/posts/video2chars-1-basics/:0:3","series":null,"tags":[],"title":"Python 视频转字符动画（一）60 行代码","uri":"/posts/video2chars-1-basics/#一准备"},{"categories":["tech"],"content":" 一、准备 1. 模块这个程序需要用到这样几个模块: opencv-python # 用来读取视频和图片 numpy # opencv-python 依赖于它 准备阶段，首先安装依赖： pip3 install numpy opencv-python 然后新建python代码文档，在开头添加上下面的导入语句 #-*- coding:utf-8 -*- # numpy 是一个矩阵运算库，图像处理需要用到。 import numpy as np 2. 材料材料就是需要转换的视频文件了，我这里用的是BadApple.mp4，下载下来和代码放到同一目录下 你也可以换成自己的，建议是学习时尽量选个短一点的视频，几十秒就行了，不然调试起来很痛苦。（或者自己稍微修改一下函数，只转换一定范围、一定数量的帧。） 此外，要选择对比度高的视频。否则的话，就需要彩色字符才能有足够好的表现，有时间我试试。 ","date":"2016-10-18","objectID":"/posts/video2chars-1-basics/:0:3","series":null,"tags":[],"title":"Python 视频转字符动画（一）60 行代码","uri":"/posts/video2chars-1-basics/#1-模块"},{"categories":["tech"],"content":" 一、准备 1. 模块这个程序需要用到这样几个模块: opencv-python # 用来读取视频和图片 numpy # opencv-python 依赖于它 准备阶段，首先安装依赖： pip3 install numpy opencv-python 然后新建python代码文档，在开头添加上下面的导入语句 #-*- coding:utf-8 -*- # numpy 是一个矩阵运算库，图像处理需要用到。 import numpy as np 2. 材料材料就是需要转换的视频文件了，我这里用的是BadApple.mp4，下载下来和代码放到同一目录下 你也可以换成自己的，建议是学习时尽量选个短一点的视频，几十秒就行了，不然调试起来很痛苦。（或者自己稍微修改一下函数，只转换一定范围、一定数量的帧。） 此外，要选择对比度高的视频。否则的话，就需要彩色字符才能有足够好的表现，有时间我试试。 ","date":"2016-10-18","objectID":"/posts/video2chars-1-basics/:0:3","series":null,"tags":[],"title":"Python 视频转字符动画（一）60 行代码","uri":"/posts/video2chars-1-basics/#2-材料"},{"categories":["tech"],"content":" 二、按帧读取视频现在继续添加代码，实现第一步：按帧读取视频。 下面这个函数，接受视频路径和字符视频的尺寸信息，返回一个img列表，其中的img是尺寸都为指定大小的灰度图。 #导入 opencv import cv2 def video2imgs(video_name, size): \"\"\" :param video_name: 字符串, 视频文件的路径 :param size: 二元组，(宽, 高)，用于指定生成的字符画的尺寸 :return: 一个 img 对象的列表，img对象实际上就是 numpy.ndarray 数组 \"\"\" img_list = [] # 从指定文件创建一个VideoCapture对象 cap = cv2.VideoCapture(video_name) # 如果cap对象已经初始化完成了，就返回true，换句话说这是一个 while true 循环 while cap.isOpened(): # cap.read() 返回值介绍： # ret 表示是否读取到图像 # frame 为图像矩阵，类型为 numpy.ndarry. ret, frame = cap.read() if ret: # 转换成灰度图，也可不做这一步，转换成彩色字符视频。 gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY) # resize 图片，保证图片转换成字符画后，能完整地在命令行中显示。 img = cv2.resize(gray, size, interpolation=cv2.INTER_AREA) # 分帧保存转换结果 img_list.append(img) else: break # 结束时要释放空间 cap.release() return img_list 写完后可以写个main方法测试一下，像这样： if __name__ == \"__main__\": imgs = video2imgs(\"BadApple.mp4\", (64, 48)) assert len(imgs) \u003e 10 如果运行没报错，就没问题 代码里的注释应该写得很清晰了，继续下一步。 ","date":"2016-10-18","objectID":"/posts/video2chars-1-basics/:0:4","series":null,"tags":[],"title":"Python 视频转字符动画（一）60 行代码","uri":"/posts/video2chars-1-basics/#二按帧读取视频"},{"categories":["tech"],"content":" 三、图像转化为字符画视频转换成了图像，这一步便是把图像转换成字符画 下面这个函数，接受一个img对象为参数，返回对应的字符画。 # 用于生成字符画的像素，越往后视觉上越明显。。这是我自己按感觉排的，你可以随意调整。 pixels = \" .,-'`:!1+*abcdefghijklmnopqrstuvwxyz\u003c\u003e()\\/{}[]?234567890ABCDEFGHIJKLMNOPQRSTUVWXYZ%\u0026@#$\" def img2chars(img): \"\"\" :param img: numpy.ndarray, 图像矩阵 :return: 字符串的列表：图像对应的字符画，其每一行对应图像的一行像素 \"\"\" res = [] # 灰度是用8位表示的，最大值为255。 # 这里将灰度转换到0-1之间 # 使用 numpy 的逐元素除法加速，这里 numpy 会直接对 img 中的所有元素都除以 255 percents = img / 255 # 将灰度值进一步转换到 0 到 (len(pixels) - 1) 之间，这样就和 pixels 里的字符对应起来了 # 同样使用 numpy 的逐元素算法，然后使用 astype 将元素全部转换成 int 值。 indexes = (percents * (len(pixels) - 1)).astype(np.int) # 要注意这里的顺序和 之前的 size 刚好相反（numpy 的 shape 返回 (行数、列数)） height, width = img.shape for row in range(height): line = \"\" for col in range(width): index = indexes[row][col] # 添加字符像素（最后面加一个空格，是因为命令行有行距却没几乎有字符间距，用空格当间距） line += pixels[index] + \" \" res.append(line) return res 上面的函数只接受一帧为参数，一次只转换一帧，可我们需要的是转换所有的帧，所以就再把它包装一下： def imgs2chars(imgs): video_chars = [] for img in imgs: video_chars.append(img2chars(img)) return video_chars 好了，现在我们可以测试一下： if __name__ == \"__main__\": imgs = video2imgs(\"BadApple.mp4\", (64, 48)) video_chars = imgs2chars(imgs) assert len(video_chars) \u003e 10 没报错的话，就可以下一步了。(这一步比较慢，测试阶段建议用短一点的视频，或者稍微改一下，只处理前30秒之类的) ","date":"2016-10-18","objectID":"/posts/video2chars-1-basics/:0:5","series":null,"tags":[],"title":"Python 视频转字符动画（一）60 行代码","uri":"/posts/video2chars-1-basics/#三图像转化为字符画"},{"categories":["tech"],"content":" 四、播放字符视频写了这么多代码，现在终于要出成果了。现在就是最激动人心的一步：播放字符画了。 同样的，我把它封装成了一个函数。下面这个函数接受一个字符画的列表并播放。 通用版（使用 shell 的 clear 命令清屏，但是因为效率不高，可能会有一闪一闪的问题） 这个版本适用于 linux/windows # 导入需要的模块 import time import subprocess def play_video(video_chars): \"\"\" 播放字符视频 :param video_chars: 字符画的列表，每个元素为一帧 :return: None \"\"\" # 获取字符画的尺寸 width, height = len(video_chars[0][0]), len(video_chars[0]) for pic_i in range(len(video_chars)): # 显示 pic_i，即第i帧字符画 for line_i in range(height): # 将pic_i的第i行写入第i列。 print(video_chars[pic_i][line_i]) time.sleep(1 / 24) # 粗略地控制播放速度。 # 调用 shell 命令清屏 subprocess.run(\"clear\", shell=True) # linux 版 # subrpocess.run(\"cls\", shell=True) # cmd 版，windows 系统请用这一行。 Unix系版本（使用了只支援 unix 系 的 curses 库，比 clear 更流畅） # 导入需要的模块 import time import curses def play_video(video_chars): \"\"\" 播放字符视频， :param video_chars: 字符画的列表，每个元素为一帧 :return: None \"\"\" # 获取字符画的尺寸 width, height = len(video_chars[0][0]), len(video_chars[0]) # 初始化curses，这个是必须的，直接抄就行 stdscr = curses.initscr() curses.start_color() try: # 调整窗口大小，宽度最好略大于字符画宽度。另外注意curses的height和width的顺序 stdscr.resize(height, width * 2) for pic_i in range(len(video_chars)): # 显示 pic_i，即第i帧字符画 for line_i in range(height): # 将pic_i的第i行写入第i列。(line_i, 0)表示从第i行的开头开始写入。最后一个参数设置字符为白色 stdscr.addstr(line_i, 0, video_chars[pic_i][line_i], curses.COLOR_WHITE) stdscr.refresh() # 写入后需要refresh才会立即更新界面 time.sleep(1 / 24) # 粗略地控制播放速度(24帧/秒)。更精确的方式是使用游戏编程里，精灵的概念 finally: # curses 使用前要初始化，用完后无论有没有异常，都要关闭 curses.endwin() return 好，接下来就是见证奇迹的时刻 不过开始前要注意，字符画的播放必须在shell窗口下运行，在pycharm里运行会看到一堆无意义字符。另外播放前要先最大化shell窗口 if __name__ == \"__main__\": imgs = video2imgs(\"BadApple.mp4\", (64, 48)) video_chars = imgs2chars(imgs) input(\"`转换完成！按enter键开始播放\") play_video(video_chars) 写完后，开个shell，最大化窗口，然后键入（文件名换成你的） python3 video2chars.py 可能要等很久。我使用示例视频大概需要 12 秒左右。看到提示的时候，按回车，开始播放！ **这样就完成了视频到字符动画的转换, 除去注释, 大概七十行代码的样子. 稍微超出了点预期, 不过效果真是挺棒的. ** ","date":"2016-10-18","objectID":"/posts/video2chars-1-basics/:0:6","series":null,"tags":[],"title":"Python 视频转字符动画（一）60 行代码","uri":"/posts/video2chars-1-basics/#四播放字符视频"},{"categories":["tech"],"content":" 五、进一步优化到了这里，核心功能基本都完成了。 不过仔细想想，其实还有很多可以做的： 能不能手动指定要转换的区间、帧率？ 每次转换都要很久的时间，能不能边转换边播放？或者转换后把数据保存起来，下次播放时，就直接读缓存。 为啥我的字符动画没有声音，看无声电影么？ 视频的播放速度能不能精确控制？ 能不能用彩色字符？ 这些东西，就不写这里了，再写下去，你们肯定要说我这标题是骗人了哈哈。 所以如果有兴趣的，请移步这个系列的下一篇：视频转字符动画（二）进阶 ","date":"2016-10-18","objectID":"/posts/video2chars-1-basics/:0:7","series":null,"tags":[],"title":"Python 视频转字符动画（一）60 行代码","uri":"/posts/video2chars-1-basics/#五进一步优化"},{"categories":["tech"],"content":" 六、总结完整代码见 video2chars.py，要注意的是代码库的代码，包含了第二篇文章的内容（音频、缓存、帧率控制等），而且相对这篇文章也有一些小改动（目的是方便使用，但是稍微增加了点代码量，所以改动没有写在这篇文章里了） 想运行起来的话，还是建议跟着文章做。。 ","date":"2016-10-18","objectID":"/posts/video2chars-1-basics/:0:8","series":null,"tags":[],"title":"Python 视频转字符动画（一）60 行代码","uri":"/posts/video2chars-1-basics/#六总结"},{"categories":["tech"],"content":" 七、参考 Opencv-Python Tutorials - Video Playing Python 图片转字符画 允许转载, 但是要求附上来源链接: Python 视频转字符动画（一）60 行代码 ","date":"2016-10-18","objectID":"/posts/video2chars-1-basics/:0:9","series":null,"tags":[],"title":"Python 视频转字符动画（一）60 行代码","uri":"/posts/video2chars-1-basics/#七参考"}]