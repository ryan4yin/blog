[{"categories":["随笔","技术"],"content":"闲言碎语 一晃一年又是过去了，这个新年，全球疫情再创新高，圣诞节后美国单日新增更是直接突破 50 万直逼 60 万大关，国内也有西安管理不力导致民众忍饥挨饿。 新冠已经两年多了啊。 言归正传，今年年初从 W 公司离职后，我非常幸运地进了现在的公司，在融入新公司的过程中也是五味杂陈。 不过总体结果我自己还是挺满意的，目前工作已经步入正轨，也在新公司发现了非常多的机会。 一些重要事情还是没怎么想通，不过毕竟风口上的猪都能飞，今年小小努力了一把，大部分时间仍然随波逐流，却也渐入佳境。 ","date":"2022-01-03","objectID":"/posts/2021-summary/:1:0","tags":["总结"],"title":"2021 年年终总结","uri":"/posts/2021-summary/"},{"categories":["随笔","技术"],"content":"生活 加入了我司的冲浪小分队，第一次冲浪、海边烧烤 跟两个堂弟一起穿越深圳东西冲海岸线，风景非常棒，不过路上也是又热又渴 定期团建，跟 SRE 小伙伴公款吃喝，今年下馆子次数估计是我去年的七八倍 又买了双轮滑鞋，学会了倒滑、压步转向，复习了大学时学过的若干基础技巧 今年只看了一部电影——《寻龙传说》，片尾曲超好听。 各种巧合下，意外发现初中同学住得离我 1km 不到，在他家吃了顿家乡菜，还有杨梅酒，味道非常棒！还有回甘强烈的城步青钱柳茶，让我念念不忘。 ","date":"2022-01-03","objectID":"/posts/2021-summary/:2:0","tags":["总结"],"title":"2021 年年终总结","uri":"/posts/2021-summary/"},{"categories":["随笔","技术"],"content":"读书 年初辞职后游山玩水，心思稍微安定了些，看了大半本《走出荒野》。 6 月份社区组织打新冠疫苗时，在等候室看了本《青春驿站——深圳打工妹写真》，讲述八九十年代打工妹的生活。很真实，感情很细腻。 年末二爷爷去世，参加完葬礼后，心态有些变化，看完了大一时买下的《月宫 Moon Palace》，讲述主角的悲剧人生。 其余大部分业余时间，无聊，又不想学点东西，也不想运动，于是看了非常多的网络小说打发时间。 ","date":"2022-01-03","objectID":"/posts/2021-summary/:3:0","tags":["总结"],"title":"2021 年年终总结","uri":"/posts/2021-summary/"},{"categories":["随笔","技术"],"content":"音乐 年初辞职后，练了一段时间的竹笛跟蓝调口琴，但后来找到工作后就基本沉寂了。 总的来说还是原地踏步吧。 ","date":"2022-01-03","objectID":"/posts/2021-summary/:4:0","tags":["总结"],"title":"2021 年年终总结","uri":"/posts/2021-summary/"},{"categories":["随笔","技术"],"content":"工作与技术 回看下了 2020 年的总结与展望，今年实际的进步，跟去年期望的差别很大。最初的目标大概只实现了 10%，但是接触到了许多意料之外的东西，总体还是满意的： 熟悉了新公司的文化与工作方式，这感觉是个很大的收获，我的工作方式有了很大的改善 接触并且熟悉了新公司的 AWS 线上环境 负责管理维护线上 Kubernetes 管理平台，第一次接触到的线上集群峰值 QPS 就有好几万。从一开始的小心翼翼，到现在也转变成了老手，这算是意义重大吧 使用 python 写了几个 Kubernetes 管理平台的服务，这也是我第一次写线上服务，很有些成就感 参与 AWS 成本的分析与管控，有了一些不错的成果 学会了 Nginx 的简单使用，刚好够用于维护公司先有的 Nginx 代理配置。 随便写了几个 go 的 demo，基本没啥进步 学了一个星期的 rust 语言，快速看完了 the book，用 rust 重写了个 video2chars 学习了 Linux 容器的底层原理：cgroups/namespace 技术，并且用 go/rust 实现了个 demo 学习了 Linux 的各种网络接口、Iptables 熟悉了 PromQL/Grafana，现在也能拷贝些 PromQL 查各种数据了 如果要给自己打分的话，那就是「良好」吧。因为并没有很强的进取心，所以出来的结果也并不能称之为「优秀」。 ","date":"2022-01-03","objectID":"/posts/2021-summary/:5:0","tags":["总结"],"title":"2021 年年终总结","uri":"/posts/2021-summary/"},{"categories":["随笔","技术"],"content":"技术方面的感受 Istio 服务网格：体会到了它有点重，而且它的发展跟我们的需求不一定匹配 Sidecar 模式的成本比较高，在未调优的情况下，它会给服务带来 1/3 到 1/4 的成本提升，以及延迟上升 比如切量权重固定为 100（新版本将会放宽限制），不支持 pod 的 warm up（社区已经有 PR，持续观望吧） 而它重点发展的虚拟机支持我们却完全不需要 一直在思考是持续往 Istio 投入，还是换其他的方案 服务网格仍然在快速发展，未来的趋势应该是 eBPF + Envoy + WASM Cilium 推出的基于 eBPF 的 Service Mesh 是一个新趋势（它使用高级特性时会退化成 Per Node Proxy 模式），成本、延迟方面都吊打 Sidecar 模式的其他服务网格，是今年服务网格领域的大新闻。 我们曾尝试使用中心化网关来替代 Sidecar 以降低成本。但是跨区流量成本、HTTP/gRPC 多协议共存，这些都是挑战。而且这也并不是社区的最佳实践，现在我觉得维持 Sidecar 其实反而能提升资源利用率，我们的集群资源利用率目前很低。如果能把控好，这部分成本或许是可以接受的。 K8s 集群的日志方面，我们目前是使用自研的基于 gelf 协议的系统，但是开源的 loki 会更好呢？ 从提升系统的可维护性、易用性等角度来说，loki 是值得探索下的 K8s 集群管理方面，觉得集群的升级迭代，可以做得更自动化、更可靠。明年可以往这个方向靠拢。 Pod 服务质量：对非核心服务，可以适当调低 requests 的资源量，而不是完全预留(Guaranteed)，以提升资源利用率。 官方的 HPA 能力是不够用的，业务侧可能会需要基于 QPS/Queue 或者业务侧的其他参数来进行扩缩容，需要持续关注 KEDA 这个项目。 成本控制方面，体会到了 ARM 架构以及 Spot 竞价实例的好处 跨区流量成本有很大的潜在优化空间 今年各云厂商故障频发，没有跨 region 的服务迁移就会很难受，需要持续关注下 karmada 这类多集群管理方案。 Google 账号系统宕机 Fastly CDN 故障 Facebook 故障 AWS 更是各种可用区故障，12/7 的故障导致 AWS 大部分服务都崩了。因此我们 SRE 今年经常是救各种大火小火… Rust/Go/WASM 蓬勃发展，未来可期。 AI 落地到各个领域，影响到了我们日常使用的语音导航、歌声合成、语音合成等多个领域，当然也包括与 SRE 工作相关的场景：AIOps ","date":"2022-01-03","objectID":"/posts/2021-summary/:6:0","tags":["总结"],"title":"2021 年年终总结","uri":"/posts/2021-summary/"},{"categories":["随笔","技术"],"content":"2022 年的展望 ","date":"2022-01-03","objectID":"/posts/2021-summary/:7:0","tags":["总结"],"title":"2021 年年终总结","uri":"/posts/2021-summary/"},{"categories":["随笔","技术"],"content":"技术侧 今年的展望写得更聚焦一些，争取能实现 50%，就是很大的突破了。 熟练掌握 Go/Rust 语言，并用于至少一个项目中 深入学习如下技术 Kubernetes 源码 网络技术 服务网格 Istio 代理工具 Envoy/APISIX 网络插件 Cilium + eBPF AWS K8s 成本与服务稳定性优化 节约跨可用区/跨域的流量成本 K8s 新特性：Topology Aware Hints Istio: Locality Load Balancing 实例类型优化： 使用更合适的实例类型 使用 ARM 架构机型，降本增效 推广 GRPC 协议 打通本地开发环境与云上的运行环境： nocalhost 探索新技术与可能性（优先级低） 基于 Kubernetes 的服务平台，未来的发展方向 kubevela buildpack 是否应该推进 gitops openkruise Serverless 平台的进展 Knative OpenFunction 跨集群的应用部署、容灾 karmada 机器学习、深度学习技术：想尝试下将 AI 应用在音乐、语音、SRE 等我感兴趣的领域，即使是调包也行啊… ","date":"2022-01-03","objectID":"/posts/2021-summary/:7:1","tags":["总结"],"title":"2021 年年终总结","uri":"/posts/2021-summary/"},{"categories":["随笔","技术"],"content":"生活侧 运动： 把轮滑练好，学会点花样吧，每个月至少两次。 进行三次以上的次短途旅行，东西冲穿越可以再来一次。 音乐： 再一次学习乐理… midi 键盘买了一直吃灰，多多练习吧 买了个 Synthesizer V Stduio Pro + 「青溯 AI」，新的一年想学下调教，翻唱些自己喜欢的歌。 阅读：清单如下，一个月至少读完其中一本。 文学类： 《人间失格》：久仰大名的一本书，曾经有同学力荐，但是一直没看。 《生命最后的读书会》：或许曾经看过，但是一点印象都没了 《百年孤独》：高中的时候读过一遍，但是都忘差不多了 《霍乱时期的爱情》 《苏菲的世界》：据说是哲学启蒙读物，曾经看过，但是对内容完全没印象了。 《你一生的故事》：我也曾是个科幻迷 《沈从文的后半生》 《我与地坛》 《将饮茶》 《吾国与吾民 - 林语堂》 《房思琪的初恋乐园》 人文社科 《在生命的尽头拥抱你-临终关怀医生手记》：今年想更多地了解下「死亡」 《怎样征服美丽少女》：哈哈 《爱的艺术》 《社会心理学》 《被讨厌的勇气》 《人体简史》 《科学革命的结构》 《邓小平时代》 《论中国》 《刘擎西方现代思想讲义》 《时间的秩序》 《极简宇宙史》 《圆圈正义-作为自由前提的信念》 《人生脚本》 技术类 《复杂》 《SRE - Google 运维解密》 《凤凰项目：一个 IT 运维的传奇故事》 《人月神话》 《绩效使能：超越 OKR》 《奈飞文化手册》 《幕后产品-打造突破式思维》 《深入 Linux 内核架构》 《Linux/UNIX 系统编程手册》 《重构 - 改善既有代码的设计》 《网络是怎样连接的》：曾经学习过《计算机网络：自顶向下方法》，不过只学到网络层。就从这本书开始重新学习吧。 ","date":"2022-01-03","objectID":"/posts/2021-summary/:8:0","tags":["总结"],"title":"2021 年年终总结","uri":"/posts/2021-summary/"},{"categories":["随笔","技术"],"content":"结语 2021 年初朋友与我给自己的期许是「拆破玉笼飞彩凤，顿开金锁走蛟龙」，感觉确实应验了。 今年我希望不论是在生活上还是在工作上，都能「更上层楼」~ ","date":"2022-01-03","objectID":"/posts/2021-summary/:9:0","tags":["总结"],"title":"2021 年年终总结","uri":"/posts/2021-summary/"},{"categories":["随笔"],"content":" 「此岸弃草，彼岸繁花。」取自前永动机主唱「河津樱/白金」的个人简介 今天想推几首歌 emmmm ","date":"2021-08-28","objectID":"/posts/weeds-on-this-side-flowers-on-the-other/:0:0","tags":["闲言碎语","音乐"],"title":"此岸弃草，彼岸繁花","uri":"/posts/weeds-on-this-side-flowers-on-the-other/"},{"categories":["技术"],"content":" 本文仅针对 ipv4 网络 本文先介绍 iptables 的基本概念及常用命令，然后分析 docker/podman 是如何利用 iptables 和 Linux 虚拟网络接口实现的单机容器网络。 一、iptables iptables 提供了包过滤、NAT 以及其他的包处理能力，iptables 应用最多的两个场景是 firewall 和 NAT iptables 及新的 nftables 都是基于 netfilter 开发的，是 netfilter 的子项目。 但是 eBPF 社区目前正在开发旨在取代 netfilter 的新项目 bpfilter，他们的目标之一是兼容 iptables/nftables 规则，让我们拭目以待吧。 ","date":"2021-08-15","objectID":"/posts/iptables-and-container-networks/:0:0","tags":["Linux","网络","虚拟化","容器","iptables","conntrack"],"title":"iptables 及 docker 容器网络分析","uri":"/posts/iptables-and-container-networks/"},{"categories":["技术"],"content":"iptables 基础概念 - 四表五链 实际上还有张 SELinux 相关的 security 表（应该是较新的内核新增的，但是不清楚是哪个版本加的），但是我基本没接触过，就略过了。 详细的说明参见 iptables详解（1）：iptables概念 - 朱双印，这篇文章写得非常棒！把 iptables 讲清楚了。 默认情况下，iptables 提供了四张表（不考虑 security 的话）和五条链，数据在这四表五链中的处理流程如下图所示： 在这里的介绍中，可以先忽略掉图中 link layer 层的链路，它属于 ebtables 的范畴。另外 conntrack 也暂时忽略，在下一小节会详细介绍 conntrack 的功能。 netfilter 数据包处理流程，来自 wikipedia\" netfilter 数据包处理流程，来自 wikipedia 对照上图，对于发送到某个用户层程序的数据而言，流量顺序如下： 首先进入 PREROUTING 链，依次经过这三个表： raw -\u003e mangle -\u003e nat 然后进入 INPUT 链，这个链上也有三个表，处理顺序是：mangle -\u003e nat -\u003e filter 过了 INPUT 链后，数据才会进入内核协议栈，最终到达用户层程序。 用户层程序发出的报文，则依次经过这几个表：OUTPUT -\u003e POSTROUTING 从图中也很容易看出，如果数据 dst ip 不是本机任一接口的 ip，那它通过的几个链依次是：PREROUTEING -\u003e FORWARD -\u003e POSTROUTING 五链的功能和名称完全一致，应该很容易理解。下面按优先级分别介绍下链中的四个表： raw: 对收到的数据包在连接跟踪前进行处理。一般用不到，可以忽略 一旦用户使用了 RAW 表，RAW 表处理完后，将跳过 NAT 表和 ip_conntrack 处理，即不再做地址转换和数据包的链接跟踪处理了 mangle: 用于修改报文、给报文打标签 nat: 主要用于做网络地址转换，SNAT 或者 DNAT filter: 主要用于过滤数据包 数据在按优先级经过四个表的处理时，一旦在某个表中匹配到一条规则 A,下一条处理规则就由规则 A 的 target 参数指定，后续的所有表都会被忽略。target 有如下几种类型： ACCEPT: 直接允许数据包通过 DROP: 直接丢弃数据包，对程序而言就是 100% 丢包 REJECT: 丢弃数据包，但是会给程序返回 RESET。这个对程序更友好，但是存在安全隐患，通常不使用。 MASQUERADE: （伪装）将 src ip 改写为网卡 ip，和 SNAT 的区别是它会自动读取网卡 ip。路由设备必备。 SNAT/DNAT: 顾名思义，做网络地址转换 REDIRECT: 在本机做端口映射 LOG: 在/var/log/messages文件中记录日志信息，然后将数据包传递给下一条规则，也就是说除了记录以外不对数据包做任何其他操作，仍然让下一条规则去匹配。 只有这个 target 特殊一些，匹配它的数据仍然可以匹配后续规则，不会直接跳过。 其他类型，可以用到的时候再查 理解了上面这张图，以及四个表的用途，就很容易理解 iptables 的命令了。 ","date":"2021-08-15","objectID":"/posts/iptables-and-container-networks/:1:0","tags":["Linux","网络","虚拟化","容器","iptables","conntrack"],"title":"iptables 及 docker 容器网络分析","uri":"/posts/iptables-and-container-networks/"},{"categories":["技术"],"content":"常用命令 注意: 下面提供的 iptables 命令做的修改是未持久化的，重启就会丢失！在下一节会简单介绍持久化配置的方法。 命令格式： iptables [-t table] {-A|-C|-D} chain [-m matchname [per-match-options]] -j targetname [per-target-options] 其中 table 默认为 filter 表，但是感觉系统管理员实际使用最多的是 INPUT 表，用于设置防火墙。 以下简单介绍在 INPUT 表上添加、修改规则，来设置防火墙： # --add 允许 80 端口通过 iptables -A INPUT -p tcp --dport 80 -j ACCEPT # --list-rules 查看所有规则 iptables -S # --list-rules 查看 INPUT 表中的所有规则 iptables -S INPUT # 查看 iptables 中的所有规则（比 -L 更详细） # ---delete 通过编号删除规则 iptables -D 1 # 或者通过完整的规则参数来删除规则 iptables -D INPUT -p tcp --dport 80 -j ACCEPT # --replace 通过编号来替换规则内容 iptables -R INPUT 1 -s 192.168.0.1 -j DROP # --insert 在指定的位置插入规则，可类比链表的插入 iptables -I INPUT 1 -p tcp --dport 80 -j ACCEPT # 在匹配条件前面使用感叹号表示取反 # 如下规则表示接受所有来自 docker0，但是目标接口不是 docker0 的流量 iptables -A FORWARD -i docker0 ! -o docker0 -j ACCEPT # --policy 设置某个链的默认规则 # 很多系统管理员会习惯将连接公网的服务器，默认规则设为 DROP，提升安全性，避免错误地开放了端口。 # 但是也要注意，默认规则设为 DROP 前，一定要先把允许 ssh 端口的规则加上，否则就尴尬了。 iptables -P INPUT DROP # --flush 清空 INPUT 表上的所有规则 iptables -F INPUT 本文后续分析时，假设用户已经清楚 linux bridge、veth 等虚拟网络接口相关知识。 如果你还缺少这些前置知识，请先阅读文章 Linux 中的虚拟网络接口。 ","date":"2021-08-15","objectID":"/posts/iptables-and-container-networks/:2:0","tags":["Linux","网络","虚拟化","容器","iptables","conntrack"],"title":"iptables 及 docker 容器网络分析","uri":"/posts/iptables-and-container-networks/"},{"categories":["技术"],"content":"conntrack 连接跟踪与 NAT 在讲 conntrack 之间，我们再回顾下前面给出过的 netfilter 数据处理流程图： netfilter 数据包处理流程，来自 wikipedia\" netfilter 数据包处理流程，来自 wikipedia 上一节中我们忽略了图中的 conntrack，它就是本节的主角——netfilter 的连接跟踪（connection tracking）模块。 netfilter/conntrack 是 iptables 实现 SNAT/DNAT/MASQUERADE 的前提条件，上面的流程图显示， conntrack 在 PREROUTEING 和 OUTPUT 表的 raw 链之后生效。 下面以 docker 默认的 bridge 网络为例详细介绍下 conntrack 的功能。 首先，这是我在「Linux 的虚拟网络接口」文中给出过的 docker0 网络架构图: +-----------------------------------------------+-----------------------------------+-----------------------------------+ | Host | Container A | Container B | | | | | | +---------------------------------------+ | +-------------------------+ | +-------------------------+ | | | Network Protocol Stack | | | Network Protocol Stack | | | Network Protocol Stack | | | +----+-------------+--------------------+ | +-----------+-------------+ | +------------+------------+ | | ^ ^ | ^ | ^ | |........|.............|........................|................|..................|.................|.................| | v v ↓ | v | v | | +----+----+ +-----+------+ | +-----+-------+ | +-----+-------+ | | | .31.101 | | 172.17.0.1 | +------+ | | 172.17.0.2 | | | 172.17.0.3 | | | +---------+ +-------------\u003c----\u003e+ veth | | +-------------+ | +-------------+ | | | eth0 | | docker0 | +--+---+ | | eth0(veth) | | | eth0(veth) | | | +----+----+ +-----+------+ ^ | +-----+-------+ | +-----+-------+ | | ^ ^ | | ^ | ^ | | | | +------------------------+ | | | | | v | | | | | | +--+---+ | | | | | | | veth | | | | | | | +--+---+ | | | | | | ^ | | | | | | +------------------------------------------------------------------------------+ | | | | | | | | | | | +-----------------------------------------------+-----------------------------------+-----------------------------------+ v Physical Network (192.168.31.0/24) docker 会在 iptables 中为 docker0 网桥添加如下规则： -t nat -A POSTROUTING -s 172.17.0.0/16 ! -o docker0 -j MASQUERADE -t filter -P DROP -t filter -A FORWARD -o docker0 -m conntrack --ctstate RELATED,ESTABLISHED -j ACCEPT 这几行规则使 docker 容器能正常访问外部网络。MASQUERADE 在请求出网时，会自动做 SNAT，将 src ip 替换成出口网卡的 ip. 这样数据包能正常出网，而且对端返回的数据包现在也能正常回到出口网卡。 现在问题就来了：出口网卡收到返回的数据包后，还能否将数据包转发到数据的初始来源端——某个 docker 容器？难道 docker 还额外添加了与 MASQUERADE 对应的 dst ip 反向转换规则？ 实际上这一步依赖的是本节的主角——iptables 提供的 conntrack 连接跟踪功能（在「参考」中有一篇文章详细介绍了此功能）。 连接跟踪对 NAT 的贡献是：在做 NAT 转换时，无需手动添加额外的规则来执行反向转换以实现数据的双向传输。netfilter/conntrack 系统会记录 NAT 的连接状态，NAT 地址的反向转换是根据这个状态自动完成的。 比如上图中的 Container A 通过 bridge 网络向 baidu.com 发起了 N 个连接，这时数据的处理流程如下： 首先 Container A 发出的数据包被 MASQUERADE 规则处理，将 src ip 替换成 eth0 的 ip，然后发送到物理网络 192..168.31.0/24。 conntrack 系统记录此连接被 NAT 处理前后的状态信息，并将其状态设置为 NEW，表示这是新发起的一个连接 对端 baidu.com 返回数据包后，会首先到达 eth0 网卡 conntrack 查表，发现返回数据包的连接已经记录在表中并且状态为 NEW，于是它将连接的状态修改为 ESTABLISHED，并且将 dst_ip 改为 172.17.0.2 然后发送出去 注意，这个和 tcp 的 ESTABLISHED 没任何关系 经过路由匹配，数据包会进入到 docker0，然后匹配上 iptables 规则：-t filter -A FORWARD -o docker0 -m conntrack --ctstate RELATED,ESTABLISHED -j ACCEPT，数据直接被放行 数据经过 veth 后，最终进入到 Container A 中，交由容器的内核协议栈处理。 数据被 Container A 的内核协议栈发送到「发起连接的应用程序」。 ","date":"2021-08-15","objectID":"/posts/iptables-and-container-networks/:3:0","tags":["Linux","网络","虚拟化","容器","iptables","conntrack"],"title":"iptables 及 docker 容器网络分析","uri":"/posts/iptables-and-container-networks/"},{"categories":["技术"],"content":"实际测试 conntrack 现在我们来实际测试一下，看看是不是这么回事： # 使用 tcpdump 分别在出口网卡 wlp4s0 （相当于 eth0）和 dcoker0 网桥上抓包，后面会用来分析 ❯ sudo tcpdump -i wlp4s0 -n \u003e wlp4s0.dump # 窗口一，抓 wlp4s0 的包 ❯ sudo tcpdump -i docker0 -n \u003e docker0.dump # 窗口二，抓 docker0 的包 现在新建窗口三，启动一个容器，通过 curl 命令低速下载一个视频文件： ❯ docker run --rm --name curl -it curlimages/curl \"https://media.w3.org/2010/05/sintel/trailer.mp4\" -o /tmp/video.mp4 --limit-rate 100k 然后新建窗口四，在宿主机查看 conntrack 状态 ❯ sudo zypper in conntrack-tools # 这个记得先提前安装好 ❯ sudo conntrack -L | grep 172.17 # curl 通过 NAT 网络发起了一个 dns 查询请求，DNS 服务器是网关上的 192.168.31.1 udp 17 22 src=172.17.0.4 dst=192.168.31.1 sport=59423 dport=53 src=192.168.31.1 dst=192.168.31.228 sport=53 dport=59423 [ASSURED] mark=0 use=1 # curl 通过 NAT 网络向 media.w3.org 发起了 tcp 连接 tcp 6 298 ESTABLISHED src=172.17.0.4 dst=198.18.5.130 sport=54636 dport=443 src=198.18.5.130 dst=192.168.31.228 sport=443 dport=54636 [ASSURED] mark=0 use=1 等 curl 命令跑个十来秒，然后关闭所有窗口及应用程序，接下来进行数据分析： # 前面查到的，本地发起请求的端口是 54636，下面以此为过滤条件查询数据 # 首先查询 wlp4s0/eth0 进来的数据，可以看到本机的 dst_ip 为 192.168.31.228.54636 ❯ cat wlp4s0.dump | grep 54636 | head -n 15 18:28:28.349321 IP 192.168.31.228.54636 \u003e 198.18.5.130.443: Flags [S], seq 750859357, win 64240, options [mss 1460,sackOK,TS val 3365688110 ecr 0,nop,wscale 7], length 0 18:28:28.350757 IP 198.18.5.130.443 \u003e 192.168.31.228.54636: Flags [S.], seq 2381759932, ack 750859358, win 28960, options [mss 1460,sackOK,TS val 22099541 ecr 3365688110,nop,wscale 5], length 0 18:28:28.350814 IP 192.168.31.228.54636 \u003e 198.18.5.130.443: Flags [.], ack 1, win 502, options [nop,nop,TS val 3365688111 ecr 22099541], length 0 18:28:28.357345 IP 192.168.31.228.54636 \u003e 198.18.5.130.443: Flags [P.], seq 1:518, ack 1, win 502, options [nop,nop,TS val 3365688118 ecr 22099541], length 517 18:28:28.359253 IP 198.18.5.130.443 \u003e 192.168.31.228.54636: Flags [.], ack 518, win 939, options [nop,nop,TS val 22099542 ecr 3365688118], length 0 18:28:28.726544 IP 198.18.5.130.443 \u003e 192.168.31.228.54636: Flags [P.], seq 1:2622, ack 518, win 939, options [nop,nop,TS val 22099579 ecr 3365688118], length 2621 18:28:28.726616 IP 192.168.31.228.54636 \u003e 198.18.5.130.443: Flags [.], ack 2622, win 482, options [nop,nop,TS val 3365688487 ecr 22099579], length 0 18:28:28.727652 IP 192.168.31.228.54636 \u003e 198.18.5.130.443: Flags [P.], seq 518:598, ack 2622, win 501, options [nop,nop,TS val 3365688488 ecr 22099579], length 80 18:28:28.727803 IP 192.168.31.228.54636 \u003e 198.18.5.130.443: Flags [P.], seq 598:644, ack 2622, win 501, options [nop,nop,TS val 3365688488 ecr 22099579], length 46 18:28:28.727828 IP 192.168.31.228.54636 \u003e 198.18.5.130.443: Flags [P.], seq 644:693, ack 2622, win 501, options [nop,nop,TS val 3365688488 ecr 22099579], length 49 18:28:28.727850 IP 192.168.31.228.54636 \u003e 198.18.5.130.443: Flags [P.], seq 693:728, ack 2622, win 501, options [nop,nop,TS val 3365688488 ecr 22099579], length 35 18:28:28.727875 IP 192.168.31.228.54636 \u003e 198.18.5.130.443: Flags [P.], seq 728:812, ack 2622, win 501, options [nop,nop,TS val 3365688488 ecr 22099579], length 84 18:28:28.729241 IP 198.18.5.130.443 \u003e 192.168.31.228.54636: Flags [.], ack 598, win 939, options [nop,nop,TS val 22099579 ecr 3365688488], length 0 18:28:28.729245 IP 198.18.5.130.443 \u003e 192.168.31.228.54636: Flags [.], ack 644, win 939, options [nop,nop,TS val 22099579 ecr 3365688488], length 0 18:28:28.729247 IP 198.18.5.130.443 \u003e 192.168.31.228.54636: Flags [.], ack 693, win 939, options [nop,nop,TS val 22099579 ecr 3365688488], length 0 # 然后再查询 docker0 上的数据，能发现本地的地址为 172.17.0.4.54636 ❯ cat docker0.dump | grep 54636 | head -n 20 18:28:28.349299 IP 172.17.0.4.54636 \u003e 198.18.5.130.443: Flags [S], seq 750859357, win 64240, options [mss 1460,sackOK,TS val 3365688110 ecr 0,nop,wscale 7], length 0 18:28:28.350780 IP 198.18.5.130.443 \u003e 172.17.0.4.54636: Flags [S.], seq 2381759932, ack 750859358, win 28960, options [mss 1460,sackOK,TS val 22099541 ecr 3365688110,nop,wscale 5], length 0 18:28:28.350812 IP 172.17.0.4.54636 \u003e 198.18.5.130.443: Flags [.], ","date":"2021-08-15","objectID":"/posts/iptables-and-container-networks/:3:1","tags":["Linux","网络","虚拟化","容器","iptables","conntrack"],"title":"iptables 及 docker 容器网络分析","uri":"/posts/iptables-and-container-networks/"},{"categories":["技术"],"content":"NAT 如何分配端口？ 上一节我们实际测试发现，docker 容器的流量在经过 iptables 的 MASQUERADE 规则处理后，只有 src ip 被修改了，而 port 仍然是一致的。 但是如果 NAT 不修改连接的端口，实际上是会有问题的。如果有两个容器同时向 ip: 198.18.5.130, port: 443 发起请求，又恰好使用了同一个 src port，在宿主机上就会出现端口冲突！ 因为这两个请求被 SNAT 时，如果只修改 src ip，那它们映射到的将是主机上的同一个连接！ 这个问题 NAT 是如何解决的呢？我想如果遇到这种情况，NAT 应该会通过一定的规则选用一个不同的端口。 有空可以翻一波源码看看这个，待续… ","date":"2021-08-15","objectID":"/posts/iptables-and-container-networks/:3:2","tags":["Linux","网络","虚拟化","容器","iptables","conntrack"],"title":"iptables 及 docker 容器网络分析","uri":"/posts/iptables-and-container-networks/"},{"categories":["技术"],"content":"如何持久化 iptables 配置 首先需要注意的是，centos7/opensuse 15 都已经切换到了 firewalld 作为防火墙配置软件， 而 ubuntu18.04 lts 也换成了 ufw 来配置防火墙。 包括 docker 应该也是在启动的时候动态添加 iptables 配置。 对于上述新系统，还是建议直接使用 firewalld/ufw 配置防火墙吧，或者网上搜下关闭 ufw/firewalld、启用 iptables 持久化的解决方案。 本文主要目的在于理解 docker 容器网络的原理，以及为后面理解 kubernetes 网络插件 calico/flannel 打好基础，因此就不多介绍持久化了。 ","date":"2021-08-15","objectID":"/posts/iptables-and-container-networks/:4:0","tags":["Linux","网络","虚拟化","容器","iptables","conntrack"],"title":"iptables 及 docker 容器网络分析","uri":"/posts/iptables-and-container-networks/"},{"categories":["技术"],"content":"如何使用 iptables + bridge + veth 实现容器网络 Docker/Podman 默认使用的都是 bridge 网络，它们的底层实现完全类似。下面以 docker 为例进行分析（Podman 的分析流程也基本一样）。 ","date":"2021-08-15","objectID":"/posts/iptables-and-container-networks/:5:0","tags":["Linux","网络","虚拟化","容器","iptables","conntrack"],"title":"iptables 及 docker 容器网络分析","uri":"/posts/iptables-and-container-networks/"},{"categories":["技术"],"content":"通过 docker run 运行容器 首先，使用 docker run 运行几个容器，检查下网络状况： # 运行一个 debian 容器和一个 nginx ❯ docker run -dit --name debian --rm debian:buster sleep 1000000 ❯ docker run -dit --name nginx --rm nginx:1.19-alpine #　查看网络接口，有两个 veth 接口（而且都没设 ip 地址），分别连接到两个容器的 eth0（dcoker0 网络架构图前面给过了，可以往前面翻翻对照下） ❯ ip addr ls ... 5: docker0: \u003cBROADCAST,MULTICAST,UP,LOWER_UP\u003e mtu 1500 qdisc noqueue state UP group default link/ether 02:42:42:c7:12:ba brd ff:ff:ff:ff:ff:ff inet 172.17.0.1/16 brd 172.17.255.255 scope global docker0 valid_lft forever preferred_lft forever inet6 fe80::42:42ff:fec7:12ba/64 scope link valid_lft forever preferred_lft forever 100: veth16b37ea@if99: \u003cBROADCAST,MULTICAST,UP,LOWER_UP\u003e mtu 1500 qdisc noqueue master docker0 state UP group default link/ether 42:af:34:ae:74:ae brd ff:ff:ff:ff:ff:ff link-netnsid 0 inet6 fe80::40af:34ff:feae:74ae/64 scope link valid_lft forever preferred_lft forever 102: veth4b4dada@if101: \u003cBROADCAST,MULTICAST,UP,LOWER_UP\u003e mtu 1500 qdisc noqueue master docker0 state UP group default link/ether 9e:f1:58:1a:cf:ae brd ff:ff:ff:ff:ff:ff link-netnsid 1 inet6 fe80::9cf1:58ff:fe1a:cfae/64 scope link valid_lft forever preferred_lft forever # 两个 veth 接口都连接到了 docker0 上面，说明两个容器都使用了 docker 默认的 bridge 网络 ❯ sudo brctl show bridge name bridge id STP enabled interfaces docker0 8000.024242c712ba no veth16b37ea veth4b4dada # 查看路由规则 ❯ ip route ls default via 192.168.31.1 dev wlp4s0 proto dhcp metric 600 #下列路由规则将 `172.17.0.0/16` 网段的所有流量转发到 docker0 172.17.0.0/16 dev docker0 proto kernel scope link src 172.17.0.1 linkdown 192.168.31.0/24 dev wlp4s0 proto kernel scope link src 192.168.31.228 metric 600 # 查看　iptables 规则 # NAT 表 ❯ sudo iptables -t nat -S -P PREROUTING ACCEPT -P INPUT ACCEPT -P OUTPUT ACCEPT -P POSTROUTING ACCEPT -N DOCKER # 所有目的地址在本机的，都先交给 DOCKER 链处理一波 -A PREROUTING -m addrtype --dst-type LOCAL -j DOCKER -A OUTPUT ! -d 127.0.0.0/8 -m addrtype --dst-type LOCAL -j DOCKER # （容器访问外部网络）所有出口不为 docker0 的流量，都做下 SNAT，把 src ip 换成出口接口的 ip 地址 -A POSTROUTING -s 172.17.0.0/16 ! -o docker0 -j MASQUERADE -A DOCKER -i docker0 -j RETURN # filter 表 ❯ sudo iptables -t filter -S -P INPUT ACCEPT -P FORWARD DROP -P OUTPUT ACCEPT -N DOCKER -N DOCKER-ISOLATION-STAGE-1 -N DOCKER-ISOLATION-STAGE-2 -N DOCKER-USER # 所有流量都必须先经过如下两个表处理，没问题才能继续往下走 -A FORWARD -j DOCKER-ISOLATION-STAGE-1 -A FORWARD -j DOCKER-USER # （容器访问外部网络）出去的流量走了 MASQUERADE，回来的流量会被 conntrack 识别并转发回来，这里允许返回的数据包通过。 # 这里直接 ACCEPT 被 conntrack 识别到的流量 -A FORWARD -o docker0 -m conntrack --ctstate RELATED,ESTABLISHED -j ACCEPT # 将所有访问 docker0 的流量都转给自定义链 DOCKER 处理 -A FORWARD -o docker0 -j DOCKER # 允许所有来自 docker0 的流量通过，不论下一跳是否是 docker0 -A FORWARD -i docker0 ! -o docker0 -j ACCEPT -A FORWARD -i docker0 -o docker0 -j ACCEPT # 下面三个表目前啥规则也没有，就是简单的 RETURN，交给后面的表继续处理 -A DOCKER-ISOLATION-STAGE-1 -j RETURN -A DOCKER-ISOLATION-STAGE-2 -j RETURN -A DOCKER-USER -j RETURN 接下来使用如下 docker-compose 配置启动一个 caddy　容器，添加自定义 network 和端口映射，待会就能验证 docker 是如何实现这两种网络的了。 docker-compose.yml 内容： version:\"3.3\"services:caddy:image:\"caddy:2.2.1-alpine\"container_name:\"caddy\"restart:alwayscommand:caddy file-server --browse --root /data/staticports:- \"8081:80\"volumes:- \"/home/ryan/Downloads:/data/static\"networks:- caddy-1networks:caddy-1: 现在先用上面的配置启动 caddy 容器，然后再查看网络状况： # 启动 caddy ❯ docker-compose up -d # 查下 caddy 容器的 ip \u003e docker inspect caddy | grep IPAddress ... \"IPAddress\": \"172.18.0.2\", # 查看网络接口，可以看到多了一个网桥，它就是上一行命令创建的 caddy-1 网络 # 还多了一个 veth，它连接到了 caddy 容器的 eth0(veth) 接口 ❯ ip addr ls ... 5: docker0: \u003cBROADCAST,MULTICAST,UP,LOWER_UP\u003e mtu 1500 qdisc noqueue state UP group default link/ether 02:42:42:c7:12:ba brd ff:ff:ff:ff:ff:ff inet 172.17.0.1/16 brd 172.17.255.255 scope global docker0 valid_lft forever preferred_lft forever inet6 fe80::42:42ff:fec7:12ba/64 scope link valid_lft forever preferred_lft forever 100: veth16b37ea@if99: \u003cBROADCAST,MULTICAST,UP,LOWER_UP\u003e mtu 1500 qdisc noqueue master docker0 state UP group default link/ether 42:af:34:ae:74:ae brd ff:ff:ff:ff:ff:ff link-netnsid 0 inet6 fe80::40af:34ff:feae:74ae/64 sco","date":"2021-08-15","objectID":"/posts/iptables-and-container-networks/:5:1","tags":["Linux","网络","虚拟化","容器","iptables","conntrack"],"title":"iptables 及 docker 容器网络分析","uri":"/posts/iptables-and-container-networks/"},{"categories":["技术"],"content":"Docker/Podman 的 macvlan/ipvlan 模式 注意：macvlan 和 wifi 好像不兼容，测试时不要使用无线网络的接口！ 我在前面介绍 Linux 虚拟网络接口的文章中，有介绍过 macvlan 和 ipvlan 两种新的虚拟接口。 目前 Podman/Docker 都支持使用 macvlan 来构建容器网络，这种模式下创建的容器直连外部网络，容器可以拥有独立的外部 IP，不需要端口映射，也不需要借助 iptables. 这和虚拟机的 Bridge 模式就很类似，主要适用于希望容器拥有独立外部 IP 的情况。 下面详细分析下 Docker 的 macvlan 网络（Podman 应该也完全类似）。 # 首先创建一个 macvlan 网络 # subnet/gateway 的参数需要和物理网络一致 # 通过 -o parent 设定父接口，我本机的以太网口名称为 eno1 $ docker network create -d macvlan \\ --subnet=192.168.31.0/24 \\ --gateway=192.168.31.1 \\ -o parent=eno1 \\ macnet0 # 现在使用 macvlan 启动一个容器试试 # 建议和我一样，通过 --ip 手动配置静态 ip 地址，当然不配也可以，DHCP 会自动分配 IP $ docker run --network macnet0 --ip=192.168.31.233 --rm -it buildpack-deps:buster-curl /bin/bash # 在容器中查看网络接口状况，能看到 eth0 是一个 macvlan 接口 root@4319488cb5e7:/# ip -d addr ls 1: lo: \u003cLOOPBACK,UP,LOWER_UP\u003e mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 promiscuity 0 minmtu 0 maxmtu 0 numtxqueues 1 numrxqueues 1 gso_max_size 65536 gso_max_segs 65535 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever 8: eth0@if2: \u003cBROADCAST,MULTICAST,UP,LOWER_UP\u003e mtu 1500 qdisc noqueue state UP group default link/ether 02:42:c0:a8:1f:e9 brd ff:ff:ff:ff:ff:ff link-netnsid 0 promiscuity 0 minmtu 68 maxmtu 9194 macvlan mode bridge numtxqueues 1 numrxqueues 1 gso_max_size 64000 gso_max_segs 64 inet 192.168.31.233/24 brd 192.168.31.255 scope global eth0 valid_lft forever preferred_lft forever # 路由表，默认 gateway 被自动配置进来了 root@4319488cb5e7:/# ip route ls default via 192.168.31.1 dev eth0 192.168.31.0/24 dev eth0 proto kernel scope link src 192.168.31.233 # 可以正常访问 baidu root@4319488cb5e7:/# curl baidu.com \u003chtml\u003e \u003cmeta http-equiv=\"refresh\" content=\"0;url=http://www.baidu.com/\"\u003e \u003c/html\u003e Docker 支持的另一种网络模式是 ipvlan（ipvlan 和 macvlan 的区别我在前一篇文章中已经介绍过，不再赘言），创建命令和 macvlan 几乎一样： # 首先创建一个 macvlan 网络 # subnet/gateway 的参数需要和物理网络一致 # 通过 -o parent 设定父接口，我本机的以太网口名称为 eno1 # ipvlan_mode 默认为 l2，表示工作在数据链路层。 $ docker network create -d ipvlan \\ --subnet=192.168.31.0/24 \\ --gateway=192.168.31.1 \\ -o parent=eno1 \\ -o ipvlan_mode=l2 \\ ipvnet0 # 现在使用 macvlan 启动一个容器试试 # 建议和我一样，通过 --ip 手动配置静态 ip 地址，当然不配也可以，DHCP 会自动分配 IP $ docker run --network ipvnet0 --ip=192.168.31.234 --rm -it buildpack-deps:buster-curl /bin/bash # 在容器中查看网络接口状况，能看到 eth0 是一个 ipvlan 接口 root@d0764ebbbf42:/# ip -d addr ls 1: lo: \u003cLOOPBACK,UP,LOWER_UP\u003e mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 promiscuity 0 minmtu 0 maxmtu 0 numtxqueues 1 numrxqueues 1 gso_max_size 65536 gso_max_segs 65535 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever 12: eth0@if2: \u003cBROADCAST,MULTICAST,UP,LOWER_UP\u003e mtu 1500 qdisc noqueue state UNKNOWN group default link/ether 38:f3:ab:a3:e6:71 brd ff:ff:ff:ff:ff:ff link-netnsid 0 promiscuity 0 minmtu 68 maxmtu 65535 ipvlan mode l2 bridge numtxqueues 1 numrxqueues 1 gso_max_size 64000 gso_max_segs 64 inet 192.168.31.234/24 brd 192.168.31.255 scope global eth0 valid_lft forever preferred_lft forever # 路由表，默认 gateway 被自动配置进来了 root@d0764ebbbf42:/# ip route ls default via 192.168.31.1 dev eth0 192.168.31.0/24 dev eth0 proto kernel scope link src 192.168.31.234 # 可以正常访问 baidu root@d0764ebbbf42:/# curl baidu.com \u003chtml\u003e \u003cmeta http-equiv=\"refresh\" content=\"0;url=http://www.baidu.com/\"\u003e \u003c/html\u003e ","date":"2021-08-15","objectID":"/posts/iptables-and-container-networks/:6:0","tags":["Linux","网络","虚拟化","容器","iptables","conntrack"],"title":"iptables 及 docker 容器网络分析","uri":"/posts/iptables-and-container-networks/"},{"categories":["技术"],"content":"Rootless 容器的网络实现 如果容器运行时也在 Rootless 模式下运行，那它就没有权限在宿主机添加 bridge/veth 等虚拟网络接口，这种情况下，我们前面描述的容器网络就无法设置了。 那么 podman/containerd(nerdctl) 目前是如何在 Rootless 模式下构建容器网络的呢？ 查看文档，发现它们都用到了 rootlesskit 相关的东西，而 rootlesskit 提供了 rootless 网络的几个实现，文档参见 rootlesskit/docs/network.md 其中目前推荐使用，而且 podman/containerd(nerdctl) 都默认使用的方案，是 rootless-containers/slirp4netns 以 containerd(nerdctl) 为例，按官方文档安装好后，随便启动几个容器，然后在宿主机查 iptables/ip addr ls，会发现啥也没有。 这显然是因为 rootless 模式下 containerd 改不了宿主机的 iptables 配置和虚拟网络接口。但是可以查看到宿主机 slirp4netns 在后台运行： ❯ ps aux | grep tap ryan 11644 0.0 0.0 5288 3312 ? S 00:01 0:02 slirp4netns --mtu 65520 -r 3 --disable-host-loopback --enable-sandbox --enable-seccomp 11625 tap0 但是我看半天文档，只看到怎么使用 rootlesskit/slirp4netns 创建新的名字空间，没看到有介绍如何进入一个已存在的 slirp4netns 名字空间… 使用 nsenter -a -t 11644 也一直报错，任何程序都是 no such binary… 以后有空再重新研究一波… 总之能确定的是，它通过在虚拟的名字空间中创建了一个 tap 虚拟接口来实现容器网络，性能相比前面介绍的网络多少是要差一点的。 ","date":"2021-08-15","objectID":"/posts/iptables-and-container-networks/:7:0","tags":["Linux","网络","虚拟化","容器","iptables","conntrack"],"title":"iptables 及 docker 容器网络分析","uri":"/posts/iptables-and-container-networks/"},{"categories":["技术"],"content":"nftables 前面介绍了 iptables 以及其在 docker 和防火墙上的应用。但是实际上目前各大 Linux 发行版都已经不建议使用 iptables 了，甚至把 iptables 重命名为了 iptables-leagacy. 目前 opensuse/debian/opensuse 都已经预装了并且推荐使用 nftables，而且 firewalld 已经默认使用 nftables 作为它的后端了。 我在 opensuse tumbleweed 上实测，firewalld 添加的是 nftables 配置，而 docker 仍然在用旧的 iptables，也就是说我现在的机器上有两套 netfilter 工具并存： # 查看 iptables 数据 \u003e iptables -S -P INPUT ACCEPT -P FORWARD DROP -P OUTPUT ACCEPT -N DOCKER -N DOCKER-ISOLATION-STAGE-1 -N DOCKER-ISOLATION-STAGE-2 -N DOCKER-USER -A FORWARD -j DOCKER-ISOLATION-STAGE-1 -A FORWARD -o br-e3fbbb7a1b3a -m conntrack --ctstate RELATED,ESTABLISHED -j ACCEPT -A FORWARD -o br-e3fbbb7a1b3a -j DOCKER ... # 确认下是否使用了 nftables 的兼容层，结果提示请我使用 iptables-legacy \u003e iptables-nft -S # Warning: iptables-legacy tables present, use iptables-legacy to see them -P INPUT ACCEPT -P FORWARD ACCEPT -P OUTPUT ACCEPT # 查看 nftables 规则，能看到三张 firewalld 生成的 table \u003e nft list ruleset table inet firewalld { ... } table ip firewalld { ... } table ip6 firewalld { ... } 但是现在 kubernetes/docker 都还是用的 iptables，nftables 我学了用处不大，以后有空再补充。 ","date":"2021-08-15","objectID":"/posts/iptables-and-container-networks/:8:0","tags":["Linux","网络","虚拟化","容器","iptables","conntrack"],"title":"iptables 及 docker 容器网络分析","uri":"/posts/iptables-and-container-networks/"},{"categories":["技术"],"content":"参考 iptables详解（1）：iptables概念 网络地址转换（NAT）之报文跟踪 容器安全拾遗 - Rootless Container初探 netfilter - wikipedia ","date":"2021-08-15","objectID":"/posts/iptables-and-container-networks/:9:0","tags":["Linux","网络","虚拟化","容器","iptables","conntrack"],"title":"iptables 及 docker 容器网络分析","uri":"/posts/iptables-and-container-networks/"},{"categories":["技术"],"content":" 本文用到的字符画工具：vscode-asciiflow2 注意: 本文中使用 ip 命令创建或修改的任何网络配置，都是未持久化的，主机重启即消失。 Linux 具有强大的虚拟网络能力，这也是 openstack 网络、docker 容器网络以及 kubernetes 网络等虚拟网络的基础。 这里介绍 Linux 常用的虚拟网络接口类型：TUN/TAP、bridge、veth、ipvlan/macvlan、vlan 以及 vxlan/geneve. ","date":"2021-08-14","objectID":"/posts/linux-virtual-network-interfaces/:0:0","tags":["Linux","网络","虚拟化","容器"],"title":"Linux 中的虚拟网络接口","uri":"/posts/linux-virtual-network-interfaces/"},{"categories":["技术"],"content":"一、tun/tap 虚拟网络接口 tun/tap 是操作系统内核中的虚拟网络设备，他们为用户层程序提供数据的接收与传输。 普通的物理网络接口如 eth0，它的两端分别是内核协议栈和外面的物理网络。 而对于 TUN/TAP 虚拟接口如 tun0，它的一端一定是连接的用户层程序，另一端则视配置方式的不同而变化，可以直连内核协议栈，也可以是某个 bridge（后面会介绍）。 Linux 通过内核模块 TUN 提供 tun/tap 功能，该模块提供了一个设备接口 /dev/net/tun 供用户层程序读写，用户层程序通过 /dev/net/tun 读写主机内核协议栈的数据。 \u003e modinfo tun filename: /lib/modules/5.13.6-1-default/kernel/drivers/net/tun.ko.xz alias: devname:net/tun alias: char-major-10-200 license: GPL author: (C) 1999-2004 Max Krasnyansky \u003cmaxk@qualcomm.com\u003e description: Universal TUN/TAP device driver ... \u003e ls /dev/net/tun /dev/net/tun 一个 TUN 设备的示例图如下： +----------------------------------------------------------------------+ | | | +--------------------+ +--------------------+ | | | User Application A | | User Application B +\u003c-----+ | | +------------+-------+ +-------+------------+ | | | | 1 | 5 | | |...............+......................+...................|...........| | ↓ ↓ | | | +----------+ +----------+ | | | | socket A | | socket B | | | | +-------+--+ +--+-------+ | | | | 2 | 6 | | |.................+.................+......................|...........| | ↓ ↓ | | | +------------------------+ +--------+-------+ | | | Network Protocol Stack | | /dev/net/tun | | | +--+-------------------+-+ +--------+-------+ | | | 7 | 3 ^ | |................+...................+.....................|...........| | ↓ ↓ | | | +----------------+ +----------------+ 4 | | | | eth0 | | tun0 | | | | +-------+--------+ +-----+----------+ | | | 10.32.0.11 | | 192.168.3.11 | | | | 8 +---------------------+ | | | | +----------------+-----------------------------------------------------+ ↓ Physical Network 因为 TUN/TAP 设备的一端是内核协议栈，显然流入 tun0 的数据包是先经过本地的路由规则匹配的。 路由匹配成功，数据包被发送到 tun0 后，tun0 发现另一端是通过 /dev/net/tun 连接到应用程序 B，就会将数据丢给应用程序 B。 应用程序对数据包进行处理后，可能会构造新的数据包，通过物理网卡发送出去。比如常见的 VPN 程序就是把原来的数据包封装/加密一遍，再发送给 VPN 服务器。 ","date":"2021-08-14","objectID":"/posts/linux-virtual-network-interfaces/:1:0","tags":["Linux","网络","虚拟化","容器"],"title":"Linux 中的虚拟网络接口","uri":"/posts/linux-virtual-network-interfaces/"},{"categories":["技术"],"content":"C 语言编程测试 TUN 设备 为了使用 tun/tap 设备，用户层程序需要通过系统调用打开 /dev/net/tun 获得一个读写该设备的文件描述符(FD)，并且调用 ioctl() 向内核注册一个 TUN 或 TAP 类型的虚拟网卡(实例化一个 tun/tap 设备)，其名称可能是 tun0/tap0 等。 此后，用户程序可以通过该 TUN/TAP 虚拟网卡与主机内核协议栈（或者其他网络设备）交互。当用户层程序关闭后，其注册的 TUN/TAP 虚拟网卡以及自动生成的路由表相关条目都会被内核释放。 可以把用户层程序看做是网络上另一台主机，他们通过 tun/tap 虚拟网卡相连。 一个简单的 C 程序示例如下，它每次收到数据后，都只单纯地打印一下收到的字节数： #include \u003clinux/if.h\u003e#include \u003clinux/if_tun.h\u003e #include \u003csys/ioctl.h\u003e #include \u003cfcntl.h\u003e#include \u003cstring.h\u003e #include \u003cunistd.h\u003e#include\u003cstdlib.h\u003e#include\u003cstdio.h\u003e int tun_alloc(int flags) { struct ifreq ifr; int fd, err; char *clonedev = \"/dev/net/tun\"; // 打开 tun 文件，获得 fd if ((fd = open(clonedev, O_RDWR)) \u003c 0) { return fd; } memset(\u0026ifr, 0, sizeof(ifr)); ifr.ifr_flags = flags; // 向内核注册一个 TUN 网卡，并与前面拿到的 fd 关联起来 // 程序关闭时，注册的 tun 网卡及自动生成的相关路由策略，会被自动释放 if ((err = ioctl(fd, TUNSETIFF, (void *) \u0026ifr)) \u003c 0) { close(fd); return err; } printf(\"Open tun/tap device: %s for reading...\\n\", ifr.ifr_name); return fd; } int main() { int tun_fd, nread; char buffer[1500]; /* Flags: IFF_TUN - TUN device (no Ethernet headers) * IFF_TAP - TAP device * IFF_NO_PI - Do not provide packet information */ tun_fd = tun_alloc(IFF_TUN | IFF_NO_PI); if (tun_fd \u003c 0) { perror(\"Allocating interface\"); exit(1); } while (1) { nread = read(tun_fd, buffer, sizeof(buffer)); if (nread \u003c 0) { perror(\"Reading from interface\"); close(tun_fd); exit(1); } printf(\"Read %d bytes from tun/tap device\\n\", nread); } return 0; } 接下来开启三个终端窗口来测试上述程序，分别运行上面的 tun 程序、tcpdump 和 iproute2 指令。 首先通过编译运行上述 c 程序，程序会阻塞住，等待数据到达： # 编译，请忽略部分 warning \u003e gcc mytun.c -o mytun # 创建并监听 tun 设备需要 root 权限 \u003e sudo mytun Open tun/tap device: tun0 for reading... 现在使用 iproute2 查看下链路层设备： # 能发现最后面有列出名为 tun0 的接口，但是状态为 down ❯ ip addr ls ...... 3: wlp4s0: \u003cBROADCAST,MULTICAST,UP,LOWER_UP\u003e mtu 1500 qdisc noqueue state UP group default qlen 1000 link/ether c0:3c:59:36:a4:16 brd ff:ff:ff:ff:ff:ff inet 192.168.31.228/24 brd 192.168.31.255 scope global dynamic noprefixroute wlp4s0 valid_lft 41010sec preferred_lft 41010sec inet6 fe80::4ab0:130f:423b:5d37/64 scope link noprefixroute valid_lft forever preferred_lft forever 7: tun0: \u003cPOINTOPOINT,MULTICAST,NOARP\u003e mtu 1500 qdisc noop state DOWN group default qlen 500 link/none # 为 tun0 设置 ip 地址，注意不要和其他接口在同一网段，会导致路由冲突 \u003e sudo ip addr add 172.21.22.23/24 dev tun0 # 启动 tun0 这个接口，这一步会自动向路由表中添加将 172.21.22.23/24 路由到 tun0 的策略 \u003e sudo ip link set tun0 up #确认上一步添加的路由策略是否存在 ❯ ip route ls default via 192.168.31.1 dev wlp4s0 proto dhcp metric 600 172.17.0.0/16 dev docker0 proto kernel scope link src 172.17.0.1 linkdown 172.21.22.0/24 dev tun0 proto kernel scope link src 172.21.22.23 192.168.31.0/24 dev wlp4s0 proto kernel scope link src 192.168.31.228 metric 600 # 此时再查看接口，发现 tun0 状态为 unknown \u003e ip addr ls ...... 8: tun0: \u003cPOINTOPOINT,MULTICAST,NOARP,UP,LOWER_UP\u003e mtu 1500 qdisc pfifo_fast state UNKNOWN group default qlen 500 link/none inet 172.21.22.23/24 scope global tun0 valid_lft forever preferred_lft forever inet6 fe80::3d52:49b5:1cf3:38fd/64 scope link stable-privacy valid_lft forever preferred_lft forever # 使用 tcpdump 尝试抓下 tun0 的数据，会阻塞在这里，等待数据到达 \u003e tcpdump -i tun0 现在再启动第三个窗口发点数据给 tun0，持续观察前面 tcpdump 和 mytun 的日志: # 直接 ping tun0 的地址，貌似有问题，数据没进 mytun 程序，而且还有响应 ❯ ping -c 4 172.21.22.23 PING 172.21.22.23 (172.21.22.23) 56(84) bytes of data. 64 bytes from 172.21.22.23: icmp_seq=1 ttl=64 time=0.167 ms 64 bytes from 172.21.22.23: icmp_seq=2 ttl=64 time=0.180 ms 64 bytes from 172.21.22.23: icmp_seq=3 ttl=64 time=0.126 ms 64 bytes from 172.21.22.23: icmp_seq=4 ttl=64 time=0.141 ms --- 172.21.22.23 ping statistics --- 4 packets transmitted, 4 received, 0% packet loss, time 3060ms rtt min/avg/max/mdev = 0.126/0.153/0.180/0.021 ms # 但是 ping 该网段下的其他地址，流量就会被转发给 mytun 程序，因为 mytun 啥数据也没回，自然丢包率 100% # tcpdump 和 mytun 都会打印出相关日志 ❯ ping -c 4 172.21.22.26 PING 172.21.22.26 (172.21.22.26) 56(84) bytes of data. --- 172.21.22.26 ping statistics --- 4 packets transmitted, 0 received, 100% packet loss, time 3055ms 下面给出 mytun 的输出： Read 84 bytes from tun/tap device Read 84 bytes fro","date":"2021-08-14","objectID":"/posts/linux-virtual-network-interfaces/:1:1","tags":["Linux","网络","虚拟化","容器"],"title":"Linux 中的虚拟网络接口","uri":"/posts/linux-virtual-network-interfaces/"},{"categories":["技术"],"content":"TUN 与 TAP 的区别 TUN 和 TAP 的区别在于工作的网络层次不同，用户程序通过 TUN 设备只能读写网络层的 IP 数据包，而 TAP 设备则支持读写链路层的数据包（通常是以太网数据包，带有 Ethernet headers）。 TUN 与 TAP 的关系，就类似于 socket 和 raw socket. TUN/TAP 应用最多的场景是 VPN 代理，比如: clash: 一个支持各种规则的隧道，也支持 TUN 模式 tun2socks: 一个全局透明代理，和 VPN 的工作模式一样，它通过创建虚拟网卡+修改路由表，在第三层网络层代理系统流量。 ","date":"2021-08-14","objectID":"/posts/linux-virtual-network-interfaces/:1:2","tags":["Linux","网络","虚拟化","容器"],"title":"Linux 中的虚拟网络接口","uri":"/posts/linux-virtual-network-interfaces/"},{"categories":["技术"],"content":"二、veth veth 接口总是成对出现，一对 veth 接口就类似一根网线，从一端进来的数据会从另一端出去。 同时 veth 又是一个虚拟网络接口，因此它和 TUN/TAP 或者其他物理网络接口一样，也都能配置 mac/ip 地址（但是并不是一定得配 mac/ip 地址）。 其主要作用就是连接不同的网络，比如在容器网络中，用于将容器的 namespace 与 root namespace 的网桥 br0 相连。 容器网络中，容器侧的 veth 自身设置了 ip/mac 地址并被重命名为 eth0，作为容器的网络接口使用，而主机侧的 veth 则直接连接在 docker0/br0 上面。 使用 veth 实现容器网络，需要结合下一小节介绍的 bridge，在下一小节将给出容器网络结构图。 ","date":"2021-08-14","objectID":"/posts/linux-virtual-network-interfaces/:2:0","tags":["Linux","网络","虚拟化","容器"],"title":"Linux 中的虚拟网络接口","uri":"/posts/linux-virtual-network-interfaces/"},{"categories":["技术"],"content":"三、bridge Linux Bridge 是工作在链路层的网络交换机，由 Linux 内核模块 brige 提供，它负责在所有连接到它的接口之间转发链路层数据包。 添加到 Bridge 上的设备被设置为只接受二层数据帧并且转发所有收到的数据包到 Bridge 中。 在 Bridge 中会进行一个类似物理交换机的查MAC端口映射表、转发、更新MAC端口映射表这样的处理逻辑，从而数据包可以被转发到另一个接口/丢弃/广播/发往上层协议栈，由此 Bridge 实现了数据转发的功能。 如果使用 tcpdump 在 Bridge 接口上抓包，可以抓到网桥上所有接口进出的包，因为这些数据包都要通过网桥进行转发。 与物理交换机不同的是，Bridge 本身可以设置 IP 地址，可以认为当使用 brctl addbr br0 新建一个 br0 网桥时，系统自动创建了一个同名的隐藏 br0 网络接口。br0 一旦设置 IP 地址，就意味着这个隐藏的 br0 接口可以作为路由接口设备，参与 IP 层的路由选择(可以使用 route -n 查看最后一列 Iface)。因此只有当 br0 设置 IP 地址时，Bridge 才有可能将数据包发往上层协议栈。 但被添加到 Bridge 上的网卡是不能配置 IP 地址的，他们工作在数据链路层，对路由系统不可见。 它常被用于在虚拟机、主机上不同的 namepsaces 之间转发数据。 ","date":"2021-08-14","objectID":"/posts/linux-virtual-network-interfaces/:3:0","tags":["Linux","网络","虚拟化","容器"],"title":"Linux 中的虚拟网络接口","uri":"/posts/linux-virtual-network-interfaces/"},{"categories":["技术"],"content":"虚拟机场景（桥接模式） 以 qemu-kvm 为例，在虚拟机的桥接模式下，qemu-kvm 会为每个虚拟机创建一个 tun/tap 虚拟网卡并连接到 br0 网桥。 虚拟机内部的网络接口 eth0 是 qemu-kvm 软件模拟的，实际上虚拟机内网络数据的收发都会被 qemu-kvm 转换成对 /dev/net/tun 的读写。 以发送数据为例，整个流程如下： 虚拟机发出去的数据包先到达 qemu-kvm 程序 数据被用户层程序 qemu-kvm 写入到 /dev/net/tun，到达 tap 设备 tap 设备把数据传送到 br0 网桥 br0 把数据交给 eth0 发送出去 整个流程跑完，数据包都不需要经过宿主机的协议栈，效率高。 +------------------------------------------------+-----------------------------------+-----------------------------------+ | Host | VirtualMachine1 | VirtualMachine2 | | | | | | +--------------------------------------+ | +-------------------------+ | +-------------------------+ | | | Network Protocol Stack | | | Network Protocol Stack | | | Network Protocol Stack | | | +--------------------------------------+ | +-------------------------+ | +-------------------------+ | | ↑ | ↑ | ↑ | |.......................|........................|................|..................|.................|.................| | ↓ | ↓ | ↓ | | +--------+ | +-------+ | +-------+ | | | .3.101 | | | .3.102| | | .3.103| | | +------+ +--------+ +-------+ | +-------+ | +-------+ | | | eth0 |\u003c---\u003e| br0 |\u003c---\u003e|tun/tap| | | eth0 | | | eth0 | | | +------+ +--------+ +-------+ | +-------+ | +-------+ | | ↑ ↑ ↑ +--------+ ↑ | ↑ | | | | +------|qemu-kvm|-----------+ | | | | | ↓ +--------+ | | | | | +-------+ | | | | | | |tun/tap| | | | | | | +-------+ | | | | | | ↑ | +--------+ | | | | | +-------------------------------------|qemu-kvm|-------------|-----------------+ | | | | +--------+ | | | | | | | +---------|--------------------------------------+-----------------------------------+-----------------------------------+ ↓ Physical Network (192.168.3.0/24) ","date":"2021-08-14","objectID":"/posts/linux-virtual-network-interfaces/:3:1","tags":["Linux","网络","虚拟化","容器"],"title":"Linux 中的虚拟网络接口","uri":"/posts/linux-virtual-network-interfaces/"},{"categories":["技术"],"content":"跨 namespace 通信场景（容器网络，NAT 模式） docker/podman 提供的 bridge 网络模式，就是使用 veth+bridge+iptalbes 实现的。我会在下一篇文章详细介绍「容器网络」。 由于容器运行在自己单独的 network namespace 里面，所以和虚拟机一样，它们也都有自己单独的协议栈。 容器网络的结构和虚拟机差不多，但是它改用了 NAT 网络，并把 tun/tap 换成了 veth，导致 docker0 过来的数据，要先经过宿主机协议栈，然后才进入 veth 接口。 多了一层 NAT，以及多走了一层宿主机协议栈，都会导致性能下降。 示意图如下： +-----------------------------------------------+-----------------------------------+-----------------------------------+ | Host | Container 1 | Container 2 | | | | | | +---------------------------------------+ | +-------------------------+ | +-------------------------+ | | | Network Protocol Stack | | | Network Protocol Stack | | | Network Protocol Stack | | | +----+-------------+--------------------+ | +-----------+-------------+ | +------------+------------+ | | ^ ^ | ^ | ^ | |........|.............|........................|................|..................|.................|.................| | v v ↓ | v | v | | +----+----+ +-----+------+ | +-----+-------+ | +-----+-------+ | | | .31.101 | | 172.17.0.1 | +------+ | | 172.17.0.2 | | | 172.17.0.3 | | | +---------+ +-------------\u003c----\u003e+ veth | | +-------------+ | +-------------+ | | | eth0 | | docker0 | +--+---+ | | eth0(veth) | | | eth0(veth) | | | +----+----+ +-----+------+ ^ | +-----+-------+ | +-----+-------+ | | ^ ^ | | ^ | ^ | | | | +------------------------+ | | | | | v | | | | | | +--+---+ | | | | | | | veth | | | | | | | +--+---+ | | | | | | ^ | | | | | | +------------------------------------------------------------------------------+ | | | | | | | | | | | +-----------------------------------------------+-----------------------------------+-----------------------------------+ v Physical Network (192.168.31.0/24) 每创建一个新容器，都会在容器的 namespace 里新建一个 veth 接口并命令为 eth0，同时在主 namespace 创建一个 veth，将容器的 eth0 与 docker0 连接。 可以在容器中通过 iproute2 查看到， eth0 的接口类型为 veth： ❯ docker run -it --rm debian:buster bash root@5facbe4ddc1e:/# ip --details addr ls 1: lo: \u003cLOOPBACK,UP,LOWER_UP\u003e mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 promiscuity 0 minmtu 0 maxmtu 0 numtxqueues 1 numrxqueues 1 gso_max_size 65536 gso_max_segs 65535 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever 20: eth0@if21: \u003cBROADCAST,MULTICAST,UP,LOWER_UP\u003e mtu 1500 qdisc noqueue state UP group default link/ether 02:42:ac:11:00:02 brd ff:ff:ff:ff:ff:ff link-netnsid 0 promiscuity 0 minmtu 68 maxmtu 65535 veth numtxqueues 1 numrxqueues 1 gso_max_size 65536 gso_max_segs 65535 inet 172.17.0.2/16 brd 172.17.255.255 scope global eth0 valid_lft forever preferred_lft forever 同时在宿主机中能看到对应的 veth 设备是绑定到了 docker0 网桥的： ❯ sudo brctl show bridge name bridge id STP enabled interfaces docker0 8000.0242fce99ef5 no vethea4171a ","date":"2021-08-14","objectID":"/posts/linux-virtual-network-interfaces/:3:2","tags":["Linux","网络","虚拟化","容器"],"title":"Linux 中的虚拟网络接口","uri":"/posts/linux-virtual-network-interfaces/"},{"categories":["技术"],"content":"四、macvlan 目前 docker/podman 都支持创建基于 macvlan 的 Linux 容器网络。 注意 macvlan 和 WiFi 存在兼容问题，如果使用笔记本测试，可能会遇到麻烦。 参考文档：linux 网络虚拟化： macvlan macvlan 是比较新的 Linux 特性，需要内核版本 \u003e= 3.9，它被用于在主机的网络接口（父接口）上配置多个虚拟子接口，这些子接口都拥有各自独立的 mac 地址，也可以配上 ip 地址进行通讯。 macvlan 下的虚拟机或者容器网络和主机在同一个网段中，共享同一个广播域。macvlan 和 bridge 比较相似，但因为它省去了 bridge 的存在，所以配置和调试起来比较简单，而且效率也相对高。除此之外，macvlan 自身也完美支持 VLAN。 如果希望容器或者虚拟机放在主机相同的网络中，享受已经存在网络栈的各种优势，可以考虑 macvlan。 我会在下一篇文章对 docker 的 macvlan/ipvlan 做个分析，这里先略过了… ","date":"2021-08-14","objectID":"/posts/linux-virtual-network-interfaces/:4:0","tags":["Linux","网络","虚拟化","容器"],"title":"Linux 中的虚拟网络接口","uri":"/posts/linux-virtual-network-interfaces/"},{"categories":["技术"],"content":"五、ipvlan linux 网络虚拟化： ipvlan cilium 1.9 已经提供了基于 ipvlan 的网络（beta 特性），用于替换传统的 veth+bridge 容器网络。详见 IPVLAN based Networking (beta) - Cilium 1.9 Docs ipvlan 和 macvlan 的功能很类似，也是用于在主机的网络接口（父接口）上配置出多个虚拟的子接口。但不同的是，ipvlan 的各子接口没有独立的 mac 地址，它们和主机的父接口共享 mac 地址。 因为 mac 地址共享，所以如果使用 DHCP，就要注意不能使用 mac 地址做 DHCP，需要额外配置唯一的 clientID. 如果你遇到以下的情况，请考虑使用 ipvlan： 父接口对 mac 地址数目有限制，或者在 mac 地址过多的情况下会造成严重的性能损失 工作在 802.11(wireless)无线网络中（macvlan 无法和无线网络共同工作） 希望搭建比较复杂的网络拓扑（不是简单的二层网络和 VLAN），比如要和 BGP 网络一起工作 基于 ipvlan/macvlan 的容器网络，比 veth+bridge+iptables 的性能要更高。 我会在下一篇文章对 docker 的 macvlan/ipvlan 做个分析，这里先略过了… ","date":"2021-08-14","objectID":"/posts/linux-virtual-network-interfaces/:5:0","tags":["Linux","网络","虚拟化","容器"],"title":"Linux 中的虚拟网络接口","uri":"/posts/linux-virtual-network-interfaces/"},{"categories":["技术"],"content":"六、vlan vlan 即虚拟局域网，是一个链路层的广播域隔离技术，可以用于切分局域网，解决广播泛滥和安全性问题。被隔离的广播域之间需要上升到第三层才能完成通讯。 常用的企业路由器如 ER-X 基本都可以设置 vlan，Linux 也直接支持了 vlan. 以太网数据包有一个专门的字段提供给 vlan 使用，vlan 数据包会在该位置记录它的 VLAN ID，交换机通过该 ID 来区分不同的 VLAN，只将该以太网报文广播到该 ID 对应的 VLAN 中。 ","date":"2021-08-14","objectID":"/posts/linux-virtual-network-interfaces/:6:0","tags":["Linux","网络","虚拟化","容器"],"title":"Linux 中的虚拟网络接口","uri":"/posts/linux-virtual-network-interfaces/"},{"categories":["技术"],"content":"七、vxlan/geneve rfc8926 - Geneve: Generic Network Virtualization Encapsulation rfc7348 - Virtual eXtensible Local Area Network (VXLAN) linux 上实现 vxlan 网络 在介绍 vxlan 前，先说明下两个名词的含义： underlay 网络：即物理网络 overlay 网络：指在现有的物理网络之上构建的虚拟网络。其实就是一种隧道技术，将原生态的二层数据帧报文进行封装后通过隧道进行传输。 vxlan 与 geneve 都是 overlay 网络协议，它俩都是使用 UDP 包来封装链路层的以太网帧。 vxlan 在 2014 年标准化，而 geneve 在 2020 年底才通过草案阶段，目前尚未形成最终标准。但是目前 linux/cilium 都已经支持了 geneve. geneve 相对 vxlan 最大的变化，是它更灵活——它的 header 长度是可变的。 目前所有 overlay 的跨主机容器网络方案，几乎都是基于 vxlan 实现的（例外：cilium 也支持 geneve）。 我们在学习单机的容器网络时，不需要接触到 vxlan，但是在学习跨主机容器网络方案如 flannel/calico/cilium 时，那 vxlan(overlay) 及 BGP(underlay) 就不可避免地要接触了。 先介绍下 vxlan 的数据包结构： VXLAN 栈帧结构\" VXLAN 栈帧结构 在创建 vxlan 的 vtep 虚拟设备时，我们需要手动设置图中的如下属性： VXLAN 目标端口：即接收方 vtep 使用的端口，这里 IANA 定义的端口是 4789，但是只有 calico 的 vxlan 模式默认使用该端口 calico，而 cilium/flannel 的默认端口都是 Linux 默认的 8472. VNID: 每个 VXLAN 网络接口都会被分配一个独立的 VNID 一个点对点的 vxlan 网络架构图如下: VXLAN 点对点网络架构\" VXLAN 点对点网络架构 可以看到每台虚拟机 VM 都会被分配一个唯一的 VNID，然后两台物理机之间通过 VTEP 虚拟网络设备建立了 VXLAN 隧道，所有 VXLAN 网络中的虚拟机，都通过 VTEP 来互相通信。 有了上面这些知识，我们就可以通过如下命令在两台 Linux 机器间建立一个点对点的 VXLAN 隧道： # 在主机 A 上创建 VTEP 设备 vxlan0 # 与另一个 vtep 接口 B（192.168.8.101）建立隧道 # 将 vxlan0 自身的 IP 地址设为 192.168.8.100 # 使用的 VXLAN 目标端口为 4789(IANA 标准) ip link add vxlan0 type vxlan \\ id 42 \\ dstport 4789 \\ remote 192.168.8.101 \\ local 192.168.8.100 \\ dev enp0s8 # 为我们的 VXLAN 网络设置虚拟网段，vxlan0 就是默认网关 ip addr add 10.20.1.2/24 dev vxlan0 # 启用我们的 vxlan0 设备，这会自动生成路由规则 ip link set vxlan0 up # 现在在主机 B 上运行如下命令，同样创建一个 VTEP 设备 vxlan0，remote 和 local 的 ip 与前面用的命令刚好相反。 # 注意 VNID 和 dstport 必须和前面完全一致 ip link add vxlan0 type vxlan \\ id 42 \\ dstport 4789 \\ remote 192.168.8.100 \\ local 192.168.8.101 \\ dev enp0s8 # 为我们的 VXLAN 网络设置虚拟网段，vxlan0 就是默认网关 ip addr add 10.20.1.3/24 dev vxlan0 ip link set vxlan0 up # 到这里，两台机器就完成连接，可以通信了。可以在主机 B 上 ping 10.20.1.2 试试，应该能收到主机 A 的回应。 ping 10.20.1.2 点对点的 vxlan 隧道实际用处不大，如果集群中的每个节点都互相建 vxlan 隧道，代价太高了。 一种更好的方式，是使用 「组播模式」的 vxlan 隧道，这种模式下一个 vtep 可以一次与组内的所有 vtep 建立隧道。 示例命令如下（这里略过了如何设置组播地址 239.1.1.1 的信息）： ip link add vxlan0 type vxlan \\ id 42 \\ dstport 4789 \\ group 239.1.1.1 \\ dev enp0s8 ip addr add 10.20.1.2/24 dev vxlan0 ip link set vxlan0 up 可以看到，只需要简单地把 local_ip/remote_ip 替换成一个组播地址就行。组播功能会将收到的数据包发送给组里的所有 vtep 接口，但是只有 VNID 能对上的 vtep 会处理该报文，其他 vtep 会直接丢弃数据。 接下来，为了能让所有的虚拟机/容器，都通过 vtep 通信，我们再添加一个 bridge 网络，充当 vtep 与容器间的交换机。架构如下： VXLAN 多播网络架构\" VXLAN 多播网络架构 使用 ip 命令创建网桥、网络名字空间、veth pairs 组成上图中的容器网络： # 创建 br0 并将 vxlan0 绑定上去 ip link add br0 type bridge ip link set vxlan0 master bridge ip link set vxlan0 up ip link set br0 up # 模拟将容器加入到网桥中的操作 ip netns add container1 ## 创建 veth pair，并把一端加到网桥上 ip link add veth0 type veth peer name veth1 ip link set dev veth0 master br0 ip link set dev veth0 up ## 配置容器内部的网络和 IP ip link set dev veth1 netns container1 ip netns exec container1 ip link set lo up ip netns exec container1 ip link set veth1 name eth0 ip netns exec container1 ip addr add 10.20.1.11/24 dev eth0 ip netns exec container1 ip link set eth0 up 然后在另一台机器上做同样的操作，并创建新容器，两个容器就能通过 vxlan 通信啦~ ","date":"2021-08-14","objectID":"/posts/linux-virtual-network-interfaces/:7:0","tags":["Linux","网络","虚拟化","容器"],"title":"Linux 中的虚拟网络接口","uri":"/posts/linux-virtual-network-interfaces/"},{"categories":["技术"],"content":"比组播更高效的 vxlan 实现 组播最大的问题在于，因为它不知道数据的目的地，所以每个 vtep 都发了一份。如果每次发数据时，如果能够精确到对应的 vtep，就能节约大量资源。 另一个问题是 ARP 查询也会被组播，要知道 vxlan 本身就是个 overlay 网络，ARP 的成本也很高。 上述问题都可以通过一个中心化的注册中心（如 etcd）来解决，所有容器、网络的注册与变更，都写入到这个注册中心，然后由程序自动维护 vtep 之间的隧道、fdb 表及 ARP 表. ","date":"2021-08-14","objectID":"/posts/linux-virtual-network-interfaces/:7:1","tags":["Linux","网络","虚拟化","容器"],"title":"Linux 中的虚拟网络接口","uri":"/posts/linux-virtual-network-interfaces/"},{"categories":["技术"],"content":"八、虚拟网络接口的速率 Loopback 和本章讲到的其他虚拟网络接口一样，都是一种软件模拟的网络设备。 他们的速率是不是也像物理链路一样，存在链路层（比如以太网）的带宽限制呢？ 比如目前很多老旧的网络设备，都是只支持到百兆以太网，这就决定了它的带宽上限。 即使是较新的设备，目前基本也都只支持到千兆，也就是 1GbE 以太网标准，那本文提到的虚拟网络接口单纯在本机内部通信，是否也存在这样的制约呢？是否也只能跑到 1GbE? 使用 ethtool 检查： # docker 容器的 veth 接口速率 \u003e ethtool vethe899841 | grep Speed Speed: 10000Mb/s # 网桥看起来没有固定的速率 \u003e ethtool docker0 | grep Speed Speed: Unknown! # tun0 设备的默认速率貌似是 10Mb/s ? \u003e ethtool tun0 | grep Speed Speed: 10Mb/s # 此外 ethtool 无法检查 lo 以及 wifi 的速率 ","date":"2021-08-14","objectID":"/posts/linux-virtual-network-interfaces/:8:0","tags":["Linux","网络","虚拟化","容器"],"title":"Linux 中的虚拟网络接口","uri":"/posts/linux-virtual-network-interfaces/"},{"categories":["技术"],"content":"网络性能实测 接下来实际测试一下，先给出机器参数： ❯ cat /etc/os-release NAME=\"openSUSE Tumbleweed\" # VERSION=\"20210810\" ... ❯ uname -a Linux legion-book 5.13.8-1-default #1 SMP Thu Aug 5 08:56:22 UTC 2021 (967c6a8) x86_64 x86_64 x86_64 GNU/Linux ❯ lscpu Architecture: x86_64 CPU(s): 16 Model name: AMD Ryzen 7 5800H with Radeon Graphics ... # 内存，单位 MB ❯ free -m total used free shared buff/cache available Mem: 27929 4482 17324 249 6122 22797 Swap: 2048 0 2048 使用 iperf3 测试： # 启动服务端 iperf3 -s ------------- # 新窗口启动客户端，通过 loopback 接口访问 iperf3-server，大概 49Gb/s ❯ iperf3 -c 127.0.0.1 Connecting to host 127.0.0.1, port 5201 [ 5] local 127.0.0.1 port 48656 connected to 127.0.0.1 port 5201 [ ID] Interval Transfer Bitrate Retr Cwnd [ 5] 0.00-1.00 sec 4.46 GBytes 38.3 Gbits/sec 0 1.62 MBytes [ 5] 1.00-2.00 sec 4.61 GBytes 39.6 Gbits/sec 0 1.62 MBytes [ 5] 2.00-3.00 sec 5.69 GBytes 48.9 Gbits/sec 0 1.62 MBytes [ 5] 3.00-4.00 sec 6.11 GBytes 52.5 Gbits/sec 0 1.62 MBytes [ 5] 4.00-5.00 sec 6.04 GBytes 51.9 Gbits/sec 0 1.62 MBytes [ 5] 5.00-6.00 sec 6.05 GBytes 52.0 Gbits/sec 0 1.62 MBytes [ 5] 6.00-7.00 sec 6.01 GBytes 51.6 Gbits/sec 0 1.62 MBytes [ 5] 7.00-8.00 sec 6.05 GBytes 52.0 Gbits/sec 0 1.62 MBytes [ 5] 8.00-9.00 sec 6.34 GBytes 54.5 Gbits/sec 0 1.62 MBytes [ 5] 9.00-10.00 sec 5.91 GBytes 50.8 Gbits/sec 0 1.62 MBytes - - - - - - - - - - - - - - - - - - - - - - - - - [ ID] Interval Transfer Bitrate Retr [ 5] 0.00-10.00 sec 57.3 GBytes 49.2 Gbits/sec 0 sender [ 5] 0.00-10.00 sec 57.3 GBytes 49.2 Gbits/sec receiver # 客户端通过 wlp4s0 wifi 网卡(192.168.31.228)访问 iperf3-server，实际还是走的本机，但是速度要比 loopback 快一点，可能是默认设置的问题 ❯ iperf3 -c 192.168.31.228 Connecting to host 192.168.31.228, port 5201 [ 5] local 192.168.31.228 port 43430 connected to 192.168.31.228 port 5201 [ ID] Interval Transfer Bitrate Retr Cwnd [ 5] 0.00-1.00 sec 5.12 GBytes 43.9 Gbits/sec 0 1.25 MBytes [ 5] 1.00-2.00 sec 5.29 GBytes 45.5 Gbits/sec 0 1.25 MBytes [ 5] 2.00-3.00 sec 5.92 GBytes 50.9 Gbits/sec 0 1.25 MBytes [ 5] 3.00-4.00 sec 6.00 GBytes 51.5 Gbits/sec 0 1.25 MBytes [ 5] 4.00-5.00 sec 5.98 GBytes 51.4 Gbits/sec 0 1.25 MBytes [ 5] 5.00-6.00 sec 6.05 GBytes 52.0 Gbits/sec 0 1.25 MBytes [ 5] 6.00-7.00 sec 6.16 GBytes 52.9 Gbits/sec 0 1.25 MBytes [ 5] 7.00-8.00 sec 6.08 GBytes 52.2 Gbits/sec 0 1.25 MBytes [ 5] 8.00-9.00 sec 6.00 GBytes 51.6 Gbits/sec 0 1.25 MBytes [ 5] 9.00-10.00 sec 6.01 GBytes 51.6 Gbits/sec 0 1.25 MBytes - - - - - - - - - - - - - - - - - - - - - - - - - [ ID] Interval Transfer Bitrate Retr [ 5] 0.00-10.00 sec 58.6 GBytes 50.3 Gbits/sec 0 sender [ 5] 0.00-10.00 sec 58.6 GBytes 50.3 Gbits/sec receiver # 从容器中访问宿主机的 iperf3-server，速度几乎没区别 ❯ docker run -it --rm --name=iperf3-server networkstatic/iperf3 -c 192.168.31.228 Connecting to host 192.168.31.228, port 5201 [ 5] local 172.17.0.2 port 43436 connected to 192.168.31.228 port 5201 [ ID] Interval Transfer Bitrate Retr Cwnd [ 5] 0.00-1.00 sec 4.49 GBytes 38.5 Gbits/sec 0 403 KBytes [ 5] 1.00-2.00 sec 5.31 GBytes 45.6 Gbits/sec 0 544 KBytes [ 5] 2.00-3.00 sec 6.14 GBytes 52.8 Gbits/sec 0 544 KBytes [ 5] 3.00-4.00 sec 5.85 GBytes 50.3 Gbits/sec 0 544 KBytes [ 5] 4.00-5.00 sec 6.14 GBytes 52.7 Gbits/sec 0 544 KBytes [ 5] 5.00-6.00 sec 5.99 GBytes 51.5 Gbits/sec 0 544 KBytes [ 5] 6.00-7.00 sec 5.86 GBytes 50.4 Gbits/sec 0 544 KBytes [ 5] 7.00-8.00 sec 6.05 GBytes 52.0 Gbits/sec 0 544 KBytes [ 5] 8.00-9.00 sec 5.99 GBytes 51.5 Gbits/sec 0 544 KBytes [ 5] 9.00-10.00 sec 6.12 GBytes 52.5 Gbits/sec 0 544 KBytes - - - - - - - - - - - - - - - - - - - - - - - - - [ ID] Interval Transfer Bitrate Retr [ 5] 0.00-10.00 sec 58.0 GBytes 49.8 Gbits/sec 0 sender [ 5] 0.00-10.00 sec 58.0 GBytes 49.8 Gbits/sec receiver 把 iperf3-server 跑在容器里再测一遍： # 在容器中启动 iperf3-server，并映射到宿主机端口 6201 \u003e docker run -it --rm --name=iperf3-server -p 6201:5201 networkstatic/iperf3 -s \u003e docker inspect --format \"{{ .NetworkSettings.IPAddress }}\" iperf3-server 172.17.0.2 ----------------------------- # 测试容器之间互访的速度，ip 为 iperf3-server 的容器 ip，速度要慢一些。 # 毕竟过了 veth -\u003e veth -\u003e docker0 -\u003e veth","date":"2021-08-14","objectID":"/posts/linux-virtual-network-interfaces/:8:1","tags":["Linux","网络","虚拟化","容器"],"title":"Linux 中的虚拟网络接口","uri":"/posts/linux-virtual-network-interfaces/"},{"categories":["技术"],"content":"参考 Linux虚拟网络设备之tun/tap Linux虚拟网络设备之veth 云计算底层技术-虚拟网络设备(Bridge,VLAN) 云计算底层技术-虚拟网络设备(tun/tap,veth) Universal TUN/TAP device driver - Kernel Docs Tun/Tap interface tutorial Linux Loopback performance with TCP_NODELAY enabled ","date":"2021-08-14","objectID":"/posts/linux-virtual-network-interfaces/:9:0","tags":["Linux","网络","虚拟化","容器"],"title":"Linux 中的虚拟网络接口","uri":"/posts/linux-virtual-network-interfaces/"},{"categories":["技术"],"content":" 文中的命令均在 macOS Big Sur 和 Opensuse Tumbleweed 上测试通过 ","date":"2021-04-11","objectID":"/posts/socat-netcat/:0:0","tags":["网络","Linux","网络调试"],"title":"Linux 网络工具中的瑞士军刀 - socat \u0026 netcat","uri":"/posts/socat-netcat/"},{"categories":["技术"],"content":"socat \u0026 netcat netcat(network cat) 是一个历史悠久的网络工具包，被称作 TCP/IP 的瑞士军刀，各大 Linux 发行版都有默认安装 openbsd 版本的 netcat，它的命令行名称为 nc. 而 socat(socket cat)，官方文档描述它是 \"netcat++\" (extended design, new implementation)，项目比较活跃，kubernetes-client(kubectl) 底层就是使用的它做各种流量转发。 在不方便安装 socat 的环境中，我们可以使用系统自带的 netcat. 而在其他环境，可以考虑优先使用 socat. ","date":"2021-04-11","objectID":"/posts/socat-netcat/:1:0","tags":["网络","Linux","网络调试"],"title":"Linux 网络工具中的瑞士军刀 - socat \u0026 netcat","uri":"/posts/socat-netcat/"},{"categories":["技术"],"content":"一、简介 socat 的基本命令格式： socat [参数] 地址1 地址2 给 socat 提供两个地址，socat 干的活就是把两个地址的流对接起来。左边地址的输出传给右边，同时又把右边地址的输出传给左边，也就是一个双向的数据管道。 听起来好像没啥特别的，但是实际上计算机网络干的活也就是数据传输而已，却影响了整个世界，不可小觑它的功能。 socat 支持非常多的地址类型：-/stdio，TCP, TCP-LISTEN, UDP, UDP-LISTEN, OPEN, EXEC, SOCKS, PROXY 等等，可用于端口监听、链接，文件和进程读写，代理桥接等等。 socat 的功能就是这么简单，命令行参数也很简洁，唯一需要花点精力学习的就是它各种地址的定义和搭配写法。 而 netcat 定义貌似没这么严谨，可以简单的理解为网络版的 cat 命令 2333 ","date":"2021-04-11","objectID":"/posts/socat-netcat/:2:0","tags":["网络","Linux","网络调试"],"title":"Linux 网络工具中的瑞士军刀 - socat \u0026 netcat","uri":"/posts/socat-netcat/"},{"categories":["技术"],"content":"二、安装方法 各发行版都自带 netcat，包名通常为 nc-openbsd，因此这里只介绍 socat 的安装方法： # Debian/Ubuntu sudo apt install socat # CentOS/RedHat sudo yum install socat # macOS brew install socat 其他发行版基本也都可以使用包管理器安装 socat ","date":"2021-04-11","objectID":"/posts/socat-netcat/:3:0","tags":["网络","Linux","网络调试"],"title":"Linux 网络工具中的瑞士军刀 - socat \u0026 netcat","uri":"/posts/socat-netcat/"},{"categories":["技术"],"content":"三、常用命令 ","date":"2021-04-11","objectID":"/posts/socat-netcat/:4:0","tags":["网络","Linux","网络调试"],"title":"Linux 网络工具中的瑞士军刀 - socat \u0026 netcat","uri":"/posts/socat-netcat/"},{"categories":["技术"],"content":"1. 网络调试 1.1 检测远程端口的可连接性（确认防火墙没问题） 以前你可能学过如何用 telnet 来做这项测试，不过现在很多发行版基本都不自带 telnet 了，还需要额外安装。 telnet 差不多已经快寿终正寝了，还是建议使用更专业的 socat/netcat 使用 socat/netcat 检测远程端口的可连接性： # -d[ddd] 增加日志详细程度，-dd Prints fatal, error, warning, and notice messages. socat -dd - TCP:192.168.1.252:3306 # -v 显示详细信息 # -z 不发送数据，效果为立即关闭连接，快速得出结果 nc -vz 192.168.1.2 8080 # -vv 显示更详细的内容 # -w2 超时时间设为 2 秒 # 使用 nc 做简单的端口扫描 nc -vv -w2 -z 192.168.1.2 20-500 1.2 测试本机端口是否能正常被外部访问（检测防火墙、路由） 在本机监听一个 TCP 端口，接收到的内容传到 stdout，同时将 stdin 的输入传给客户端： # 服务端启动命令，socat/nc 二选一 socat TCP-LISTEN:7000 - # -l --listening nc -l 7000 # 客户端连接命令，socat/nc 二选一 socat TCP:192.168.31.123:7000 - nc 192.168.11.123 7000 UDP 协议的测试也非常类似，使用 netcat 的示例如下： # 服务端，只监听 ipv4 nc -u -l 8080 # 客户端 nc -u 192.168.31.123 8080 # 客户端本机测试，注意 localhost 会被优先解析为 ipv6! 这会导致服务端(ipv4)的 nc 接收不到数据！ nc -u localhost 8080 使用 socat 的 UDP 测试示例如下： socat UDP-LISTEN:7000 - socat UDP:192.168.31.123:7000 - 1.3 调试 TLS 协议 参考 socat 官方文档：Securing Traffic Between two Socat Instances Using SSL 测试证书及私钥的生成参见 TLS 协议、TLS 证书、TLS 证书的配置方法、TLS 加密的破解手段 模拟一个 mTLS 服务器，监听 4433 端口，接收到的数据同样输出到 stdout： # socat 需要使用同时包含证书和私钥的 pem 文件，生成方法如下 cat server.key server.crt \u003e server.pem cat client.key client.crt \u003e client.pem # 服务端启动命令 socat openssl-listen:4433,reuseaddr,cert=server.pem,cafile=client.crt - # 客户端连接命令 socat - openssl-connect:192.168.31.123:4433,cert=client.pem,cafile=server.crt # 或者使用 curl 连接(我们知道 ca.crt 和 server.crt 都能被用做 cacert/cafile) curl -v --cacert ca.crt --cert client.crt --key client.key --tls-max 1.2 https://192.168.31.123:4433 上面的命令使用了 mTLS 双向认证的协议，可通过设定 verify=0 来关掉客户端认证，示例如下： # socat 需要使用同时包含证书和私钥的 pem 文件，生成方法如下 cat server.key server.crt \u003e server.pem # 服务端启动命令 socat openssl-listen:4433,reuseaddr,cert=server.pem,verify=0 - # 客户端连接命令，如果 ip/域名不受证书保护，就也需要添加 verify=0 socat - openssl-connect:192.168.31.123:4433,cafile=server.crt # 或者使用 curl 连接，证书无效请添加 -k 跳过证书验证 curl -v --cacert server.crt https://192.168.31.123:4433 ","date":"2021-04-11","objectID":"/posts/socat-netcat/:4:1","tags":["网络","Linux","网络调试"],"title":"Linux 网络工具中的瑞士军刀 - socat \u0026 netcat","uri":"/posts/socat-netcat/"},{"categories":["技术"],"content":"2. 数据传输 通常传输文件时，我都习惯使用 scp/ssh/rsync，但是 socat 其实也可以传输文件。 以将 demo.tar.gz 从主机 A 发送到主机 B 为例， 首先在数据发送方 A 执行如下命令： # -u 表示数据只从左边的地址单向传输给右边（socat 默认是一个双向管道） # -U 和 -u 相反，数据只从右边单向传输给左边 socat -u open:demo.tar.gz tcp-listen:2000,reuseaddr 然后在数据接收方 B 执行如下命令，就能把文件接收到： socat -u tcp:192.168.1.252:2000 open:demo.tar.gz,create # 如果觉得太繁琐，也可以直接通过 stdout 重定向 socat -u tcp:192.168.1.252:2000 - \u003e demo.tar.gz 使用 netcat 也可以实现数据传输： # 先在接收方启动服务端 nc -l -p 8080 \u003e demo.tar.gz # 再在发送方启动客户端发送数据 nc 192.168.1.2 8080 \u003c demo.tar.gz ","date":"2021-04-11","objectID":"/posts/socat-netcat/:5:0","tags":["网络","Linux","网络调试"],"title":"Linux 网络工具中的瑞士军刀 - socat \u0026 netcat","uri":"/posts/socat-netcat/"},{"categories":["技术"],"content":"3. 担当临时的 web 服务器 使用 fork reuseaddr SYSTEM 三个命令，再用 systemd/supervisor 管理一下，就可以用几行命令实现一个简单的后台服务器。 下面的命令将监听 8080 端口，并将数据流和 web.py 的 stdio 连接起来，可以直接使用浏览器访问 http://\u003cip\u003e:8080 来查看效果。 socat TCP-LISTEN:8080,reuseaddr,fork SYSTEM:\"python3 web.py\" 假设 web.py 的内容为： print(\"hello world\") 那 curl localhost:8080 就应该会输出 hello world ","date":"2021-04-11","objectID":"/posts/socat-netcat/:6:0","tags":["网络","Linux","网络调试"],"title":"Linux 网络工具中的瑞士军刀 - socat \u0026 netcat","uri":"/posts/socat-netcat/"},{"categories":["技术"],"content":"4. 端口转发 监听 8080 端口，建立该端口与 baidu.com:80 之间的双向管道: socat TCP-LISTEN:8080,fork,reuseaddr TCP:baidu.com:80 拿 curl 命令测试一下，应该能正常访问到百度： # 注意指定 Host curl -v -H 'Host: baidu.com' localhost:8080 ","date":"2021-04-11","objectID":"/posts/socat-netcat/:7:0","tags":["网络","Linux","网络调试"],"title":"Linux 网络工具中的瑞士军刀 - socat \u0026 netcat","uri":"/posts/socat-netcat/"},{"categories":["技术"],"content":"参考 新版瑞士军刀：socat - 韦易笑 - 知乎 用好你的瑞士军刀/netcat - 韦易笑 - 知乎 socat - Multipurpose relay ","date":"2021-04-11","objectID":"/posts/socat-netcat/:8:0","tags":["网络","Linux","网络调试"],"title":"Linux 网络工具中的瑞士军刀 - socat \u0026 netcat","uri":"/posts/socat-netcat/"},{"categories":["随笔"],"content":"年轻真好 最近看了些前辈们的博客，很多是在计算机行业工作几十年的前辈，还有许嵩的文章。 我更深刻地认识到了一件事：我当下的很多文章，都能看得出我在很认真的思考、总结，但是总是有很明显的稚嫩的感觉在里面——我自认为这是「学生型思维」。 我总是喜欢讲「且行且寻」、「自己的眼界还太狭窄了，我对世界还很缺乏了解」、「根本看不清好坏，就无法独立做出决策」诸如此类。 我把这样的文章写出来，前辈们给我留言「博主只是沉淀的时间还远远不够。憋着急，年轻就是最大的资本。」、「只想说年轻真好，使劲折腾才知道要什么东西」。 嗯，我理解到了，因为我「年轻」，所以写出这样的文章没问题，可以使劲去折腾、去探索、去思考。 ","date":"2021-02-13","objectID":"/posts/no-more-dreams/:1:0","tags":[],"title":"脚踏实地，仰望星空","uri":"/posts/no-more-dreams/"},{"categories":["随笔"],"content":"三十而立 子曰：吾，十有五，而志于学，三十而立，四十而不惑，五十而知天命，六十而耳顺，七十而从心所欲，不逾矩。 孔子说：“我十五岁立志学习，三十岁在人生道路上站稳脚跟，四十岁心中不再迷惘，五十岁知道上天给我安排的命运，六十岁听到别人说话就能分辨是非真假，七十岁能随心所欲地说话做事，又不会超越规矩。” 「四十而不惑」对我而言可能还太远，但「三十而立」却是已经能预见到了的，没几年了。 三十而立，人到了三十岁，就应该知道自己如何立身处世，尘世滚滚中能守住自己的一点本真不失。 三十岁，已不是一个年轻的年纪了。 如果我到了三十岁，还去写些「自己的眼界还太狭窄了，我对世界还很缺乏了解」、「我根本看不清好坏，很多时候无法独立做出决策」，那就贻笑大方了。 所以即使说「年轻就是最大的资本」，也不是能随意挥霍的！人生这条道路上我们踽踽独行，道阻且长，眼光要放长远一点、多看一点，不要把自己限制住了，更不应该原地踏步！ ","date":"2021-02-13","objectID":"/posts/no-more-dreams/:2:0","tags":[],"title":"脚踏实地，仰望星空","uri":"/posts/no-more-dreams/"},{"categories":["随笔"],"content":"许嵩——我没有梦想 这两天看多了前辈们的博客，就想找点非虚构的书藉看看，补充点阅历。 昨天向朋友们讨书看，@rea.ink 就给我推荐许嵩的《海上灵光》。意外地发现了许嵩的新浪博客。 博客的内容都很老了，最新的一篇是 2013 年。但是这并不妨碍其中见解的价值 海上灵光——许嵩 以前媒体问我接下来有什么计划或梦想时我总是很愣的回答，我没有梦想。 真的，一个年过半百的人还把梦想这种字眼挂在嘴上是很乏味的。 睁大眼看看眼前的生活，周遭的一切吧。 脚踏实地认真过好每一天的生活吧。 至于心底的信念——是决计不必拿出来高谈阔论的。 出离心——许嵩 这几个月，走过了不少地方。 每到一处，采访我的媒体通常会有这么一问：你的音乐理想是什么？ 而当答案是“我从来没有理想”时，我迎接那些错愕的眼神。 年轻的时候，拥有一些世俗的念想（比如声名远播？）、一些物质上的期待（比如大房子好车子？）、一些精神上的憧憬（比如寻得佳偶？）、一些相对崇高的目标（比如造福子孙？！），似乎的确能让一些人更有动力的过活每一天。 但如果，岁月在你脸上已然留下不少年轮——你坐船的动机仍然只是到达一座岛，别人把岛上的一切美妙和宝藏说给你听就可以让你划船划的更带劲儿——那我能对你说些什么呢？ ","date":"2021-02-13","objectID":"/posts/no-more-dreams/:3:0","tags":[],"title":"脚踏实地，仰望星空","uri":"/posts/no-more-dreams/"},{"categories":["随笔"],"content":"池建强——你老了 这两天读到了一篇池建强写的《你老了》，作者是极客时间创始人，真的是年过半百的技术人了。 你老了 - 池建强的随想录 40 以后，不惑是不可能的，恐慌是与日俱增的。四十不惑，说得不是你想明白了，而是你想不明白的，可能就想不明白了，生日变成另一种仪式，它严肃的告诉你，同学，不要有任何幻想了，接受这个现实，你已经不再年轻了。再卖萌也改变不了这个事实。 人们总会长大，成熟，衰老，一如万事万物。今何在说，人从一出生开始，就踏上了自己的西游路，一路向西，到了尽头，就是虚无，人就没了。所有人都不可避免要奔向那个归宿，你没办法选择，没办法回头。 你老了 - 池建强的随想录 你跳不出这个世界，是因为你不知道这个世界有多大，一旦你知道了，你就超出了它。 年龄也是如此。 ","date":"2021-02-13","objectID":"/posts/no-more-dreams/:4:0","tags":[],"title":"脚踏实地，仰望星空","uri":"/posts/no-more-dreams/"},{"categories":["随笔"],"content":"梦想不要多的，想看世界也不是靠说的 既然说了要多走走看看，那就多看多想。 就像许嵩写的那样，不必去高谈阔论什么理想与信念，实际行动才是最有力的证明。 Keep eyes on the stars, and feet on the ground. ","date":"2021-02-13","objectID":"/posts/no-more-dreams/:5:0","tags":[],"title":"脚踏实地，仰望星空","uri":"/posts/no-more-dreams/"},{"categories":null,"content":"一、我的学习清单 根据个人兴趣，以及工作需求划分优先级。 ","date":"2021-02-01","objectID":"/now/:1:0","tags":null,"title":"此时此刻的我","uri":"/now/"},{"categories":null,"content":"最高优先级 技术： go 语言、web 编程、kubebuilder 服务网格 学习与测试各种负载均衡策略: 需要持续更新这份文档 研究 istio 的 gRPC 支持 考虑如何通过 istio envoyfilter 实现 pod 的 warm_up/slow_start 研究 istio 的指标与 access_log（添加 host/uri 等自定义指标），以支持 HTTP host/path 级别的监控与请求分析能力。 详细研究 istio 的限流限并发能力 生活： 娱乐+运动： 轮滑：倒滑后压步 音乐：Synthesizer V, 练习键盘 阅读（一二月份，就读这两本吧）： 《人间失格》 《在生命的尽头拥抱你-临终关怀医生手记》 ","date":"2021-02-01","objectID":"/now/:1:1","tags":null,"title":"此时此刻的我","uri":"/now/"},{"categories":null,"content":"高优先级 服务网格 Cilium Service Mesh - 使用 eBPF + per-node proxy 实现 的服务网格，很有前景。 Zone Aware Load Balancing - 减少跨区流量 如何调优数据面，降低 CPU 使用率及延迟 日志方案调研：grafana loki 配置管理：研究如何使用 vault 实现跨集群的动态配置支持，如何落地此项能力 阅读（三四月份的书单）： 《房思琪的初恋乐园》 《圆圈正义-作为自由前提的信念》 《网络是怎样连接的》 k8s 网络插件 - Cilium Kubernetes：阅读源码，熟悉底层细节 计算机网络： BGP 路由协议 vxlan ","date":"2021-02-01","objectID":"/now/:1:2","tags":null,"title":"此时此刻的我","uri":"/now/"},{"categories":null,"content":"中优先级 rust 语言 容器底层原理 容器镜像的文件系统：overlayfs 镜像的结构分析 镜像的构建流程 写几个小项目（使用 rust/go） 实现一个文本编辑器 https://viewsourcecode.org/snaptoken/kilo/ 实现一个简单的 Linux 容器 https://blog.lizzie.io/linux-containers-in-500-loc.html 网络代理（不到 2000 行的 TUN 库） https://github.com/songgao/water 实现简单的数据库：tinysql/tinykv ","date":"2021-02-01","objectID":"/now/:1:3","tags":null,"title":"此时此刻的我","uri":"/now/"},{"categories":null,"content":"低优先级 Openresty 技术栈：（暂时感觉兴趣不大） 阅读 Lua 程序设计 阅读 APISIX 源码 + Openresty 深入学习 Nginx 及 epoll [进阶]数据库、数据结构与算法 MIT 6.824：《数据密集型应用系统设计》raft redis 底层 mysql/postgresql 底层 英语词汇量 学习简单的 Parser 原理：《编程语言实现模式》 ","date":"2021-02-01","objectID":"/now/:1:4","tags":null,"title":"此时此刻的我","uri":"/now/"},{"categories":null,"content":"其他比较感兴趣的东西 微积分、线代、概率论、数学物理方法 信号与系统、数字信号处理、音视频处理 机器学习、深度学习 《声学基础》、《理论声学》、《空间声学》：虽然大学学的一塌糊涂，现在居然又有些兴趣想学来玩玩，写些声学仿真工具试试。 语音合成、歌声合成 声学模拟：揉搓声模拟 ","date":"2021-02-01","objectID":"/now/:1:5","tags":null,"title":"此时此刻的我","uri":"/now/"},{"categories":null,"content":"二、我想读的书 Computer Systems: A Programmer’s Perspective, 3/E (CS:APP3e) Designing Data-Intensive Applications The Linux Programming Interface Computer Networking: A Top-Down Approach, 7th Edition Systems Performance: Enterprise and the Cloud, 2nd Edition (2020) ","date":"2021-02-01","objectID":"/now/:2:0","tags":null,"title":"此时此刻的我","uri":"/now/"},{"categories":null,"content":"三、此时此刻的我 主要记录下业余时间我都在干些啥。 ","date":"2021-02-01","objectID":"/now/:3:0","tags":null,"title":"此时此刻的我","uri":"/now/"},{"categories":null,"content":"now 目前想做的： Istio/Envoy、Cilium 搞一搞 rust 编程，go web 编程 ","date":"2021-02-01","objectID":"/now/:3:1","tags":null,"title":"此时此刻的我","uri":"/now/"},{"categories":null,"content":"2022-01-01 了解 APISIX/Nginx/Envoy 中的各种负载均衡算法，及其适用场景、局限性。 ","date":"2021-02-01","objectID":"/now/:3:2","tags":null,"title":"此时此刻的我","uri":"/now/"},{"categories":null,"content":"2021-12-12 练习二个半小时，学会了压步转弯技术 无聊，但是又啥都不想干，耽于网络小说… 感觉有点现充了，感觉需要找个更明确的、能给人动力的目标 做个三年的职业规划以及生活规划？ ","date":"2021-02-01","objectID":"/now/:3:3","tags":null,"title":"此时此刻的我","uri":"/now/"},{"categories":null,"content":"2021-11-21 轮滑：复习前双鱼、前剪、前蛇，尝试侧压步、倒滑 ","date":"2021-02-01","objectID":"/now/:3:4","tags":null,"title":"此时此刻的我","uri":"/now/"},{"categories":null,"content":"2021-11-08 - 2021-11-12 将上次 EKS 升级过程中，有问题的服务迁移到 1.21 的 EKS 集群，直接切线上流量测试。 复现了问题，通过 JFR + pods 数量监控，确认到是服务链路上的个别服务频繁扩缩容导致的，这些服务比较重，对扩缩容比较敏感。 测试在 HPA 中添加 behavior 降低缩容速率，同时添加上 PodDisruptionBudget 以避免节点回收导致大量 Pod 被回收，经测试问题基本解决。 遭遇 AWS EKS 托管的控制平面故障，controller-manager 挂了一整天。现象非常奇怪，又是第一次遇到，导致长时间未排查到问题。 确认问题来自 HPA behavior 的 Bug 储存于 etcd 中的 object 仅会有一个版本，透过 apiserver 读取时会转换成请求的 autoscaling API 版本。 autoscaling/v2beta2 scaleUp 及 scaleDown 对象不能为 null，并在其 Kubernetse 代码可以查看到相应的检查机制。 当使用 autoscaling/v1 时，v2beta2 版本中的相关对象字段将作为 annotation 保留，apiserver 不会检查 ScaleUp/ScaleDown 的 annotation是否为 non-null，而导致 kube-controller-manager panic 问题。 我们可以使用 v1 或 v2beta2 创建一个 HPA 对象，然后使用 v1 或 v2beta2 读取、更新或删除该对象。 etcd 中存储的对象只有一个版本，每当您使用 v1 或 v2beta2 获取 HPA 对象时，apiserver 从 etcd 读取它，然后将其转换为您请求的版本。 在使用 kubectl 时，客户端将默认使用 v1(kubectl get hpa)，因此我们必须明确请求 v2beta2 才能使用这些功能(kubectl get hpa.v2beta2.autoscaling) 如果在更新 v1 版本的 HPA 时（kubectl 默认用 v1），手动修改了 v2beta2 功能相关的 annotation 将 scaleUp/scaleDown 设为 null，会导致 controller-manager 挂掉. ","date":"2021-02-01","objectID":"/now/:3:5","tags":null,"title":"此时此刻的我","uri":"/now/"},{"categories":null,"content":"2021-10-23 跟公司冲浪小分队，第一次玩冲浪，最佳成绩是在板上站了大概 6s… ","date":"2021-02-01","objectID":"/now/:3:6","tags":null,"title":"此时此刻的我","uri":"/now/"},{"categories":null,"content":"2021-10/11 - 2021-10-19 将 EKS 集群从 1.17 升级到 1.21（新建集群切量的方式），但是遇到部分服务迁移后可用率抖动。 未定位到原因，升级失败，回滚了流量。 ","date":"2021-02-01","objectID":"/now/:3:7","tags":null,"title":"此时此刻的我","uri":"/now/"},{"categories":null,"content":"2021-09-13 - 2021-09-17 学习极客时间《10X程序员工作法》 以终推始 识别关键问题 ownership ","date":"2021-02-01","objectID":"/now/:3:8","tags":null,"title":"此时此刻的我","uri":"/now/"},{"categories":null,"content":"2021-09-02 - 2021-09-11 EKS 集群升级 了解 EKS 集群的原地升级的细节 输出 EKS 集群原地升级的测试方案，以及生产环境的 EKS 集群升级方案 学习使用 kubeadm+containerd 部署 k8s 测试集群 涉及到的组件：Kuberntes 控制面、网络插件 Cilium、kube-proxy、coredns、containerd ","date":"2021-02-01","objectID":"/now/:3:9","tags":null,"title":"此时此刻的我","uri":"/now/"},{"categories":null,"content":"2021-08-31 - 2021-09-01 思考我在工作中遇到的一些非技术问题，寻找解法 效率：如何在没人 push 的情况下（没有外部压力），维持住高效率的工作状态（早早干完活下班它不香么？）。 建立有效的「自检」与「纠错」机制 自检： 列出目前已知的「异常」和「健康」两类工作状态，每日做一个对比。 每日都列一下详细的工作计划，精确到小时（预留 1 小时 buffer 应对临时需求）。 沟通：遇到问题（各种意义上的问题）时，及时沟通清楚再继续推进，是一件 ROI 非常高的事。否则几乎肯定会在后面的某个节点，被这个问题坑一把。 目前的关键目标是啥？存在哪些关键问题（实现关键目标最大的阻碍）？我最近做的主要工作，是不是在为关键目标服务？ 如何把安排到手上的事情做好？ 思考这件事情真正的目标的什么？ 比如任务是排查下某服务状态码有无问题，真正的目的应该是想知道服务有没有异常 达成真正的目标，需要做哪些事？ 不仅仅状态码需要排查，还有服务负载、内存、延迟的分位值，或许都可以看看。 跟需求方沟通，询问是否真正需要做的，是前面分析得出的事情。 这些问题都是有解法的，关键是思路的转换。 ","date":"2021-02-01","objectID":"/now/:3:10","tags":null,"title":"此时此刻的我","uri":"/now/"},{"categories":null,"content":"2021-08-28 =\u003e 2021-08-29 容器底层原理： linux namespace 与 cgroups linux 虚拟网络接口 macvlan/ipvlan、vlan、vxlan ","date":"2021-02-01","objectID":"/now/:3:11","tags":null,"title":"此时此刻的我","uri":"/now/"},{"categories":null,"content":"2021-08-19 =\u003e 2021-08-23 阅读 rust 语言的官方文档：the book 边读文档边做 rustlings 的小习题 目前完成了除 macros 之外的所有题 遇到的最难的题：conversions/{try_from_into, from_str} 使用 rust 重写了一版 video2chars ","date":"2021-02-01","objectID":"/now/:3:12","tags":null,"title":"此时此刻的我","uri":"/now/"},{"categories":null,"content":"2021-08-12 =\u003e 2021-08-16 Linux 的虚拟网络接口 Linux 的 netfilter 网络处理框架，以及其子项目 iptables/conntrack ","date":"2021-02-01","objectID":"/now/:3:13","tags":null,"title":"此时此刻的我","uri":"/now/"},{"categories":null,"content":"2021-03-11 =\u003e 2021-08-09 学习 nginx - openresty - apisix 工作中，在自己负责的领域，建立起 ownership 学习新公司的工作模式：OKR 工作法 学习新公司的思维模式（识别关键问题） 如何从公司的角度去思考问题，找到我们目前最应该做的事情 从以下角度去评价一件事情的重要性 这件事情对我们目前的目标有多大帮助？ 需要投入多少资源和人力？ 在推进过程中，有哪些阶段性成果或者 check point？ ","date":"2021-02-01","objectID":"/now/:3:14","tags":null,"title":"此时此刻的我","uri":"/now/"},{"categories":["技术"],"content":" 注意：这篇文章并不是一篇入门教程，学习 Argo Workflows 请移步官方文档 Argo Documentation Argo Workflows 是一个云原生工作流引擎，专注于编排并行任务。它的特点如下： 使用 Kubernetes 自定义资源(CR)定义工作流，其中工作流中的每个步骤都是一个容器。 将多步骤工作流建模为一系列任务，或者使用有向无环图（DAG）描述任务之间的依赖关系。 可以在短时间内轻松运行用于机器学习或数据处理的计算密集型作业。 Argo Workflows 可以看作 Tekton 的加强版，因此显然也可以通过 Argo Workflows 运行 CI/CD 流水线(Pipielines)。 阿里云是 Argo Workflows 的深度使用者和贡献者，另外 Kubeflow 底层的工作流引擎也是 Argo Workflows. ","date":"2021-01-27","objectID":"/posts/expirence-of-argo-workflow/:0:0","tags":["云原生","CI","持续集成","流水线"],"title":"云原生流水线 Argo Workflows 的安装、使用以及个人体验","uri":"/posts/expirence-of-argo-workflow/"},{"categories":["技术"],"content":"一、Argo Workflows 对比 Jenkins 我们在切换到 Argo Workflows 之前，使用的 CI/CD 工具是 Jenkins，下面对 Argo Workflows 和 Jenkins 做一个比较详细的对比， 以了解 Argo Workflows 的优缺点。 ","date":"2021-01-27","objectID":"/posts/expirence-of-argo-workflow/:1:0","tags":["云原生","CI","持续集成","流水线"],"title":"云原生流水线 Argo Workflows 的安装、使用以及个人体验","uri":"/posts/expirence-of-argo-workflow/"},{"categories":["技术"],"content":"1. Workflow 的定义 Workflow 使用 kubernetes CR 进行定义，因此显然是一份 yaml 配置。 一个 Workflow，就是一个运行在 Kubernetes 上的流水线，对应 Jenkins 的一次 Build. 而 WorkflowTemplate 则是一个可重用的 Workflow 模板，对应 Jenkins 的一个 Job. WorkflowTemplate 的 yaml 定义和 Workflow 完全一致，只有 Kind 不同！ WorkflowTemplate 可以被其他 Workflow 引用并触发，也可以手动传参以生成一个 Workflow 工作流。 ","date":"2021-01-27","objectID":"/posts/expirence-of-argo-workflow/:1:1","tags":["云原生","CI","持续集成","流水线"],"title":"云原生流水线 Argo Workflows 的安装、使用以及个人体验","uri":"/posts/expirence-of-argo-workflow/"},{"categories":["技术"],"content":"2. Workflow 的编排 Argo Workflows 相比其他流水线项目(Jenkins/Tekton/Drone/Gitlab-CI)而言，最大的特点，就是它强大的流水线编排能力。 其他流水线项目，对流水线之间的关联性考虑得很少，基本都假设流水线都是互相独立的。 而 Argo Workflows 则假设「任务」之间是有依赖关系的，针对这个依赖关系，它提供了两种协调编排「任务」的方法：Steps 和 DAG 再借助 templateRef 或者 Workflow of Workflows，就能实现 Workflows 的编排了。 我们之所以选择 Argo Workflows 而不是 Tekton，主要就是因为 Argo 的流水线编排能力比 Tekton 强大得多。（也许是因为我们的后端中台结构比较特殊，导致我们的 CI 流水线需要具备复杂的编排能力） 一个复杂工作流的示例如下： https://github.com/argoproj/argo/issues/1088#issuecomment-445884543\" https://github.com/argoproj/argo/issues/1088#issuecomment-445884543 ","date":"2021-01-27","objectID":"/posts/expirence-of-argo-workflow/:1:2","tags":["云原生","CI","持续集成","流水线"],"title":"云原生流水线 Argo Workflows 的安装、使用以及个人体验","uri":"/posts/expirence-of-argo-workflow/"},{"categories":["技术"],"content":"3. Workflow 的声明式配置 Argo 使用 Kubernetes 自定义资源(CR)来定义 Workflow，熟悉 Kubernetes Yaml 的同学上手应该都很快。 下面对 Workflow 定义文件和 Jenkinsfile 做个对比： argo 完全使用 yaml 来定义流水线，学习成本比 Jenkinsfile 的 groovy 低。对熟悉 Kubernetes 的同学尤其如此。 将 jenkinsfile 用 argo 重写后，代码量出现了明显的膨胀。一个 20 行的 Jenkinsfile，用 Argo 重写可能就变成了 60 行。 配置出现了膨胀是个问题，但是考虑到它的可读性还算不错， 而且 Argo 的 Workflow 编排功能，能替代掉我们目前维护的部分 Python 构建代码，以及一些其他优点，配置膨胀这个问题也就可以接受了。 ","date":"2021-01-27","objectID":"/posts/expirence-of-argo-workflow/:1:3","tags":["云原生","CI","持续集成","流水线"],"title":"云原生流水线 Argo Workflows 的安装、使用以及个人体验","uri":"/posts/expirence-of-argo-workflow/"},{"categories":["技术"],"content":"4. Web UI Argo Workflows 的 Web UI 感觉还很原始。确实该支持的功能都有，但是它貌似不是面向「用户」的，功能比较底层。 它不像 Jenkins 一样，有很友好的使用界面(虽然说 Jenkins 的 UI 也很显老…) 另外它所有的 Workflow 都是相互独立的，没办法直观地找到一个 WorkflowTemplate 的所有构建记录，只能通过 label/namespace 进行分类，通过任务名称进行搜索。 而 Jenkins 可以很方便地看到同一个 Job 的所有构建历史。 ","date":"2021-01-27","objectID":"/posts/expirence-of-argo-workflow/:1:4","tags":["云原生","CI","持续集成","流水线"],"title":"云原生流水线 Argo Workflows 的安装、使用以及个人体验","uri":"/posts/expirence-of-argo-workflow/"},{"categories":["技术"],"content":"5. Workflow 的分类 为何需要对 Workflow 做细致的分类 常见的微服务项目，往往会拆分成众多 Git 仓库（微服务）进行开发，众多的 Git 仓库会使我们创建众多的 CI/CD 流水线。 如果没有任何的分类，这一大堆的流水线如何管理，就成了一个难题。 最显见的需求：前端和后端的流水线最好能区分一下，往下细分，前端的 Web 端和客户端最好也能区分，后端的业务层和中台最好也区分开来。 另外我们还希望将运维、自动化测试相关的任务也集成到这个系统中来（目前我们就是使用 Jenkins 完成运维、自动化测试任务的）， 如果没有任何分类，这一大堆流水线将混乱无比。 Argo Workflows 的分类能力 当 Workflow 越来越多的时候，如果不做分类，一堆 WorkflowTemplate 堆在一起就会显得特别混乱。（没错，我觉得 Drone 就有这个问题…） Argo 是完全基于 Kubernetes 的，因此目前它也只能通过 namespace/labels 进行分类。 这样的分类结构和 Jenkins 的视图-文件夹体系大相径庭，目前感觉不是很好用（也可能纯粹是 Web UI 的锅）。 ","date":"2021-01-27","objectID":"/posts/expirence-of-argo-workflow/:1:5","tags":["云原生","CI","持续集成","流水线"],"title":"云原生流水线 Argo Workflows 的安装、使用以及个人体验","uri":"/posts/expirence-of-argo-workflow/"},{"categories":["技术"],"content":"6. 触发构建的方式 Argo Workflows 的流水线有多种触发方式： 手动触发：手动提交一个 Workflow，就能触发一次构建。可以通过 workflowTemplateRef 直接引用一个现成的流水线模板。 定时触发：CronWorkflow 通过 Git 仓库变更触发：借助 argo-events 可以实现此功能，详见其文档。 另外目前也不清楚 WebHook 的可靠程度如何，会不会因为宕机、断网等故障，导致 Git 仓库变更了，而 Workflow 却没触发，而且还没有任何显眼的错误通知？如果这个错误就这样藏起来了，就可能会导致很严重的问题！ ","date":"2021-01-27","objectID":"/posts/expirence-of-argo-workflow/:1:6","tags":["云原生","CI","持续集成","流水线"],"title":"云原生流水线 Argo Workflows 的安装、使用以及个人体验","uri":"/posts/expirence-of-argo-workflow/"},{"categories":["技术"],"content":"7. secrets 管理 Argo Workflows 的流水线，可以从 kubernetes secrets/configmap 中获取信息，将信息注入到环境变量中、或者以文件形式挂载到 Pod 中。 Git 私钥、Harbor 仓库凭据、CD 需要的 kubeconfig，都可以直接从 secrets/configmap 中获取到。 另外因为 Vault 很流行，也可以将 secrets 保存在 Vault 中，再通过 vault agent 将配置注入进 Pod。 ","date":"2021-01-27","objectID":"/posts/expirence-of-argo-workflow/:1:7","tags":["云原生","CI","持续集成","流水线"],"title":"云原生流水线 Argo Workflows 的安装、使用以及个人体验","uri":"/posts/expirence-of-argo-workflow/"},{"categories":["技术"],"content":"8. Artifacts Argo 支持接入对象存储，做全局的 Artifact 仓库，本地可以使用 MinIO. 使用对象存储存储 Artifact，最大的好处就是可以在 Pod 之间随意传数据，Pod 可以完全分布式地运行在 Kubernetes 集群的任何节点上。 另外也可以考虑借助 Artifact 仓库实现跨流水线的缓存复用（未测试），提升构建速度。 ","date":"2021-01-27","objectID":"/posts/expirence-of-argo-workflow/:1:8","tags":["云原生","CI","持续集成","流水线"],"title":"云原生流水线 Argo Workflows 的安装、使用以及个人体验","uri":"/posts/expirence-of-argo-workflow/"},{"categories":["技术"],"content":"9. 容器镜像的构建 借助 Buildkit 等容器镜像构建工具，可以实现容器镜像的分布式构建。 Buildkit 对构建缓存的支持也很好，可以直接将缓存存储在容器镜像仓库中。 不建议使用 Google 的 Kaniko，它对缓存复用的支持不咋地，社区也不活跃。 ","date":"2021-01-27","objectID":"/posts/expirence-of-argo-workflow/:1:9","tags":["云原生","CI","持续集成","流水线"],"title":"云原生流水线 Argo Workflows 的安装、使用以及个人体验","uri":"/posts/expirence-of-argo-workflow/"},{"categories":["技术"],"content":"10. 客户端/SDK Argo 有提供一个命令行客户端，也有 HTTP API 可供使用。 如下项目值得试用： argo-client-python: Argo Workflows 的 Python 客户端 说实话，感觉和 kubernetes-client/python 一样难用，毕竟都是 openapi-generator 生成出来的… argo-python-dsl: 使用 Python DSL 编写 Argo Workflows 感觉使用难度比 yaml 高，也不太好用。 couler: 为 Argo/Tekton/Airflow 提供统一的构建与管理接口 理念倒是很好，待研究 感觉 couler 挺不错的，可以直接用 Python 写 WorkflowTemplate，这样就一步到位，所有 CI/CD 代码全部是 Python 了。 此外，因为 Argo Workflows 是 kubernetes 自定义资源 CR，也可以使用 helm/kustomize 来做 workflow 的生成。 目前我们一些步骤非常多，但是重复度也很高的 Argo 流水线配置，就是使用 helm 生成的——关键数据抽取到 values.yaml 中，使用 helm 模板 + range 循环来生成 workflow 配置。 ","date":"2021-01-27","objectID":"/posts/expirence-of-argo-workflow/:1:10","tags":["云原生","CI","持续集成","流水线"],"title":"云原生流水线 Argo Workflows 的安装、使用以及个人体验","uri":"/posts/expirence-of-argo-workflow/"},{"categories":["技术"],"content":"二、安装 Argo Workflows 安装一个集群版(cluster wide)的 Argo Workflows，使用 MinIO 做 artifacts 存储： kubectl apply -f https://raw.githubusercontent.com/argoproj/argo/stable/manifests/install.yaml 部署 MinIO: helm repo add minio https://helm.min.io/ # official minio Helm charts # 查看历史版本 helm search repo minio/minio -l | head # 下载并解压 chart helm pull minio/minio --untar --version 8.0.9 # 编写 custom-values.yaml，然后部署 minio kubectl create namespace minio helm install minio ./minio -n argo -f custom-values.yaml minio 部署好后，它会将默认的 accesskey 和 secretkey 保存在名为 minio 的 secret 中。 我们需要修改 argo 的配置，将 minio 作为它的默认 artifact 仓库。 在 configmap workflow-controller-configmap 的 data 中添加如下字段： artifactRepository: | # 是否将 main 容器的日志保存为 artifact，这样 pod 被删除后，仍然可以在 artifact 中找到日志 archiveLogs: true s3: bucket: argo-bucket # bucket 名称，这个 bucket 需要先手动创建好！ endpoint: minio:9000 # minio 地址 insecure: true # 从 minio 这个 secret 中获取 key/secret accessKeySecret: name: minio key: accesskey secretKeySecret: name: minio key: secretkey 现在还差最后一步：手动进入 minio 的 Web UI，创建好 argo-bucket 这个 bucket. 直接访问 minio 的 9000 端口（需要使用 nodeport/ingress 等方式暴露此端口）就能进入 Web UI，使用前面提到的 secret minio 中的 key/secret 登录，就能创建 bucket. ","date":"2021-01-27","objectID":"/posts/expirence-of-argo-workflow/:2:0","tags":["云原生","CI","持续集成","流水线"],"title":"云原生流水线 Argo Workflows 的安装、使用以及个人体验","uri":"/posts/expirence-of-argo-workflow/"},{"categories":["技术"],"content":"ServiceAccount 配置 Argo Workflows 依赖于 ServiceAccount 进行验证与授权，而且默认情况下，它使用所在 namespace 的 default ServiceAccount 运行 workflow. 可 default 这个 ServiceAccount 默认根本没有任何权限！所以 Argo 的 artifacts, outputs, access to secrets 等功能全都会因为权限不足而无法使用！ 为此，Argo 的官方文档提供了两个解决方法。 方法一，直接给 default 绑定 cluster-admin ClusterRole，给它集群管理员的权限，只要一行命令（但是显然安全性堪忧）： kubectl create rolebinding default-admin --clusterrole=admin --serviceaccount=\u003cnamespace\u003e:default -n \u003cnamespace\u003e 方法二，官方给出了Argo Workflows 需要的最小权限的 Role 定义，方便起见我将它改成一个 ClusterRole: apiVersion:rbac.authorization.k8s.io/v1kind:ClusterRolemetadata:name:argo-workflow-rolerules:# pod get/watch is used to identify the container IDs of the current pod# pod patch is used to annotate the step's outputs back to controller (e.g. artifact location)- apiGroups:- \"\"resources:- podsverbs:- get- watch- patch# logs get/watch are used to get the pods logs for script outputs, and for log archival- apiGroups:- \"\"resources:- pods/logverbs:- get- watch 创建好上面这个最小的 ClusterRole，然后为每个名字空间，跑一下如下命令，给 default 账号绑定这个 clusterrole: kubectl create rolebinding default-argo-workflow --clusterrole=argo-workflow-role --serviceaccount=\u003cnamespace\u003e:default -n \u003cnamespace\u003e 这样就能给 default 账号提供最小的 workflow 运行权限。 或者如果你希望使用别的 ServiceAccount 来运行 workflow，也可以自行创建 ServiceAccount，然后再走上面方法二的流程，但是最后，要记得在 workflow 的 spec.serviceAccountName 中设定好 ServiceAccount 名称。 ","date":"2021-01-27","objectID":"/posts/expirence-of-argo-workflow/:2:1","tags":["云原生","CI","持续集成","流水线"],"title":"云原生流水线 Argo Workflows 的安装、使用以及个人体验","uri":"/posts/expirence-of-argo-workflow/"},{"categories":["技术"],"content":"Workflow Executors Workflow Executor 是符合特定接口的一个进程(Process)，Argo 可以通过它执行一些动作，如监控 Pod 日志、收集 Artifacts、管理容器生命周期等等… Workflow Executor 有多种实现，可以通过前面提到的 configmap workflow-controller-configmap 来选择。 可选项如下： docker(默认): 目前使用范围最广，但是安全性最差。它要求一定要挂载访问 docker.sock，因此一定要 root 权限！ kubelet: 应用非常少，目前功能也有些欠缺，目前也必须提供 root 权限 Kubernetes API (k8sapi): 直接通过调用 k8sapi 实现日志监控、Artifacts 手机等功能，非常安全，但是性能欠佳。 Process Namespace Sharing (pns): 安全性比 k8sapi 差一点，因为 Process 对其他所有容器都可见了。但是相对的性能好很多。 在 docker 被 kubernetes 抛弃的当下，如果你已经改用 containerd 做为 kubernetes 运行时，那 argo 将会无法工作，因为它默认使用 docker 作为运行时！ 我们建议将 workflow executore 改为 pns，兼顾安全性与性能，workflow-controller-configmap 按照如下方式修改： apiVersion:v1kind:ConfigMapmetadata:name:workflow-controller-configmapdata:config:|# ...省略若干配置... # Specifies the container runtime interface to use (default: docker) # must be one of: docker, kubelet, k8sapi, pns containerRuntimeExecutor: pns # ... ","date":"2021-01-27","objectID":"/posts/expirence-of-argo-workflow/:2:2","tags":["云原生","CI","持续集成","流水线"],"title":"云原生流水线 Argo Workflows 的安装、使用以及个人体验","uri":"/posts/expirence-of-argo-workflow/"},{"categories":["技术"],"content":"三、使用 Argo Workflows 做 CI 工具 官方的 Reference 还算详细，也有提供非常多的 examples 供我们参考，这里提供我们几个常用的 workflow 定义。 使用 buildkit 构建镜像：https://github.com/argoproj/argo-workflows/blob/master/examples/buildkit-template.yaml buildkit 支持缓存，可以在这个 example 的基础上自定义参数 注意使用 PVC 来跨 step 共享存储空间这种手段，速度会比通过 artifacts 高很多。 ","date":"2021-01-27","objectID":"/posts/expirence-of-argo-workflow/:3:0","tags":["云原生","CI","持续集成","流水线"],"title":"云原生流水线 Argo Workflows 的安装、使用以及个人体验","uri":"/posts/expirence-of-argo-workflow/"},{"categories":["技术"],"content":"四、常见问题 ","date":"2021-01-27","objectID":"/posts/expirence-of-argo-workflow/:4:0","tags":["云原生","CI","持续集成","流水线"],"title":"云原生流水线 Argo Workflows 的安装、使用以及个人体验","uri":"/posts/expirence-of-argo-workflow/"},{"categories":["技术"],"content":"1. workflow 默认使用 root 账号？ workflow 的流程默认使用 root 账号，如果你的镜像默认使用非 root 账号，而且要修改文件，就很可能遇到 Permission Denined 的问题。 解决方法：通过 Pod Security Context 手动设定容器的 user/group: Workflow Pod Security Context 安全起见，我建议所有的 workflow 都手动设定 securityContext，示例： apiVersion:argoproj.io/v1alpha1kind:WorkflowTemplatemetadata:name:xxxspec:securityContext:runAsNonRoot:truerunAsUser:1000 或者也可以通过 workflow-controller-configmap 的 workflowDefaults 设定默认的 workflow 配置。 ","date":"2021-01-27","objectID":"/posts/expirence-of-argo-workflow/:4:1","tags":["云原生","CI","持续集成","流水线"],"title":"云原生流水线 Argo Workflows 的安装、使用以及个人体验","uri":"/posts/expirence-of-argo-workflow/"},{"categories":["技术"],"content":"2. 如何从 hashicorp vault 中读取 secrets? 参考 Support to get secrets from Vault hashicorp vault 目前可以说是云原生领域最受欢迎的 secrets 管理工具。 我们在生产环境用它做为分布式配置中心，同时在本地 CI/CD 中，也使用它存储相关的敏感信息。 现在迁移到 argo，我们当然希望能够有一个好的方法从 vault 中读取配置。 目前最推荐的方法，是使用 vault 的 vault-agent，将 secrets 以文件的形式注入到 pod 中。 通过 valut-policy - vault-role - k8s-serviceaccount 一系列认证授权配置，可以制定非常细粒度的 secrets 权限规则，而且配置信息阅后即焚，安全性很高。 ","date":"2021-01-27","objectID":"/posts/expirence-of-argo-workflow/:4:2","tags":["云原生","CI","持续集成","流水线"],"title":"云原生流水线 Argo Workflows 的安装、使用以及个人体验","uri":"/posts/expirence-of-argo-workflow/"},{"categories":["技术"],"content":"3. 如何在多个名字空间中使用同一个 secrets? 使用 Namespace 对 workflow 进行分类时，遇到的一个常见问题就是，如何在多个名字空间使用 private-git-creds/docker-config/minio/vault 等 workflow 必要的 secrets. 常见的方法是把 secrets 在所有名字空间 create 一次。 但是也有更方便的 secrets 同步工具： 比如，使用 kyverno 进行 secrets 同步的配置： apiVersion:kyverno.io/v1kind:ClusterPolicymetadata:name:sync-secretsspec:background:falserules:# 将 secret vault 从 argo Namespace 同步到其他所有 Namespace- name:sync-vault-secretmatch:resources:kinds:- Namespacegenerate:kind:Secretname:regcrednamespace:\"{{request.object.metadata.name}}\"synchronize:trueclone:namespace:argoname:vault# 可以配置多个 rules，每个 rules 同步一个 secret 上面提供的 kyverno 配置，会实时地监控所有 Namespace 变更，一但有新 Namespace 被创建，它就会立即将 vault secret 同步到该 Namespace. 或者，使用专门的 secrets/configmap 复制工具：kubernetes-replicator ","date":"2021-01-27","objectID":"/posts/expirence-of-argo-workflow/:4:3","tags":["云原生","CI","持续集成","流水线"],"title":"云原生流水线 Argo Workflows 的安装、使用以及个人体验","uri":"/posts/expirence-of-argo-workflow/"},{"categories":["技术"],"content":"4. Argo 对 CR 资源的验证不够严谨，写错了 key 都不报错 待研究 ","date":"2021-01-27","objectID":"/posts/expirence-of-argo-workflow/:4:4","tags":["云原生","CI","持续集成","流水线"],"title":"云原生流水线 Argo Workflows 的安装、使用以及个人体验","uri":"/posts/expirence-of-argo-workflow/"},{"categories":["技术"],"content":"5. 如何归档历史数据？ Argo 用的时间长了，跑过的 Workflows/Pods 全都保存在 Kubernetes/Argo Server 中，导致 Argo 越用越慢。 为了解决这个问题，Argo 提供了一些配置来限制 Workflows 和 Pods 的数量，详见：Limit The Total Number Of Workflows And Pods 这些限制都是 Workflow 的参数，如果希望设置一个全局默认的限制，可以按照如下示例修改 argo 的 workflow-controller-configmap 这个 configmap: apiVersion:v1kind:ConfigMapmetadata:name:workflow-controller-configmapdata:config:|# Default values that will apply to all Workflows from this controller, unless overridden on the Workflow-level # See more: docs/default-workflow-specs.md workflowDefaults: spec: # must complete in 8h (28,800 seconds) activeDeadlineSeconds: 28800 # keep workflows for 1d (86,400 seconds) ttlStrategy: secondsAfterCompletion: 86400 # secondsAfterSuccess: 5 # secondsAfterFailure: 500 # delete all pods as soon as they complete podGC: # 可选项：\"OnPodCompletion\", \"OnPodSuccess\", \"OnWorkflowCompletion\", \"OnWorkflowSuccess\" strategy: OnPodCompletion ","date":"2021-01-27","objectID":"/posts/expirence-of-argo-workflow/:4:5","tags":["云原生","CI","持续集成","流水线"],"title":"云原生流水线 Argo Workflows 的安装、使用以及个人体验","uri":"/posts/expirence-of-argo-workflow/"},{"categories":["技术"],"content":"6. Argo 的其他进阶配置 Argo Workflows 的配置，都保存在 workflow-controller-configmap 这个 configmap 中，我们前面已经接触到了它的部分内容。 这里给出此配置文件的完整 examples: https://github.com/argoproj/argo/blob/master/docs/workflow-controller-configmap.yaml 其中一些可能需要自定义的参数如下： parallelism: workflow 的最大并行数量 persistence: 将完成的 workflows 保存到 postgresql/mysql 中，这样即使 k8s 中的 workflow 被删除了，还能查看 workflow 记录 也支持配置过期时间 sso: 启用单点登录 ","date":"2021-01-27","objectID":"/posts/expirence-of-argo-workflow/:4:6","tags":["云原生","CI","持续集成","流水线"],"title":"云原生流水线 Argo Workflows 的安装、使用以及个人体验","uri":"/posts/expirence-of-argo-workflow/"},{"categories":["技术"],"content":"7. 是否应该尽量使用 CI/CD 工具提供的功能？ 我从同事以及网络上，了解到部分 DevOps 人员主张尽量自己使用 Python/Go 来实现 CI/CD 流水线，CI/CD 工具提供的功能能不使用就不要使用。 因此有此一问。下面做下详细的分析： 尽量使用 CI/CD 工具提供的插件/功能，好处是不需要自己去实现，可以降低维护成本。 但是相对的运维人员就需要深入学习这个 CI/CD 工具的使用，另外还会和 CI/CD 工具绑定，会增加迁移难度。 而尽量自己用 Python 等代码去实现流水线，让 CI/CD 工具只负责调度与运行这些 Python 代码， 那 CI/CD 就可以很方便地随便换，运维人员也不需要去深入学习 CI/CD 工具的使用。 缺点是可能会增加 CI/CD 代码的复杂性。 我观察到 argo/drone 的一些 examples，发现它们的特征是： 所有 CI/CD 相关的逻辑，全都实现在流水线中，不需要其他构建代码 每一个 step 都使用专用镜像：golang/nodejs/python 比如先使用 golang 镜像进行测试、构建，再使用 kaniko 将打包成容器镜像 那是否应该尽量使用 CI/CD 工具提供的功能呢？ 其实这就是有多种方法实现同一件事，该用哪种方法的问题。这个问题在各个领域都很常见。 以我目前的经验来看，需要具体问题具体分析，以 Argo Workflows 为例： 流水线本身非常简单，那完全可以直接使用 argo 来实现，没必要自己再搞个 python 脚本 简单的流水线，迁移起来往往也非常简单。没必要为了可迁移性，非要用 argo 去调用 python 脚本。 流水线的步骤之间包含很多逻辑判断/数据传递，那很可能是你的流水线设计有问题！ 流水线的步骤之间传递的数据应该尽可能少！复杂的逻辑判断应该尽量封装在其中一个步骤中！ 这种情况下，就应该使用 python 脚本来封装复杂的逻辑，而不应该将这些逻辑暴露到 Argo Workflows 中！ 我需要批量运行很多的流水线，而且它们之间还有复杂的依赖关系：那显然应该利用上 argo wrokflow 的高级特性。 argo 的 dag/steps 和 workflow of workflows 这两个功能结合，可以简单地实现上述功能。 ","date":"2021-01-27","objectID":"/posts/expirence-of-argo-workflow/:4:7","tags":["云原生","CI","持续集成","流水线"],"title":"云原生流水线 Argo Workflows 的安装、使用以及个人体验","uri":"/posts/expirence-of-argo-workflow/"},{"categories":["技术"],"content":"8. 如何提升 Argo Workflows 的创建和销毁速度？ 我们发现 workflow 的 pod，创建和销毁消耗了大量时间，尤其是销毁。 这导致我们单个流水线在 argo 上跑，还没在 jenkins 上跑更快。 ","date":"2021-01-27","objectID":"/posts/expirence-of-argo-workflow/:5:0","tags":["云原生","CI","持续集成","流水线"],"title":"云原生流水线 Argo Workflows 的安装、使用以及个人体验","uri":"/posts/expirence-of-argo-workflow/"},{"categories":["技术"],"content":"使用体验 目前已经使用 Argo Workflows 一个月多了，总的来说，最难用的就是 Web UI。 其他的都是小问题，只有 Web UI 是真的超难用，感觉根本就没有好好做过设计… 急需一个第三方 Web UI… ","date":"2021-01-27","objectID":"/posts/expirence-of-argo-workflow/:6:0","tags":["云原生","CI","持续集成","流水线"],"title":"云原生流水线 Argo Workflows 的安装、使用以及个人体验","uri":"/posts/expirence-of-argo-workflow/"},{"categories":["技术"],"content":"画外 - 如何处理其他 Kubernetes 资源之间的依赖关系 Argo 相比其他 CI 工具，最大的特点，是它假设「任务」之间是有依赖关系的，因此它提供了多种协调编排「任务」的方法。 但是貌似 Argo CD 并没有继承这个理念，Argo CD 部署时，并不能在 kubernetes 资源之间，通过 DAG 等方法定义依赖关系。 微服务之间存在依赖关系，希望能按依赖关系进行部署，而 ArgoCD/FluxCD 部署 kubernetes yaml 时都是不考虑任何依赖关系的。这里就存在一些矛盾。 解决这个矛盾的方法有很多，我查阅了很多资料，也自己做了一些思考，得到的最佳实践来自解决服务依赖 - 阿里云 ACK 容器服务，它给出了两种方案： 应用端服务依赖检查: 即在微服务的入口添加依赖检查逻辑，确保所有依赖的微服务/数据库都可访问了，就续探针才能返回 200. 如果超时就直接 Crash 独立的服务依赖检查逻辑: 部分遗留代码使用方法一改造起来或许会很困难，这时可以考虑使用 pod initContainer 或者容器的启动脚本中，加入依赖检查逻辑。 但是这两个方案也还是存在一些问题，在说明问题前，我先说明一下我们「按序部署」的应用场景。 我们是一个很小的团队，后端做 RPC 接口升级时，通常是直接在开发环境做全量升级+测试。 因此运维这边也是，每次都是做全量升级。 因为没有协议协商机制，新的微服务的「RPC 服务端」将兼容 v1 v2 新旧两种协议，而新的「RPC 客户端」将直接使用 v2 协议去请求其他微服务。 这就导致我们必须先升级「RPC 服务端」，然后才能升级「RPC 客户端」。 为此，在进行微服务的全量升级时，就需要沿着 RPC 调用链路按序升级，这里就涉及到了 Kubernetes 资源之间的依赖关系。 我目前获知的关键问题在于：我们使用的并不是真正的微服务开发模式，而是在把整个微服务系统当成一个「单体服务」在看待，所以引申出了这样的依赖关键的问题。 我进入的新公司完全没有这样的问题，所有的服务之间在 CI/CD 这个阶段都是解耦的，CI/CD 不需要考虑服务之间的依赖关系，也没有自动按照依赖关系进行微服务批量发布的功能，这些都由开发人员自行维护。 或许这才是正确的使用姿势，如果动不动就要批量更新一大批服务，那微服务体系的设计、拆分肯定是有问题了，生产环境也不会允许这么轻率的更新。 前面讲了，阿里云提供的「应用端服务依赖检查」和「独立的服务依赖检查逻辑」是最佳实践。它们的优点有： 简化部署逻辑，每次直接做全量部署就 OK。 提升部署速度，具体体现在：GitOps 部署流程只需要走一次（按序部署要很多次）、所有镜像都提前拉取好了、所有 Pod 也都提前启动了。 但是这里有个问题是「灰度发布」或者「滚动更新」，这两种情况下都存在新旧版本共存的问题。 如果出现了 RPC 接口升级，那就必须先完成「RPC 服务端」的「灰度发布」或者「滚动更新」，再去更新「RPC 客户端」。 否则如果直接对所有微服务做灰度更新，只依靠「服务依赖检查」，就会出现这样的问题——「RPC 服务端」处于「薛定谔」状态，你调用到的服务端版本是新还是旧，取决于负载均衡的策略和概率。 **因此在做 RPC 接口的全量升级时，只依靠「服务依赖检查」是行不通的。**我目前想到的方案，有如下几种： 我们当前的使用方案：直接在 yaml 部署这一步实现按序部署，每次部署后就轮询 kube-apiserver，确认全部灰度完成，再进行下一阶段的 yaml 部署。 让后端加个参数来控制客户端使用的 RPC 协议版本，或者搞一个协议协商。这样就不需要控制微服务发布顺序了。 社区很多有状态应用的部署都涉及到部署顺序等复杂操作，目前流行的解决方案是使用 Operator+CRD 来实现这类应用的部署。Operator 会自行处理好各个组件的部署顺序。 ","date":"2021-01-27","objectID":"/posts/expirence-of-argo-workflow/:7:0","tags":["云原生","CI","持续集成","流水线"],"title":"云原生流水线 Argo Workflows 的安装、使用以及个人体验","uri":"/posts/expirence-of-argo-workflow/"},{"categories":["技术"],"content":"参考文档 Argo加入CNCF孵化器，一文解析Kubernetes原生工作流 视频: How to Multiply the Power of Argo Projects By Using Them Together - Hong Wang ","date":"2021-01-27","objectID":"/posts/expirence-of-argo-workflow/:8:0","tags":["云原生","CI","持续集成","流水线"],"title":"云原生流水线 Argo Workflows 的安装、使用以及个人体验","uri":"/posts/expirence-of-argo-workflow/"},{"categories":["技术"],"content":"Vault 是 hashicorp 推出的 secrets 管理、加密即服务与权限管理工具。它的功能简介如下： secrets 管理：支持保存各种自定义信息、自动生成各类密钥，vault 自动生成的密钥还能自动轮转(rotate) 认证方式：支持接入各大云厂商的账号体系（比如阿里云RAM子账号体系）或者 LDAP 等进行身份验证，不需要创建额外的账号体系。 权限管理：通过 policy，可以设定非常细致的 ACL 权限。 密钥引擎：也支持接管各大云厂商的账号体系（比如阿里云RAM子账号体系），实现 API Key 的自动轮转。 支持接入 kubernetes rbac 权限体系，通过 serviceaccount+role 为每个 Pod 单独配置权限。 支持通过 sidecar/init-container 将 secrets 注入到 pod 中，或者通过 k8s operator 将 vault 数据同步到 k8s secrets 中 在使用 Vault 之前，我们是以携程开源的 Apollo 作为微服务的分布式配置中心。 Apollo 在国内非常流行。它功能强大，支持配置的继承，也有提供 HTTP API 方便自动化。 缺点是权限管理和 secrets 管理比较弱，也不支持信息加密，不适合直接存储敏感信息。因此我们现在切换到了 Vault. 目前我们本地的 CI/CD 流水线和云上的微服务体系，都是使用的 Vault 做 secrets 管理. ","date":"2021-01-24","objectID":"/posts/expirence-of-vault/:0:0","tags":["Vault","云原生","Secrets","配置","配置管理"],"title":"secrets 管理工具 Vault 的介绍、安装及使用","uri":"/posts/expirence-of-vault/"},{"categories":["技术"],"content":"一、Vault 基础概念 「基本概念」这一节，基本都翻译自官方文档: https://www.vaultproject.io/docs/internals/architecture 首先看一下 Vault 的架构图： vault layers\" vault layers 可以看到，几乎所有的 Vault 组件都被统称为「屏障(Barrier)」， Vault 可以简单地被划分为 Storage Backend、Barrier 和 HTTP/S API 三个部分。 类比银行金库，「屏障」就是 Vault(金库) 周围的「钢铁」和「混凝土」，Storage Backend 和客户端之间的所有数据流动都需要经过它。 「屏障」确保只有加密数据会被写入 Storage Backend，加密数据在经过「屏障」被读出的过程中被验证与解密。 和银行金库的大门非常类似，Barrier 也必须先解封，才能解密 Storage Backend 中的数据。 ","date":"2021-01-24","objectID":"/posts/expirence-of-vault/:1:0","tags":["Vault","云原生","Secrets","配置","配置管理"],"title":"secrets 管理工具 Vault 的介绍、安装及使用","uri":"/posts/expirence-of-vault/"},{"categories":["技术"],"content":"1. 数据存储及加密解密 Storage Backend(后端存储): Vault 自身不存储数据，因此需要为它配置一个「Storage Backend」。 「Storage Backend」是不受信任的，只用于存储加密数据。 Initialization(初始化): Vault 在首次启动时需要初始化，这一步生成一个「加密密钥(Encryption Key)」用于加密数据，加密完成的数据才能被保存到 Storage Backend. Unseal(解封): Vault 启动后，因为不知道「加密密钥」，它会进入「封印(Sealed)」状态，在「解封(Unseal)」前无法进行任何操作。 「加密密钥」被「master key」保护，我们必须提供「master key」才能完成 Unseal 操作。 默认情况下，Vault 使用沙米尔密钥共享算法 将「master key」分割成五个「Key Shares(分享密钥)」，必须要提供其中任意三个「Key Shares」才能重建出「master key」从而完成 Unseal. vault-shamir-secret-sharing\" vault-shamir-secret-sharing 「Key Shares」的数量，以及重建「master key」最少需要的 key shares 数量，都是可以调整的。 沙米尔密钥共享算法也可以关闭，这样 master key 将被直接用于 Unseal. ","date":"2021-01-24","objectID":"/posts/expirence-of-vault/:1:1","tags":["Vault","云原生","Secrets","配置","配置管理"],"title":"secrets 管理工具 Vault 的介绍、安装及使用","uri":"/posts/expirence-of-vault/"},{"categories":["技术"],"content":"2. 认证系统及权限系统 在解封完成后，Vault 就可以开始处理请求了。 HTTP 请求进入后的整个处理流程都由 vault core 管理，core 会强制进行 ACL 检查，并确保审计日志(audit logging)完成记录。 客户端首次连接 vault 时，需要先完成身份认证，vault 的「auth methods」模块有很多身份认证方法可选： 用户友好的认证方法，适合管理员使用：username/password、云服务商、ldap 在创建 user 的时候，需要为 user 绑定 policy，给予合适的权限。 应用友好的方法，适合应用程序使用：public/private keys、tokens、kubernetes、jwt 身份验证请求流经 Core 并进入 auth methods，auth methods 确定请求是否有效并返回「关联策略(policies)」的列表。 ACL Policies 由 policy store 负责管理与存储，由 core 进行 ACL 检查。 ACL 的默认行为是拒绝，这意味着除非明确配置 Policy 允许某项操作，否则该操作将被拒绝。 在通过 auth methods 完成了身份认证，并且返回的「关联策略」也没毛病之后，「token store」将会生成并管理一个新的 token， 这个 token 会被返回给客户端，用于进行后续请求。 类似 web 网站的 cookie，token 也都存在一个 lease 租期或者说有效期，这加强了安全性。 token 关联了相关的策略 policies，这些策略将被用于验证请求的权限。 请求经过验证后，将被路由到 secret engine。如果 secret engine 返回了一个 secret（由 vault 自动生成的 secret）， Core 会将其注册到 expiration manager，并给它附加一个 lease ID。lease ID 被客户端用于更新(renew)或吊销(revoke)它得到的 secret. 如果客户端允许租约(lease)到期，expiration manager 将自动吊销这个 secret. Core 负责处理审核代理(audit broker)的请求及响应日志，将请求发送到所有已配置的审核设备(audit devices)。 ","date":"2021-01-24","objectID":"/posts/expirence-of-vault/:1:2","tags":["Vault","云原生","Secrets","配置","配置管理"],"title":"secrets 管理工具 Vault 的介绍、安装及使用","uri":"/posts/expirence-of-vault/"},{"categories":["技术"],"content":"3. Secret Engine Secret Engine 是保存、生成或者加密数据的组件，它非常灵活。 有的 Secret Engines 只是单纯地存储与读取数据，比如 kv 就可以看作一个加密的 Redis。 而其他的 Secret Engines 则连接到其他的服务并按需生成动态凭证。 还有些 Secret Engines 提供「加密即服务(encryption as a service)」的能力，如 transit、证书管理等。 常用的 engine 举例： AliCloud Secrets Engine: 基于 RAM 策略动态生成 AliCloud Access Token，或基于 RAM 角色动态生成 AliCloud STS 凭据 Access Token 会自动更新(Renew)，而 STS 凭据是临时使用的，过期后就失效了。 kv: 键值存储，可用于存储一些静态的配置。它一定程度上能替代掉携程的 Apollo 配置中心。 Transit Secrets Engine: 提供加密即服务的功能，它只负责加密和解密，不负责存储。主要应用场景是帮 app 加解密数据，但是数据仍旧存储在 MySQL 等数据库中。 ","date":"2021-01-24","objectID":"/posts/expirence-of-vault/:1:3","tags":["Vault","云原生","Secrets","配置","配置管理"],"title":"secrets 管理工具 Vault 的介绍、安装及使用","uri":"/posts/expirence-of-vault/"},{"categories":["技术"],"content":"二、部署 Vault 官方建议通过 Helm 部署 vault，大概流程： 使用 helm/docker 部署运行 vault. 初始化/解封 vault: vault 安全措施，每次重启必须解封(可设置自动解封). ","date":"2021-01-24","objectID":"/posts/expirence-of-vault/:2:0","tags":["Vault","云原生","Secrets","配置","配置管理"],"title":"secrets 管理工具 Vault 的介绍、安装及使用","uri":"/posts/expirence-of-vault/"},{"categories":["技术"],"content":"0. 如何选择存储后端？ 首先，我们肯定需要 HA，至少要保留能升级到 HA 的能力，所以不建议选择不支持 HA 的后端。 而具体的选择，就因团队经验而异了，人们往往倾向于使用自己熟悉的、知根知底的后端，或者选用云服务。 比如我们对 MySQL/PostgreSQL 比较熟悉，而且使用云服务提供的数据库不需要考虑太多的维护问题，MySQL 作为一个通用协议也不会被云厂商绑架，那我们就倾向于使用 MySQL/PostgreSQL. 而如果你们是本地自建，那你可能更倾向于使用 Etcd/Consul/Raft 做后端存储。 ","date":"2021-01-24","objectID":"/posts/expirence-of-vault/:2:1","tags":["Vault","云原生","Secrets","配置","配置管理"],"title":"secrets 管理工具 Vault 的介绍、安装及使用","uri":"/posts/expirence-of-vault/"},{"categories":["技术"],"content":"1. docker-compose 部署（非 HA） 推荐用于本地开发测试环境，或者其他不需要高可用的环境。 docker-compose.yml 示例如下： version:'3.3'services:vault:# 文档：https://hub.docker.com/_/vaultimage:vault:1.6.0container_name:vaultports:# rootless 容器，内部不能使用标准端口 443- \"443:8200\"restart:alwaysvolumes:# 审计日志存储目录，默认不写审计日志，启用 `file` audit backend 时必须提供一个此文件夹下的路径- ./logs:/vault/logs# 当使用 file data storage 插件时，数据被存储在这里。默认不往这写任何数据。- ./file:/vault/file# 配置目录，vault 默认 `/valut/config/` 中所有以 .hcl/.json 结尾的文件# config.hcl 文件内容，参考 cutom-vaules.yaml- ./config.hcl:/vault/config/config.hcl# TLS 证书- ./certs:/certs# vault 需要锁定内存以防止敏感值信息被交换(swapped)到磁盘中# 为此需要添加如下能力cap_add:- IPC_LOCK# 必须手动设置 entrypoint，否则 vault 将以 development 模式运行entrypoint:vault server -config /vault/config/config.hcl config.hcl 内容如下： ui = true // 使用文件做数据存储（单节点） storage \"file\" { path = \"/vault/file\" } listener \"tcp\" { address = \"[::]:8200\" tls_disable = false tls_cert_file = \"/certs/server.crt\" tls_key_file = \"/certs/server.key\" } 将如上两份配置保存在同一非文件夹内，同时在 ./certs 中提供 TLS 证书 server.crt 和私钥 server.key。 然后 docker-compose up -d 就能启动运行一个 vault 实例。 ","date":"2021-01-24","objectID":"/posts/expirence-of-vault/:2:2","tags":["Vault","云原生","Secrets","配置","配置管理"],"title":"secrets 管理工具 Vault 的介绍、安装及使用","uri":"/posts/expirence-of-vault/"},{"categories":["技术"],"content":"2. 通过 helm 部署高可用的 vault 推荐用于生产环境 通过 helm 部署： # 添加 valut 仓库 helm repo add hashicorp https://helm.releases.hashicorp.com # 查看 vault 版本号 helm search repo hashicorp/vault -l | head # 下载某个版本号的 vault helm pull hashicorp/vault --version 0.11.0 --untar 参照下载下来的 ./vault/values.yaml 编写 custom-values.yaml， 部署一个以 mysql 为后端存储的 HA vault，配置示例如下: 配置内容虽然多，但是大都是直接拷贝自 ./vault/values.yaml，改动很少。 测试 Vault 时可以忽略掉其中大多数的配置项。 global:# enabled is the master enabled switch. Setting this to true or false# will enable or disable all the components within this chart by default.enabled:true# TLS for end-to-end encrypted transporttlsDisable:falseinjector:# True if you want to enable vault agent injection.enabled:truereplicas:1# If true, will enable a node exporter metrics endpoint at /metrics.metrics:enabled:false# Mount Path of the Vault Kubernetes Auth Method.authPath:\"auth/kubernetes\"certs:# secretName is the name of the secret that has the TLS certificate and# private key to serve the injector webhook. If this is null, then the# injector will default to its automatic management mode that will assign# a service account to the injector to generate its own certificates.secretName:null# caBundle is a base64-encoded PEM-encoded certificate bundle for the# CA that signed the TLS certificate that the webhook serves. This must# be set if secretName is non-null.caBundle:\"\"# certName and keyName are the names of the files within the secret for# the TLS cert and private key, respectively. These have reasonable# defaults but can be customized if necessary.certName:tls.crtkeyName:tls.keyserver:# Resource requests, limits, etc. for the server cluster placement. This# should map directly to the value of the resources field for a PodSpec.# By default no direct resource request is made.# Enables a headless service to be used by the Vault Statefulsetservice:enabled:true# Port on which Vault server is listeningport:8200# Target port to which the service should be mapped totargetPort:8200# This configures the Vault Statefulset to create a PVC for audit# logs. Once Vault is deployed, initialized and unseal, Vault must# be configured to use this for audit logs. This will be mounted to# /vault/audit# See https://www.vaultproject.io/docs/audit/index.html to know moreauditStorage:enabled:false# Run Vault in \"HA\" mode. There are no storage requirements unless audit log# persistence is required. In HA mode Vault will configure itself to use Consul# for its storage backend. The default configuration provided will work the Consul# Helm project by default. It is possible to manually configure Vault to use a# different HA backend.ha:enabled:truereplicas:3# Set the api_addr configuration for Vault HA# See https://www.vaultproject.io/docs/configuration#api_addr# If set to null, this will be set to the Pod IP AddressapiAddr:null# config is a raw string of default configuration when using a Stateful# deployment. Default is to use a Consul for its HA storage backend.# This should be HCL.# Note: Configuration files are stored in ConfigMaps so sensitive data # such as passwords should be either mounted through extraSecretEnvironmentVars# or through a Kube secret. For more information see: # https://www.vaultproject.io/docs/platform/k8s/helm/run#protecting-sensitive-vault-configurationsconfig:|ui = true listener \"tcp\" { address = \"[::]:8200\" cluster_address = \"[::]:8201\" # 注意，这个值要和 helm 的参数 global.tlsDisable 一致 tls_disable = false tls_cert_file = \"/etc/certs/vault.crt\" tls_key_file = \"/etc/certs/vault.key\" } # storage \"postgresql\" { # connection_url = \"postgres://username:password@\u003chost\u003e:5432/vault?sslmode=disable\" # ha_enabled = true # } service_registration \"kubernetes\" {} # Example configuration for using auto-unseal, using AWS KMS. # the cluster must have a service account that is authorized to access AWS KMS, throught an IAM Role. # seal \"awskms\" { # region = \"us-east-1\" # kms_key_id = \"\u003csome-key-id\u003e\" # 默认情况下插件会使用 awskms 的公网 enpoint，但是也可以使用如下参数，改用自行创建的 vpc 内网 endpoint # endpoint = \"https:/","date":"2021-01-24","objectID":"/posts/expirence-of-vault/:2:3","tags":["Vault","云原生","Secrets","配置","配置管理"],"title":"secrets 管理工具 Vault 的介绍、安装及使用","uri":"/posts/expirence-of-vault/"},{"categories":["技术"],"content":"3. 初始化并解封 vault 官方文档：Initialize and unseal Vault - Vault on Kubernetes Deployment Guide 通过 helm 部署 vault，默认会部署一个三副本的 StatefulSet，但是这三个副本都会处于 NotReady 状态（docker 方式部署的也一样）。 接下来还需要手动初始化并解封 vault，才能 Ready: 第一步：从三个副本中随便选择一个，运行 vault 的初始化命令：kubectl exec -ti vault-0 -- vault operator init 初始化操作会返回 5 个 unseal keys，以及一个 Initial Root Token，这些数据非常敏感非常重要，一定要保存到安全的地方！ 第二步：在每个副本上，使用任意三个 unseal keys 进行解封操作。 一共有三个副本，也就是说要解封 3*3 次，才能完成 vault 的完整解封！ # 每个实例都需要解封三次！ ## Unseal the first vault server until it reaches the key threshold $ kubectl exec -ti vault-0 -- vault operator unseal # ... Unseal Key 1 $ kubectl exec -ti vault-0 -- vault operator unseal # ... Unseal Key 2 $ kubectl exec -ti vault-0 -- vault operator unseal # ... Unseal Key 3 这样就完成了部署，但是要注意，vault 实例每次重启后，都需要重新解封！也就是重新进行第二步操作！ ","date":"2021-01-24","objectID":"/posts/expirence-of-vault/:2:4","tags":["Vault","云原生","Secrets","配置","配置管理"],"title":"secrets 管理工具 Vault 的介绍、安装及使用","uri":"/posts/expirence-of-vault/"},{"categories":["技术"],"content":"4. 初始化并设置自动解封 在未设置 auto unseal 的情况下，vault 每次重启都要手动解封所有 vault 实例，实在是很麻烦，在云上自动扩缩容的情况下，vault 实例会被自动调度，这种情况就更麻烦了。 为了简化这个流程，可以考虑配置 auto unseal 让 vault 自动解封。 自动解封目前有两种方法： 使用阿里云/AWS/Azure 等云服务提供的密钥库来管理 encryption key AWS: awskms Seal 如果是 k8s 集群，vault 使用的 ServiceAccount 需要有权限使用 AWS KMS，它可替代掉 config.hcl 中的 access_key/secret_key 两个属性 阿里云：alicloudkms Seal 如果你不想用云服务，那可以考虑 autounseal-transit，这种方法使用另一个 vault 实例提供的 transit 引擎来实现 auto-unseal. 简单粗暴：直接写个 crontab 或者在 CI 平台上加个定时任务去执行解封命令，以实现自动解封。不过这样安全性就不好说了。 以使用 awskms 为例，首先创建 aws IAM 的 policy 内容如下: { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Sid\": \"VaultKMSUnseal\", \"Effect\": \"Allow\", \"Action\": [ \"kms:Decrypt\", \"kms:Encrypt\", \"kms:DescribeKey\" ], \"Resource\": \"*\" } ] } 然后创建 IAM Role 绑定上面的 policy，并为 vault 的 k8s serviceaccount 创建一个 IAM Role，绑定上这个 policy. 这样 vault 使用的 serviceaccount 自身就拥有了访问 awskms 的权限，也就不需要额外通过 access_key/secret_key 来访问 awskms. 关于 IAM Role 和 k8s serviceaccount 如何绑定，参见官方文档：IAM roles for EKS service accounts 完事后再修改好前面提供的 helm 配置，部署它，最后使用如下命令初始化一下： # 初始化命令和普通模式并无不同 kubectl exec -ti vault-0 -- vault operator init # 会打印出一个 root token，以及五个 Recovery Key（而不是 Unseal Key） # Recover Key 不再用于解封，但是重新生成 root token 等操作仍然会需要用到它. 然后就大功告成了，可以尝试下删除 vault 的 pod，新建的 Pod 应该会自动解封。 ","date":"2021-01-24","objectID":"/posts/expirence-of-vault/:2:5","tags":["Vault","云原生","Secrets","配置","配置管理"],"title":"secrets 管理工具 Vault 的介绍、安装及使用","uri":"/posts/expirence-of-vault/"},{"categories":["技术"],"content":"三、Vault 自身的配置管理 Vault 本身是一个复杂的 secrets 工具，它提供了 Web UI 和 CLI 用于手动管理与查看 Vault 的内容。 但是作为一名 DevOps，我们当然更喜欢自动化的方法，这有两种选择: 使用 vault 的 sdk: python-hvac 使用 terraform-provider-vault 或者 pulumi-vault 实现 vault 配置的自动化管理。 Web UI 适合手工操作，而 sdk/terraform-provider-vault 则适合用于自动化管理 vault. 我们的测试环境就是使用 pulumi-vault 完成的自动化配置 vault policy 和 kubernetes role，然后自动化注入所有测试用的 secrets. ","date":"2021-01-24","objectID":"/posts/expirence-of-vault/:3:0","tags":["Vault","云原生","Secrets","配置","配置管理"],"title":"secrets 管理工具 Vault 的介绍、安装及使用","uri":"/posts/expirence-of-vault/"},{"categories":["技术"],"content":"1. 使用 pulumi 自动化配置 vault 使用 pulumi 管理 vault 配置的优势是很大的，因为云上资源的敏感信息（数据库账号密码、资源 ID、RAM子账号）都是 pulumi 创建的。 再结合使用 pulumi_valut，就能实现敏感信息自动生成后，立即保存到 vault 中，实现完全自动化。 后续微服务就可以通过 kubernetes 认证，直接从 vault 读取敏感信息。 或者是写入到本地的 vault 中留做备份，在需要的时候，管理员能登入进去查看相关敏感信息。 1.1 Token 的生成 pulumi_vault 本身挺简单的，声明式的配置嘛，直接用就是了。 但是它一定要求提供 VAULT_TOKEN 作为身份认证的凭证（实测 userpass/approle 都不能直接使用，会报错 no vault token found），而且 pulumi 还会先生成临时用的 child token，然后用这个 child token 进行后续的操作。 首先安全起见，肯定不应该直接提供 root token！root token 应该封存，除了紧急情况不应该启用。 那么应该如何生成一个权限有限的 token 给 vault 使用呢？ 我的方法是创建一个 userpass 账号，通过 policy 给予它有限的权限。 然后先手动(或者自动)登录获取到 token，再将 token 提供给 pulumi_vault 使用。 这里面有个坑，就是必须给 userpass 账号创建 child token 的权限： path \"local/*\" { capabilities = [\"read\", \"list\"] } // 允许创建 child token path \"auth/token/create\" { capabilities = [\"create\", \"read\", \"update\", \"delete\", \"list\"] } 不给这个权限，pulumi_vault 就会一直报错。。 然后还得给它「自动化配置」需要的权限，比如自动创建/更新 policy/secrets/kubernetes 等等，示例如下: # To list policies - Step 3 path \"sys/policy\" { capabilities = [\"read\"] }# Create and manage ACL policies broadly across Vault path \"sys/policy/*\" { capabilities = [\"create\", \"read\", \"update\", \"delete\", \"list\", \"sudo\"] }# List, create, update, and delete key/value secrets path \"secret/*\" { capabilities = [\"create\", \"read\", \"update\", \"delete\", \"list\", \"sudo\"] } path \"auth/kubernetes/role/*\" { capabilities = [\"create\", \"read\", \"update\", \"list\"] } ","date":"2021-01-24","objectID":"/posts/expirence-of-vault/:3:1","tags":["Vault","云原生","Secrets","配置","配置管理"],"title":"secrets 管理工具 Vault 的介绍、安装及使用","uri":"/posts/expirence-of-vault/"},{"categories":["技术"],"content":"四、在 Kubernetes 中使用 vault 注入 secrets vault-k8s-auth-workflow\" vault-k8s-auth-workflow 前面提到过 vault 支持通过 Kubernetes 的 ServiceAccount 为每个 Pod 单独分配权限。 应用程序有两种方式去读取 vault 中的配置： 借助 Vault Sidecar，将 secrets 以文件的形式自动注入到 Pod 中，比如 /vault/secrets/config.json vault sidecar 在常驻模式下每 15 秒更新一次配置，应用程序可以使用 watchdog 实时监控 secrets 文件的变更。 应用程序自己使用 SDK 直接访问 vault api 获取 secrets 上述两种方式，都可以借助 Kubernetes ServiceAccount 进行身份验证和权限分配。 下面以 Sidecar 模式为例，介绍如何将 secrets 以文件形式注入到 Pod 中。 ","date":"2021-01-24","objectID":"/posts/expirence-of-vault/:4:0","tags":["Vault","云原生","Secrets","配置","配置管理"],"title":"secrets 管理工具 Vault 的介绍、安装及使用","uri":"/posts/expirence-of-vault/"},{"categories":["技术"],"content":"1. 部署并配置 vault agent 首先启用 Vault 的 Kubernetes 身份验证: # 配置身份认证需要在 vault pod 中执行，启动 vault-0 的交互式会话 kubectl exec -n vault -it vault-0 -- /bin/sh export VAULT_TOKEN='\u003cyour-root-token\u003e' export VAULT_ADDR='http://localhost:8200' # 启用 Kubernetes 身份验证 vault auth enable kubernetes # kube-apiserver API 配置，vault 需要通过 kube-apiserver 完成对 serviceAccount 的身份验证 vault write auth/kubernetes/config \\ token_reviewer_jwt=\"$(cat /var/run/secrets/kubernetes.io/serviceaccount/token)\" \\ kubernetes_host=\"https://$KUBERNETES_PORT_443_TCP_ADDR:443\" \\ kubernetes_ca_cert=@/var/run/secrets/kubernetes.io/serviceaccount/ca.crt 1.1 使用集群外部的 valut 实例 如果你没这个需求，请跳过这一节。 详见 Install the Vault Helm chart configured to address an external Vault kubernetes 也可以和外部的 vault 实例集成，集群中只部署 vault-agent. 这适用于多个 kubernetes 集群以及其他 APP 共用一个 vault 实例的情况，比如我们本地的多个开发测试集群，就都共用着同一个 vault 实例，方便统一管理应用的 secrets. 首先，使用 helm chart 部署 vault-agent，接入外部的 vault 实例。使用的 custom-values.yaml 示例如下： global:# enabled is the master enabled switch. Setting this to true or false# will enable or disable all the components within this chart by default.enabled:true# TLS for end-to-end encrypted transporttlsDisable:falseinjector:# True if you want to enable vault agent injection.enabled:truereplicas:1# If multiple replicas are specified, by default a leader-elector side-car# will be created so that only one injector attempts to create TLS certificates.leaderElector:enabled:trueimage:repository:\"gcr.io/google_containers/leader-elector\"tag:\"0.4\"ttl:60s# If true, will enable a node exporter metrics endpoint at /metrics.metrics:enabled:false# External vault server address for the injector to use. Setting this will# disable deployment of a vault server along with the injector.# TODO 这里的 https ca.crt 要怎么设置？mTLS 又该如何配置？externalVaultAddr:\"https://\u003cexternal-vault-url\u003e\"# Mount Path of the Vault Kubernetes Auth Method.authPath:\"auth/kubernetes\"certs:# secretName is the name of the secret that has the TLS certificate and# private key to serve the injector webhook. If this is null, then the# injector will default to its automatic management mode that will assign# a service account to the injector to generate its own certificates.secretName:null# caBundle is a base64-encoded PEM-encoded certificate bundle for the# CA that signed the TLS certificate that the webhook serves. This must# be set if secretName is non-null.caBundle:\"\"# certName and keyName are the names of the files within the secret for# the TLS cert and private key, respectively. These have reasonable# defaults but can be customized if necessary.certName:tls.crtkeyName:tls.key 部署命令和 通过 helm 部署 vault 一致，只要更换 custom-values.yaml 就行。 vault-agent 部署完成后，第二步是为 vault 创建 serviceAccount、secret 和 ClusterRoleBinding，以允许 vault 审查 kubernetes 的 token, 完成对 pod 的身份验证. yaml 配置如下： ---apiVersion:v1kind:ServiceAccountmetadata:name:vault-authnamespace:vault---apiVersion:v1kind:Secretmetadata:name:vault-authnamespace:vaultannotations:kubernetes.io/service-account.name:vault-authtype:kubernetes.io/service-account-token---apiVersion:rbac.authorization.k8s.io/v1beta1kind:ClusterRoleBindingmetadata:name:role-tokenreview-bindingroleRef:apiGroup:rbac.authorization.k8s.iokind:ClusterRolename:system:auth-delegatorsubjects:- kind:ServiceAccountname:vault-authnamespace:vault 现在在 vault 实例这边，启用 kubernetes 身份验证，在 vault 实例内，执行如下命令： vault 实例内显然没有 kubectl 和 kubeconfig，简便起见，下列的 vault 命令也可以通过 Web UI 完成。 export VAULT_TOKEN='\u003cyour-root-token\u003e' export VAULT_ADDR='http://localhost:8200' # 启用 Kubernetes 身份验证 vault auth enable kubernetes # kube-apiserver API 配置，vault 需要通过 kube-apiserver 完成对 serviceAccount 的身份验证 # TOKEN_REVIEW_JWT: 就是我们前面创建的 secret `vault-auth` TOKEN_REVIEW_JWT=$(kubectl -n vault get secret vault-auth -o go-template='{{ .data.token }}' | base64 --decode) # kube-apiserver 的 ca 证书 KUBE_CA_CERT=$(kubectl -n vault config view --raw --minify --flatten -o jsonpath='{.clusters[].cluster.certificate-authority-data}' | base64 --decode) # kube-apiserver 的 url KUBE_HOST=$(kubectl config view --raw --minify","date":"2021-01-24","objectID":"/posts/expirence-of-vault/:4:1","tags":["Vault","云原生","Secrets","配置","配置管理"],"title":"secrets 管理工具 Vault 的介绍、安装及使用","uri":"/posts/expirence-of-vault/"},{"categories":["技术"],"content":"2. 关联 k8s rbac 权限系统和 vault 接下来需要做的事： 通过 vault policy 定义好每个 role（微服务）能访问哪些资源。 为每个微服务生成一个 role，这个 role 需要绑定对应的 vault policy 及 kubernetes serviceaccount 这个 role 是 vault 的 kubernetes 插件自身的属性，它和 kubernetes role 没有半毛钱关系。 创建一个 ServiceAccount，并使用这个 使用这个 ServiceAccount 部署微服务 其中第一步和第二步都可以通过 vault api 自动化完成. 第三步可以通过 kubectl 部署时完成。 方便起见，vault policy / role / k8s serviceaccount 这三个配置，都建议和微服务使用相同的名称。 上述配置中，role 起到一个承上启下的作用，它关联了 k8s serviceaccount 和 vault policy 两个配置。 比如创建一个名为 my-app-policy 的 vault policy，内容为: # 允许读取数据 path \"my-app/data/*\" { capabilities = [\"read\", \"list\"] } // 允许列出 myapp 中的所有数据(kv v2) path \"myapp/metadata/*\" { capabilities = [\"read\", \"list\"] } 然后在 vault 的 kuberntes 插件配置中，创建 role my-app-role，配置如下: 关联 k8s default 名字空间中的 serviceaccount my-app-account，并创建好这个 serviceaccount. 关联 vault token policy，这就是前面创建的 my-app-policy 设置 token period（有效期） 这之后，每个微服务就能通过 serviceaccount 从 vault 中读取 my-app 中的所有信息了。 ","date":"2021-01-24","objectID":"/posts/expirence-of-vault/:4:2","tags":["Vault","云原生","Secrets","配置","配置管理"],"title":"secrets 管理工具 Vault 的介绍、安装及使用","uri":"/posts/expirence-of-vault/"},{"categories":["技术"],"content":"3. 部署 Pod 参考文档：https://www.vaultproject.io/docs/platform/k8s/injector 下一步就是将配置注入到微服务容器中，这需要使用到 Agent Sidecar Injector。 vault 通过 sidecar 实现配置的自动注入与动态更新。 具体而言就是在 Pod 上加上一堆 Agent Sidecar Injector 的注解，如果配置比较多，也可以使用 configmap 保存，在注解中引用。 需要注意的是 vault-inject-agent 有两种运行模式： init 模式: 仅在 Pod 启动前初始化一次，跑完就退出（Completed） 常驻模式: 容器不退出，持续监控 vault 的配置更新，维持 Pod 配置和 vualt 配置的同步。 示例： apiVersion:apps/v1kind:Deploymentmetadata:labels:app:my-appname:my-appnamespace:defaultspec:minReadySeconds:3progressDeadlineSeconds:60revisionHistoryLimit:3selector:matchLabels:app:my-appstrategy:rollingUpdate:maxUnavailable:1type:RollingUpdatetemplate:metadata:annotations:vault.hashicorp.com/agent-init-first:'true'# 是否使用 initContainer 提前初始化配置文件vault.hashicorp.com/agent-inject:'true'vault.hashicorp.com/secret-volume-path:vaultvault.hashicorp.com/role:\"my-app-role\"# vault kubernetes 插件的 role 名称vault.hashicorp.com/agent-inject-template-config.json:|# 渲染模板的语法在后面介绍vault.hashicorp.com/agent-limits-cpu:250mvault.hashicorp.com/agent-requests-cpu:100m# 包含 vault 配置的 configmap，可以做更精细的控制# vault.hashicorp.com/agent-configmap: my-app-vault-configlabels:app:my-appspec:containers:- image:registry.svc.local/xx/my-app:latestimagePullPolicy:IfNotPresent# 此处省略若干配置...serviceAccountName:my-app-account 常见错误： vault-agent(sidecar) 报错: namespace not authorized auth/kubernetes/config 中的 role 没有绑定 Pod 的 namespace vault-agent(sidecar) 报错: permission denied 检查 vault 实例的日志，应该有对应的错误日志，很可能是 auth/kubernetes/config 没配对，vault 无法验证 kube-apiserver 的 tls 证书，或者使用的 kubernetes token 没有权限。 vault-agent(sidecar) 报错: service account not authorized auth/kubernetes/config 中的 role 没有绑定 Pod 使用的 serviceAccount ","date":"2021-01-24","objectID":"/posts/expirence-of-vault/:4:3","tags":["Vault","云原生","Secrets","配置","配置管理"],"title":"secrets 管理工具 Vault 的介绍、安装及使用","uri":"/posts/expirence-of-vault/"},{"categories":["技术"],"content":"4. vault agent 配置 vault-agent 的配置，需要注意的有： 如果使用 configmap 提供完整的 config.hcl 配置，注意 agent-init vautl-agent 的 template 说明： 目前来说最流行的配置文件格式应该是 json/yaml，以 json 为例， 对每个微服务的 kv 数据，可以考虑将它所有的个性化配置都保存在 \u003cengine-name\u003e/\u003cservice-name\u003e/ 下面，然后使用如下 template 注入配置： { {{ range secrets \"\u003cengine-name\u003e/metadata/\u003cservice-name\u003e/\" }} \"{{ printf \"%s\" . }}\": {{ with secret (printf \"\u003cengine-name\u003e/\u003cservice-name\u003e/%s\" .) }} {{ .Data.data | toJSONPretty }}, {{ end }} {{ end }} } template 的详细语法参见: https://github.com/hashicorp/consul-template#secret 注意：v2 版本的 kv secrets，它的 list 接口有变更，因此在遍历 v2 kv secrets 时， 必须要写成 range secrets \"\u003cengine-name\u003e/metadata/\u003cservice-name\u003e/\"，也就是中间要插入 metadata，而且 policy 中必须开放 \u003cengine-name\u003e/metadata/\u003cservice-name\u003e/ 的 read/list 权限！ 官方文档完全没提到这一点，我通过 wireshark 抓包调试，对照官方的 KV Secrets Engine - Version 2 (API) 才搞明白这个。 这样生成出来的内容将是 json 格式，不过有个不兼容的地方：最后一个 secrets 的末尾有逗号 , 渲染出的效果示例： { \"secret-a\": { \"a\": \"b\", \"c\": \"d\" }, \"secret-b\": { \"v\": \"g\", \"r\": \"c\" }, } 因为存在尾部逗号(trailing comma)，直接使用 json 标准库解析它会报错。 那该如何去解析它呢？我在万能的 stackoverflow 上找到了解决方案：yaml 完全兼容 json 语法，并且支持尾部逗号！ 以 python 为例，直接 yaml.safe_load() 就能完美解析 vault 生成出的 json 内容。 ","date":"2021-01-24","objectID":"/posts/expirence-of-vault/:4:4","tags":["Vault","云原生","Secrets","配置","配置管理"],"title":"secrets 管理工具 Vault 的介绍、安装及使用","uri":"/posts/expirence-of-vault/"},{"categories":["技术"],"content":"5. 拓展：在 kubernetes 中使用 vault 的其他姿势 除了使用官方提供的 sidecar 模式进行 secrets 注入，社区也提供了一些别的方案，可以参考： hashicorp/vault-csi-provider: 官方的 Beta 项目，通过 Secrets Store CSI 驱动将 vault secrets 以数据卷的形式挂载到 pod 中 kubernetes-external-secrets: 提供 CRD 定义，根据定义将 secret 从 vault 中同步到 kubernetes secrets 官方的 sidecar/init-container 模式仍然是最推荐使用的。 ","date":"2021-01-24","objectID":"/posts/expirence-of-vault/:4:5","tags":["Vault","云原生","Secrets","配置","配置管理"],"title":"secrets 管理工具 Vault 的介绍、安装及使用","uri":"/posts/expirence-of-vault/"},{"categories":["技术"],"content":"五、使用 vault 实现 AWS IAM Credentials 的自动轮转 待续。。。 ","date":"2021-01-24","objectID":"/posts/expirence-of-vault/:5:0","tags":["Vault","云原生","Secrets","配置","配置管理"],"title":"secrets 管理工具 Vault 的介绍、安装及使用","uri":"/posts/expirence-of-vault/"},{"categories":["技术"],"content":" 个人笔记，不保证正确性。本人在密码学方面完全是个外行，如果希望深入学习，建议阅读 Practical Cryptography for Developers ","date":"2021-01-17","objectID":"/posts/about-tls-cert/:0:0","tags":["HTTPS","TLS","OpenSSL","加密","破解"],"title":"TLS 协议、TLS 证书、TLS 证书的配置方法、TLS 加密的破解手段","uri":"/posts/about-tls-cert/"},{"categories":["技术"],"content":"更新记录 补充 TLS 协议的详细流程 完成 TLS 证书的详细介绍 ","date":"2021-01-17","objectID":"/posts/about-tls-cert/:1:0","tags":["HTTPS","TLS","OpenSSL","加密","破解"],"title":"TLS 协议、TLS 证书、TLS 证书的配置方法、TLS 加密的破解手段","uri":"/posts/about-tls-cert/"},{"categories":["技术"],"content":"一、TLS 协议 我们需要加密网络数据以实现安全通信，但是有一个现实的问题： 非对称加密算法（RSA/ECC 等）可以方便地对数据进行签名/验证，但是计算速度慢。 对称加密算法（ChaCha20/AES 等）计算速度快，强度高，但是无法安全地生成与保管密钥。 于是 TLS 协议在握手阶段使用非对称算法验证服务端，并使用 ECDHE 密钥交换算法（Elliptic Curve Diffie-Hellman key exchange）安全地生成一个临时的对称密钥，然后使用对称算法进行加密通信。 然后在后续的每次数据交换过程中，都使用 ECDHE 算法生成新的对称密钥，然后使用新密钥加密解密数据。 上述的 TLS 协议流程，提供了「完美前向保密（Perfect Forward Secrecy）」特性，前向保密能够保护过去进行的通讯不受密码或密钥在未来暴露的威胁。 即使攻击者破解出了一个「对称密钥」，也只能获取到一次事务中的数据，黑客必须破解出整个 TLS 连接中所有事务的对称密钥，才能得到完整的数据。 tls1.1/tls1.2 也可以使用非前向安全的算法！要注意！ 本文的主要介绍 TLS 协议在使用方面的内容，ECDHE 等算法及 TLS 握手流程的详细内容，请查阅其他文档。 TLS 通过两个证书来实现服务端身份验证，以及对称密钥的安全生成： CA 证书：浏览器/操作系统自带，用于验证服务端的 TLS 证书的签名。保证服务端证书可信。 TLS 证书：使用 CA 证书验证了 TLS 证书可信后，将使用这个 TLS 证书进行协商，以安全地生成一个对称密钥。 CA 证书和 TLS 证书，都只在 TLS 握手阶段有用到，之后的通信就与它们无关了。 ","date":"2021-01-17","objectID":"/posts/about-tls-cert/:2:0","tags":["HTTPS","TLS","OpenSSL","加密","破解"],"title":"TLS 协议、TLS 证书、TLS 证书的配置方法、TLS 加密的破解手段","uri":"/posts/about-tls-cert/"},{"categories":["技术"],"content":"二、TLS 证书介绍 ","date":"2021-01-17","objectID":"/posts/about-tls-cert/:3:0","tags":["HTTPS","TLS","OpenSSL","加密","破解"],"title":"TLS 协议、TLS 证书、TLS 证书的配置方法、TLS 加密的破解手段","uri":"/posts/about-tls-cert/"},{"categories":["技术"],"content":"1. 证书是什么？ 证书，其实就是非对称加密中的公钥，加上一些别的信息组成的一个文件。 比如 CA 证书，就是 CA 公钥+CA机构相关信息构成的一个文件。 而 TLS 证书，则包含公钥+申请者信息(你)，颁发者(CA)的信息+签名(使用 CA 私钥加密的证书 Hash)，以及一些其他信息。 CA 证书中的公钥，能用于验证 TLS 证书中签名的正确性，也就能用于判断证书是否可信。 你可以尝试使用浏览器查看 Google 的证书详情，我使用 Firefox 查看到的内容如下： 证书文件的格式叫做 X.509，由 RFC5280 规范详细定义。 ","date":"2021-01-17","objectID":"/posts/about-tls-cert/:3:1","tags":["HTTPS","TLS","OpenSSL","加密","破解"],"title":"TLS 协议、TLS 证书、TLS 证书的配置方法、TLS 加密的破解手段","uri":"/posts/about-tls-cert/"},{"categories":["技术"],"content":"2. TLS 证书支持保护的域名类型 TLS 证书支持配置多个域名，并且支持所谓的通配符（泛）域名。 但是通配符域名证书的匹配规则，和 DNS 解析中的匹配规则并不一致！ 根据证书选型和购买 - 阿里云文档 的解释，通配符证书只支持同级匹配，详细说明如下： 一级通配符域名：可保护该通配符域名（主域名）自身和该域名所有的一级子域名。 例如：一级通配符域名 *.aliyun.com 可以用于保护 aliyun.com、www.aliyun.com 以及其他所有一级子域名。 但是不能用于保护任何二级子域名，如 xx.aa.aliyun.com 二级或二级以上通配符域名：只能保护该域名同级的所有通配域名，不支持保护该通配符域名本身。 例如：*.a.aliyun.com 只支持保护它的所有同级域名，不能用于保护三级子域名。 要想保护多个二三级子域，只能在生成 TLS 证书时，添加多个通配符域名。 因此设计域名规则时，要考虑到这点，尽量不要使用层级太深的域名！有些信息可以通过 - 来拼接以减少域名层级，比如阿里云的 oss 域名： 公网：oss-cn-shenzhen.aliyuncs.com 内网：oss-cn-shenzhen-internal.aliyuncs.com ","date":"2021-01-17","objectID":"/posts/about-tls-cert/:3:2","tags":["HTTPS","TLS","OpenSSL","加密","破解"],"title":"TLS 协议、TLS 证书、TLS 证书的配置方法、TLS 加密的破解手段","uri":"/posts/about-tls-cert/"},{"categories":["技术"],"content":"三、TLS 证书的生成 OpenSSL 是目前使用最广泛的网络加密算法库，这里以它为例介绍证书的生成。 另外也可以考虑使用 cfssl. 前面讲到了 TLS 协议的握手需要使用到两个证书： TLS 证书（服务端证书）：这个是服务端需要配置的数据加密证书。 服务端需要持有这个 TLS 证书本身，以及证书的私钥。 握手时服务端需要将 TLS 证书发送给客户端。 CA 证书：这是受信的根证书，客户端可用于验证所有使用它进行签名的 TLS 证书。 CA 证书的私钥由权威机构持有，客户端（比如浏览器）则保有 CA 证书自身。 在 TLS 连接的建立阶段，客户端（如浏览器）会使用 CA 证书的公钥对服务端的证书签名进行验证，验证成功则说明该证书是受信任的。 ","date":"2021-01-17","objectID":"/posts/about-tls-cert/:4:0","tags":["HTTPS","TLS","OpenSSL","加密","破解"],"title":"TLS 协议、TLS 证书、TLS 证书的配置方法、TLS 加密的破解手段","uri":"/posts/about-tls-cert/"},{"categories":["技术"],"content":"1. TLS 证书的类型 按照证书的生成方式进行分类，证书有三种类型： 由权威 CA 机构签名的 TLS 证书：这类证书会被浏览器、小程序等第三方应用/服务商信任。申请证书时需要验证你的所有权，也就使证书无法伪造。 如果你的 API 需要提供给第三方应用/服务商/浏览器访问，那就必须向权威 CA 机构申请此类证书。 本地签名证书 - tls_locally_signed_cert：即由本地 CA 证书签名的 TLS 证书 本地 CA 证书，就是自己使用 openssl 等工具生成的 CA 证书。 这类证书不会被浏览器/小程序等第三方应用/服务商信任，证书就可以被伪造。 这类证书的缺点是无法与第三方应用/服务商建立安全的连接。 如果客户端是完全可控的（比如是自家的 APP），那可以自行验证证书的可靠性（公钥锁定、双向 TLS 验证）。这种场景下使用此类证书是安全可靠的。可以不使用权威CA机构颁发的证书。 自签名证书 - tls_self_signed_cert: 和 tls_locally_signed_cert 类似，但使用 TLS 证书自己充当 CA 证书（我签我自己），生成出的证书就叫自签名证书。 注意:更广义地讲，自签名证书，就是「并非由权威 CA 机构签名的 TLS 证书」，也就是同时指代了 tls_self_signed_cert 和 tls_locally_signed_cert。这也是「自签名证书」应用最广泛的一种含义。 总的来说，权威CA机构颁发的证书，可以被第三方的应用信任，但是自己生成的不行。 而越贵的权威证书，安全性与可信度就越高，或者可以保护更多的域名。 在客户端可控的情况下，可以考虑使用「本地签名证书」（方便、省钱），将这个证书预先埋入客户端中用于验证。 而「自签名证书」主要是方便，能不用还是尽量不要使用。 ","date":"2021-01-17","objectID":"/posts/about-tls-cert/:4:1","tags":["HTTPS","TLS","OpenSSL","加密","破解"],"title":"TLS 协议、TLS 证书、TLS 证书的配置方法、TLS 加密的破解手段","uri":"/posts/about-tls-cert/"},{"categories":["技术"],"content":"2. 向权威CA机构申请「受信 TLS 证书」 免费的 TLS 证书有两种方式获取： 部分 TLS 提供商有提供免费证书的申请，有效期为一年，但是不支持泛域名。 申请 Let’s Encrypt 免费证书 很多代理工具都有提供 Let’s Encrypt 证书的 Auto Renewal，比如: Traefik Caddy docker-letsencrypt-nginx-proxy-companion 网上也有一些 certbot 插件，可以通过 DNS 提供商的 API 进行 Let’s Encrypt 证书的 Auto Renewal，比如： certbot-dns-aliyun terraform 也有相关 provider: terraform-provider-acme 收费证书可以在各 TLS 提供商处购买，比如国内的阿里云腾讯云等。 完整的证书申请流程如下： 为了方便用户，图中的申请人(Applicant)自行处理的部分，目前很多证书申请网站也可以自动处理，用户只需要提供相关信息即可。 ","date":"2021-01-17","objectID":"/posts/about-tls-cert/:4:2","tags":["HTTPS","TLS","OpenSSL","加密","破解"],"title":"TLS 协议、TLS 证书、TLS 证书的配置方法、TLS 加密的破解手段","uri":"/posts/about-tls-cert/"},{"categories":["技术"],"content":"3. 生成「本地签名证书」或者「自签名证书」 除了被第三方信任的「受信 TLS 证书」，在内网环境，我们需要也使用 TLS 证书保障通信安全，这时我们可能会选择自己生成证书，而不是向权威机构申请证书。 可能的原因如下： 要向权威机构申请证书，那是要给钱的。而在内网环境下，并无必要使用权威证书。 内网环境使用的可能是非公网域名（xxx.local/xxx.lan/xxx.srv 等），权威机构不签发这种域名的证书。（因为没有人唯一地拥有这个域名） 前面介绍过，自己生成的证书有两种方类型： 本地签名证书：生成两个独立的密钥对，一个用于 CA 证书，另一个用于 TLS 证书。使用 CA 证书对 TLS 证书进行签名。 自签名证书（我签我自己）：TLS 证书和 CA 证书都使用同一个密钥对，使用 TLS 证书对它自己进行签名。 测试发现这种方式得到的证书貌似不包含 SAN 属性！因此不支持多域名。 一般来说，直接生成一个泛域名的「自签名证书」就够了，但是它不方便拓展——客户端对每个「自签名证书」，都需要单独添加一次信任。 而「本地签名证书」就没这个问题，one ca.crt rules them all. 总的来说，使用「自签名证书」不方便进行拓展，未来可能会遇到麻烦。因此建议使用「本地签名证书」。 另外介绍下这里涉及到的几种文件类型： xxx.key: 就是一个私钥，一般是一个 RSA 私钥，长度通常指定为 2048 位。 CA 证书和 TLS 证书的私钥都是通过这种方式生成的。 xxx.csr: 即 Certificate Sign Request，证书签名请求。使用 openssl 等工具，通过 TLS 密钥+TLS 证书的相关信息，可生成出一个 CSR 文件。 域名（Common Name, CN）就是在这里指定的，可以使用泛域名。 用户将 csr 文件发送给 CA 机构，进行进一步处理。 xxx.crt: 这就是我们所说的 TLS 证书，CA 证书和服务端 TLS 证书都是这个格式。 使用 CA 证书、CA 密钥对 csr 文件进行签名，就能得到最终的服务端 TLS 证书——一个 crt 文件。 生成一个「自签名证书」或者「本地签名证书」（RSA256 算法），有两个步骤： 编写证书签名请求的配置文件 csr.conf: [ req ] default_bits = 2048 prompt = no default_md = sha256 req_extensions = req_ext distinguished_name = dn [ dn ] C = CN # Contountry ST = \u003cstate\u003e L = \u003ccity\u003e O = \u003corganization\u003e OU = \u003corganization unit\u003e CN = *.svc.local # 泛域名，这个字段已经被 chrome/apple 弃用了。 [ alt_names ] # 备用名称，chrome/apple 目前只信任这里面的域名。 DNS.1 = *.svc.local # 一级泛域名 DNS.2 = *.aaa.svc.local # 二级泛域名 DNS.3 = *.bbb.svc.local # 二级泛域名 [ req_ext ] subjectAltName = @alt_names [ v3_ext ] subjectAltName=@alt_names # Chrome 要求必须要有 subjectAltName(SAN) authorityKeyIdentifier=keyid,issuer:always basicConstraints=CA:FALSE keyUsage=keyEncipherment,dataEncipherment,digitalSignature extendedKeyUsage=serverAuth,clientAuth 此文件的详细文档：OpenSSL file formats and conventions 生成证书： # 1. 生成 2048 位 的 RSA 密钥 openssl genrsa -out server.key 2048 # 2. 通过第一步编写的配置文件，生成证书签名请求（公钥+申请者信息） openssl req -new -key server.key -out server.csr -config csr.conf # 3. 生成最终的证书，这里指定证书有效期 3650 天 ## 3.1 方法一（自签名）：使用 server.key 进行自签名。这种方式得到的证书不包含 SAN！不支持多域名！ openssl req -x509 -sha256 -days 3650 -key server.key -in server.csr -out server.crt ## 3.2 方法二（本地签名）：生成 ca 证书，并且使用 CA 证书、CA 密钥对 `csr` 文件进行签名 ### 3.2.1 ca 私钥 openssl genrsa -out ca.key 2048 ### 3.2.2 ca 证书，ca 证书的有效期尽量设长一点，因为不方便更新换代。 openssl req -x509 -new -nodes -key ca.key -subj \"/CN=MyLocalRootCA\" -days 10000 -out ca.crt ### 3.2.3 签名，得到最终的 TLS 证书，它包含四部分内容：公钥+申请者信息 + 颁发者(CA)的信息+签名(使用 CA 私钥加密) openssl x509 -req -in server.csr -CA ca.crt -CAkey ca.key \\ -CAcreateserial -out server.crt -days 3650 \\ -extensions v3_ext -extfile csr.conf 上述流程生成一个 x509 证书链，详细的参数说明，参见 RFC5280 - Internet X.509 Public Key Infrastructure Certificate and Certificate Revocation List (CRL) Profile ","date":"2021-01-17","objectID":"/posts/about-tls-cert/:4:3","tags":["HTTPS","TLS","OpenSSL","加密","破解"],"title":"TLS 协议、TLS 证书、TLS 证书的配置方法、TLS 加密的破解手段","uri":"/posts/about-tls-cert/"},{"categories":["技术"],"content":"4. 关于证书寿命 对于公开服务，服务端证书的有效期不要超过 825 天（27 个月）！而 2020 年 11 月起，新申请的服务端证书有效期缩短到了 398 天（13 个月）。目前 Apple/Mozilla/Chrome 都发表了相应声明，证书有效期超过上述限制的，将被浏览器/Apple设备禁止使用。 对于其他用途的证书，如果更换起来很麻烦，可以考虑放宽条件。 比如 kubernetes 集群的加密证书，可以考虑有效期设长一些，比如 10 年。 据云原生安全破局｜如何管理周期越来越短的数字证书？所述，大量知名企业如 特斯拉/微软/领英/爱立信 都曾因未及时更换 TLS 证书导致服务暂时不可用。 因此 TLS 证书最好是设置自动轮转！人工维护不可靠！ 目前很多 Web 服务器/代理，都支持自动轮转 Let’s Encrypt 证书。 另外 Vault 等安全工具，也支持自动轮转私有证书。 ","date":"2021-01-17","objectID":"/posts/about-tls-cert/:4:4","tags":["HTTPS","TLS","OpenSSL","加密","破解"],"title":"TLS 协议、TLS 证书、TLS 证书的配置方法、TLS 加密的破解手段","uri":"/posts/about-tls-cert/"},{"categories":["技术"],"content":"5. 拓展1：基于 ECC 算法的 TLS 证书 Let’s Encrypt 目前也已经支持了 ECC 证书。 ECC(Elliptic Curve Cryptography) 算法被认为是比 RSA 更优秀的算法。与 RSA 算法相比，ECC 算法使用更小的密钥大小，但可提供同样的安全性，这使计算更快，降低了能耗，并节省了内存和带宽。 对于 RSA 密钥，可以提供不同的密钥大小（密钥大小越大，加密效果越好）。 而对于 ECC 密钥，您应选择要用哪种曲线生成密钥对。各个组织（ANSI X9.62、NIST、SECG）命名了多种曲线，可通过如下命名查看 openssl 支持的所有椭圆曲线名称： openssl ecparam -list_curves 生成一个自签名的 ECC 证书的命令示例如下： # 生成 ec 算法的私钥，使用 prime256v1 算法，密钥长度 256 位。（强度大于 2048 位的 RSA 密钥） openssl ecparam -genkey -name prime256v1 -out key.pem # 生成证书签名请求，需要输入域名(Common Name, CN)等相关信息 openssl req -new -sha256 -key key.pem -out csr.csr -config csr.conf # 生成最终的证书，这里指定证书有效期 10 年 ## 方法一：自签名证书 openssl req -x509 -sha256 -days 3650 -key key.pem -in csr.csr -out certificate.pem ## 方法二：使用 ca 进行签名，方法参考前面 ","date":"2021-01-17","objectID":"/posts/about-tls-cert/:4:5","tags":["HTTPS","TLS","OpenSSL","加密","破解"],"title":"TLS 协议、TLS 证书、TLS 证书的配置方法、TLS 加密的破解手段","uri":"/posts/about-tls-cert/"},{"categories":["技术"],"content":"6. 拓展2：使用 OpenSSL 生成 SSH/JWT 密钥对 RSA/ECC 这两类非对称加密算法被广泛的应用在各类加密通讯中。 SSH/JWT 都支持 RSA-SHA256 及 ECDSA-SHA256 等基于 RSA/ECDSA 的签名/加密算法，因此使用 OpenSSL 生成的密钥对，也应该能用于 SSH 协议加密、JWT 签名等场景。 目前有两种基于 ECC 和 DSA 的椭圆曲线签名算法：ECDSA 和 EdDSA(ed25519)，其中 ECDSA 的文档在曲线选择方面语焉不详，被认为可能存在安全隐患（政治和技术两方面带来的）。 既然 SSH/TLS/JWT 使用的是相同的密钥对，那理所当然地，SSH/JWT 密钥对应该也可以通过 OpenSSL 生成出来。 生成 RSA 密钥对的命令如下： # 1. 生成 2048 位（不是 256 位）的 RSA 密钥 openssl genrsa -out rsa-private-key.pem 2048 # 2. 通过密钥生成公钥，JWT 使用此公钥验证签名 openssl rsa -in rsa-private-key.pem -pubout -out rsa-public-key.pem # 3. SSH 使用专用的公钥格式，需要使用 ssh-keygen 转换下格式 ssh-keygen -i -mPKCS8 -f rsa-public-key.pem -y \u003e rsa-public.pub 生成 ECC 密钥对的命令如下： # 1. 生成 ec 算法的私钥，使用 prime256v1 算法，密钥长度 256 位。（强度大于 2048 位的 RSA 密钥） openssl ecparam -genkey -name prime256v1 -out ecc-private-key.pem # 2. 通过密钥生成公钥 openssl ec -in ecc-private-key.pem -pubout -out ecc-public-key.pem # 3. SSH 使用专用的公钥格式，需要使用 ssh-keygen 转换下格式 ssh-keygen -i -mPKCS8 -f ecc-public-key.pem -y \u003e ecc-public.pub JWT 签名及验证只需要使用标准的私钥-公钥对，即 ecc-private-key.pem/ecc-public-key.pem. 而 SSH 需要使用专用的公钥格式，因此它的使用的密钥对应该是 ecc-private-key.pem/ecc-public.pub 注：SSH 目前推荐使用 ed25519 算法，而 JWT 目前推荐使用 ECDSA 算法。 6.1 加密与签名 加密与解密：公钥用于对数据进行加密，私钥用于对数据进行解密 签名与验证：私钥用于对数据进行签名，公钥用于对签名进行验证 加密与签名的公私钥，用途刚好相反！ ","date":"2021-01-17","objectID":"/posts/about-tls-cert/:4:6","tags":["HTTPS","TLS","OpenSSL","加密","破解"],"title":"TLS 协议、TLS 证书、TLS 证书的配置方法、TLS 加密的破解手段","uri":"/posts/about-tls-cert/"},{"categories":["技术"],"content":"四、服务端与客户端的证书配置 ","date":"2021-01-17","objectID":"/posts/about-tls-cert/:5:0","tags":["HTTPS","TLS","OpenSSL","加密","破解"],"title":"TLS 协议、TLS 证书、TLS 证书的配置方法、TLS 加密的破解手段","uri":"/posts/about-tls-cert/"},{"categories":["技术"],"content":"1. 服务端的 TLS 证书配置 要支持 HTTPS 协议，服务端需要两个文件： TLS 证书私钥(RSA 私钥或 EC 私钥)：server.key TLS 证书（包含公钥）：server.crt 一般如 Nginx 等服务端应用，都可以通过配置文件指定这两个文件的位置。修改配置后重新启动，就有 TLS 了，可以通过 https 协议访问测试。 1.1 完美前向保密 旧版本的 TLS 协议并不一定能保证前向保密，为了保证前向安全，需要在服务端配置中进行一定设置。 具体的设置方法参见 ssl-config - mozilla，该网站提供三个安全等级的配置： 「Intermediate」：查看生成出的 ssl-cipher 属性，发现它只支持 ECDHE/DHE 开头的算法。因此它保证前向保密。 对于需要通过浏览器访问的 API，推荐选择这个等级。 「Mordern」：只支持 TLSv1.3，该协议废弃掉了过往所有不安全的算法，保证前向保密，安全性极高，性能也更好。 对于不需要通过浏览器等旧终端访问的 API，请直接选择这个等级。 「Old」：除非你的用户使用非常老的终端进行访问，否则请不要考虑这个选项！ 另外阿里云负载均衡器配置前向保密的方法参见：管理TLS安全策略 - 负载均衡 - 阿里云文档 ","date":"2021-01-17","objectID":"/posts/about-tls-cert/:5:1","tags":["HTTPS","TLS","OpenSSL","加密","破解"],"title":"TLS 协议、TLS 证书、TLS 证书的配置方法、TLS 加密的破解手段","uri":"/posts/about-tls-cert/"},{"categories":["技术"],"content":"2. 客户端的 TLS 证书配置（针对本地签名的证书） 如果你在证书不是权威 CA 机构颁发的（比如是一个自签名证书），那就需要在客户端将这个 TLS 证书添加到受信证书列表中。 这个步骤可以在 OS 层面进行——添加到 OS 的默认证书列表中，也可以在代码层面完成——指定使用某个证书进行 TLS 验证。 Linux: 使用如下命令安装证书 sudo cp ca.crt /usr/local/share/ca-certificates/server.crt sudo update-ca-certificates Windows: 通过证书管理器 certmgr.msc 将证书安装到 受信任的根证书颁发机构，Chrome 的小锁就能变绿了。 编程：使用 HTTPS 客户端的 api 指定使用的 TLS 证书 Docker-Client: 参见 Use self-signed certificates - Docker Docs 2.1 证书锁定(Certifacte Pining)技术 即使使用了 TLS 协议对流量进行加密，并且保证了前向保密，也无法保证流量不被代理！ 这是因为客户端大多是直接依靠了操作系统内置的证书链进行 TLS 证书验证，而 Fiddler 等代理工具可以将自己的 TLS 证书添加到该证书链中。 为了防止流量被 Fiddler 等工具使用上述方式监听流量，出现了「证书锁定」技术。 方法是在客户端中硬编码证书的指纹（Hash值，或者直接保存整个证书的内容也行），在建立 TLS 连接前，先计算使用的证书的指纹是否匹配，否则就中断连接。 这种锁定方式需要以下几个前提才能确保流量不被监听： 客户端中硬编码的证书指纹不会被篡改。 指纹验证不能被绕过。 目前有公开技术（XPosed+JustTrustMe）能破解 Android 上常见的 HTTPS 请求库，直接绕过证书检查。 针对上述问题，可以考虑加大绕过的难度。或者 App 检测自己是否运行在 Xposed 等虚拟环境下。 用于 TLS 协议的证书不会频繁更换。（如果更换了，指纹就对不上了。） 而对于第三方的 API，因为我们不知道它们会不会更换 TLS 证书，就不能直接将证书指纹硬编码在客户端中。 这时可以考虑从服务端获取这些 API 的证书指纹（附带私钥签名用于防伪造）。 为了实现证书的轮转(rotation)，可以在新版本的客户端中包含多个证书指纹，这样能保证同时有多个可信证书，达成证书的轮转。（类比 JWT 的公钥轮转机制） 证书锁定技术几乎等同于 SSH 协议的 StrictHostKeyChecking 选项，客户端会验证服务端的公钥指纹（key fingerprint），验证不通过则断开连接。 2.2 公钥锁定(Public Key Pining) 前面提到过，TLS 证书其实就是公钥+申请者(你)和颁发者(CA)的信息+签名(使用 CA 私钥加密)，因此我们也可以考虑只锁定其中的公钥。 「公钥锁定」比「证书锁定」更灵活，这样证书本身其实就可以直接轮转了（证书有过期时间），而不需要一个旧证书和新证书共存的中间时期。 如果不考虑实现难度的话，「公钥锁定」是更推荐的技术。 ","date":"2021-01-17","objectID":"/posts/about-tls-cert/:5:2","tags":["HTTPS","TLS","OpenSSL","加密","破解"],"title":"TLS 协议、TLS 证书、TLS 证书的配置方法、TLS 加密的破解手段","uri":"/posts/about-tls-cert/"},{"categories":["技术"],"content":"五、TLS 双向认证(Mutual TLS authentication, mTLS) TLS 协议（tls1.0+，RFC: TLS1.2 - RFC5246）中，定义了服务端请求验证客户端证书的方法。这 个方法是可选的。如果使用上这个方法，那客户端和服务端就会在 TLS 协议的握手阶段进行互相认证。这种验证方式被称为双向 TLS 认证(mTLS, mutual TLS)。 传统的「TLS 单向认证」技术，只在客户端去验证服务端是否可信。 而「TLS 双向认证（mTLS）」，则添加了服务端验证客户端是否可信的步骤（第三步）： 客户端发起请求 「验证服务端是否可信」：服务端将自己的 TLS 证书发送给客户端，客户端通过自己的 CA 证书链验证这个服务端证书。 「验证客户端是否可信」：客户端将自己的 TLS 证书发送给服务端，服务端使用它的 CA 证书链验证该客户端证书。 协商对称加密算法及密钥 使用对称加密进行后续通信。 因为相比传统的 TLS，mTLS 只是添加了「验证客户端」这样一个步骤，所以这项技术也被称为「Client Authetication」. mTLS 需要用到两套 TLS 证书： 服务端证书：这个证书签名已经介绍过了。 客户端证书：客户端证书貌似对证书信息（如 CN/SAN 域名）没有任何要求，只要证书能通过 CA 签名验证就行。 使用 openssl 生成 TLS 客户端证书（ca 和 csr.conf 可以直接使用前面生成服务端证书用到的，也可以另外生成）： # 1. 生成 2048 位 的 RSA 密钥 openssl genrsa -out client.key 2048 # 2. 通过第一步编写的配置文件，生成证书签名请求 openssl req -new -key client.key -out client.csr -config csr.conf # 3. 生成最终的证书，这里指定证书有效期 3650 天 ### 使用前面生成的 ca 证书对客户端证书进行签名（客户端和服务端共用 ca 证书） openssl x509 -req -in client.csr -CA ca.crt -CAkey ca.key \\ -CAcreateserial -out client.crt -days 3650 \\ -extensions v3_ext -extfile csr.conf mTLS 的应用场景主要在「零信任网络架构」，或者叫「无边界网络」中。 比如微服务之间的互相访问，就可以使用 mTLS。 这样就能保证每个 RPC 调用的客户端，都是其他微服务（或者别的可信方），防止黑客入侵后为所欲为。 目前查到如下几个Web服务器/代理支持 mTLS: Traefik: Docs - Client Authentication (mTLS) Nginx: Using NGINX Reverse Proxy for client certificate authentication 主要参数是两个：ssl_client_certificate /etc/nginx/client-ca.pem 和 ssl_verify_client on ","date":"2021-01-17","objectID":"/posts/about-tls-cert/:6:0","tags":["HTTPS","TLS","OpenSSL","加密","破解"],"title":"TLS 协议、TLS 证书、TLS 证书的配置方法、TLS 加密的破解手段","uri":"/posts/about-tls-cert/"},{"categories":["技术"],"content":"mTLS 的安全性 如果将 mTLS 用在 App 安全上，存在的风险是： 客户端中隐藏的证书是否可以被提取出来，或者黑客能否 Hook 进 App 中，直接使用证书发送信息。 如果客户端私钥设置了「密码（passphrase）」，那这个密码是否能很容易被逆向出来？ mTLS 和「公钥锁定/证书锁定」对比： 公钥锁定/证书锁定：只在客户端进行验证。 但是在服务端没有进行验证。这样就无法鉴别并拒绝第三方应用（爬虫）的请求。 加强安全的方法，是通过某种算法生成动态的签名。爬虫生成不出来这个签名，请求就被拒绝。 mTLS: 服务端和客户端都要验证对方。 保证双边可信，在客户端证书不被破解的情况下，就能 Ban 掉所有的爬虫或代理技术。 ","date":"2021-01-17","objectID":"/posts/about-tls-cert/:6:1","tags":["HTTPS","TLS","OpenSSL","加密","破解"],"title":"TLS 协议、TLS 证书、TLS 证书的配置方法、TLS 加密的破解手段","uri":"/posts/about-tls-cert/"},{"categories":["技术"],"content":"六 TLS 协议的破解手段 ","date":"2021-01-17","objectID":"/posts/about-tls-cert/:7:0","tags":["HTTPS","TLS","OpenSSL","加密","破解"],"title":"TLS 协议、TLS 证书、TLS 证书的配置方法、TLS 加密的破解手段","uri":"/posts/about-tls-cert/"},{"categories":["技术"],"content":"1. 客户端逆向/破解手段总结 要获取一个应用的数据，有两个方向： 服务端入侵：现代应用的服务端突破难度通常都比较客户端高，注入等漏洞底层框架就有处理。 客户端逆向+爬虫：客户端是离用户最近的地方，也是最容易被突破的地方。 mTLS 常见的破解手段，是找到老版本的安装包，发现很容易就能提取出客户端证书。。 待续 ","date":"2021-01-17","objectID":"/posts/about-tls-cert/:7:1","tags":["HTTPS","TLS","OpenSSL","加密","破解"],"title":"TLS 协议、TLS 证书、TLS 证书的配置方法、TLS 加密的破解手段","uri":"/posts/about-tls-cert/"},{"categories":["技术"],"content":"七、通过 OpenSSL 对 TLS 证书进行 CURD（增删查改） ","date":"2021-01-17","objectID":"/posts/about-tls-cert/:8:0","tags":["HTTPS","TLS","OpenSSL","加密","破解"],"title":"TLS 协议、TLS 证书、TLS 证书的配置方法、TLS 加密的破解手段","uri":"/posts/about-tls-cert/"},{"categories":["技术"],"content":"1. 查询与验证 # 查看证书(crt)信息 openssl x509 -noout -text -in server.crt # 查看证书请求(csr)信息 openssl req -noout -text -in server.csr # 查看 RSA 私钥(key)信息 openssl rsa -noout -text -in server.key # 验证证书是否可信 ## 1. 使用系统的证书链进行验证。因为是自签名证书，会验证失败 openssl verify server.crt ## 2. 使用 ca.crt 进行验证。验证成功。 openssl verify -CAfile ca.crt server.crt ","date":"2021-01-17","objectID":"/posts/about-tls-cert/:8:1","tags":["HTTPS","TLS","OpenSSL","加密","破解"],"title":"TLS 协议、TLS 证书、TLS 证书的配置方法、TLS 加密的破解手段","uri":"/posts/about-tls-cert/"},{"categories":["技术"],"content":"2. 证书格式转换 openssl 模式使用 PEM 格式的证书，这个证书格式将证书编码为 base64 格式进行保存。 另外一类使用广泛的证书，是 PKCS#12、PKCS#8，以及 Windows 上常用的 DER 格式。 # pem 格式转 pkcs12，公钥和私钥都放里面 openssl pkcs12 -export -in client.crt -inkey client.key -out client.p12 # 按提示输入保护密码 或者 p12 转 pem: openssl pkcs12 -in xxx.p12 -out xxx.crt -clcerts -nokeys openssl pkcs12 -in xxx.p12 -out xxx.key -nocerts -nodes 微信/支付宝等支付相关的数字证书，通常就使用 pkcs12 格式，使用商户号做加密密码，然后编码为 base64 再提供给用户。 ","date":"2021-01-17","objectID":"/posts/about-tls-cert/:8:2","tags":["HTTPS","TLS","OpenSSL","加密","破解"],"title":"TLS 协议、TLS 证书、TLS 证书的配置方法、TLS 加密的破解手段","uri":"/posts/about-tls-cert/"},{"categories":["技术"],"content":"参考 ","date":"2021-01-17","objectID":"/posts/about-tls-cert/:9:0","tags":["HTTPS","TLS","OpenSSL","加密","破解"],"title":"TLS 协议、TLS 证书、TLS 证书的配置方法、TLS 加密的破解手段","uri":"/posts/about-tls-cert/"},{"categories":["技术"],"content":"TLS 协议 HTTPS 温故知新（三） —— 直观感受 TLS 握手流程(上) HTTPS 温故知新（五） —— TLS 中的密钥计算 ","date":"2021-01-17","objectID":"/posts/about-tls-cert/:9:1","tags":["HTTPS","TLS","OpenSSL","加密","破解"],"title":"TLS 协议、TLS 证书、TLS 证书的配置方法、TLS 加密的破解手段","uri":"/posts/about-tls-cert/"},{"categories":["技术"],"content":"TLS 证书 Certificates - Kubernetes Docs TLS/HTTPS 证书生成与验证 ECC作为SSL/TLS证书加密算法的优势 ECC证书的生成和验签 前向保密(Forward Secrecy) - WikiPedia 证书选型和购买 - 阿里云文档 云原生安全破局｜如何管理周期越来越短的数字证书？ 另外两个关于 CN(Common Name) 和 SAN(Subject Altnative Name) 的问答： Can not get rid of net::ERR_CERT_COMMON_NAME_INVALID error in chrome with self-signed certificates SSL - How do Common Names (CN) and Subject Alternative Names (SAN) work together? 关于证书锁定/公钥锁定技术： Certificate and Public Key Pinning - OWASP Difference between certificate pinning and public key pinning 加密/签名算法相关： RSA算法原理（二） 其他： OpenSSL ManPage openssl 查看证书 ","date":"2021-01-17","objectID":"/posts/about-tls-cert/:9:2","tags":["HTTPS","TLS","OpenSSL","加密","破解"],"title":"TLS 协议、TLS 证书、TLS 证书的配置方法、TLS 加密的破解手段","uri":"/posts/about-tls-cert/"},{"categories":["技术"],"content":"QEMU/KVM 虚拟化 QEMU/KVM 是目前最流行的虚拟化技术，它基于 Linux 内核提供的 kvm 模块，结构精简，性能损失小，而且开源免费（对比收费的 vmware），因此成了大部分企业的首选虚拟化方案。 目前各大云厂商的虚拟化方案，新的服务器实例基本都是用的 KVM 技术。即使是起步最早，一直重度使用 Xen 的 AWS，从 EC2 C5 开始就改用了基于 KVM 定制的 Nitro 虚拟化技术。 但是 KVM 作为一个企业级的底层虚拟化技术，却没有对桌面使用做深入的优化，因此如果想把它当成桌面虚拟化软件来使用，替代掉 VirtualBox/VMware，有一定难度。 本文是我个人学习 KVM 的一个总结性文档，其目标是使用 KVM 作为桌面虚拟化软件。 ","date":"2021-01-17","objectID":"/posts/qemu-kvm-usage/:1:0","tags":["虚拟化","KVM","libvirt","QEMU"],"title":"QEMU-KVM 虚拟化环境的搭建与使用","uri":"/posts/qemu-kvm-usage/"},{"categories":["技术"],"content":"一、安装 QUEU/KVM QEMU/KVM 环境需要安装很多的组件，它们各司其职： qemu: 模拟各类输入输出设备（网卡、磁盘、USB端口等） qemu 底层使用 kvm 模拟 CPU 和 RAM，比软件模拟的方式快很多。 libvirt: 提供简单且统一的工具和 API，用于管理虚拟机，屏蔽了底层的复杂结构。（支持 qemu-kvm/virtualbox/vmware） ovmf: 为虚拟机启用 UEFI 支持 virt-manager: 用于管理虚拟机的 GUI 界面（可以管理远程 kvm 主机）。 virt-viewer: 通过 GUI 界面直接与虚拟机交互（可以管理远程 kvm 主机）。 dnsmasq vde2 bridge-utils openbsd-netcat: 网络相关组件，提供了以太网虚拟化、网络桥接、NAT网络等虚拟网络功能。 dnsmasq 提供了 NAT 虚拟网络的 DHCP 及 DNS 解析功能。 vde2: 以太网虚拟化 bridge-utils: 顾名思义，提供网络桥接相关的工具。 openbsd-netcat: TCP/IP 的瑞士军刀，详见 socat \u0026 netcat，这里不清楚是哪个网络组件会用到它。 安装命令： # archlinux/manjaro sudo pacman -S qemu virt-manager virt-viewer dnsmasq vde2 bridge-utils openbsd-netcat # ubuntu,参考了官方文档，但未测试 sudo apt install qemu-kvm libvirt-daemon-system virt-manager virt-viewer virtinst bridge-utils # centos,参考了官方文档，但未测试 sudo yum groupinstall \"Virtualization Host\" sudo yum install virt-manager virt-viewer virt-install # opensuse # see: https://doc.opensuse.org/documentation/leap/virtualization/html/book-virt/cha-vt-installation.html sudo yast2 virtualization # enter to terminal ui, select kvm + kvm tools, and then install it. 安装完成后，还不能直接使用，需要做些额外的工作。请继续往下走。 ","date":"2021-01-17","objectID":"/posts/qemu-kvm-usage/:2:0","tags":["虚拟化","KVM","libvirt","QEMU"],"title":"QEMU-KVM 虚拟化环境的搭建与使用","uri":"/posts/qemu-kvm-usage/"},{"categories":["技术"],"content":"1. libguestfs - 虚拟机磁盘映像处理工具 libguestfs 是一个虚拟机磁盘映像处理工具，可用于直接修改/查看/虚拟机映像、转换映像格式等。 它提供的命令列表如下： virt-df centos.img: 查看硬盘使用情况 virt-ls centos.img /: 列出目录文件 virt-copy-out -d domain /etc/passwd /tmp：在虚拟映像中执行文件复制 virt-list-filesystems /file/xx.img：查看文件系统信息 virt-list-partitions /file/xx.img：查看分区信息 guestmount -a /file/xx.qcow2(raw/qcow2都支持) -m /dev/VolGroup/lv_root --rw /mnt：直接将分区挂载到宿主机 guestfish: 交互式 shell，可运行上述所有命令。 virt-v2v: 将其他格式的虚拟机(比如 ova) 转换成 kvm 虚拟机。 virt-p2v: 将一台物理机转换成虚拟机。 学习过程中可能会使用到上述命令，提前安装好总不会有错，安装命令如下： # opensuse sudo zypper install libguestfs # archlinux/manjaro，目前缺少 virt-v2v/virt-p2v 组件 sudo pacman -S libguestfs # ubuntu sudo apt install libguestfs-tools # centos sudo yum install libguestfs-tools ","date":"2021-01-17","objectID":"/posts/qemu-kvm-usage/:2:1","tags":["虚拟化","KVM","libvirt","QEMU"],"title":"QEMU-KVM 虚拟化环境的搭建与使用","uri":"/posts/qemu-kvm-usage/"},{"categories":["技术"],"content":"2. 启动 QEMU/KVM 通过 systemd 启动 libvirtd 后台服务： sudo systemctl enable libvirtd.service sudo systemctl start libvirtd.service ","date":"2021-01-17","objectID":"/posts/qemu-kvm-usage/:2:2","tags":["虚拟化","KVM","libvirt","QEMU"],"title":"QEMU-KVM 虚拟化环境的搭建与使用","uri":"/posts/qemu-kvm-usage/"},{"categories":["技术"],"content":"3. 让非 root 用户能正常使用 kvm qumu/kvm 装好后，默认情况下需要 root 权限才能正常使用它。 为了方便使用，首先编辑文件 /etc/libvirt/libvirtd.conf: unix_sock_group = \"libvirt\"，取消这一行的注释，使 libvirt 用户组能使用 unix 套接字。 unix_sock_rw_perms = \"0770\"，取消这一行的注释，使用户能读写 unix 套接字。 然后新建 libvirt 用户组，将当前用户加入该组： newgrp libvirt sudo usermod -aG libvirt $USER 最后重启 libvirtd 服务，应该就能正常使用了： sudo systemctl restart libvirtd.service ","date":"2021-01-17","objectID":"/posts/qemu-kvm-usage/:2:3","tags":["虚拟化","KVM","libvirt","QEMU"],"title":"QEMU-KVM 虚拟化环境的搭建与使用","uri":"/posts/qemu-kvm-usage/"},{"categories":["技术"],"content":"3. 启用嵌套虚拟化 如果你需要在虚拟机中运行虚拟机（比如在虚拟机里测试 katacontainers 等安全容器技术），那就需要启用内核模块 kvm_intel 实现嵌套虚拟化。 # 临时启用 kvm_intel 嵌套虚拟化 sudo modprobe -r kvm_intel sudo modprobe kvm_intel nested=1 # 修改配置，永久启用嵌套虚拟化 echo \"options kvm-intel nested=1\" | sudo tee /etc/modprobe.d/kvm-intel.conf 验证嵌套虚拟化已经启用： $ cat /sys/module/kvm_intel/parameters/nested Y 至此，KVM 的安装就大功告成啦，现在应该可以在系统中找到 virt-manager 的图标，进去就可以使用了。 virt-manager 的使用方法和 virtualbox/vmware workstation 大同小异，这里就不详细介绍了，自己摸索摸索应该就会了。 如下内容是进阶篇，主要介绍如何通过命令行来管理虚拟机磁盘，以及 KVM。 如果你还是 kvm 新手，建议先通过图形界面 virt-manager 熟悉熟悉，再往下继续读。 ","date":"2021-01-17","objectID":"/posts/qemu-kvm-usage/:2:4","tags":["虚拟化","KVM","libvirt","QEMU"],"title":"QEMU-KVM 虚拟化环境的搭建与使用","uri":"/posts/qemu-kvm-usage/"},{"categories":["技术"],"content":"二、虚拟机磁盘映像管理 这需要用到两个工具： libguestfs: 虚拟机磁盘映像管理工具，前面介绍过了 qemu-img: qemu 的磁盘映像管理工具，用于创建磁盘、扩缩容磁盘、生成磁盘快照、查看磁盘信息、转换磁盘格式等等。 # 创建磁盘 qemu-img create -f qcow2 -o cluster_size=128K virt_disk.qcow2 20G # 扩容磁盘 qemu-img resize ubuntu-server-cloudimg-amd64.img 30G # 查看磁盘信息 qemu-img info ubuntu-server-cloudimg-amd64.img # 转换磁盘格式 qemu-img convert -f raw -O qcow2 vm01.img vm01.qcow2 # raw =\u003e qcow2 qemu-img convert -f qcow2 -O raw vm01.qcow2 vm01.img # qcow2 =\u003e raw ","date":"2021-01-17","objectID":"/posts/qemu-kvm-usage/:3:0","tags":["虚拟化","KVM","libvirt","QEMU"],"title":"QEMU-KVM 虚拟化环境的搭建与使用","uri":"/posts/qemu-kvm-usage/"},{"categories":["技术"],"content":"1. 导入 vmware 镜像 直接从 vmware ova 文件导入 kvm，这种方式转换得到的镜像应该能直接用（网卡需要重新配置）： virt-v2v -i ova centos7-test01.ova -o local -os /vmhost/centos7-01 -of qcow2 也可以先从 ova 中解压出 vmdk 磁盘映像，将 vmware 的 vmdk 文件转换成 qcow2 格式，然后再导入 kvm（网卡需要重新配置）： # 转换映像格式 qemu-img convert -p -f vmdk -O qcow2 centos7-test01-disk1.vmdk centos7-test01.qcow2 # 查看转换后的映像信息 qemu-img info centos7-test01.qcow2 直接转换 vmdk 文件得到的 qcow2 镜像，启会报错，比如「磁盘无法挂载」。 根据 Importing Virtual Machines and disk images - ProxmoxVE Docs 文档所言，需要在网上下载安装 MergeIDE.zip 组件， 另外启动虚拟机前，需要将硬盘类型改为 IDE，才能解决这个问题。 ","date":"2021-01-17","objectID":"/posts/qemu-kvm-usage/:3:1","tags":["虚拟化","KVM","libvirt","QEMU"],"title":"QEMU-KVM 虚拟化环境的搭建与使用","uri":"/posts/qemu-kvm-usage/"},{"categories":["技术"],"content":"2. 导入 img 镜像 img 镜像文件，就是所谓的 raw 格式镜像，也被称为裸镜像，IO 速度比 qcow2 快，但是体积大，而且不支持快照等高级特性。 如果不追求 IO 性能的话，建议将它转换成 qcow2 再使用。 qemu-img convert -f raw -O qcow2 vm01.img vm01.qcow2 ","date":"2021-01-17","objectID":"/posts/qemu-kvm-usage/:3:2","tags":["虚拟化","KVM","libvirt","QEMU"],"title":"QEMU-KVM 虚拟化环境的搭建与使用","uri":"/posts/qemu-kvm-usage/"},{"categories":["技术"],"content":"三、虚拟机管理 虚拟机管理可以使用命令行工具 virsh/virt-install，也可以使用 GUI 工具 virt-manager. GUI 很傻瓜式，就不介绍了，这里主要介绍命令行工具 virsh/virt-install 先介绍下 libvirt 中的几个概念： Domain: 指代运行在虚拟机器上的操作系统的实例 - 一个虚拟机，或者用于启动虚拟机的配置。 Guest OS: 运行在 domain 中的虚拟操作系统。 大部分情况下，你都可以把下面命令中涉及到的 domain 理解成虚拟机。 ","date":"2021-01-17","objectID":"/posts/qemu-kvm-usage/:4:0","tags":["虚拟化","KVM","libvirt","QEMU"],"title":"QEMU-KVM 虚拟化环境的搭建与使用","uri":"/posts/qemu-kvm-usage/"},{"categories":["技术"],"content":"0. 设置默认 URI virsh/virt-install/virt-viewer 等一系列 libvirt 命令，sudo virsh net-list –all 默认情况下会使用 qemu:///session 作为 URI 去连接 QEMU/KVM，只有 root 账号才会默认使用 qemu:///system. 另一方面 virt-manager 这个 GUI 工具，默认也会使用 qemu:///system 去连接 QEMU/KVM（和 root 账号一致） qemu:///system 是系统全局的 qemu 环境，而 qemu:///session 的环境是按用户隔离的。 另外 qemu:///session 没有默认的 network，创建虚拟机时会出毛病。。。 因此，你需要将默认的 URI 改为 qemu:///system，否则绝对会被坑: echo 'export LIBVIRT_DEFAULT_URI=\"qemu:///system\"' \u003e\u003e ~/.bashrc ","date":"2021-01-17","objectID":"/posts/qemu-kvm-usage/:4:1","tags":["虚拟化","KVM","libvirt","QEMU"],"title":"QEMU-KVM 虚拟化环境的搭建与使用","uri":"/posts/qemu-kvm-usage/"},{"categories":["技术"],"content":"1. 虚拟机网络 qemu-kvm 安装完成后，qemu:///system 环境中默认会创建一个 default 网络，而 qemu:///session 不提供默认的网络，需要手动创建。 我们通常使用 qemu:///system 环境就好，可以使用如下方法查看并启动 default 网络，这样后面创建虚拟机时才有网络可用。 # 列出所有虚拟机网络 $ sudo virsh net-list --all Name State Autostart Persistent ---------------------------------------------- default inactive no yes # 启动默认网络 $ virsh net-start default Network default started # 将 default 网络设为自启动 $ virsh net-autostart --network default Network default marked as autostarted # 再次检查网络状况，已经是 active 了 $ sudo virsh net-list --all Name State Autostart Persistent -------------------------------------------- default active yes yes 也可以创建新的虚拟机网络，这需要手动编写网络的 xml 配置，然后通过 virsh net-define --file my-network.xml 创建，这里就不详细介绍了，因为暂时用不到… ","date":"2021-01-17","objectID":"/posts/qemu-kvm-usage/:4:2","tags":["虚拟化","KVM","libvirt","QEMU"],"title":"QEMU-KVM 虚拟化环境的搭建与使用","uri":"/posts/qemu-kvm-usage/"},{"categories":["技术"],"content":"2. 创建虚拟机 - virt-intall # 使用 iso 镜像创建全新的 proxmox 虚拟机，自动创建一个 60G 的磁盘。 virt-install --virt-type kvm \\ --name pve-1 \\ --vcpus 4 --memory 8096 \\ --disk size=60 \\ --network network=default,model=virtio \\ --os-type linux \\ --os-variant generic \\ --graphics vnc \\ --cdrom proxmox-ve_6.3-1.iso # 使用已存在的 opensuse cloud 磁盘创建虚拟机 virt-install --virt-type kvm \\ --name opensuse15-2 \\ --vcpus 2 --memory 2048 \\ --disk opensuse15.2-openstack.qcow2,device=disk,bus=virtio \\ --disk seed.iso,device=cdrom \\ --os-type linux \\ --os-variant opensuse15.2 \\ --network network=default,model=virtio \\ --graphics vnc \\ --import 其中的 --os-variant 用于设定 OS 相关的优化配置，官方文档强烈推荐设定，其可选参数可以通过 osinfo-query os 查看。 ","date":"2021-01-17","objectID":"/posts/qemu-kvm-usage/:4:3","tags":["虚拟化","KVM","libvirt","QEMU"],"title":"QEMU-KVM 虚拟化环境的搭建与使用","uri":"/posts/qemu-kvm-usage/"},{"categories":["技术"],"content":"3. 虚拟机管理 - virsh 虚拟机创建好后，可使用 virsh 管理虚拟机： 查看虚拟机列表： # 查看正在运行的虚拟机 virsh list # 查看所有虚拟机，包括 inactive 的虚拟机 virsh list --all 使用 virt-viewer 以 vnc 协议登入虚拟机终端： # 使用虚拟机 ID 连接 virt-viewer 8 # 使用虚拟机名称连接，并且等待虚拟机启动 virt-viewer --wait opensuse15 启动、关闭、暂停(休眠)、重启虚拟机： virsh start opensuse15 virsh suuspend opensuse15 virsh resume opensuse15 virsh reboot opensuse15 # 优雅关机 virsh shutdown opensuse15 # 强制关机 virsh destroy opensuse15 # 启用自动开机 virsh autostart opensuse15 # 禁用自动开机 virsh autostart --disable opensuse15 虚拟机快照管理： # 列出一个虚拟机的所有快照 virsh snapshot-list --domain opensuse15 # 给某个虚拟机生成一个新快照 virsh snapshot-create \u003cdomain\u003e # 使用快照将虚拟机还原 virsh snapshot-restore \u003cdomain\u003e \u003csnapshotname\u003e # 删除快照 virsh snapshot-delete \u003cdomain\u003e \u003csnapshotname\u003e 删除虚拟机： virsh undefine opensuse15 迁移虚拟机： # 使用默认参数进行离线迁移，将已关机的服务器迁移到另一个 qemu 实例 virsh migrate 37 qemu+ssh://tux@jupiter.example.com/system # 还支持在线实时迁移，待续 cpu/内存修改： # 改成 4 核 virsh setvcpus opensuse15 4 # 改成 4G virsh setmem opensuse15 4096 虚拟机监控： # 待续 修改磁盘、网络及其他设备： # 添加新设备 virsh attach-device virsh attach-disk virsh attach-interface # 删除设备 virsh detach-disk virsh detach-device virsh detach-interface ","date":"2021-01-17","objectID":"/posts/qemu-kvm-usage/:4:4","tags":["虚拟化","KVM","libvirt","QEMU"],"title":"QEMU-KVM 虚拟化环境的搭建与使用","uri":"/posts/qemu-kvm-usage/"},{"categories":["技术"],"content":"参考 Virtualization Guide - OpenSUSE Complete Installation of KVM, QEMU and Virt Manager on Arch Linux and Manjaro virtualization-libvirt - ubuntu docs RedHat Docs - KVM ","date":"2021-01-17","objectID":"/posts/qemu-kvm-usage/:5:0","tags":["虚拟化","KVM","libvirt","QEMU"],"title":"QEMU-KVM 虚拟化环境的搭建与使用","uri":"/posts/qemu-kvm-usage/"},{"categories":null,"content":"关于我 有很多的绝望，但也有美的时刻，只不过在美的时刻，时间是不同于以往的。 ──《刺猬的优雅》 昵称：中文昵称「於清樂」「二花」，英文 ID「ryan4yin」「ryan_yin」 音乐：喜欢听后摇、蓝草、民谣、器乐 运动爱好：乐器方面喜欢玩竹笛和口琴，另外想要学习键盘；运动方面喜欢轮滑以及游泳。但是目前全都是半吊子哈哈~ 书籍：读得最多的正经书是技术书籍，另外也喜欢看科幻，以及戒不掉的网文/轻小说 影视：看得最多的是动漫，另外就是欧美科幻片、温情片（This Is Us） 中文输入方案：小鹤音形 专业：大学学的是声学，没错就是初中物理课上敲音叉的那个声学。本人专业知识战五渣，就不展开了hhh 自然语言 English: Good at reading technical articles, but weak in writing, listening and speaking. 中文：母语，高中语文中等水准。希望能学会用中文写小说，就先从短篇开始吧。 编程语言 Python: 目前的主力，也是我最熟悉的语言。 Go: 学习中，云原生圈子里最流行的语言。 Rust: 学习中，大量函数式的语法糖，贴心的编译器提示，感觉很好用。 Lua: 为了搞 Openresty/APISIX 网关，也在学习中 C: 勉强能看懂代码，荒废比较久了。在学习 Nginx/Linux 的过程中慢慢补吧… 曾经使用过但已经荒废的语言：Java/Julia/Mathematica 职业：SRE 维护与优化计算平台及流量链路，稳定高效地支撑快速成长的业务。 工具或技术：Linux/Kubernetes/Istio/APISIX ","date":"2021-01-16","objectID":"/about/:1:0","tags":null,"title":"关于","uri":"/about/"},{"categories":null,"content":"关于此博客 “对我来说，博客首先是一种知识管理工具，其次才是传播工具。我的技术文章，主要用来整理我还不懂的知识。我只写那些我还没有完全掌握的东西，那些我精通的东西，往往没有动力写。炫耀从来不是我的动机，好奇才是。\" ──阮一峰 博客时间线 2016-06-17：在博客园 - https://www.cnblogs.com/kirito-c/ 2021-01-16：开设独立博客 https://ryan4yin.space 博客快照 博客园快照\" 博客园快照 学习技术时，边学边做笔记是我的个人习惯。 偶尔有些伤春悲秋的想法，也会随便写写吐槽一下，这也是我大学写日记养成的习惯。 最开始是 2016 年 6 月，我开始在「博客园」上写文章。 中间有用过 hexo+next-theme 搭个人博客，但是个人博客没啥流量，它的存在貌似毫无意义，因此域名到期后就没续。 有人阅读我的文章，为它点个赞、分享或者评论交流，我才能感受到自己写的东西是有意义的，因此我又回到了「博客园」。 「博客园」的 SEO 做得很好，「博客园首页」也是一个很好的引流途径，这些结合起来，就能满足我的小小虚荣心。 大概我写的部分文章确实对别人有些价值，因此几年下来在博客园也积累了十多万访问量。 但是 2020 年通过独立博客，见识了各种各样的大佬们。大家的博客，通过一条条友链连接在一起，我觉得这非常有趣。 在这个友链串联起的世界中游览，每次总能有些新的收获。 而且个人博客的主题更丰富多彩，通过 github actions + github pages 来搭博客，也没啥成本。 恰好最近又买了一个域名 ryan4yin.space，十年有效期。 于是就有了这个小站。 ","date":"2021-01-16","objectID":"/about/:2:0","tags":null,"title":"关于","uri":"/about/"},{"categories":null,"content":"博客内容 我以前用博客园的时候，习惯将它当作笔记本使用，各色各样的笔记都直接往「博客园」里放，方便自己查阅。 偶尔有一两篇自觉写得还不错，对别人可能有帮助的，我会发布到「博客园首页」。 但是这种做法其实不好，笔记主要是给自己看的，写得不一定清楚明白，不适合直接分享出来（网络垃圾制造者…） 应该有专门的地方来放这些东西，所以我现在将所有的个人笔记，都保存在了 ryan4yin/knowledge 里面。 而这个独立博客的内容，和 ryan4yin/knowledge 应该是相辅相成的。 日常的一些笔记我仍然会记录在 ryan4yin/knowledge 里面，然后从中选择部分有意思的笔记，整理润色后，发布到这个博客，和大家分享。 另一方面，博客既是一个发声的地方，我偶尔可能也会发些自己的闲言碎语。 ","date":"2021-01-16","objectID":"/about/:2:1","tags":null,"title":"关于","uri":"/about/"},{"categories":null,"content":"注意事项 博客中的内容说到底也只是我一家之言，我只能尽量去减少错漏，但不能保证内容的正确性！ 因此请带着批判的眼光看待本博客中的任何内容。 ","date":"2021-01-16","objectID":"/about/:2:2","tags":null,"title":"关于","uri":"/about/"},{"categories":null,"content":"画外 互联网浩如烟海，这个小站偏安一隅，如果它有幸被你发现，而且其中文字对你还有些帮助，那可真是太棒了！感谢有你~ ","date":"2021-01-16","objectID":"/about/:3:0","tags":null,"title":"关于","uri":"/about/"},{"categories":null,"content":" 感谢 @芝士部落格 提供了友链页面模板~ 在友链形成的网络中漫游，是一件很有意思的事情。 以前的人们通过信笺交流，而我们通过友链串联起一个「世界」。希望你我都能在这个「世界」中有所收获 注： 下方友链次序每次刷新页面随机排列。 ","date":"2021-01-16","objectID":"/friends/:0:0","tags":null,"title":"我的小伙伴们","uri":"/friends/"},{"categories":null,"content":"交换友链 如果你觉得我的博客有些意思，而且也有自己的独立博客，欢迎与我交换友链~ 可通过 Issues 或者评论区提交友链申请，格式如下： 站点名称：Ryan4Yin's Space 站点地址：https://ryan4yin.space/ 个人形象：https://www.gravatar.com/avatar/2362ce7bdf2845a92240cc2f6609e001?s=240 站点描述：赞美快乐~ // 以下为样例内容，按照格式可以随意修改 var myFriends = [ [\"https://chee5e.space\", \"https://chee5e.space/images/avatar.jpg\", \"@芝士部落格\", \"有思想，也有忧伤和理想，芝士就是力量\"], [\"https://sanshiliuxiao.top/\", \"https://cdn.jsdelivr.net/gh/vensing/static@latest/avatar/sanshiliuxiao.jpg\", \"@三十六咲\", \"快走吧，趁风停止之前\"], [\"https://rea.ink/\", \"https://rea.ink/head.png\", \"@倾书\", \"清风皓月，光景常新 0) { var rndNum = Math.floor(Math.random()*myFriends.length); var friendNode = document.createElement(\"li\"); var friend_link = document.createElement(\"a\"), friend_img = document.createElement(\"img\"), friend_name = document.createElement(\"h4\"), friend_about = document.createElement(\"p\") ; friend_link.target = \"_blank\"; friend_link.href = myFriends[rndNum][0]; friend_img.src=myFriends[rndNum][1]; friend_name.innerText = myFriends[rndNum][2]; friend_about.innerText = myFriends[rndNum][3]; friend_link.appendChild(friend_img); friend_link.appendChild(friend_name); friend_link.appendChild(friend_about); friendNode.appendChild(friend_link); targetList.appendChild(friendNode); myFriends.splice(rndNum, 1); } .linkpage ul { color: rgba(255,255,255,.15) } .linkpage ul:after { content: \" \"; clear: both; display: block } .linkpage li { float: left; width: 48%; position: relative; -webkit-transition: .3s ease-out; transition: .3s ease-out; border-radius: 5px; line-height: 1.3; height: 90px; display: block } .linkpage h3 { margin: 15px -25px; padding: 0 25px; border-left: 5px solid #51aded; background-color: #f7f7f7; font-size: 25px; line-height: 40px } .linkpage li:hover { background: rgba(230,244,250,.5); cursor: pointer } .linkpage li a { padding: 0 10px 0 90px } .linkpage li a img { width: 60px; height: 60px; border-radius: 50%; position: absolute; top: 15px; left: 15px; cursor: pointer; margin: auto; border: none } .linkpage li a h4 { color: #333; font-size: 18px; margin: 0 0 7px; padding-left: 90px } .linkpage li a h4:hover { color: #51aded } .linkpage li a h4, .linkpage li a p { cursor: pointer; white-space: nowrap; text-overflow: ellipsis; overflow: hidden; line-height: 1.4; margin: 0 !important; } .linkpage li a p { font-size: 12px; color: #999; padding-left: 90px } @media(max-width: 460px) { .linkpage li { width:97% } .linkpage ul { padding-left: 5px } } ","date":"2021-01-16","objectID":"/friends/:1:0","tags":null,"title":"我的小伙伴们","uri":"/friends/"},{"categories":["技术"],"content":"Pulumi 是一个基础设施的自动管理工具，使用 Python/TypeScript/Go/Dotnet 编写好声明式的资源配置，就能实现一键创建/修改/销毁各类资源，这里的资源可以是： AWS/阿里云等云上的负载均衡、云服务器、TLS 证书、DNS、CDN、OSS、数据库…几乎所有的云上资源 本地自建的 vSphere/Kubernetes/ProxmoxVE/libvirt 环境中的虚拟机、容器等资源 相比直接调用 AWS/阿里云/Kubernetes 的 API，使用 pulumi 的好处有： 声明式配置：你只需要声明你的资源属性就 OK，所有的状态管理、异常处理都由 pulumi 完成。 统一的配置方式：提供统一的配置方法，来声明式的配置所有 AWS/阿里云/Kubernetes 资源。 声明式配置的可读性更好，更便于维护 试想一下，通过传统的手段去从零搭建一个云上测试环境、或者本地开发环境，需要手工做多少繁琐的工作。 而依靠 Pulumi 这类「基础设施即代码」的工具，只需要一行命令就能搭建好一个可复现的云上测试环境或本地开发环境。 比如我们的阿里云测试环境，包括两个 kubernetes 集群、负载均衡、VPC 网络、数据库、云监控告警/日志告警、RAM账号权限体系等等，是一个比较复杂的体系。 人工去配置这么多东西，想要复现是很困难的，非常繁琐而且容易出错。 但是使用 pulumi，只需要一行命令，就能创建并配置好这五花八门一大堆的玩意儿。 销毁整个测试环境也只需要一行命令。 实际使用体验：我们使用 Pulumi 自动化了阿里云测试环境搭建 95%+ 的操作，这个比例随着阿里云的 pulumi provider 的完善，还可以进一步提高！ ","date":"2021-01-08","objectID":"/posts/expirence-of-pulumi/:0:0","tags":["基础设施即代码","云原生","Pulumi","Terraform"],"title":"Pulumi 使用体验 - 基础设施代码化","uri":"/posts/expirence-of-pulumi/"},{"categories":["技术"],"content":"Pulumi vs Terraform 有一个「基础设施即代码」的工具比 Pulumi 更流行，它就是 Terraform. 实际上我们一开始使用的也是 Terraform，但是后来使用 Pulumi 完全重写了一遍。 主要原因是，Pulumi 解决了 Terraform 配置的一个痛点：配置语法太过简单，导致配置繁琐。而且还要额外学习一门 DSL - HCL Terraform 虽然应用广泛，但是它默认使用的 HCL 语言太简单，表现力不够强。 这就导致在一些场景下使用 Terraform，会出现大量的重复配置。 一个典型的场景是「批量创建资源，动态生成资源参数」。比如批量创建一批名称类似的 ECS 服务器/VPC交换机。如果使用 terraform，就会出现大量的重复配置。 改用 terraform 提供的 module 能在一定程度上实现配置的复用，但是它还是解决不了问题。 要使用 module，你需要付出时间去学习 module 的概念，为了拼接参数，你还需要学习 HCL 的一些高级用法。 但是付出了这么多，最后写出的 module 还是不够灵活——它被 HCL 局限住了。 为了实现如此的参数化动态化，我们不得不引入 Python 等其他编程语言。于是构建流程就变成了： 借助 Python 等其他语言先生成出 HCL 配置 通过 terraform 命令行进行 plan 与 apply 通过 Python 代码解析 terraform.tfstat，获取 apply 结果，再进行进一步操作。 这显然非常繁琐，主要困难就在于 Python 和 Terraform 之间的交互。 进一步思考，既然其他编程语言如 Python/Go 的引入不可避免，那是不是能使用它们彻底替代掉 HCL 呢？能不能直接使用 Python/Go 编写配置？如果 Terraform 原生就支持 Python/Go 来编写配置，那就不存在交互问题了。 相比于使用领域特定语言 HCL，使用通用编程语言编写配置，好处有： Python/Go/TypeScript 等通用的编程语言，能满足你的一切需求。 作为一个开发人员/DevOps，你应该对 Python/Go 等语言相当熟悉，可以直接利用上已有的经验。 更方便测试：可以使用各编程语言中流行的测试框架来测试 pulumi 配置！ 于是 Pulumi 横空出世。 另一个和 Pulumi 功能类似的工具，是刚出炉没多久的 terraform-cdk，但是目前它还很不成熟。 ","date":"2021-01-08","objectID":"/posts/expirence-of-pulumi/:1:0","tags":["基础设施即代码","云原生","Pulumi","Terraform"],"title":"Pulumi 使用体验 - 基础设施代码化","uri":"/posts/expirence-of-pulumi/"},{"categories":["技术"],"content":"Pulumi 特点介绍 原生支持通过 Python/Go/TypeScript/Dotnet 等语言编写配置，也就完全解决了上述的 terraform 和 python 的交互问题。 pulumi 是目前最流行的 真-IaaS 工具，对各语言的支持都很成熟。 兼容 terraform 的所有 provider，只是需要自行使用 pulumi-tf-provider-boilerplate 重新打包，有些麻烦。 pulumi 官方的 provider 几乎全都是封装的 terraform provider，包括 aws/azure/alicloud，目前只发现 kubernetes 是原生的（独苗啊）。 状态管理和 secrets 管理有如下几种选择： 使用 app.pulumi.com（默认）:免费版提供 stack 历史管理，可以看到所有的历史记录。另外还提供一个资源关系的可视化面板。总之很方便，但是多人合作就需要收费。 本地文件存储：pulumi login file:///app/data 云端对象存储，支持 s3 等对象存储协议，因此可以使用 AWS 或者本地的 MinIO 来做 Backend. pulumi login 's3://\u003cbucket-path\u003e?endpoint=my.minio.local:8080\u0026disableSSL=true\u0026s3ForcePathStyle=true' minio/aws 的 creadential 可以通过 AWS_ACCESS_KEY_ID 和 AWS_SECRET_ACCESS_KEY 两个环境变量设置。另外即使是使用 MinIO，AWS_REGION 这个没啥用的环境变量也必须设置！否则会报错。 gitlab 13 支持 Terraform HTTP State 协议，等这个 pr 合并，pulumi 也能以 gitlab 为 backend 了。 使用 pulumi 企业版（自建服务）：比 app.pulumi.com 提供更多的特性，但是显然是收费的。。 总之，非常香，强烈推荐各位 DevOps 试用。 以下内容是我对 pulumi 的一些思考，以及使用 pulumi 遇到的各种问题+解决方法，适合对 pulumi 有一定了解的同学阅读。 如果你刚接触 Pulumi 而且有兴趣学习，建议先移步 pulumi get started 入个门，再接着看下面的内容。 ","date":"2021-01-08","objectID":"/posts/expirence-of-pulumi/:2:0","tags":["基础设施即代码","云原生","Pulumi","Terraform"],"title":"Pulumi 使用体验 - 基础设施代码化","uri":"/posts/expirence-of-pulumi/"},{"categories":["技术"],"content":"使用建议 建议查看对应的 terraform provider 文档：pulumi 的 provider 基本都是封装的 terraform 版本，而且文档是自动生成的，比（简）较（直）难（一）看（坨）懂（shi），examples 也少。 stack: pulumi 官方提供了两种 stack 用法：「单体」和「微-stack」 单体: one stack rule them all，通过 stack 参数来控制步骤。stack 用来区分环境 dev/pro 等。 微-stack: 每一个 stack 是一个步骤，所有 stack 组成一个完整的项目。 实际使用中，我发现「微-stack」模式需要使用到 pulumi 的 inter-stack dependencies，报一堆的错，而且不够灵活。因此目前更推荐「单体」模式。 我们最近使用 pulumi 完全重写了以前用 terraform 编写的云上配置，简化了很多繁琐的配置，也降低了我们 Python 运维代码和 terraform 之间的交互难度。 另外我们还充分利用上了 Python 的类型检查和语法检查，很多错误 IDE 都能直接给出提示，强化了配置的一致性和可维护性。 不过由于阿里云 provider 暂时还： 不支持管理 ASM 服务网格、DTS 数据传输等资源 OSS 等产品的部分参数也暂时不支持配置（比如 OSS 不支持配置图片样式、ElasticSearch 暂时不支持自动创建 7.x 版本） 不支持创建 ElasticSearch 7.x 这些问题，导致我们仍然有部分配置需要手动处理，另外一些耗时长的资源，需要单独去创建。 因此还不能实现完全的「一键」。 ","date":"2021-01-08","objectID":"/posts/expirence-of-pulumi/:3:0","tags":["基础设施即代码","云原生","Pulumi","Terraform"],"title":"Pulumi 使用体验 - 基础设施代码化","uri":"/posts/expirence-of-pulumi/"},{"categories":["技术"],"content":"常见问题 ","date":"2021-01-08","objectID":"/posts/expirence-of-pulumi/:4:0","tags":["基础设施即代码","云原生","Pulumi","Terraform"],"title":"Pulumi 使用体验 - 基础设施代码化","uri":"/posts/expirence-of-pulumi/"},{"categories":["技术"],"content":"1. Output 的用法 pulumi 通过资源之间的属性引用（Output[str]）来确定依赖关系，如果你通过自定义的属性(str)解耦了资源依赖，会导致资源创建顺序错误而创建失败。 Output[str] 是一个异步属性，类似 Future，不能被用在 pulumi 参数之外的地方！ Output[str] 提供两种方法能直接对 Output[str] 进行一些操作： Output.concat(\"http://\", domain, \"/\", path): 此方法将 str 与 Output[str] 拼接起来，返回一个新的 Output[str] 对象，可用做 pulumi 属性。 domain.apply(lambda it: print(it)): Output[str] 的 apply 方法接收一个函数。在异步获取到数据后，pulumi 会调用这个函数，把具体的数据作为参数传入。 另外 apply 也会将传入函数的返回值包装成 Output 类型返回出来。 可用于：在获取到数据后，将数据打印出来/发送到邮箱/调用某个 API 上传数据等等。 Output.all(output1, output2, ...).apply(lambda it: print(it)) 可用于将多个 output 值，拼接成一个 Output 类型，其内部的 raw 值为一个 tuple 对象 (str1, str2, ...). 官方举例：connection_string = Output.all(sql_server.name, database.name).apply(lambda args: f\"Server=tcp:{args[0]}.database.windows.net;initial catalog={args[1]}...\") ","date":"2021-01-08","objectID":"/posts/expirence-of-pulumi/:4:1","tags":["基础设施即代码","云原生","Pulumi","Terraform"],"title":"Pulumi 使用体验 - 基础设施代码化","uri":"/posts/expirence-of-pulumi/"},{"categories":["技术"],"content":"2. 如何使用多个云账号/多个 k8s 集群？ 默认情况下 pulumi 使用默认的 provider，但是 pulumi 所有的资源都有一个额外的 opts 参数，可用于设定其他 provider。 通过这个 opts，我们可以实现在一个 pulumi 项目中，使用多个云账号，或者管理多个 k8s 集群。 示例： from pulumi import get_stack, ResourceOptions, StackReference from pulumi_alicloud import Provider, oss # 自定义 provider，key/secret 通过参数设定，而不是从默认的环境变量读取。 # 可以自定义很多个 providers provider = pulumi_alicloud.Provider( \"custom-alicloud-provider\", region=\"cn-hangzhou\", access_key=\"xxx\", secret_key=\"jjj\", ) # 通过 opts，让 pulumi 使用自定义的 provider（替换掉默认的） bucket = oss.Bucket(..., opts=ResourceOptions(provider=provider)) ","date":"2021-01-08","objectID":"/posts/expirence-of-pulumi/:4:2","tags":["基础设施即代码","云原生","Pulumi","Terraform"],"title":"Pulumi 使用体验 - 基础设施代码化","uri":"/posts/expirence-of-pulumi/"},{"categories":["技术"],"content":"3. inter-stack 属性传递 这东西还没搞透，待研究。 多个 stack 之间要互相传递参数，需要通过 pulumi.export 导出属性，通过 stack.require_xxx 获取属性。 从另一个 stack 读取属性的示例： from pulumi import StackReference cfg = pulumi.Config() stack_name = pulumi.get_stack() # stack 名称 project = pulumi.get_project() infra = StackReference(f\"ryan4yin/{project}/{stack_name}\") # 这个属性在上一个 stack 中被 export 出来 vpc_id = infra.require(\"resources.vpc.id\") ","date":"2021-01-08","objectID":"/posts/expirence-of-pulumi/:4:3","tags":["基础设施即代码","云原生","Pulumi","Terraform"],"title":"Pulumi 使用体验 - 基础设施代码化","uri":"/posts/expirence-of-pulumi/"},{"categories":["技术"],"content":"4. pulumi up 被中断，或者对资源做了手动修改，会发生什么？ 强行中断 pulumi up，会导致资源进入 pending 状态，必须手动修复。 修复方法：pulumi stack export，删除 pending 资源，再 pulumi stack import 手动删除了云上资源，或者修改了一些对资源管理无影响的参数，对 pulumi 没有影响，它能正确检测到这种情况。 可以通过 pulumi refresh 手动从云上拉取最新的资源状态。 手动更改了资源之间的依赖关系（比如绑定 EIP 之类的），很可能导致 pulumi 无法正确管理资源之间的依赖。 这种情况必须先手动还原依赖关系（或者把相关资源全部手动删除掉），然后才能继续使用 pulumi。 ","date":"2021-01-08","objectID":"/posts/expirence-of-pulumi/:4:4","tags":["基础设施即代码","云原生","Pulumi","Terraform"],"title":"Pulumi 使用体验 - 基础设施代码化","uri":"/posts/expirence-of-pulumi/"},{"categories":["技术"],"content":"5. 如何手动声明资源间的依赖关系？ 有时候因为一些问题（比如 pulumi provider 功能缺失，使用了 restful api 实现部分功能），pulumi 可能无法识别到某些资源之间的依赖关系。 这时可以为资源添加 dependsOn 属性，这个属性能显式地声明依赖关系。 ","date":"2021-01-08","objectID":"/posts/expirence-of-pulumi/:4:5","tags":["基础设施即代码","云原生","Pulumi","Terraform"],"title":"Pulumi 使用体验 - 基础设施代码化","uri":"/posts/expirence-of-pulumi/"},{"categories":["技术"],"content":"6. 如何导入已经存在的资源？ 由于历史原因，我们可能有部分资源是手动创建或者由其他 IaC 工具管理的，该如何将它们纳入 pulumi 管辖呢？ 官方有提供一篇相关文档 Importing Infrastructure. 文档有提到三种资源导入的方法： 使用 pulumi import 命令，这个命令能导入资源同时自动生成对应的代码。 感觉这个命令也很适合用来做资源的配置备份，不需要对照资源手写 pulumi 代码了，好评。 批量导入资源：文档的 Bulk Import Operations 这一节介绍了如何通过 json 列出资源清单，然后使用 pulumi import -f resources.json 自动生成所有导入资源的 pulumi 代码。 ","date":"2021-01-08","objectID":"/posts/expirence-of-pulumi/:4:6","tags":["基础设施即代码","云原生","Pulumi","Terraform"],"title":"Pulumi 使用体验 - 基础设施代码化","uri":"/posts/expirence-of-pulumi/"},{"categories":["技术"],"content":"5. pulumi-kubernetes？ pulumi-kubernetes 是一条龙服务： 在 yaml 配置生成这一步，它能结合/替代掉 helm/kustomize，或者你高度自定义的 Python 脚本。 在 yaml 部署这一步，它能替代掉 argo-cd 这类 gitops 工具。 强大的状态管理，argo-cd 也有状态管理，可以对比看看。 也可以仅通过 kubernetes_pulumi 生成 yaml，再通过 argo-cd 部署，这样 pulumi_kubernetes 就仅用来简化 yaml 的编写，仍然通过 gitops 工具/kubectl 来部署。 使用 pulumi-kubernetes 写配置，要警惕逻辑和数据的混合程度。 因为 kubernetes 的配置复杂度比较高，如果动态配置比较多，很容易就会写出难以维护的 python 代码来。 渲染 yaml 的示例： from pulumi import get_stack, ResourceOptions, StackReference from pulumi_kubernetes import Provider from pulumi_kubernetes.apps.v1 import Deployment, DeploymentSpecArgs from pulumi_kubernetes.core.v1 import ( ContainerArgs, ContainerPortArgs, EnvVarArgs, PodSpecArgs, PodTemplateSpecArgs, ResourceRequirementsArgs, Service, ServicePortArgs, ServiceSpecArgs, ) from pulumi_kubernetes.meta.v1 import LabelSelectorArgs, ObjectMetaArgs provider = Provider( \"render-yaml\", render_yaml_to_directory=\"rendered\", ) deployment = Deployment( \"redis\", spec=DeploymentSpecArgs(...), opts=ResourceOptions(provider=provider), ) 如示例所示，pulumi-kubernetes 的配置是完全结构化的，比 yaml/helm/kustomize 要灵活非常多。 总之它非常灵活，既可以和 helm/kustomize 结合使用，替代掉 argocd/kubectl。 也可以和 argocd/kubectl 使用，替代掉 helm/kustomize。 具体怎么使用好？我也还在研究。 ","date":"2021-01-08","objectID":"/posts/expirence-of-pulumi/:4:7","tags":["基础设施即代码","云原生","Pulumi","Terraform"],"title":"Pulumi 使用体验 - 基础设施代码化","uri":"/posts/expirence-of-pulumi/"},{"categories":["技术"],"content":"6. 阿里云资源 replace 报错？ 阿里云有部分资源，只能创建删除，不允许修改，比如「资源组」。 对这类资源做变更时，pulumi 会直接报错：「Resources aleardy exists」， 这类资源，通常都有一个「force」参数，指示是否强制修改——即先删除再重建。 ","date":"2021-01-08","objectID":"/posts/expirence-of-pulumi/:4:8","tags":["基础设施即代码","云原生","Pulumi","Terraform"],"title":"Pulumi 使用体验 - 基础设施代码化","uri":"/posts/expirence-of-pulumi/"},{"categories":["技术"],"content":"7. 有些资源属性无法使用 pulumi 配置？ 这得看各云服务提供商的支持情况。 比如阿里云很多资源的属性，pulumi 都无法完全配置，因为 alicloud provider 的功能还不够全面。 目前我们生产环境，大概 95%+ 的东西，都可以使用 pulumi 实现自动化配置。 而其他 OSS 的高级参数、新出的 ASM 服务网格、kubernetes 的授权管理、ElasticSearch7 等资源，还是需要手动配置。 这个没办法，只能等阿里云提供支持。 ","date":"2021-01-08","objectID":"/posts/expirence-of-pulumi/:4:9","tags":["基础设施即代码","云原生","Pulumi","Terraform"],"title":"Pulumi 使用体验 - 基础设施代码化","uri":"/posts/expirence-of-pulumi/"},{"categories":["技术"],"content":"8. CI/CD 中如何使 pulumi 将状态保存到文件？ CI/CD 中我们可能会希望 pulumi 将状态保存到本地，避免连接 pulumi 中心服务器。 这一方面能加快速度，另一方面一些临时状态我们可能根本不想存储，可以直接丢弃。 方法： # 指定状态文件路径 pulumi login file://\u003cfile-path\u003e # 保存到默认位置: ~/.pulumi/credentials.json pulumi login --local # 保存到远程 S3 存储（minio/ceph 或者各类云对象存储服务，都兼容 aws 的 s3 协议） pulumi login s3://\u003cbucket-path\u003e 登录完成后，再进行 pulumi up 操作，数据就会直接保存到你设定的路径下。 ","date":"2021-01-08","objectID":"/posts/expirence-of-pulumi/:4:10","tags":["基础设施即代码","云原生","Pulumi","Terraform"],"title":"Pulumi 使用体验 - 基础设施代码化","uri":"/posts/expirence-of-pulumi/"},{"categories":["技术"],"content":"缺点 ","date":"2021-01-08","objectID":"/posts/expirence-of-pulumi/:5:0","tags":["基础设施即代码","云原生","Pulumi","Terraform"],"title":"Pulumi 使用体验 - 基础设施代码化","uri":"/posts/expirence-of-pulumi/"},{"categories":["技术"],"content":"1. 报错信息不直观 pulumi 和 terraform 都有一个缺点，就是封装层次太高了。 封装的层次很高，优点是方便了我们使用，可以使用很统一很简洁的声明式语法编写配置。 而缺点，则是出了 bug，报错信息往往不够直观，导致问题不好排查。 ","date":"2021-01-08","objectID":"/posts/expirence-of-pulumi/:5:1","tags":["基础设施即代码","云原生","Pulumi","Terraform"],"title":"Pulumi 使用体验 - 基础设施代码化","uri":"/posts/expirence-of-pulumi/"},{"categories":["技术"],"content":"2. 资源状态被破坏时，修复起来非常麻烦 在很多情况下，都可能发生资源状态被破坏的问题： 在创建资源 A，因为参数是已知的，你直接使用了常量而不是 Output。这会导致 pulumi 无法识别到依赖关系！从而创建失败，或者删除时资源状态被破坏！ 有一个 pulumi stack 一次在三台物理机上创建资源。你白天创建资源晚上删除资源，但是某一台物理机晚上会关机。这将导致 pulumi 无法查询到这台物理机上的资源状态，这个 pulumi stack 在晚上就无法使用，它会一直报错！ ","date":"2021-01-08","objectID":"/posts/expirence-of-pulumi/:5:2","tags":["基础设施即代码","云原生","Pulumi","Terraform"],"title":"Pulumi 使用体验 - 基础设施代码化","uri":"/posts/expirence-of-pulumi/"},{"categories":["技术"],"content":"常用 Provider pulumi-alicloud: 管理阿里云资源 pulumi-vault: 我这边用它来快速初始化 vault，创建与管理 vault 的所有配置。 ","date":"2021-01-08","objectID":"/posts/expirence-of-pulumi/:6:0","tags":["基础设施即代码","云原生","Pulumi","Terraform"],"title":"Pulumi 使用体验 - 基础设施代码化","uri":"/posts/expirence-of-pulumi/"},{"categories":["技术"],"content":"我创建维护的 Provider 由于 Pulumi 生态还比较小，有些 provider 只有 terraform 才有。 我为了造(方)福(便)大(自)众(己)，创建并维护了两个本地虚拟机相关的 Providers: ryan4yin/pulumi-proxmox: 目前只用来自动创建 PVE 虚拟机 可以考虑结合 kubespray/kubeadm 快速创建 k8s 集群 ryan4yin/pulumi-libvirt: 快速创建 kvm 虚拟机 可以考虑结合 kubespray/kubeadm 快速创建 k8s 集群 ","date":"2021-01-08","objectID":"/posts/expirence-of-pulumi/:7:0","tags":["基础设施即代码","云原生","Pulumi","Terraform"],"title":"Pulumi 使用体验 - 基础设施代码化","uri":"/posts/expirence-of-pulumi/"},{"categories":["技术"],"content":"openSUSE 是一个基于 RPM 的发行版，这和 RHEL/CentOS 一致。 但是它的官方包管理器是专有的 zypper，挺好用的，软件也很新。 我最近从 Manjaro 切换到了 openSUSE，发现 KDE 桌面确实比 Manjaro 更丝滑，而且社区源 OBS 体验下来比 AUR 更舒服。 尤其是容器/Kubernetes 方面，源里面的东西比 AUR 更丰富，而且是官方维护的。 本文算是对迁移流程做的一个总结。 本文以 openSUSE Tumbleweed 为基础编写，这是一个和 Manjaro/Arch 一样的滚动发行版，软件源都很新。 openSUSE 社区的大部分用户都是使用的 Tumbleweed. 它的硬件兼容性也要比 openSUSE Leap（稳定版）好——实测小米游戏本安装 Leap，休眠后 Touchpad 会失灵。 ","date":"2021-01-04","objectID":"/posts/opensuse-instruction/:0:0","tags":["openSUSE","Linux"],"title":"openSUSE 使用指南","uri":"/posts/opensuse-instruction/"},{"categories":["技术"],"content":"一、zypper 的基础命令 zypper 的源在国内比较慢，但实际上下载的时候，zypper 会智能选择最快的镜像源下载软件包，比如国内的清华源等。 但是我发现官方的源索引更新太慢，甚至经常失败。因此没办法，还是得手动设置镜像源： # 禁用原有的官方软件源 sudo zypper mr --disable repo-oss repo-non-oss repo-update repo-update-non-oss repo-debug # 添加北外镜像源 sudo zypper ar -fcg https://mirrors.bfsu.edu.cn/opensuse/tumbleweed/repo/oss/ bfsu-oss sudo zypper ar -fcg https://mirrors.bfsu.edu.cn/opensuse/tumbleweed/repo/non-oss/ bfsu-non-oss 然后就是 zypper 的常用命令： sudo zypper refresh # refresh all repos sudo zypper update # update all softwares sudo zypper search --installed-only \u003cpackage-name\u003e # 查找本地安装的程序 sudo zypper search \u003cpackage-name\u003e # 查找本地和软件源中的程序 sudo zypper install \u003cpackage-name\u003e # 安装程序 sudo zypper remove --clean-deps \u003cpackage-name\u003e # 卸载程序，注意添加 --clean-deps 或者 -u，否则不会卸载依赖项！ sudo zypper clean # 清理本地的包缓存 ","date":"2021-01-04","objectID":"/posts/opensuse-instruction/:1:0","tags":["openSUSE","Linux"],"title":"openSUSE 使用指南","uri":"/posts/opensuse-instruction/"},{"categories":["技术"],"content":"Install Softwares 这里需要用到 OBS(Open Build Service, 类似 arch 的 AUR，但是是预编译的包)，因为 OBS 东西太多了，因此不存在完整的国内镜像，平均速度大概 300kb/s。 建议有条件可以在路由器上加智能代理提速。 安装需要用到的各类软件: # 启用 Packman 仓库，使用北交镜像源 sudo zypper ar -cfp 90 'https://mirror.bjtu.edu.cn/packman/suse/openSUSE_Tumbleweed/' packman-bjtu # install video player and web browser sudo zypper install mpv ffmpeg-4 chromium firefox # install screenshot and other utils # 安装好后可以配个截图快捷键 alt+a =\u003e `flameshot gui` sudo zypper install flameshot peek nomacs # install git clang/make/cmake sudo zypper install git gcc clang make cmake # install wireshark sudo zypper install wireshark sudo gpasswd --add $USER wireshark # 将你添加到 wireshark 用户组中 ","date":"2021-01-04","objectID":"/posts/opensuse-instruction/:2:0","tags":["openSUSE","Linux"],"title":"openSUSE 使用指南","uri":"/posts/opensuse-instruction/"},{"categories":["技术"],"content":"IDE + 编程语言 # install vscode: https://en.openSUSE.org/Visual_Studio_Code sudo rpm --import https://packages.microsoft.com/keys/microsoft.asc sudo zypper addrepo https://packages.microsoft.com/yumrepos/vscode vscode sudo zypper refresh sudo zypper install code # 安装 dotnet 5: https://docs.microsoft.com/en-us/dotnet/core/install/linux-openSUSE#openSUSE-15- sudo rpm --import https://packages.microsoft.com/keys/microsoft.asc sudo zypper addrepo https://packages.microsoft.com/openSUSE/15/prod/ microsoft-prod sudo zypper refresh sudo zypper install dotnet-sdk-5.0 # 安装新版本的 go（源中的版本比较低，更建议从 go 官网下载安装） sudo zypper install go 通过 tarball/script 安装： # rustup，rust 环境管理器 curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh # jetbrains toolbox app，用于安装和管理 pycharm/idea/goland/android studio 等 IDE # 参见：https://www.jetbrains.com/toolbox-app/ # 不使用系统 python，改用 miniconda 装 python3.8 # 参考：https://github.com/ContinuumIO/docker-images/blob/master/miniconda3/debian/Dockerfile wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh -O /tmp/miniconda.sh sudo /bin/bash /tmp/miniconda.sh -b -p /opt/conda rm /tmp/miniconda.sh sudo /opt/conda/bin/conda clean -tipsy sudo ln -s /opt/conda/etc/profile.d/conda.sh /etc/profile.d/conda.sh echo \". /opt/conda/etc/profile.d/conda.sh\" \u003e\u003e ~/.bashrc echo \"conda activate base\" \u003e\u003e ~/.bashrc # miniconda 的 entrypoint 默认安装在如下目录，添加到 PATH 中 echo \"export PATH=\\$PATH:\\$HOME/.local/bin\" \u003e\u003e ~/.bashrc 接下来安装 VSCode 插件，下列是我的插件列表： 语言： python/go/rust/c#/julia/flutter xml/yaml/toml vscode proto3 ansible/terraform markdown all in one + Markdown Preview Enhanced 美化： community material theme vscode icons glasslt-vsc docker/kubernetes IntelliJ IDEA Keybindings gitlens prettier utils comment translate path intellisense svg visual studio intellicode antlr4 remote ssh + remote containers rest client vscode databases ","date":"2021-01-04","objectID":"/posts/opensuse-instruction/:2:1","tags":["openSUSE","Linux"],"title":"openSUSE 使用指南","uri":"/posts/opensuse-instruction/"},{"categories":["技术"],"content":"容器 + Kubernetes # 时髦的新容器套装: https://documentation.suse.com/sles/15-SP2/html/SLES-all/cha-podman-overview.html sudo zypper in podman kompose skopeo buildah katacontainers # 安装 kubernetes 相关工具，tumbleweed 官方仓库的包都非常新！很舒服 sudo zypper in helm k9s kubernetes-client # 本地测试目前还是 docker-compose 最方便，docker 仍有必要安装 sudo zypper in docker sudo gpasswd --add $USER docker sudo systemctl enable docker sudo systemctl start docker # 简单起见，直接用 pip 安装 docker-compose 和 podman-compose sudo pip install docker-compose podman-compose ","date":"2021-01-04","objectID":"/posts/opensuse-instruction/:2:2","tags":["openSUSE","Linux"],"title":"openSUSE 使用指南","uri":"/posts/opensuse-instruction/"},{"categories":["技术"],"content":"办公、音乐、聊天 # 添加 openSUSE_zh 源：https://build.opensuse.org/project/show/home:opensuse_zh sudo zypper addrepo 'https://download.opensuse.org/repositories/home:/opensuse_zh/openSUSE_Tumbleweed' openSUSE_zh sudo zypper refresh sudo zypper install wps-office netease-cloud-music # linux qq: https://im.qq.com/linuxqq/download.html # 虽然简陋但也够用，发送文件比 KDE Connect 要方便一些。 sudo rpm -ivh linux_qq.rpm ","date":"2021-01-04","objectID":"/posts/opensuse-instruction/:2:3","tags":["openSUSE","Linux"],"title":"openSUSE 使用指南","uri":"/posts/opensuse-instruction/"},{"categories":["技术"],"content":"安装输入法 我用的输入法是小鹤音形，首先安装 fcitx-rime: # 添加 m17n obs 源：https://build.openSUSE.org/repositories/M17N sudo zypper addrepo 'https://download.opensuse.org/repositories/M17N/openSUSE_Tumbleweed' m17n sudo zypper refresh sudo zypper install fcitx5 fcitx5-configtool fcitx5-qt5 fcitx5-rime 然后，从 http://flypy.ys168.com/ 下载最新的鼠须管（MacOS）配置文件，将解压得到的 rime 文件夹拷贝到 ~/.local/share/fcitx5/ 下： mv rime ~/.local/share/fcitx5/ 现在重启系统，在 fcitx5 配置里面添加 rime「中州韵」，就可以正常使用小鹤音形了。 ","date":"2021-01-04","objectID":"/posts/opensuse-instruction/:2:4","tags":["openSUSE","Linux"],"title":"openSUSE 使用指南","uri":"/posts/opensuse-instruction/"},{"categories":["技术"],"content":"QEMU/KVM 不得不说，openSUSE 安装 KVM 真的超方便，纯 GUI 操作： # see: https://doc.openSUSE.org/documentation/leap/virtualization/html/book-virt/cha-vt-installation.html sudo yast2 virtualization # enter to terminal ui, select kvm + kvm tools, and then install it. KVM 的详细文档参见 KVM/README.md ","date":"2021-01-04","objectID":"/posts/opensuse-instruction/:2:5","tags":["openSUSE","Linux"],"title":"openSUSE 使用指南","uri":"/posts/opensuse-instruction/"},{"categories":["技术"],"content":"KDE Connect KDE Connect 是一个 PC 手机协同工具，可以在电脑和手机之间共享剪切版、远程输入、发送文件、共享文件夹、通知同步等等。 总而言之非常好用，只要手机和 PC 处于同一个局域网就行，不需要什么数据线。 如果安装系统时选择了打开防火墙，KDE Connect 是连不上的，需要手动开放端口号： # see: https://userbase.kde.org/KDEConnect#firewalld # 还可以使用 --add-source=xx.xx.xx.xx/xx 设置 ip 白名单 sudo firewall-cmd --zone=public --permanent --add-port=1714-1764/tcp sudo firewall-cmd --zone=public --permanent --add-port=1714-1764/udp sudo systemctl restart firewalld.service 然后手机（Android）安装好 KDE Connect，就能开始享受了。 目前存在的 Bug: Android 10 禁止了后台应用读取剪切版，这导致 KDE Connect 只能从 PC 同步到手机，而无法反向同步。 如果你有 ROOT 权限，可以参考 Fix clipboard permission on Android 10 的方法，安装 ClipboardWhitelist 来打开权限。 否则，貌似就只能使用手机端的「远程输入」模块来手动传输文本了。 ","date":"2021-01-04","objectID":"/posts/opensuse-instruction/:2:6","tags":["openSUSE","Linux"],"title":"openSUSE 使用指南","uri":"/posts/opensuse-instruction/"},{"categories":["技术"],"content":"Qv2ray 代理 Qv2ray 是我用过的比较好用的 GUI 代理工具，通过插件可支持常见的所有代理协议。 # see: https://build.openSUSE.org/repositories/home:zzndb sudo zypper addrepo 'https://download.opensuse.org/repositories/home:/zzndb/openSUSE_Tumbleweed' qv2ray sudo zypper refresh sudo zypper install Qv2ray QvPlugin-Trojan QvPlugin-SS ","date":"2021-01-04","objectID":"/posts/opensuse-instruction/:2:7","tags":["openSUSE","Linux"],"title":"openSUSE 使用指南","uri":"/posts/opensuse-instruction/"},{"categories":["技术"],"content":"VPN 连接与防火墙 防火墙默认会禁用 pptp 等 vpn 协议的端口，需要手动打开. 允许使用 PPTP 协议： # 允许 gre 数据包流入网络 sudo firewall-cmd --permanent --zone=public --direct --add-rule ipv4 filter INPUT 0 -p gre -j ACCEPT sudo firewall-cmd --permanent --zone=public --direct --add-rule ipv6 filter INPUT 0 -p gre -j ACCEPT # masquerade: 自动使用 interface 地址伪装所有流量（将主机当作路由器使用，vpn 是虚拟网络，需要这个功能） sudo firewall-cmd --permanent --zone=public --add-masquerade # pptp 客户端使用固定端口 1723/tcp 通信 firewall-cmd --add-port=1723/tcp --permanent sudo firewall-cmd --reload 允许使用 wireguard 协议，此协议只使用 tcp 协议，而且可以端口号可以自定义。不过 wireguard 自身的配置文件 /etc/wireguard/xxx.conf 就能配置 iptables 参数放行相关端口，这里就不赘述了。 ","date":"2021-01-04","objectID":"/posts/opensuse-instruction/:2:8","tags":["openSUSE","Linux"],"title":"openSUSE 使用指南","uri":"/posts/opensuse-instruction/"},{"categories":["技术"],"content":"其他设置 从 Windows 带过来的习惯是单击选中文件，双击才打开，这个可以在「系统设置」-「工作空间行为」-「常规行为」-「点击行为」中修改。 ","date":"2021-01-04","objectID":"/posts/opensuse-instruction/:3:0","tags":["openSUSE","Linux"],"title":"openSUSE 使用指南","uri":"/posts/opensuse-instruction/"},{"categories":["随笔","技术"],"content":"闲言碎语 一晃一年又过去了，今年可真是魔幻的一年，口罩带了一年没能摘下来，美国疫情感染人数 1500 万。 上面这段话要是让去年的我看到了，没准都以为今年真的生化危机了hhh… 言归正传，从去年 6 月底入职，到现在有一年半了，这一年半学到的东西真的非常多，完全重塑了我的技术栈。现在我的整个技术栈，基本都是围绕着云原生这一块发展了。 ","date":"2020-12-12","objectID":"/posts/2020-summary/:1:0","tags":["总结"],"title":"2020 年年终总结","uri":"/posts/2020-summary/"},{"categories":["随笔","技术"],"content":"活动 今年也参加了几个技术沙龙，有些收获，但是没去年那么新奇了，主要是很多东西自己已经懂了hhh。大概有这么几个活动： 2019 年腾讯蓝鲸第5届运维技术沙龙：在深圳腾讯大厦参加的，点心和咖啡很棒，讲的东西里，腾讯自己分享的「研发运维一体化平台」比较有收获，我收藏了那一份 PPT Rancher - 企业云原生的探索与落地：去年参加 Rancher 的沙龙觉得很高大上，因为自己很多东西都不懂。但是今年来听，明显就感觉他们讲的很基础，对我没什么价值了。也侧面说明我确实进步了非常多哈哈。 2020 PyconChina 深圳场：额，也觉得没什么干货，好几个都是在推销自家的产品（Azure AI 平台和一个 Django 写的低代码平台），有个讲 Nix 包管理的大佬但是没讲好，后面我们就直接溜了… 另外就是，今年心血来潮买了四张 Live House 的演出票，体验下来觉得钱花得很值，给我充值了不少正能量。 景德镇文艺复兴《小歌行》：这是我超级喜欢的一个乐队，演出效果超棒！听到了完整的故事，而且见到了九三姑娘本人，太高兴了！ 徐海俏 - 游离片刻：这位歌手我之前其实没接触过，但是听了下她的《空》发现很不错很帅气，就买了。但是整场下来感觉俏俏状态不佳，有点唱不动的感觉。中场问歌迷们有没有带野格酒，末了又问深圳现在能游泳么哈哈，是个很随性的歌手。后面可能还真游泳去了。 夏小虎 - 逝年：这是个民谣歌手，以前上大学的时候听过，只有吉他和人声，其实是有些伤感的歌。因为我最近状态很好，我去之前还担心氛围不适合我。然后夏小虎说开心最重要，带了个乐队来伴奏，架子鼓就是灵魂，整个演出都因鼓点而欢快了起来。效果也非常棒！ 时光胶囊乐队：这也是一个国风后摇乐队，在一个深圳福田一个小酒吧「红糖罐上步店」演出的，比较简陋，人也不多，就四五十人的样子（出乎意料）。但是演出效果很棒，《旅途》《忆长安》《磐石》都非常好听。尤其是在这样的场合唱《我不知道你的名字》，挺有感触的。 ","date":"2020-12-12","objectID":"/posts/2020-summary/:2:0","tags":["总结"],"title":"2020 年年终总结","uri":"/posts/2020-summary/"},{"categories":["随笔","技术"],"content":"技术能力总结 今年我的工作重点有这么几个： 重构及维护 CI/CD 代码，让它能在多个产品线上复用 云上环境管理：今年熟悉了阿里云这一套东西，并且用上了自动化工具对云上环境进行管理。 一开始是使用 terraform，但是 terraform 的 hcl 语法不够灵活，最近切换到了 pulumi+python，不得不说真香。目前云上的资源及配置 95% 都完全用 pulumi 管理了（还剩大概 5% 因为各种原因，需要手动配置）。 kubernetes云原生: 今年我在这个领域的进步最大，熟悉了 k8s/istio/flagger/vault/prometheus/helm/traefik 等等。不过目前这里面大部分工具还停留在「会用」的状态。 服务器虚拟化系统从 vSphere 切换到 PVE VMware 的 vCenter 吃的资源太多，而且还不能自动扩缩容硬盘，Python SDK 也超难用。因此我在公司尝试使用 PVE 替换 vSphere 这一套，效果很不错。 PVE+pulumi/terraform+cloudinit 能实现自动化部署虚拟机，自动配置网络、账号及SSH密钥、自动扩缩容硬盘，非常方便！ 而且 PVE 不收费，去中心化，一套用下来舒服太多了。只是 pve+cloud-init 门槛稍微高一点，需要一定时间去熟悉。 CI/CD 系统：基于 Jenkins 的 CI/CD 在我司各种水土不服，小毛病不断。Jenkins 本身就存在单点故障，不适合云原生，加上 Jenkinsfile 有学习成本，而且不方便复用，我就想把 Jenkins 换掉。我在这一年里调研了大量的开源 CI/CD 工具，都各有不足。主要还是因为我们当下的 Jenkins 承载了太多的功能，已经是一个CI/CD、自动化测试、自动化运维平台了，另一方面公司后端的流水线还存在依赖关系，需要进行复杂的编排。 目前我就找到 Argo Workflows 的功能很符合我们的需求，目前正在尝试迁移一部分功能到 Argo Workflows 试用。 因为 argo 的 UI 和 jenkins 差别过大，暂定仍以 jenkins 为前端，通过 python 将任务分派给 argo 运行。这样 argo 对使用者而言是隐形的，用户体验基本上没区别。 杂事：修水电、修服务器、组装办公电脑、搬机房… ","date":"2020-12-12","objectID":"/posts/2020-summary/:3:0","tags":["总结"],"title":"2020 年年终总结","uri":"/posts/2020-summary/"},{"categories":["随笔","技术"],"content":"今年在技术方面的感受 Podman/Skopeo/Buildah/Kaniko 等技术进一步发展，正在逐渐蚕食 Docker 的地盘. kubernetes 已经弃用 docker-shim，直接对接 containerd，下一步应该是彻底切换到 CRI-O。 Istio 1.5 合并为单体架构效果很明显，各微信公众号三天两头就讲服务网格，服务网格是毋庸置疑的未来 阿里云的 OAM 进一步发展，目前阿里基于 OAM 研发的 Kubevela 致力于封装 Kubernetes 的功能，让小白也能用上 Kubernetes。而这同时还能保留 k8s 完整的能力，值得期待。 云上安全越来越引起重视了，目前 CNCF 社区上容器安全相关的项目在快速发展。包括镜像安全/安全容器(kata containers)等。 使用 Kubernetes 来管理数据库已经是大趋势，毕竟成本优势太明显了。 很多公司已经在使用 docker 运行数据库，毕竟性能没啥损失，就能方便很多。但是仍然手动搭建集群，也不使用分布式存储。 目前好像只有大厂如阿里京东才有这个实力，使用 kubernetes 和分布式存储来跑数据库。容器化的分布式存储系统维护(如 ceph)，其中的难点我目前还不是很清楚，不过无外乎性能、稳定性、故障恢复这些。 ","date":"2020-12-12","objectID":"/posts/2020-summary/:4:0","tags":["总结"],"title":"2020 年年终总结","uri":"/posts/2020-summary/"},{"categories":["随笔","技术"],"content":"明年的展望 Go 语言必须学起来，今年入门了两遍语法，但是没写过啥东西，又忘差不多了。 要进一步熟悉 k8s/istio/flagger/vault/prometheus/helm/traefik/caddy 这些工具，会用还不够，要深入底层。 深入学习计算机网络 + Linux 网络 + Kubernetes 网络！这非常重要。 学习 Podman/Docker 的底层原理，学习 katacontainers 等安全容器技术。 为 kubevela/dapr/knative 等云原生项目做一些贡献，要参与到开源中去。 掌握 Argo Workflows/tekton，将 CI/CD 搬到 k8s 上去。 学习设计模式 有机会的话，熟悉下分布式存储、分布式数据库。这方面我目前还相当欠缺。 学习 KVM 虚拟化 如果学（xian）有（de）余（dan）力（teng）的话，也可以考虑搞搞下面这几个： rust 语言：rust 通过 owner+lifetime 实现内存的智能管理，性能很高，而且编译器提示非常友好，值得一学。 机器学习/深度学习: 这个领域可是当下的大热门，可以用 python/julia 玩一玩，顺便补一补微积分线代概率论。。 《编程语言实现模式》：嗯，希望能自己造轮子，写些简单的 parser。 elixir: ruby 语法+ erlang 并发模型(actor), 如果有时间的话，也可以玩玩，了解下原生分布式的函数式语言的特点。 回看了下去年的总结，发现我 go/c# 都没学多少，设计模式也没动，python 还在原地踏步hhh… 去年的展望很多都没实现。 不过云原生这一块倒是进步很快，总体很满意今年的成果哈哈~ 明年加油！ ","date":"2020-12-12","objectID":"/posts/2020-summary/:5:0","tags":["总结"],"title":"2020 年年终总结","uri":"/posts/2020-summary/"},{"categories":["音乐"],"content":"前言 2020 年 11 月 28 日，我第一次参加 Live House，演出乐队是「景德镇文艺复兴」。 「景德镇文艺复兴」是我很喜欢的一支后摇乐队，我喜欢上这支乐队，还得从我的昵称「於清樂」说起。 17 年的时候，听了许多后摇，网易云就给我推荐「景德镇文艺复兴」的歌，如此结缘。 其中有一首歌是「满世」，后摇嘛，歌词只有四句： 月下灵鸟吟 花香无处寻 再看破土人 一满又一世 听这首歌的时候，我想起看过韩寒的《长安乱》，里面女主名叫「喜乐」，这名字里寄托着家人对她的期许——平安喜乐。 我心里也升起一个不可能实现的愿望： 希冀能于这尘世之中，享得半世清闲，一生喜乐 这愿望已然不可能实现，过往的岁月里，我有过太多苦恼，做过了太多错事；可遇见的未来，也还没到清闲享乐的时候。 那至少把昵称改成「於清樂」，提醒着自己，你有过这样一个愿望。 ","date":"2020-11-28","objectID":"/posts/jingdezhen-renaissance-band-2020-shenzhen/:1:0","tags":["景德镇文艺复兴","后摇"],"title":"「小歌行」-景德镇文艺复兴-2020巡演-深圳","uri":"/posts/jingdezhen-renaissance-band-2020-shenzhen/"},{"categories":["音乐"],"content":"演出 演出的内容是《小歌行》这张专辑，乐队通过一个自创的神话故事，将整张专辑串成了一个类似音乐剧的形式，进行演出，效果很棒！ 我用手机录下了几乎全程，因为第一次参加 Live House，又是自己这么喜欢的乐队，想要录下来，留做纪念。 视频已经上传到了 Bilibili: 录到最后手机没电了，为了留点电量刷公交车卡和门禁卡，最后一首《水码头》没有录完。（到家时真的差点刷不了门禁hhh） 好了，下面是演出的照片集锦： Live House 入口的宣传海报\" Live House 入口的宣传海报 老村长1\" 老村长1 老村长2\" 老村长2 老村长3\" 老村长3 拉小提琴的小哥哥\" 拉小提琴的小哥哥 九三舞蹈\" 九三舞蹈 小提琴伴奏\" 小提琴伴奏 九三是朝廷命官\" 九三是朝廷命官 阿弥陀佛\" 阿弥陀佛 唱\" 唱 唱\" 唱 唱\" 唱 唱\" 唱 唱\" 唱 九三最漂亮的一瞬间\" 九三最漂亮的一瞬间 九三超帅气\" 九三超帅气 结束： Live House 后台\" Live House 后台 结束鞠躬\" 结束鞠躬 大合照\" 大合照 签售： 签售\" 签售 ","date":"2020-11-28","objectID":"/posts/jingdezhen-renaissance-band-2020-shenzhen/:2:0","tags":["景德镇文艺复兴","后摇"],"title":"「小歌行」-景德镇文艺复兴-2020巡演-深圳","uri":"/posts/jingdezhen-renaissance-band-2020-shenzhen/"},{"categories":["技术"],"content":"抓包分析 抓包分析工具主要有两种： http/https 网络代理工具：mitmproxy/fiddler 都属于这一类，用于分析 http 非常方便。但是只支持 http/https，有局限性。 tcp/udp/icmp 等网络嗅探工具：tcpdump/tshark 都属于这一类，网络故障分析等场景常用。 这里主要介绍如何使用 tcpdump + wireshark 进行远程实时抓包分析。 而 mitmproxy 抓包和 wireshark 本地抓包都相当简单，就不介绍了。 P.S. tshark 是 wireshark 的命令行版本，用法 tcpdump 非常相似。 ","date":"2020-05-28","objectID":"/posts/tcpdump-and-wireshark/:0:0","tags":["网络","Wireshark","tcpdump","抓包分析"],"title":"使用 tcpdump 和 Wireshark 进行远程实时抓包分析","uri":"/posts/tcpdump-and-wireshark/"},{"categories":["技术"],"content":"一、wireshark 的基本用法 WireShark 的 UI 界面如何使用，网上能搜得到各种类型的 wireshark 演示，多看几篇博客就会了。 搜索 [xxx 协议 wireshark 抓包分析] 就能找到各种各样的演示，比如 「gRPC 协议 wireshark 抓包分析」 「WebSocket 协议 wireshark 抓包分析」 「TCP 协议 wireshark 抓包分析」 等等 主要需要介绍的，应该是 wireshark 的数据包过滤器。 wireshark 中有两种包过滤器： 捕获过滤器：在抓包的时候使用它对数据包进行过滤。 显示过滤器：对抓到的所有数据包进行过滤。 显示过滤器是最有用的，下面简要介绍下显示过滤器的语法。 可以直接通过「协议名称」进行过滤： # 只看 tcp 流量 tcp # 只看 http 流量 http # 使用感叹号（或 not）进行反向过滤 !arp # 过滤掉所有 arp 数据包 也可以通过「协议名称.协议属性」和「比较操作符（比如 ==）」进行更精确的过滤： # 通过 ip 的源地址 src 或 dst 进行过滤 ip.src==192.168.1.33 # 通过 IP 地址（ip.addr）进行过滤（匹配 ip.src 或 ip.dst） ip.addr==192.168.0.5 # 上一条过滤表达式等价于： ip.src==192.168.0.5 or ip.dst==192.168.0.5 # 通过 tcp 端口号进行过滤 tcp.port==80 tcp.port\u003e4000 # 通过 http 的 host 属性进行过滤 http.host != \"xxx.baidu.com\" # 通过 http.referer 属性进行过滤 http.referer == \"xxx.baidu.com\" # 多个过滤器之间用 and、or 进行组合 http.host != \"xxx.baidu.com\" and http.referer == \"xxx.baidu.com\" ","date":"2020-05-28","objectID":"/posts/tcpdump-and-wireshark/:1:0","tags":["网络","Wireshark","tcpdump","抓包分析"],"title":"使用 tcpdump 和 Wireshark 进行远程实时抓包分析","uri":"/posts/tcpdump-and-wireshark/"},{"categories":["技术"],"content":"二、tcpdump + ssh + wireshark 远程实时抓包 在进行远程网络抓包分析时，我们通常的做法是： 使用 tcpdump 在远程主机上抓包，保存为 pcap 文件。 将 pcap 文件拷贝到本地，使用 wireshark 对其进行分析。 但是这样做没有时效性，而且数据拷贝来去也比较麻烦。 考虑使用流的方式，在远程主机上使用 tcpdump 抓包，本地使用 wireshark 进行实时分析。 使用 ssh 协议进行流式传输的示例如下： # eth0 更换成你的机器 interface 名称，虚拟机可能是 ens33 ssh root@some.host \"tcpdump -i eth0 -l -w -\" | wireshark -k -i - 在不方便使用 ssh 协议的情况下（比如容器抓包、Android 抓包），可以考虑使用 nc(netcat) 进行数据流的转发： # 1. 远程主机抓包：将数据流通过 11111 端口暴露出去 tcpdump -i wlan0 -s0 -w - | nc -l -p 11111 # 2. 本地主机从远程主机的 11111 端口读取数据，提供给 wireshark nc \u003cremote-host\u003e 11111 | wireshark -k -S -i - 如果是抓取 Android 手机的数据，方便起见，可以通过 adb 多进行一次数据转发： # 方案一：root 手机后，将 arm 版的 tcpdump 拷贝到手机内进行抓包 # 1. 在 adb shell 里使用 tcpdump 抓 WiFi 的数据包，转发到 11111 端口 ## 需要先获取到 root 权限，将 tcpdump 拷贝到 /system/bin/ 目录下 tcpdump -i wlan0 -s0 -w - | nc -l -p 11111 # 2. 在本机使用 adb forward 将手机的 11111 端口绑定到本机(PC)的 11111 端口 adb forward tcp:11111 tcp:11111 # 3. 直接从本机(PC)的 11111 端口读取数据，提供给 wireshark nc localhost 11111 | wireshark -k -S -i - ## 通过数据转发，本机 11111 端口的数据，就是安卓手机内 tcmpdump 的 stdout 内容。 # 方案二： # 如果手机不方便 root，推荐 PC 开启 WiFi 热点，手机连上这个热点访问网络。 # 这样手机的数据就一定会走 PC，直接在 PC 上通过 wireshark 抓包就行。 # 如果你只需要简单地抓 http/https 包，请使用 fiddler/mitmproxy 如果需要对 Kubernetes 集群中的容器进行抓包，推荐直接使用 ksniff! ","date":"2020-05-28","objectID":"/posts/tcpdump-and-wireshark/:2:0","tags":["网络","Wireshark","tcpdump","抓包分析"],"title":"使用 tcpdump 和 Wireshark 进行远程实时抓包分析","uri":"/posts/tcpdump-and-wireshark/"},{"categories":["技术"],"content":"Windows 系统 另外如果你本机是 Windows 系统，要分 shell 讨论： cmd: 可以直接使用上述命令。 powershell: **PowerShell 管道对 native commands 的支持不是很好，管道两边的命令貌似是串行执行的，这会导致 wireshark 无法启动！**目前没有找到好的解决方法。。 另外如果你使用 wsl，那么可以通过如下命令使 wsl 调用 windows 中的 wireshark 进行抓包分析： # 添加软链接 sudo ln -s \"$(which wireshark.exe)\" /usr/local/bin/wireshark 添加了上述软链接后，就可以正常地在 wsl 中使用前面介绍的所有抓包指令了（包括 ksniff）。 它能正常调用 windows 中的 wireshark，数据流也能正常地通过 shell 管道传输。 ","date":"2020-05-28","objectID":"/posts/tcpdump-and-wireshark/:2:1","tags":["网络","Wireshark","tcpdump","抓包分析"],"title":"使用 tcpdump 和 Wireshark 进行远程实时抓包分析","uri":"/posts/tcpdump-and-wireshark/"},{"categories":["技术"],"content":"2. termshark: 直接通过命令行 UI 进行实时抓包分析 有的时候，远程实时抓包因为某些原因无法实现，而把 pcap 数据拷贝到本地分析又比较麻烦。 这时你可以考虑直接使用命令行版本的 wireshark UI: termshark，直接在命令行进行实时的抓包分析。 kubectl-debug 默认的调试镜像中，就自带 termshark. ","date":"2020-05-28","objectID":"/posts/tcpdump-and-wireshark/:3:0","tags":["网络","Wireshark","tcpdump","抓包分析"],"title":"使用 tcpdump 和 Wireshark 进行远程实时抓包分析","uri":"/posts/tcpdump-and-wireshark/"},{"categories":["技术"],"content":"参考 WireShark使用教程 Tracing network traffic using tcpdump and tshark Android remote sniffing using Tcpdump, nc and Wireshark 聊聊tcpdump与Wireshark抓包分析 ","date":"2020-05-28","objectID":"/posts/tcpdump-and-wireshark/:4:0","tags":["网络","Wireshark","tcpdump","抓包分析"],"title":"使用 tcpdump 和 Wireshark 进行远程实时抓包分析","uri":"/posts/tcpdump-and-wireshark/"},{"categories":["技术"],"content":" 本文基于 Istio1.5 编写测试 Istio 支持使用 JWT 对终端用户进行身份验证（Istio End User Authentication），支持多种 JWT 签名算法。 目前主流的 JWT 算法是 RS256/ES256。（请忽略 HS256，该算法不适合分布式 JWT 验证） 这里以 RSA256 算法为例进行介绍，ES256 的配置方式也是一样的。 ","date":"2020-04-06","objectID":"/posts/use-istio-for-jwt-auth/:0:0","tags":["Kubernetes","Istio","服务网格"],"title":"使用 Istio 进行 JWT 身份验证（充当 API 网关）","uri":"/posts/use-istio-for-jwt-auth/"},{"categories":["技术"],"content":"1. 介绍 JWK 与 JWKS Istio 要求提供 JWKS 格式的信息，用于 JWT 签名验证。因此这里得先介绍一下 JWK 和 JWKS. JWKS ，也就是 JWK Set，json 结构如下： { \"keys\": [ \u003cjwk-1\u003e, \u003cjwk-2\u003e, ... ]} JWKS 描述一组 JWK 密钥。它能同时描述多个可用的公钥，应用场景之一是密钥的 Rotate. 而 JWK，全称是 Json Web Key，它描述了一个加密密钥（公钥或私钥）的各项属性，包括密钥的值。 Istio 使用 JWK 描述验证 JWT 签名所需要的信息。在使用 RSA 签名算法时，JWK 描述的应该是用于验证的 RSA 公钥。 一个 RSA 公钥的 JWK 描述如下： { \"alg\": \"RS256\", # 算法「可选参数」 \"kty\": \"RSA\", # 密钥类型 \"use\": \"sig\", # 被用于签名「可选参数」 \"kid\": \"NjVBRjY5MDlCMUIwNzU4RTA2QzZFMDQ4QzQ2MDAyQjVDNjk1RTM2Qg\", # key 的唯一 id \"n\": \"yeNlzlub94YgerT030codqEztjfU_S6X4DbDA_iVKkjAWtYfPHDzz_sPCT1Axz6isZdf3lHpq_gYX4Sz-cbe4rjmigxUxr-FgKHQy3HeCdK6hNq9ASQvMK9LBOpXDNn7mei6RZWom4wo3CMvvsY1w8tjtfLb-yQwJPltHxShZq5-ihC9irpLI9xEBTgG12q5lGIFPhTl_7inA1PFK97LuSLnTJzW0bj096v_TMDg7pOWm_zHtF53qbVsI0e3v5nmdKXdFf9BjIARRfVrbxVxiZHjU6zL6jY5QJdh1QCmENoejj_ytspMmGW7yMRxzUqgxcAqOBpVm0b-_mW3HoBdjQ\", \"e\": \"AQAB\" } RSA 是基于大数分解的加密/签名算法，上述参数中，e 是公钥的模数(modulus)，n 是公钥的指数(exponent)，两个参数都是 base64 字符串。 JWK 中 RSA 公钥的具体定义参见 RSA Keys - JSON Web Algorithms (JWA) ","date":"2020-04-06","objectID":"/posts/use-istio-for-jwt-auth/:0:1","tags":["Kubernetes","Istio","服务网格"],"title":"使用 Istio 进行 JWT 身份验证（充当 API 网关）","uri":"/posts/use-istio-for-jwt-auth/"},{"categories":["技术"],"content":"2. JWK 的生成 要生成 JWK 公钥，需要先生成私钥，生成方法参见 JWT 签名算法 HS256、RS256 及 ES256 及密钥生成。 公钥不需要用上述方法生成，因为我们需要的是 JWK 格式的公钥。后面会通过 python 生成出 JWK 公钥。 上面的命令会将生成出的 RSA 私钥写入 key.pem 中，查看一下私钥内容。 ryan@RYAN-MI-DESKTOP:~/istio$ cat key.pem -----BEGIN RSA PRIVATE KEY----- MIIEpAIBAAKCAQEAt1cKkQqPh8iOv5BhKh7Rx6A2+1ldpO/jczML/0GBKu4X+lHr Y8YbJrt29jyAXlWM8vHC7tXsqgUG+WziRD0D8nhnh10XC14SeH+3mVuBqph+TqhX TWsh9gtAIbeUHJjEI4I79QK4/wquPHHIGZBQDQQnuMh6vAS3VaUYJdEIoKvUBnAy Y35kJZgyJSbrxLsEExL2zujUD/OY+/In2bq/3rFtDGNlgHyC7Gu2zXSXvfOA4O5m 9BBXOc7eEqj7PoOKNaTxLN3YcuRtgR6NIXL4KLb6oyvIzoeiprt4+9q7sc3Dnkc5 EV9kwWlEW2DHzhP6VYca0WXIIXc53U1AM3ewxwIDAQABAoIBABIKhaaqJF+XM7zU B0uuxrPfJynqrFVbqcUfQ9H1bzF7Rm7CeuhRiUBxeA5Y+8TMpFcPxT/dWzGL1xja RxWx715/zKg8V9Uth6HF55o2r/bKlLtGw3iBz1C34LKwrul1eu+HlEDS6MNoGKco BynE0qvFOedsCu/Pgv7xhQPLow60Ty1uM0AhbcPgi6yJ5ksRB1XjtEnW0t+c8yQS nU3mU8k230SdMhf4Ifud/5TPLjmXdFpyPi9uYiVdJ5oWsmMWEvekXoBnHWDDF/eT VkVMiTBorT4qn+Ax1VjHL2VOMO5ZbXEcpbIc3Uer7eZAaDQ0NPZK37IkIn9TiZ21 cqzgbCkCgYEA5enHZbD5JgfwSNWCaiNrcBhYjpCtvfbT82yGW+J4/Qe/H+bY/hmJ RRTKf0kVPdRwZzq7GphVMWIuezbOk0aFGhk/SzIveW8QpLY0FV/5xFnGNjV9AuNc xrmgVshUsyQvr1TFkbdkC6yuvNgQfXfnbEoaPsXYEMCii2zqdF5lWGUCgYEAzCR2 6g8vEQx0hdRS5d0zD2/9IRYNzfP5oK0+F3KHH2OuwlmQVIo7IhCiUgqserXNBDef hj+GNcU8O/yXLomAXG7VG/cLWRrpY8d9bcRMrwb0/SkNr0yNrkqHiWQ/PvR+2MLk viWFZTTp8YizPA+8pSC/oFd1jkZF0UhKVAREM7sCgYB5+mfxyczFopyW58ADM7uC g0goixXCnTuiAEfgY+0wwXVjJYSme0HaxscQdOOyJA1ml0BBQeShCKgEcvVyKY3g ZNixunR5hrVbzdcgKAVJaR/CDuq+J4ZHYKByqmJVkLND4EPZpWSM1Rb31eIZzw2W 5FG8UBbr/GfAdQ6GorY+CQKBgQCzWQHkBmz6VG/2t6AQ9LIMSP4hWEfOfh78q9dW MDdIO4JomtkzfLIQ7n49B8WalShGITwUbLDTgrG1neeQahsMmg6+X99nbD5JfBTV H9WjG8CWvb+ZF++NhUroSNtLyu+6LhdaeopkbQVvPwMArG62wDu6ebv8v/5MrG8o uwrUSwKBgQCxV43ZqTRnEuDlF7jMN+2JZWhpbrucTG5INoMPOC0ZVatePszZjYm8 LrmqQZHer2nqtFpyslwgKMWgmVLJTH7sVf0hS9po0+iSYY/r8e/c85UdUreb0xyT x8whrOnMMODCAqu4W/Rx1Lgf2vXIx0pZmlt8Df9i2AVg/ePR6jO3Nw== -----END RSA PRIVATE KEY----- 接下来通过 Python 编程生成 RSA Public Key 和 JWK（jwk 其实就是公钥的另一个表述形式而已）: # 需要先安装依赖: pip install jwcrypto from jwcrypto.jwk import JWK from pathlib import Path private_key = Path(\"key.pem\").read_bytes() jwk = JWK.from_pem(private_key) # 导出公钥 RSA Public Key public_key = jwk.public().export_to_pem() print(public_key) print(\"=\"*30) # 导出 JWK jwk_bytes = jwk.public().export() print(jwk_bytes) Istio 需要 JWK 进行 JWT 验证，而我们手动验证 JWT 时一般需要用到 Public Key. 方便起见，上述代码把这两个都打印了出来。内容如下： # Public Key 内容，不包含这行注释 -----BEGIN PUBLIC KEY----- MIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEAt1cKkQqPh8iOv5BhKh7R x6A2+1ldpO/jczML/0GBKu4X+lHrY8YbJrt29jyAXlWM8vHC7tXsqgUG+WziRD0D 8nhnh10XC14SeH+3mVuBqph+TqhXTWsh9gtAIbeUHJjEI4I79QK4/wquPHHIGZBQ DQQnuMh6vAS3VaUYJdEIoKvUBnAyY35kJZgyJSbrxLsEExL2zujUD/OY+/In2bq/ 3rFtDGNlgHyC7Gu2zXSXvfOA4O5m9BBXOc7eEqj7PoOKNaTxLN3YcuRtgR6NIXL4 KLb6oyvIzoeiprt4+9q7sc3Dnkc5EV9kwWlEW2DHzhP6VYca0WXIIXc53U1AM3ew xwIDAQAB -----END PUBLIC KEY----- # jwk 内容 { 'e': 'AQAB', 'kid': 'oyYwZSLCLVVPHdVp0jXIcLNpGn6dMCumlY-6wSenmFo', 'kty': 'RSA', 'n': 't1cKkQqPh8iOv5BhKh7Rx6A2-1ldpO_jczML_0GBKu4X-lHrY8YbJrt29jyAXlWM8vHC7tXsqgUG-WziRD0D8nhnh10XC14SeH-3mVuBqph-TqhXTWsh9gtAIbeUHJjEI4I79QK4_wquPHHIGZBQDQQnuMh6vAS3VaUYJdEIoKvUBnAyY35kJZgyJSbrxLsEExL2zujUD_OY-_In2bq_3rFtDGNlgHyC7Gu2zXSXvfOA4O5m9BBXOc7eEqj7PoOKNaTxLN3YcuRtgR6NIXL4KLb6oyvIzoeiprt4-9q7sc3Dnkc5EV9kwWlEW2DHzhP6VYca0WXIIXc53U1AM3ewxw' } ","date":"2020-04-06","objectID":"/posts/use-istio-for-jwt-auth/:0:2","tags":["Kubernetes","Istio","服务网格"],"title":"使用 Istio 进行 JWT 身份验证（充当 API 网关）","uri":"/posts/use-istio-for-jwt-auth/"},{"categories":["技术"],"content":"4. 测试密钥可用性 接下来在 jwt.io 中填入测试用的公钥私钥，还有 Header/Payload。一是测试公私钥的可用性，二是生成出 JWT 供后续测试 Istio JWT 验证功能的可用性。 可以看到左下角显示「Signature Verified」，成功地生成出了 JWT。后续可以使用这个 JWT 访问 Istio 网关，测试 Istio JWT 验证功能。 ","date":"2020-04-06","objectID":"/posts/use-istio-for-jwt-auth/:0:3","tags":["Kubernetes","Istio","服务网格"],"title":"使用 Istio 进行 JWT 身份验证（充当 API 网关）","uri":"/posts/use-istio-for-jwt-auth/"},{"categories":["技术"],"content":"5. 启用 Istio 的身份验证 编写 istio 配置： apiVersion:\"security.istio.io/v1beta1\"kind:\"RequestAuthentication\"metadata:name:\"jwt-example\"namespace:istio-system # istio-system 名字空间中的配置，默认情况下会应用到所有名字空间spec:selector:matchLabels:istio:ingressgatewayjwtRules:# issuer 即签发者，需要和 JWT payload 中的 iss 属性完全一致。- issuer:\"testing@secure.istio.io\"jwks:|{ \"keys\": [ { \"e\": \"AQAB\", \"kid\": \"oyYwZSLCLVVPHdVp0jXIcLNpGn6dMCumlY-6wSenmFo\", # kid 需要与 jwt header 中的 kid 完全一致。 \"kty\": \"RSA\", \"n\": \"t1cKkQqPh8iOv5BhKh7Rx6A2-1ldpO_jczML_0GBKu4X-lHrY8YbJrt29jyAXlWM8vHC7tXsqgUG-WziRD0D8nhnh10XC14SeH-3mVuBqph-TqhXTWsh9gtAIbeUHJjEI4I79QK4_wquPHHIGZBQDQQnuMh6vAS3VaUYJdEIoKvUBnAyY35kJZgyJSbrxLsEExL2zujUD_OY-_In2bq_3rFtDGNlgHyC7Gu2zXSXvfOA4O5m9BBXOc7eEqj7PoOKNaTxLN3YcuRtgR6NIXL4KLb6oyvIzoeiprt4-9q7sc3Dnkc5EV9kwWlEW2DHzhP6VYca0WXIIXc53U1AM3ewxw\" } ] } # jwks 或 jwksUri 二选其一 # jwksUri: \"http://nginx.test.local/istio/jwks.json\" 现在 kubectl apply 一下，JWT 验证就添加到全局了。 可以看到 jwtRules 是一个列表，因此可以为每个 issuers 配置不同的 jwtRule. 对同一个 issuers（jwt 签发者），可以通过 jwks 设置多个公钥，以实现JWT签名密钥的轮转。 JWT 的验证规则是： JWT 的 payload 中有 issuer 属性，首先通过 issuer 匹配到对应的 istio 中配置的 jwks。 JWT 的 header 中有 kid 属性，第二步在 jwks 的公钥列表中，中找到 kid 相同的公钥。 使用找到的公钥进行 JWT 签名验证。 ","date":"2020-04-06","objectID":"/posts/use-istio-for-jwt-auth/:0:4","tags":["Kubernetes","Istio","服务网格"],"title":"使用 Istio 进行 JWT 身份验证（充当 API 网关）","uri":"/posts/use-istio-for-jwt-auth/"},{"categories":["技术"],"content":"6. 启用 Payload 转发/Authorization 转发 默认情况下，Istio 在完成了身份验证之后，会去掉 Authorization 请求头再进行转发。 这将导致我们的后端服务获取不到对应的 Payload，无法判断 End User 的身份。 因此我们需要启用 Istio 的 Authorization 请求头的转发功能，在前述的 RequestAuthentication yaml 配置中添加一个参数就行： apiVersion:\"security.istio.io/v1beta1\"kind:\"RequestAuthentication\"metadata:name:\"jwt-example\"namespace:istio-systemspec:selector:matchLabels:istio:ingressgatewayjwtRules:- issuer:\"testing@secure.istio.io\"jwks:|{ \"keys\": [ { \"e\": \"AQAB\", \"kid\": \"oyYwZSLCLVVPHdVp0jXIcLNpGn6dMCumlY-6wSenmFo\", \"kty\": \"RSA\", \"n\": \"t1cKkQqPh8iOv5BhKh7Rx6A2-1ldpO_jczML_0GBKu4X-lHrY8YbJrt29jyAXlWM8vHC7tXsqgUG-WziRD0D8nhnh10XC14SeH-3mVuBqph-TqhXTWsh9gtAIbeUHJjEI4I79QK4_wquPHHIGZBQDQQnuMh6vAS3VaUYJdEIoKvUBnAyY35kJZgyJSbrxLsEExL2zujUD_OY-_In2bq_3rFtDGNlgHyC7Gu2zXSXvfOA4O5m9BBXOc7eEqj7PoOKNaTxLN3YcuRtgR6NIXL4KLb6oyvIzoeiprt4-9q7sc3Dnkc5EV9kwWlEW2DHzhP6VYca0WXIIXc53U1AM3ewxw\" } ] }# ===================== 添加如下参数===========================forwardOriginalToken:true# 转发 Authorization 请求头 加了转发后，流程图如下（需要 mermaid 渲染）： ","date":"2020-04-06","objectID":"/posts/use-istio-for-jwt-auth/:0:5","tags":["Kubernetes","Istio","服务网格"],"title":"使用 Istio 进行 JWT 身份验证（充当 API 网关）","uri":"/posts/use-istio-for-jwt-auth/"},{"categories":["技术"],"content":"其他问题 ","date":"2020-04-06","objectID":"/posts/use-istio-for-jwt-auth/:1:0","tags":["Kubernetes","Istio","服务网格"],"title":"使用 Istio 进行 JWT 身份验证（充当 API 网关）","uri":"/posts/use-istio-for-jwt-auth/"},{"categories":["技术"],"content":"1. AuthorizationPolicy Istio 的 JWT 验证规则，默认情况下会直接忽略不带 Authorization 请求头的流量，因此这类流量能直接进入网格内部。如果需要禁止不带 Authorization 头的流量，需要额外配置 AuthorizationPolicy 策略。 RequestsAuthentication 验证失败的请求，Istio 会返回 401 状态码。 AuthorizationPolicy 验证失败的请求，Istio 会返回 403 状态码。 这会导致在使用 AuthorizationPolicy 禁止了不带 Authorization 头的流量后，这类请求会直接被返回 403。。。在使用 RESTful API 时，这种情况可能会造成一定的问题。 ","date":"2020-04-06","objectID":"/posts/use-istio-for-jwt-auth/:1:1","tags":["Kubernetes","Istio","服务网格"],"title":"使用 Istio 进行 JWT 身份验证（充当 API 网关）","uri":"/posts/use-istio-for-jwt-auth/"},{"categories":["技术"],"content":"2. Response Headers RequestsAuthentication 不支持自定义响应头信息，这导致对于前后端分离的 Web API 而言， 一旦 JWT 失效，Istio 会直接将 401 返回给前端 Web 页面。 因为响应头中不包含 Access-Crontrol-Allow-Origin，响应将被浏览器拦截！ 这可能需要通过 EnvoyFilter 自定义响应头，添加跨域信息。 ","date":"2020-04-06","objectID":"/posts/use-istio-for-jwt-auth/:1:2","tags":["Kubernetes","Istio","服务网格"],"title":"使用 Istio 进行 JWT 身份验证（充当 API 网关）","uri":"/posts/use-istio-for-jwt-auth/"},{"categories":["技术"],"content":"参考 JSON Web Key Set Properties - Auth0 JWK - RFC7517 Sample JWT and JWKS data for demo - Istio Security End User Authentication - Istio JWTRule - Istio jwt.io - 动态生成 jwt ","date":"2020-04-06","objectID":"/posts/use-istio-for-jwt-auth/:2:0","tags":["Kubernetes","Istio","服务网格"],"title":"使用 Istio 进行 JWT 身份验证（充当 API 网关）","uri":"/posts/use-istio-for-jwt-auth/"},{"categories":["技术"],"content":" 个人笔记，观点不一定正确. 适合对 Kubernetes 有一定了解的同学。 ","date":"2020-03-29","objectID":"/posts/about-dns-protocol/:0:0","tags":["DNS","CoreDNS","网络"],"title":"Linux网络学习笔记（二）：域名解析(DNS)——以 CoreDNS 为例","uri":"/posts/about-dns-protocol/"},{"categories":["技术"],"content":"前言 最近一直在学习 Kubernetes，但是手头没有个自有域名，要测试 ingress 就比较麻烦，每次都是手动改 hosts 文件。。 今天突然想到——K8s 内部就是用 DNS 做的服务发现，我为啥不自己弄一个 DNS 服务器呢？然后所有节点的 DNS 都配成它，这样有需要时直接改这个 DNS 服务器的配置就行， 一劳永逸。 我首先想到的是 群晖/Windows Server 自带的那种自带图形化界面的 DNS 服务器，但是这俩都是平台特定的。 网上搜一圈没找到类似带 UI 的 DNS 工具，搜到的 powerdns/bind 相比 coredns 也没看出啥优势来，所以决定就用 CoreDNS，刚好熟悉一下它的具体使用。 不过讲 CoreDNS 前，我们还是先来熟悉一下 DNS 的基础概念吧。 ","date":"2020-03-29","objectID":"/posts/about-dns-protocol/:1:0","tags":["DNS","CoreDNS","网络"],"title":"Linux网络学习笔记（二）：域名解析(DNS)——以 CoreDNS 为例","uri":"/posts/about-dns-protocol/"},{"categories":["技术"],"content":"一、DNS 是个啥？ 没有写得很清楚，不适合初学。建议先通过别的资料熟悉下 DNS 基础。 DNS，即域名系统（Domain Name System），是一项负责将一个 human readable 的所谓域名，转换成一个 ip 地址的协议。 而域名的好处，有如下几项： 域名对人类更友好，可读的字符串比一串 ip 数字可好记多了。 一个域名可以对应多个 ip，可实现所谓的负载均衡。 多个域名可以对应同一个 ip，以不同的域名访问该 ip，能访问不同的应用。（通过 nginx 做代理实现） DNS 协议是一个基于 UDP 的应用层协议，它默认使用 53 端口进行通信。 应用程序通常将 DNS 解析委派给操作系统的 DNS Resolver 来执行，程序员对它几乎无感知。 DNS 虽然说一般只用来查个 ip 地址，但是它提供的记录类型还蛮多的，主要有如下几种： A 记录：它记录域名与 IPv4 地址的对应关系。目前用的最多的 DNS 记录就是这个。 AAAA 记录：它对应的是 IPv6，可以理解成新一代的 A 记录。以后会用的越来越多的。 NS 记录：记录 DNS 域对应的权威服务器域名，权威服务器域名必须要有对应的 A 记录。 通过这个记录，可以将子域名的解析分配给别的 DNS 服务器。 CNAME 记录: 记录域名与另一个域名的对应关系，用于给域名起别名。这个用得也挺多的。 MX 记录：记录域名对应的邮件服务器域名，邮件服务器的域名必须要有对应的 A 记录。 SRV 记录：SRV 记录用于提供服务发现，看名字也能知道它和 SERVICE 有关。 SRV 记录的内容有固定格式：优先级 权重 端口 目标地址，例如 0 5 5060 sipserver.example.com 主要用于企业域控(AD)、微服务发现（Kubernetes） 上述的所有 DNS 记录，都是属于将域名解析为 IP 地址，或者另一个域名，这被称做** DNS 正向解析**。 除了这个正向解析外，还有个非常冷门的**反向解析**，基本上只在设置邮件服务器时才会用到。（Kubernetes 可能也有用到） 反向解析主要的记录类型是：PTR 记录，它提供将 IP 地址反向解析为域名的功能。 而且因为域名是从右往左读的（最右侧是根, www.baidu.com.），而 IP 的网段（如 192.168.0.0/16）刚好相反，是左边优先。 因此 PTR 记录的“域名”必须将 IP 地址反着写，末尾再加上 .in-addr.arpa. 表示这是一个反向解析的域名。（ipv6 使用 ip6.arpa.） 拿 baidu.com 的邮件服务器测试一下： 其他还有些 TXT、CAA 等奇奇怪怪的记录，就用到的时候自己再查了。 ","date":"2020-03-29","objectID":"/posts/about-dns-protocol/:2:0","tags":["DNS","CoreDNS","网络"],"title":"Linux网络学习笔记（二）：域名解析(DNS)——以 CoreDNS 为例","uri":"/posts/about-dns-protocol/"},{"categories":["技术"],"content":"二、域名的分层结构 国际域名系统被分成四层： 根域（Root Zone）：所有域名的根。 根域名服务器负责解析顶级域名，给出顶级域名的 DNS 服务器地址。 全世界仅有十三组根域名服务器，这些服务器的 ip 地址基本不会变动。 它的域名是 “\"，空字符串。而它的**全限定域名（FQDN）**是 .，因为 FQDN 总是以 . 结尾。（FQDN 在后面解释，可暂时忽略） 顶级域（Top Level Domains, TLD）：.com .cn 等国际、国家级的域名 顶级域名服务器负责解析次级域名，给出次级域名的 DNS 服务器地址。 每个顶级域名都对应各自的服务器，它们之间是完全独立的。.cn 的域名解析仅由 .cn 顶级域名服务器提供。 目前国际 DNS 系统中已有上千个 TLD，包括中文「.我爱你」甚至藏文域名，详细列表参见 IANA TLD 数据库 除了国际可用的 TLD，还有一类类似「内网 IP 地址」的“私有 TLD”，最常见的比如 xxx.local xxx.lan，被广泛用在集群通信中。后面详细介绍 次级域（Second Level Domains）：这个才是个人/企业能够买到的域名，比如 baidu.com 每个次级域名都有一到多个权威 DNS 服务器，这些 DNS 服务器会以 NS 记录的形式保存在对应的顶级域名（TLD）服务器中。 权威域名服务器则负责给出最终的解析结果：ip 地址(A 记录 )，另一个域名（CNAME 记录）、另一个 DNS 服务器（NS 记录）等。 子域（Sub Domians）：*.baidu.com 统统都是 baidu.com 的子域。 每一个子域都可以有自己独立的权威 DNS 服务器，这通过在子域中添加 NS 记录实现。 普通用户通常是通过域名提供商如阿里云购买的次级域名，接下来我们以 rea.ink 为例介绍域名的购买到可用的整个流程。 域名的购买与使用流程： 你在某域名提供商处购买了一个域名 rea.ink 域名提供商向 .ink 对应的顶级域名服务器中插入一条以上的 NS 记录，指向它自己的次级 DNS 服务器，如 dns25.hichina.com. 阿里云会向 TLD 中插入几条 NS 记录，指向阿里云的次级 DNS 服务器（如 vip1.alidns.com）。 你在该域名提供商的 DNS 管理界面中添加 A 记录，值为你的服务器 IP。 OK 现在 ping 一下 rea.ink，就会发现它已经解析到你自己的服务器了。 上述流程中忽略了我大天朝的特殊国情——备案，勿介意。 ","date":"2020-03-29","objectID":"/posts/about-dns-protocol/:3:0","tags":["DNS","CoreDNS","网络"],"title":"Linux网络学习笔记（二）：域名解析(DNS)——以 CoreDNS 为例","uri":"/posts/about-dns-protocol/"},{"categories":["技术"],"content":"三、DNS 递归解析器：在浏览器中输入域名后发生了什么？ 下面的图片拷贝自 Amazon Aws 文档，它展示了在不考虑任何 DNS 缓存的情况下，一次 Web 请求的经过，详细描绘了 DNS 解析的部分。 其中的第 3 4 5 步按顺序向前面讲过的根域名服务器、顶级域名服务器、权威域名服务器发起请求，以获得下一个 DNS 服务器的信息。这很清晰。 图中当前还没介绍的部分，是紫色的 DNS Resolver(域名解析器)，也叫 Recursive DNS resolver（DNS 递归解析器）。 它本身只负责递归地请求 3 4 5 步中的上游服务器，然后把获取的最终结果返回给客户端，同时将记录缓存到本地以加快解析速度。 这个 DNS 解析器，其实就是所谓的公共 DNS 服务器：Google 的 8.8.8.8，国内著名的 114.114.114.114。 这些公共 DNS 用户量大，缓存了大量的 DNS 记录，有效地降低了上游 DNS 服务器的压力，也加快了网络上的 DNS 查询速度。 接下来使用 dig +trace baidu.com 复现一下上述的查询流程（这种情况下 dig 自己就是一个 DNS 递归解析器）： 另外前面有讲过 DNS 的反向解析，也是同样的层级结构，是从根服务器开始往下查询的，下面拿 baidu 的一个邮件服务器进行测试： dig 工具未来可能会被 drill 取代。 ","date":"2020-03-29","objectID":"/posts/about-dns-protocol/:4:0","tags":["DNS","CoreDNS","网络"],"title":"Linux网络学习笔记（二）：域名解析(DNS)——以 CoreDNS 为例","uri":"/posts/about-dns-protocol/"},{"categories":["技术"],"content":"DNS 泛解析通配符 * DNS 记录允许使用通配符 *，并且该通配符可匹配任意级数的子域！！！比如 *.example.com 就可以匹配所有的一二三四级域名等等，但是无法匹配 example.com 本身！ ","date":"2020-03-29","objectID":"/posts/about-dns-protocol/:4:1","tags":["DNS","CoreDNS","网络"],"title":"Linux网络学习笔记（二）：域名解析(DNS)——以 CoreDNS 为例","uri":"/posts/about-dns-protocol/"},{"categories":["技术"],"content":"TTL（Time To Live） 上面讲了公共 DNS 服务器通过缓存技术，降低了上游 DNS 服务器的压力，也加快了网络上的 DNS 查询速度。 可缓存总得有个过期时间吧！为了精确地控制 DNS 记录的过期时间，每条 DNS 记录都要求设置一个时间属性——TTL，单位为秒。这个时间可以自定义。 任何一条 DNS 缓存，在超过过期时间后都必须丢弃！ 另外在没超时的时候，DNS 缓存也可以被主动或者被动地刷新。 ","date":"2020-03-29","objectID":"/posts/about-dns-protocol/:4:2","tags":["DNS","CoreDNS","网络"],"title":"Linux网络学习笔记（二）：域名解析(DNS)——以 CoreDNS 为例","uri":"/posts/about-dns-protocol/"},{"categories":["技术"],"content":"四、本地 DNS 服务器与私有 DNS 域 这类服务器只在当前局域网内有效，是一个私有的 DNS 服务器，企业常用。一般通过 DHCP 或者手动配置的方式，使内网的服务器都默认使用局域网 DNS 服务器进行解析。该服务器可以只解析自己的私有 DNS 域，而将其他 DNS 域的解析 forward 到公网 DNS 解析器去。 这个私有 DNS 域，会覆盖掉公网的同名域(如果公网上有这个域的话)。 私有 dns 域也可使用公网不存在的 TLD，比如 xxx.local xxx.lan 等。vmware vcenter 就默认使用 vsphere.local 作为它的 sso (单点登录)系统的域名。kubernetes 默认使用 svc.cluster.local 作为集群内部域名。 私有 DNS 域的选择，参见 DNS 私有域的选择：internal.xxx.com xxx.local 还是 xxx.zone？ 局域网 DNS 服务器的规模与层级，视局域网的大小而定。一般小公司一个就行，要容灾设三个副本也够了。 以 CoreDNS 为例，局域网 DNS 服务器也可以被设置成一个 DNS Resolver，可以设置只转发特定域名的 DNS 解析。这叫将某个域设为「转发区域」。 ","date":"2020-03-29","objectID":"/posts/about-dns-protocol/:5:0","tags":["DNS","CoreDNS","网络"],"title":"Linux网络学习笔记（二）：域名解析(DNS)——以 CoreDNS 为例","uri":"/posts/about-dns-protocol/"},{"categories":["技术"],"content":"五、操作系统的 DNS 解析器 应用程序实际上都是调用的操作系统的 DNS Resolver 进行域名解析的。在 Linux 中 DNS Resolver 由 glibc/musl 提供，配置文件为 /etc/resolv.conf。 比如 Python 的 DNS 解析，就来自于标准库的 socket 包，这个包只是对底层 c 语言库的一个简单封装。 基本上只有专门用于网络诊断的 DNS 工具包，才会自己实现 DNS 协议。 ","date":"2020-03-29","objectID":"/posts/about-dns-protocol/:6:0","tags":["DNS","CoreDNS","网络"],"title":"Linux网络学习笔记（二）：域名解析(DNS)——以 CoreDNS 为例","uri":"/posts/about-dns-protocol/"},{"categories":["技术"],"content":"1. hosts 文件 操作系统中还有一个特殊文件：Linux 中的 /etc/hosts 和 Windows 中的 C:\\Windows\\System32\\drivers\\etc\\hosts 系统中的 DNS resolver 会首先查看这个 hosts 文件中有没有该域名的记录，如果有就直接返回了。没找到才会去查找本地 DNS 缓存、别的 DNS 服务器。 只有部分专门用于网络诊断的应用程序（e.g. dig）不会依赖 OS 的 DNS 解析器，因此这个 hosts 会失效。hosts 对于绝大部分程序都有效。 移动设备上 hosts 可能会失效，部分 app 会绕过系统，使用新兴的 HTTPDNS 协议进行 DNS 解析。 ","date":"2020-03-29","objectID":"/posts/about-dns-protocol/:6:1","tags":["DNS","CoreDNS","网络"],"title":"Linux网络学习笔记（二）：域名解析(DNS)——以 CoreDNS 为例","uri":"/posts/about-dns-protocol/"},{"categories":["技术"],"content":"2. HTTPDNS 传统的 DNS 协议因为使用了明文的 UDP 协议，很容易被劫持。顺应移动互联网的兴起，目前一种新型的 DNS 协议——HTTPDNS 应用越来越广泛，国内的阿里云腾讯云都提供了这项功能。 HTTPDNS 通过 HTTP 协议直接向权威 DNS 服务器发起请求，绕过了一堆中间的 DNS 递归解析器。好处有二： 权威 DNS 服务器能直接获取到客户端的真实 IP（而不是某个中间 DNS 递归解析器的 IP），能实现就近调度。 因为是直接与权威 DNS 服务器连接，避免了 DNS 缓存污染的问题。 HTTPDNS 协议需要程序自己引入 SDK，或者直接请求 HTTP API。 ","date":"2020-03-29","objectID":"/posts/about-dns-protocol/:6:2","tags":["DNS","CoreDNS","网络"],"title":"Linux网络学习笔记（二）：域名解析(DNS)——以 CoreDNS 为例","uri":"/posts/about-dns-protocol/"},{"categories":["技术"],"content":"3. 默认 DNS 服务器 操作系统的 DNS 解析器通常会允许我们配置多个上游 Name Servers，比如 Linux 就是通过 /etc/resolv.conf 配置 DNS 服务器的。 $ cat /etc/resolv.conf nameserver 8.8.8.8 nameserver 8.8.4.4 search lan 不过现在这个文件基本不会手动修改了，各 Linux 发行版都推出了自己的网络配置工具，由这些工具自动生成 Linux 的各种网络配置，更方便。 比如 Ubuntu 就推荐使用 netplan 工具进行网络设置。 Kubernetes 就是通过使用容器卷映射的功能，修改 /etc/resolv.conf，使集群的所有容器都使用集群 DNS 服务器（CoreDNS）进行 DNS 解析。 通过重复使用 nameserver 字段，可以指定多个 DNS 服务器（Linux 最多三个）。DNS 查询会按配置中的顺序选用 DNS 服务器。 **仅在靠前的 DNS 服务器没有响应（timeout）时，才会使用后续的 DNS 服务器！所以指定的服务器中的 DNS 记录最好完全一致！！！**不要把第一个配内网 DNS，第二个配外网！！！ ","date":"2020-03-29","objectID":"/posts/about-dns-protocol/:6:3","tags":["DNS","CoreDNS","网络"],"title":"Linux网络学习笔记（二）：域名解析(DNS)——以 CoreDNS 为例","uri":"/posts/about-dns-protocol/"},{"categories":["技术"],"content":"4. DNS 搜索域 上一小节给出的 /etc/resolv.conf 文件内容的末尾，有这样一行: search lan，它指定的，是所谓的 DNS 搜索域。 讲到 DNS 搜索域，就不得不提到一个名词：全限定域名（Full Qulified Domain Name, FQDN），即一个域名的完整名称，www.baidu.com。 一个普通的域名，有下列四种可能： www.baidu.com.: 末尾的 . 表示根域，说明 www.baidu.com 是一个 FQDN，因此不会使用搜索域！ www.baidu.com: 末尾没 .，但是域名包含不止一个 .。首先当作 FQDN 进行查询，没查找再按顺序在各搜索域中查询。 /etc/resolv.conf 的 options 参数中，可以指定域名中包含 . 的临界个数，默认是 1. local: 不包含 .，被当作 host 名称，非 FQDN。首先在 /etc/hosts 中查找，没找到的话，再按顺序在各搜索域中查找。 上述搜索顺序可以通过 host -v \u003cdomain-name\u003e 进行测试，该命令会输出它尝试过的所有 FQDN。 修改 /etc/resolv.conf 中的 search 属性并测试，然后查看输出。 就如上面说例举的，在没有 DNS 搜索域 这个东西的条件下，我们访问任何域名，都必须输入一个全限定域名 FQDN。 有了搜索域我们就可以稍微偷点懒，省略掉域名的一部分后缀，让 DNS Resolver 自己去在各搜索域中搜索。 在 Kubernetes 中就使用到了搜索域，k8s 中默认的域名 FQDN 是 service.namespace.svc.cluster.local， 但是对于 default namespace 中的 service，我们可以直接通过 service 名称查询到它的 IP。 对于其他名字空间中的 service，也可以通过 service.namespace 查询到它们的 IP，不需要给出 FQDN。 Kubernetes 中 /etc/resolv.conf 的示例如下： nameserver 10.43.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5 可以看到 k8s 设置了一系列的搜索域，并且将 . 的临界值设为了 5。 也就是少于 5 个 dots 的域名，都首先当作非 FQDN 看待，优先在搜索域里面查找。 该配置文件的详细描述参见 manpage - resolv.conf，或者在 Linux 中使用 man resolv.conf 命令查看。 ","date":"2020-03-29","objectID":"/posts/about-dns-protocol/:6:4","tags":["DNS","CoreDNS","网络"],"title":"Linux网络学习笔记（二）：域名解析(DNS)——以 CoreDNS 为例","uri":"/posts/about-dns-protocol/"},{"categories":["技术"],"content":"六、DNS 诊断的命令行工具 dig +trace baidu.com # 诊断 dns 的主要工具，非常强大 host -a baidu.com # host 基本就是 dig 的弱化版，不过 host 有个有点就是能打印出它测试过的所有 FQDN nslookup baidu.com # 和 host 没啥大差别，多个交互式查询不过一般用不到 whois baidu.com # 查询域名注册信息，内网诊断用不到 详细的使用请 man dig ","date":"2020-03-29","objectID":"/posts/about-dns-protocol/:7:0","tags":["DNS","CoreDNS","网络"],"title":"Linux网络学习笔记（二）：域名解析(DNS)——以 CoreDNS 为例","uri":"/posts/about-dns-protocol/"},{"categories":["技术"],"content":"七、CoreDNS 的使用 主流的本地 DNS 服务器中，提供 UI 界面的有 Windows DNS Server 和群晖 DNS Server，很方便，不过这两个都是操作系统绑定的。 开源的 DNS 服务器里边儿，BIND 好像是最有名的，各大 Linux 发行版自带的 dig/host/nslookup，最初都是 Bind 提供的命令行工具。 不过为了一举两得（DNS+K8s），咱还是直接学习 CoreDNS 的使用。 CoreDNS 最大的特点是灵活，可以很方便地给它编写插件以提供新功能。功能非常强大，相比传统 DNS 服务器，它非常“现代化”。在 K8s 中它被用于提供服务发现功能。 接下来以 CoreDNS 为例，讲述如何配置一个 DNS 服务器，添加私有的 DNS 记录，并设置转发规则以解析公网域名。 ","date":"2020-03-29","objectID":"/posts/about-dns-protocol/:8:0","tags":["DNS","CoreDNS","网络"],"title":"Linux网络学习笔记（二）：域名解析(DNS)——以 CoreDNS 为例","uri":"/posts/about-dns-protocol/"},{"categories":["技术"],"content":"1. 配置文件：Corefile CoreDNS 因为是 Go 语言写的，编译结果是单个可执行文件，它默认以当前文件夹下的 Corefile 为配置文件。以 kubernetes 中的 Corefile 为例： .:53 { errors # 启用错误日志 health # 启用健康检查 api ready # 启用 readiness 就绪 api # 启用 kubernetes 集群支持，详见 https://coredns.io/plugins/kubernetes/ # 此插件只处理 cluster.local 域，以及 PTR 解析 kubernetes cluster.local in-addr.arpa ip6.arpa { pods insecure upstream # fallthrough in-addr.arpa ip6.arpa # 向下传递 DNS 反向查询 ttl 30 # 过期时间 } prometheus :9153 # 启用 prometheus metrics 支持 forward . 114.114.114.114 19.29.29.29 # 将非集群域名的 DNS 请求，转发给公网 DNS 服务器。 cache 30 # 启用前端缓存，缓存的 TTL 设为 30 loop # 检测并停止死循环解析 reload # 支持动态更新 Corefile # 随机化 A/AAAA/MX 记录的顺序以实现负载均衡。 # 因为 DNS resolver 通常使用第一条记录，而第一条记录是随机的。这样客户端的请求就能被随机分配到多个后端。 loadbalance } Corefile 首先定义 DNS 域，域后的代码块内定义需要使用的各种插件。**注意这里的插件顺序是没有任何意义的！**插件的调用链是在 CoreDNS 编译时就定义好的，不能在运行时更改。 通过上述配置启动的 CoreDNS 是无状态的，它以 Kubernetes ApiServer 为数据源，CoreDNS 本身只相当于一个查询器/缓存，因此它可以很方便地扩缩容。 ","date":"2020-03-29","objectID":"/posts/about-dns-protocol/:8:1","tags":["DNS","CoreDNS","网络"],"title":"Linux网络学习笔记（二）：域名解析(DNS)——以 CoreDNS 为例","uri":"/posts/about-dns-protocol/"},{"categories":["技术"],"content":"2. 将 CoreDNS 设置成一个私有 DNS 服务器 现在清楚了 Corefile 的结构，让我们来设计一个通过文件配置 DNS 条目的 Corefile 配置： # 定义可复用 Block (common) { log errors cache loop # 检测并停止死循环解析 } # 本地开发环境的 DNS 解析 dev-env.local:53 { import common # 导入 Block file dev-env.local { # 从文件 `dev-env.local` 中读取 DNS 数据 reload 30s # 每 30s 检查一次配置的 Serial，若该值有变更则重载整个 Zone 的配置。 } } # 本地测试环境 test-env.local:53 { import common file test-env.local { reload 30s } } # 其他 .:53 { forward . 114.114.114.114 # 解析公网域名 log errors cache } 上面的 Corefile 定义了两个本地域名 dev-env.local 和 test-env.local，它们的 DNS 数据分别保存在 file 指定的文件中。 这个 file 指定的文件和 bind9 一样，都是使用在 rfc1035 中定义的 Master File 格式，dig 命令输出的就是这种格式的内容。示例如下： ;; 與整個領域相關性較高的設定包括 NS, A, MX, SOA 等標誌的設定處！ $TTL 30 @ IN SOA dev-env.local. devops.dev-env.local. ( 20200202 ; SERIAL，每次修改此文件，都应该同步修改这个“版本号”，可将它设为修改时间。 7200 ; REFRESH 600 ; RETRY 3600000 ; EXPIRE 60) ; MINIMUM @ IN NS dns1.dev-env.local. ; DNS 伺服器名稱 dns1.dev-env.local. IN A 192.168.23.2 ; DNS 伺服器 IP redis.dev-env.local. IN A 192.168.23.21 mysql.dev-env.local. IN A 192.168.23.22 elasticsearch.dev-env.local. IN A 192.168.23.23 ftp IN A 192.168.23.25 ; 這是簡化的寫法！ 详细的格式说明参见 鳥哥的 Linux 私房菜 - DNS 正解資料庫檔案的設定 test-env.local 也是一样的格式，根据上面的模板修改就行。这两个配置文件和 Corefile 放在同一个目录下： root@test-ubuntu:~/dns-server# tree . ├── coredns # coredns binary ├── Corefile ├── dev-env.local └── test-env.local 然后通过 ./coredns 启动 coredns。通过 dig 检验： 可以看到 ftp.dev-env.local 已经被成功解析了。 ","date":"2020-03-29","objectID":"/posts/about-dns-protocol/:8:2","tags":["DNS","CoreDNS","网络"],"title":"Linux网络学习笔记（二）：域名解析(DNS)——以 CoreDNS 为例","uri":"/posts/about-dns-protocol/"},{"categories":["技术"],"content":"3. 可选插件（External Plugins） CoreDNS 提供的预编译版本，不包含 External Plugins 中列出的部分，如果你需要，可以自行修改 plugin.cfg，然后手动编译。 不得不说 Go 语言的编译，比 C 语言是方便太多了。自动拉取依赖，一行命令编译！只要配好 GOPROXY，启用可选插件其实相当简单。 ","date":"2020-03-29","objectID":"/posts/about-dns-protocol/:8:3","tags":["DNS","CoreDNS","网络"],"title":"Linux网络学习笔记（二）：域名解析(DNS)——以 CoreDNS 为例","uri":"/posts/about-dns-protocol/"},{"categories":["技术"],"content":"4. 设置 DNS 集群 单台 DNS 服务器的性能是有限的，而且存在单点故障问题。因此在要求高可用或者高性能的情况下，就需要设置 DNS 集群。 虽然说 CoreDNS 本身也支持各种 DNS Zone 传输，主从 DNS 服务器等功能，不过我想最简单的，可能还是直接用 K8s。 直接用 ConfigMap 存配置，通过 Deployment 扩容就行，多方便。 要修改起来更方便，还可以启用可选插件：redis，直接把配置以 json 的形式存在 redis 里，通过 redis-desktop-manager 进行查看与修改。 ","date":"2020-03-29","objectID":"/posts/about-dns-protocol/:8:4","tags":["DNS","CoreDNS","网络"],"title":"Linux网络学习笔记（二）：域名解析(DNS)——以 CoreDNS 为例","uri":"/posts/about-dns-protocol/"},{"categories":["技术"],"content":"参考 DNS 原理入门 What Is DNS? | How DNS Works - Cloudflare What is DNS? - Amazon AWS 鸟哥的 Linux 私房菜——主機名稱控制者： DNS 伺服器 CoreDNS - Manual Kubernetes - DNS for Services and Pods Kubernetes - Customizing DNS Service ","date":"2020-03-29","objectID":"/posts/about-dns-protocol/:9:0","tags":["DNS","CoreDNS","网络"],"title":"Linux网络学习笔记（二）：域名解析(DNS)——以 CoreDNS 为例","uri":"/posts/about-dns-protocol/"},{"categories":["技术"],"content":"签名算法 介绍具体的 JWT 签名算法前，先解释一下签名、摘要/指纹、加密这几个名词的含义： 数字签名(Digital Signature):就和我们日常办理各种手续时需要在文件上签上自己的名字一样，数字签名的主要用途也是防止伪造签名。 数字摘要(digest)/数字指纹(fingerprint): 指的都是数据的 Hash 值。 加密算法：这个应该不需要解释，就是对数据进行加密。。 数字签名的具体实现，通常是先对数据进行一次 Hash 摘要(SHA1/SHA256/SHA512 等)，然后再使用非对称加密算法(RSA/ECDSA 等)的私钥对这个摘要进行加密，这样得到的结果就是原始数据的一个签名。 用户在验证数据时，只需要使用公钥解密出 Hash 摘要，然后自己再对数据进行一次同样的摘要，对比两个摘要是否相同即可。 注意：签名算法是使用私钥加密，确保得到的签名无法被伪造，同时所有人都可以使用公钥解密来验证签名。这和正常的数据加密算法是相反的。 因为数字签名多了非对称加密这一步，就能保证只有拥有私钥的人才能生成出正确的数字签名，达到了防止伪造签名的目的。 而数字摘要（Hash）则谁都可以计算出来，通常由可信方公布数据的 Hash 值，用户下载数据后，可通过 Hash 值对比来判断数据是否损坏，或者被人调包。 重点在于，Hash 摘要必须由可信方公布出来，否则不能保证安全性。而数字签名可以随数据一起提供，不需要担心被伪造。 JWT 是签名和数据一起提供的，因此必须使用签名才能保证安全性。 P.S. 在 Android/IOS 开发中，经常会遇到各类 API 或者 APP 商店要求提供 APP 的签名，还指明需要的是 MD5/SHA1 值。 这个地方需要填的 MD5/SHA1 值，实际上只是你「签名证书(=公钥+证书拥有者信息)」的「数字指纹/摘要」，和 JWT 的签名不是一回事。 前言 JWT 规范的详细说明请见「参考」部分的链接。这里主要说明一下 JWT 最常见的几种签名算法(JWA)：HS256(HMAC-SHA256) 、RS256(RSA-SHA256) 还有 ES256(ECDSA-SHA256)。 这三种算法都是一种消息签名算法，得到的都只是一段无法还原的签名。区别在于消息签名与签名验证需要的 「key」不同。 HS256 使用同一个「secret_key」进行签名与验证（对称加密）。一旦 secret_key 泄漏，就毫无安全性可言了。 因此 HS256 只适合集中式认证，签名和验证都必须由可信方进行。 传统的单体应用广泛使用这种算法，但是请不要在任何分布式的架构中使用它！ RS256 是使用 RSA 私钥进行签名，使用 RSA 公钥进行验证。公钥即使泄漏也毫无影响，只要确保私钥安全就行。 RS256 可以将验证委托给其他应用，只要将公钥给他们就行。 ES256 和 RS256 一样，都使用私钥签名，公钥验证。算法速度上差距也不大，但是它的签名长度相对短很多（省流量），并且算法强度和 RS256 差不多。 对于单体应用而言，HS256 和 RS256 的安全性没有多大差别。 而对于需要进行多方验证的微服务架构而言，显然只有 RS256/ES256 才能提供足够的安全性。 在使用 RS256 时，只有「身份认证的微服务(auth)」需要用 RSA 私钥生成 JWT，其他微服务使用公开的公钥即可进行签名验证，私钥得到了更好的保护。 更进一步，「JWT 生成」和「JWT 公钥分发」都可以直接委托给第三方的通用工具，比如 hydra。 甚至「JWT 验证」也可以委托给「API 网关」来处理，应用自身可以把认证鉴权完全委托给外部的平台，而应用自身只需要专注于业务。这也是目前的发展趋势。 RFC 7518 - JSON Web Algorithms (JWA) 中给出的 JWT 算法列表如下： +--------------+-------------------------------+--------------------+ | \"alg\" Param | Digital Signature or MAC | Implementation | | Value | Algorithm | Requirements | +--------------+-------------------------------+--------------------+ | HS256 | HMAC using SHA-256 | Required | | HS384 | HMAC using SHA-384 | Optional | | HS512 | HMAC using SHA-512 | Optional | | RS256 | RSASSA-PKCS1-v1_5 using | Recommended | | | SHA-256 | | | RS384 | RSASSA-PKCS1-v1_5 using | Optional | | | SHA-384 | | | RS512 | RSASSA-PKCS1-v1_5 using | Optional | | | SHA-512 | | | ES256 | ECDSA using P-256 and SHA-256 | Recommended+ | | ES384 | ECDSA using P-384 and SHA-384 | Optional | | ES512 | ECDSA using P-521 and SHA-512 | Optional | | PS256 | RSASSA-PSS using SHA-256 and | Optional | | | MGF1 with SHA-256 | | | PS384 | RSASSA-PSS using SHA-384 and | Optional | | | MGF1 with SHA-384 | | | PS512 | RSASSA-PSS using SHA-512 and | Optional | | | MGF1 with SHA-512 | | | none | No digital signature or MAC | Optional | | | performed | | +--------------+-------------------------------+--------------------+ The use of \"+\" in the Implementation Requirements column indicates that the requirement strength is likely to be increased in a future version of the specification. 可以看到被标记为 Recommended 的只有 RS256 和 ES256(纠正：最后一列只是对 JWA 具体实现的一些要求，和算法推荐没有任何关系！）。 ES256 使用 ECDSA 进行签名，它的安全性和运算速度目前和 RS256 差距不大，但是拥有更短的签名长度。 对于需要频繁发送的 JWT 而言，更短的长度长期下来可以节约大量流量。 因此更推荐使用 ES256 算法。 ","date":"2020-03-03","objectID":"/posts/jwt-algorithm-key-generation/:0:0","tags":["JWT","算法","OpenSSL"],"title":"JWT 签名算法 HS256、RS256 及 ES256 及密钥生成","uri":"/posts/jwt-algorithm-key-generation/"},{"categories":["技术"],"content":"使用 OpenSSL 生成 RSA/ECC 公私钥 RS256 使用 RSA 算法进行签名，可通过如下命令生成 RSA 密钥： # 1. 生成 2048 位（不是 256 位）的 RSA 密钥 openssl genrsa -out rsa-private-key.pem 2048 # 2. 通过密钥生成公钥 openssl rsa -in rsa-private-key.pem -pubout -out rsa-public-key.pem ES256 使用 ECDSA 算法进行签名，该算法使用 ECC 密钥，生成命令如下： # 1. 生成 ec 算法的私钥，使用 prime256v1 算法，密钥长度 256 位。（强度大于 2048 位的 RSA 密钥） openssl ecparam -genkey -name prime256v1 -out ecc-private-key.pem # 2. 通过密钥生成公钥 openssl ec -in ecc-private-key.pem -pubout -out ecc-public-key.pem 密钥的使用应该就不需要介绍了，各类语言都有对应 JWT 库处理这些，请自行查看文档。 如果是调试/学习 JWT，需要手动签名与验证的话，推荐使用 jwt 工具网站 - jwt.io ","date":"2020-03-03","objectID":"/posts/jwt-algorithm-key-generation/:1:0","tags":["JWT","算法","OpenSSL"],"title":"JWT 签名算法 HS256、RS256 及 ES256 及密钥生成","uri":"/posts/jwt-algorithm-key-generation/"},{"categories":["技术"],"content":"参考 RFC 7518 - JSON Web Algorithms (JWA) 什么是 JWT – JSON WEB TOKEN jwt 工具网站 - jwt.io JWT 算法比较 ","date":"2020-03-03","objectID":"/posts/jwt-algorithm-key-generation/:2:0","tags":["JWT","算法","OpenSSL"],"title":"JWT 签名算法 HS256、RS256 及 ES256 及密钥生成","uri":"/posts/jwt-algorithm-key-generation/"},{"categories":["技术"],"content":" OOMKilled: Pod 的内存使用超出了 resources.limits 中的限制，被强制杀死。 CrashLoopBackoff: Pod 进入 崩溃-重启循环，重启间隔时间从 10 20 40 80 一直翻倍到上限 300 秒，然后以 300 秒为间隔无限重启。 Pod 一直 Pending: 这说明没有任何节点能满足 Pod 的要求，容器无法被调度。比如端口被别的容器用 hostPort 占用，节点有污点等。 FailedCreateSandBox: Failed create pod sandbox: rpc error: code = DeadlineExceeded desc = context deadline exceeded：很可能是 CNI 网络插件的问题（比如 ip 地址溢出）， SandboxChanged: Pod sandbox changed, it will be killed and re-created: 很可能是由于内存限制导致容器被 OOMKilled，或者其他资源不足 FailedSync: error determining status: rpc error: code = DeadlineExceeded desc = context deadline exceeded: 常和前两个错误先后出现，很可能是 CNI 网络插件的问题。 开发集群，一次性部署所有服务时，各 Pod 互相争抢资源，导致 Pod 生存探针失败，不断重启，重启进一步加重资源使用。恶性循环。 需要给每个 Pod 加上 resources.requests，这样资源不足时，后续 Pod 会停止调度，直到资源恢复正常。 Pod 出现大量的 Failed 记录，Deployment 一直重复建立 Pod: 通过 kubectl describe/edit pod \u003cpod-name\u003e 查看 pod Events 和 Status，一般会看到失败信息，如节点异常导致 Pod 被驱逐。 Kubernetes 问题排查：Pod 状态一直 Terminating 创建了 Deployment 后，却没有自动创建 Pod: 缺少某些创建 Pod 必要的东西，比如设定的 ServiceAccount 不存在。 Pod 运行失败，状态为 MatchNodeSelector: 对主节点进行关机、迁移等操作，导致主调度器下线时，会在一段时间内导致 Pod 调度失败，调度失败会报这个错。 Pod 仍然存在，但是 Service 的 Endpoints 却为空，找不到对应的 Pod IPs: 遇到过一次，是因为时间跳变（从未来的时间改回了当前时间）导致的问题。 Pod 状态一直 ContainerCreating，Error syncing pod: 可能是节点的内存碎片化严重，导致无法创建pod。临时解决：echo 3 \u003e /proc/sys/vm/drop_caches 详细分析参见 容器创建失败(runc_page allocation failure)排查 收集的其他故障解决文章： TKE操作指南 - TKE K8S问题排查（十八） ","date":"2019-11-24","objectID":"/posts/common-kubernetes-errors-causes-and-solutions/:0:0","tags":["Kubernetes"],"title":"Kubernetes 常见错误、原因及处理方法","uri":"/posts/common-kubernetes-errors-causes-and-solutions/"},{"categories":["技术"],"content":"Pod 无法删除 可能是某些资源无法被GC，这会导致容器已经 Exited 了，但是 Pod 一直处于 Terminating 状态。 这个问题在网上能搜到很多案例,但大都只是提供了如下的强制清理命令，未分析具体原因： kubectl delete pods \u003cpod\u003e --grace-period=0 --force 最近找到几篇详细的原因分析文章，值得一看： 腾讯云原生 -【Pod Terminating原因追踪系列】之 containerd 中被漏掉的 runc 错误信息 腾讯云原生 -【Pod Terminating原因追踪系列之二】exec连接未关闭导致的事件阻塞 腾讯云原生 -【Pod Terminating原因追踪系列之三】让docker事件处理罢工的cancel状态码 Pod terminating - 问题排查 - KaKu Li 大致总结一下，主要原因来自 docker 18.06 以及 kubernetes 的 docker-shim 运行时的底层逻辑，已经在新版本被修复了。 ","date":"2019-11-24","objectID":"/posts/common-kubernetes-errors-causes-and-solutions/:0:1","tags":["Kubernetes"],"title":"Kubernetes 常见错误、原因及处理方法","uri":"/posts/common-kubernetes-errors-causes-and-solutions/"},{"categories":["技术"],"content":"节点常见错误 DiskPressure：节点的可用空间不足。（通过df -h 查看，保证可用空间不小于 15%） The node was low on resource: ephemeral-storage: 同上，节点的存储空间不够了。 ","date":"2019-11-24","objectID":"/posts/common-kubernetes-errors-causes-and-solutions/:1:0","tags":["Kubernetes"],"title":"Kubernetes 常见错误、原因及处理方法","uri":"/posts/common-kubernetes-errors-causes-and-solutions/"},{"categories":["技术"],"content":"网络常见错误 ","date":"2019-11-24","objectID":"/posts/common-kubernetes-errors-causes-and-solutions/:2:0","tags":["Kubernetes"],"title":"Kubernetes 常见错误、原因及处理方法","uri":"/posts/common-kubernetes-errors-causes-and-solutions/"},{"categories":["技术"],"content":"1. Ingress/Istio Gateway 返回值 404：不存在该 Service/Istio Gateway 503：服务不可用。原因基本都是 Service 对应的 Pods NotReady 504：网关请求下游超时。主要有两种可能 考虑是不是 Ingress Controller 的 IP 表未更新，将请求代理到了不存在的 Pod ip，导致得不到响应。 Pod 响应太慢，代码问题。 Ingress 相关网络问题的排查流程： Which ingress controller? Timeout between client and ingress controller, or between ingress controller and backend service/pod? HTTP/504 generated by the ingress controller, proven by logs from the ingress controller? If you port-forward to skip the internet between client and ingress controller, does the timeout still happen? ","date":"2019-11-24","objectID":"/posts/common-kubernetes-errors-causes-and-solutions/:2:1","tags":["Kubernetes"],"title":"Kubernetes 常见错误、原因及处理方法","uri":"/posts/common-kubernetes-errors-causes-and-solutions/"},{"categories":["技术"],"content":"名字空间常见错误 ","date":"2019-11-24","objectID":"/posts/common-kubernetes-errors-causes-and-solutions/:3:0","tags":["Kubernetes"],"title":"Kubernetes 常见错误、原因及处理方法","uri":"/posts/common-kubernetes-errors-causes-and-solutions/"},{"categories":["技术"],"content":"名字空间无法删除 这通常是某些资源如 CR(custom resources)/存储等资源无法释放导致的。 比如常见的 monitoring 名字空间无法删除，应该就是 CR 无法 GC 导致的。 可手动删除 namespace 配置中的析构器（spec.finalizer，在名字空间生命周期结束前会生成的配置项），这样名字空间就会直接跳过 GC 步骤： # 编辑名字空间的配置 kubectl edit namespace \u003cns-name\u003e # 将 spec.finalizers 改成空列表 [] 如果上述方法也无法删除名字空间，也找不到具体的问题，就只能直接从 etcd 中删除掉它了(有风险，谨慎操作！)。方法如下： # 登录到 etcd 容器中，执行如下命令： export ETCDCTL_API=3 cd /etc/kubernetes/pki/etcd/ # 列出所有名字空间 etcdctl --cacert ca.crt --cert peer.crt --key peer.key get /registry/namespaces --prefix --keys-only # （谨慎操作！！！）强制删除名字空间 `monitoring`。这可能导致相关资源无法被 GC！ etcdctl --cacert ca.crt --cert peer.crt --key peer.key del /registry/namespaces/monitoring ","date":"2019-11-24","objectID":"/posts/common-kubernetes-errors-causes-and-solutions/:3:1","tags":["Kubernetes"],"title":"Kubernetes 常见错误、原因及处理方法","uri":"/posts/common-kubernetes-errors-causes-and-solutions/"},{"categories":["技术"],"content":"kubectl/istioctl 等客户端工具异常 socat not found: kubectl 使用 socat 进行端口转发，集群的所有节点，以及本机都必须安装有 socat 工具。 ","date":"2019-11-24","objectID":"/posts/common-kubernetes-errors-causes-and-solutions/:4:0","tags":["Kubernetes"],"title":"Kubernetes 常见错误、原因及处理方法","uri":"/posts/common-kubernetes-errors-causes-and-solutions/"},{"categories":["技术"],"content":"批量清理 Evicted 记录 有时候 Pod 因为节点选择器的问题，被不断调度到有问题的 Node 上，就会不断被 Evicted，导致出现大量的 Evicted Pods。 排查完问题后，需要手动清理掉这些 Evicted Pods. 批量删除 Evicted 记录: kubectl get pods | grep Evicted | awk '{print $1}' | xargs kubectl delete pod ","date":"2019-11-24","objectID":"/posts/common-kubernetes-errors-causes-and-solutions/:5:0","tags":["Kubernetes"],"title":"Kubernetes 常见错误、原因及处理方法","uri":"/posts/common-kubernetes-errors-causes-and-solutions/"},{"categories":["技术"],"content":"容器镜像GC、Pod驱逐以及节点压力 节点压力 DiskPressure 会导致 Pod 被驱逐，也会触发容器镜像的 GC。 根据官方文档 配置资源不足时的处理方式，Kubelet 提供如下用于配置容器 GC 及 Evicetion 的阈值： --eviction-hard 和 eviction-soft: 对应旧参数 --image-gc-high-threshold，这两个参数配置镜像 GC 及驱逐的触发阈值。磁盘使用率的阈值默认为 85% 区别在于 eviction-hard 是立即驱逐，而 eviction-soft 在超过 eviction-soft-grace-period 之后才驱逐。 --eviction-minimum-reclaim: 对应旧参数 --image-gc-low-threshold。这是进行资源回收（镜像GC、Pod驱逐等）后期望达到的磁盘使用率百分比。磁盘使用率的阈值默认值为 80%。 问：能否为 ImageGC 设置一个比 DiskPressure 更低的阈值？因为我们希望能自动进行镜像 GC，但是不想立即触发 Pod 驱逐。 答：这应该可以通过设置 eviction-soft 和长一点的 eviction-soft-grace-period 来实现。 另外 --eviction-minimum-reclaim 也可以设小一点，清理得更干净。示例如下： --eviction-soft=memory.available\u003c1Gi,nodefs.available\u003c2Gi,imagefs.available\u003c200Gi --eviction-soft-grace-period=3m --eviction-minimum-reclaim=memory.available=0Mi,nodefs.available=1Gi,imagefs.available=2Gi ","date":"2019-11-24","objectID":"/posts/common-kubernetes-errors-causes-and-solutions/:6:0","tags":["Kubernetes"],"title":"Kubernetes 常见错误、原因及处理方法","uri":"/posts/common-kubernetes-errors-causes-and-solutions/"},{"categories":["技术"],"content":"其他问题 ","date":"2019-11-24","objectID":"/posts/common-kubernetes-errors-causes-and-solutions/:7:0","tags":["Kubernetes"],"title":"Kubernetes 常见错误、原因及处理方法","uri":"/posts/common-kubernetes-errors-causes-and-solutions/"},{"categories":["技术"],"content":"如何重新运行一个 Job？ 我们有一个 Job 因为外部原因运行失败了，修复好后就需要重新运行它。 方法是：删除旧的 Job，再使用同一份配置重建 Job. 如果你使用的是 fluxcd 这类 GitOps 工具，就只需要手工删除旧 Pod，fluxcd 会定时自动 apply 所有配置，这就完成了 Job 的重建。 ","date":"2019-11-24","objectID":"/posts/common-kubernetes-errors-causes-and-solutions/:7:1","tags":["Kubernetes"],"title":"Kubernetes 常见错误、原因及处理方法","uri":"/posts/common-kubernetes-errors-causes-and-solutions/"},{"categories":["技术"],"content":"参考 Kubernetes管理经验 504 Gateway Timeout when accessing workload via ingress Kubernetes Failure Stories ","date":"2019-11-24","objectID":"/posts/common-kubernetes-errors-causes-and-solutions/:8:0","tags":["Kubernetes"],"title":"Kubernetes 常见错误、原因及处理方法","uri":"/posts/common-kubernetes-errors-causes-and-solutions/"},{"categories":["技术"],"content":"Manjaro 是一个基于 Arch Linux 的 Linux 滚动发行版，用着挺舒服的。 最大的特点，是包仓库很丰富，而且都很新。代价是偶尔会出些小毛病。 2021-09-22 更新：今天被群友科普，可能我下面列举的几个滚挂事件，可能都和我使用了 archlinuxcn 这个源有关，这确实有可能。 我一年多的使用中，遇到过 qv2-ray 动态链接库炸掉的问题，没专门去找修复方法，好像是等了一两个月，升级了两个大版本才恢复。 另一个就是 VSCode - Incorrect locale ‘en-US’ used everywhere 还遇到过 libguestfs 的一个问题：vrit-v2v/virt-p2v 两个工具被拆分出去，导致 manjaro 只能通过源码安装这俩货。这貌似目前仍旧没有解决。 总的来说体验很不错，能很及时地用上各种新版本的软件。 ","date":"2019-07-13","objectID":"/posts/manjaro-instruction/:0:0","tags":["Manjaro","Linux","Arch Linux"],"title":"Manjaro 使用指南","uri":"/posts/manjaro-instruction/"},{"categories":["技术"],"content":"一、pacman/yay 的基础命令 Manjaro 装好后，需要运行的第一条命令： sudo pacman -Syy ## 强制更新 package 目录 sudo pacman-mirrors --interactive --country China # 列出所有国内的镜像源，并提供交互式的界面手动选择镜像源 sudo pacman -Syyu # 强制更新 package 目录，并尝试更新已安装的所有 packages. sudo pacman -S yay # 安装 yay pacman 是 arch/manjaro 的官方包管理器，而刚刚安装的 yay，则是一个能查询 arch linux 的 aur 仓库的第三方包管理器，非常流行。 pacman 的常用命令语法： pacman -S package_name # 安装软件 pacman -S extra/package_name # 安装不同仓库中的版本 pacman -Syu # 升级整个系统，y是更新数据库，yy是强制更新，u是升级软件 pacman -Ss string # 在包数据库中查询软件 pacman -Si package_name # 显示软件的详细信息 pacman -Sc # 清除软件缓存，即/var/cache/pacman/pkg目录下的文件 pacman -R package_name # 删除单个软件 pacman -Rs package_name # 删除指定软件及其没有被其他已安装软件使用的依赖关系 pacman -Qs string # 查询已安装的软件包 pacman -Qi package_name # 查询本地安装包的详细信息 pacman -Ql package_name # 获取已安装软件所包含的文件的列表 pacman -U package.tar.zx # 从本地文件安装 pactree package_name # 显示软件的依赖树 yay 的用法和 pacman 完全类似，上述所有 pacman xxx 命令，均可替换成 yay xxx 执行。 此外，还有一条 yay 命令值得记一下： yay -c # 卸载所有无用的依赖。类比 apt-get autoremove ","date":"2019-07-13","objectID":"/posts/manjaro-instruction/:1:0","tags":["Manjaro","Linux","Arch Linux"],"title":"Manjaro 使用指南","uri":"/posts/manjaro-instruction/"},{"categories":["技术"],"content":"常用软件与配置 ","date":"2019-07-13","objectID":"/posts/manjaro-instruction/:2:0","tags":["Manjaro","Linux","Arch Linux"],"title":"Manjaro 使用指南","uri":"/posts/manjaro-instruction/"},{"categories":["技术"],"content":"1. 添加 archlinux 中文社区仓库 Arch Linux 中文社区仓库 是由 Arch Linux 中文社区驱动的非官方用户仓库，包含一些额外的软件包以及已有软件的 git 版本等变种。部分软件包的打包脚本来源于 AUR。 一些国内软件，如果直接从 aur 安装，那就会有一个编译过程，有点慢。而 archlinuxcn 有已经编译好的包，可以直接安装。更新速度也很快，推荐使用。 配置方法见 Arch Linux Chinese Community Repository。 ","date":"2019-07-13","objectID":"/posts/manjaro-instruction/:2:1","tags":["Manjaro","Linux","Arch Linux"],"title":"Manjaro 使用指南","uri":"/posts/manjaro-instruction/"},{"categories":["技术"],"content":"2. 安装常用软件 sudo pacman -S google-chrome firefox # 浏览器 sudo pacman -S netease-cloud-music # 网易云音乐 sudo pacman -S noto-fonts-cjk wqy-bitmapfont wqy-microhei wqy-zenhei # 中文字体：思源系列、文泉系列 sudo pacman -S wps-office ttf-wps-fonts sudo pacman -S vim # 命令行编辑器 sudo pacman -S git # 版本管理工具 sudo pacman -S clang make cmake gdb # 编译调试环境 sudo pacman -S visual-studio-code-bin # 代码编辑器 sudo pacman -S wireshark-qt mitmproxy # 抓包工具 sudo pacman -S docker # docker 容器 其中 docker 和 wireshark 需要额外配置，否则会要求管理员权限： sudo groupadd wireshark sudo gpasswd --add $USER wireshark # 将你添加到 wireshark 用户组中 sudo groupadd docker sudo gpasswd --add $USER docker # 同上 ","date":"2019-07-13","objectID":"/posts/manjaro-instruction/:2:2","tags":["Manjaro","Linux","Arch Linux"],"title":"Manjaro 使用指南","uri":"/posts/manjaro-instruction/"},{"categories":["技术"],"content":"3. 中文输入法 有两个选择：中州韵（rime）和搜狗拼音（sogoupinyin）。 简单省事用搜狗，要用特殊的输入方案（五笔、音形、二笔等等）就只有 rime 可选了。 3.1 fcitx5-rime 配置小鹤音形 首先安装 fcitx5-rime, 注意这些组件一个都不能省略： sudo pacman -S fcitx5 fcitx5-chinese-addons fcitx5-gtk fcitx5-qt kcm-fcitx5 fcitx5-rime 第二步是修改环境变量，将 fcitx5-rime 设为默认输入法并自动启动。 添加 ~/.pam_environment 文件，内容如下： INPUT_METHOD DEFAULT=fcitx5 GTK_IM_MODULE DEFAULT=fcitx5 QT_IM_MODULE DEFAULT=fcitx5 XMODIFIERS DEFAULT=@im=fcitx5 pam-env 模块会在所有登录会话中读取上面的配置文件，包括 X11 会话和 Wayland 会话。 添加自动启动： # ~/.xprofile 是 x11 GUI 的环境变量配置文件 echo 'fcitx5 \u0026' \u003e\u003e ~/.xprofile 然后，从 http://flypy.ys168.com/ 下载最新的鼠须管（MacOS）配置文件，将解压得到的 rime 文件夹拷贝到 ~/.local/share/fcitx5/ 下： mv rime ~/.local/share/fcitx5/ 现在重启系统，在 fcitx5 配置里面添加 rime，就可以正常使用小鹤音形了。 ","date":"2019-07-13","objectID":"/posts/manjaro-instruction/:2:3","tags":["Manjaro","Linux","Arch Linux"],"title":"Manjaro 使用指南","uri":"/posts/manjaro-instruction/"},{"categories":["技术"],"content":"坑 使用过程中，我也遇到了一些坑： 安装软件包时，无法在线安装旧版本！除非你本地有旧版本的安装包没清除，才可以通过缓存安装旧版本。 这种问题没遇到时好说，但有时候新版本有问题，旧安装包也清理掉了无法回退，就非常麻烦。 而且就算你回退了版本，一升级它就又更新了。。 ","date":"2019-07-13","objectID":"/posts/manjaro-instruction/:3:0","tags":["Manjaro","Linux","Arch Linux"],"title":"Manjaro 使用指南","uri":"/posts/manjaro-instruction/"},{"categories":["技术"],"content":"彻底删除 Manjaro 及其引导项 最近(2021-01)切换到了 OpenSUSE，体验很好，于是决定删除掉 Manjaro。 一番操作，总结出的删除流程如下（以下命令均需要 root 权限）： # 1. 删除 EFI 引导项 ## 查看 efi 的所有启动项，找到 Manjaro 的编号 efibootmgr ## 删除掉 Manjaro 启动项 sudo efibootmgr --delete-bootnum -b 2 # 2. 删除 manjaro 的 bootloader ## 我使用了 manjaro 默认的安装策略，bootloader 被安装在了和 windows 相同的 EFI 分区下 ## 首先通过 opnsuse 的分区工具，找到 EFI 分区的设备号，然后挂载它 mkdir efi mount /dev/nvme0n1p1 efi # 删除 Manjaro bootloader rm -r EFI/Manjaro # 3. 重建 grub2 引导项 grub2-mkconfig \u003e /boot/grub2/grub.cfg # 4. 最后，通过分区工具删除 Manjaro 的所有分区，我是 SSD，只有一个分区 # 5. 重启系统，所有东西就全删除干净了。 ","date":"2019-07-13","objectID":"/posts/manjaro-instruction/:4:0","tags":["Manjaro","Linux","Arch Linux"],"title":"Manjaro 使用指南","uri":"/posts/manjaro-instruction/"},{"categories":["技术"],"content":"参考 Arch Linux Wiki - 中文 AUR 仓库 Arch Linux 中文社区仓库 yay - Yet another Yogurt - An AUR Helper written in Go 安装Manjaro之后的配置 Arch Linux Wiki - Fcitx5 ","date":"2019-07-13","objectID":"/posts/manjaro-instruction/:5:0","tags":["Manjaro","Linux","Arch Linux"],"title":"Manjaro 使用指南","uri":"/posts/manjaro-instruction/"},{"categories":["技术"],"content":"一、WebSocket WebSocket 是一个双向通信协议，它在握手阶段采用 HTTP/1.1 协议（暂时不支持 HTTP/2）。 握手过程如下： 首先客户端向服务端发起一个特殊的 HTTP 请求，其消息头如下： GET /chat HTTP/1.1 // 请求行 Host: server.example.com Upgrade: websocket // required Connection: Upgrade // required Sec-WebSocket-Key: dGhlIHNhbXBsZSBub25jZQ== // required，一个 16bits 编码得到的 base64 串 Origin: http://example.com // 用于防止未认证的跨域脚本使用浏览器 websocket api 与服务端进行通信 Sec-WebSocket-Protocol: chat, superchat // optional, 子协议协商字段 Sec-WebSocket-Version: 13 如果服务端支持该版本的 WebSocket，会返回 101 响应，响应标头如下： HTTP/1.1 101 Switching Protocols // 状态行 Upgrade: websocket // required Connection: Upgrade // required Sec-WebSocket-Accept: s3pPLMBiTxaQ9kYGzzhZRbK+xOo= // required，加密后的 Sec-WebSocket-Key Sec-WebSocket-Protocol: chat // 表明选择的子协议 握手完成后，接下来的 TCP 数据包就都是 WebSocket 协议的帧了。 可以看到，这里的握手不是 TCP 的握手，而是在 TCP 连接内部，从 HTTP/1.1 upgrade 到 WebSocket 的握手。 WebSocket 提供两种协议：不加密的 ws:// 和 加密的 wss://. 因为是用 HTTP 握手，它和 HTTP 使用同样的端口：ws 是 80（HTTP），wss 是 443（HTTPS） 在 Python 编程中，可使用 websockets 实现的异步 WebSocket 客户端与服务端。此外 aiohttp 也提供了 WebSocket 支持。 Note：如果你搜索 Flask 的 WebScoket 插件，得到的第一个结果很可能是 Flask-SocketIO。但是 Flask-ScoektIO 使用的是它独有的 SocketIO 协议，并不是标准的 WebSocket。只是它刚好提供与 WebSocket 相同的功能而已。 SocketIO 的优势在于只要 Web 端使用了 SocketIO.js，就能支持该协议。而纯 WS 协议，只有较新的浏览器才支持。对于客户端非 Web 的情况，更好的选择可能是使用 Flask-Sockets。 ","date":"2019-02-11","objectID":"/posts/websocket-http2-and-grpc/:1:0","tags":["WebSocket","gRPC","HTTP/2"],"title":"WebSocket、HTTP/2 与 gRPC","uri":"/posts/websocket-http2-and-grpc/"},{"categories":["技术"],"content":"JS API // WebSocket API var socket = new WebSocket('ws://websocket.example.com'); // Show a connected message when the WebSocket is opened. socket.onopen = function(event) { console.log('WebSocket is connected.'); }; // Handle messages sent by the server. socket.onmessage = function(event) { var message = event.data; console.log(message); }; // Handle any error that occurs. socket.onerror = function(error) { console.log('WebSocket Error: ' + error); }; ","date":"2019-02-11","objectID":"/posts/websocket-http2-and-grpc/:1:1","tags":["WebSocket","gRPC","HTTP/2"],"title":"WebSocket、HTTP/2 与 gRPC","uri":"/posts/websocket-http2-and-grpc/"},{"categories":["技术"],"content":"二、HTTP/2 HTTP/2 于 2015 年标准化，主要目的是优化性能。其特性如下： 二进制协议：HTTP/2 的消息头使用二进制格式，而非文本格式。并且使用专门设计的 HPack 算法压缩。 多路复用（Multiplexing）：就是说 HTTP/2 可以重复使用同一个 TCP 连接，并且连接是多路的，多个请求或响应可以同时传输。 对比之下，HTTP/1.1 的长连接也能复用 TCP 连接，但是只能串行，不能“多路”。 服务器推送：服务端能够直接把资源推送给客户端，当客户端需要这些文件的时候，它已经在客户端了。（该推送对 Web App 是隐藏的，由浏览器处理） HTTP/2 允许取消某个正在传输的数据流（通过发送 RST_STREAM 帧），而不关闭 TCP 连接。 这正是二进制协议的好处之一，可以定义多种功能的数据帧。 它允许服务端将资源推送到客户端缓存，我们访问淘宝等网站时，经常会发现很多请求的请求头部分会提示“provisional headers are shown”，这通常就是直接从缓存加载了资源，因此请求根本没有被发送。观察 Chrome Network 的 Size 列，这种请求的该字段一般都是 from disk cache 或者 from memroy cache. Chrome 可以通过如下方式查看请求使用的协议： 2019-02-10: 使用 Chrome 查看，目前主流网站基本都已经部分使用了 HTTP/2，知乎、bilibili、GIthub 使用了 wss 协议，也有很多网站使用了 SSE（格式如 data:image/png;base64,\u003cbase64 string\u003e） 而且很多网站都有使用 HTTP/2 + QUIC，该协议的新名称是 HTTP/3，它是基于 UDP 的 HTTP 协议。 ","date":"2019-02-11","objectID":"/posts/websocket-http2-and-grpc/:2:0","tags":["WebSocket","gRPC","HTTP/2"],"title":"WebSocket、HTTP/2 与 gRPC","uri":"/posts/websocket-http2-and-grpc/"},{"categories":["技术"],"content":"SSE 服务端推送事件，是通过 HTTP 长连接进行信息推送的一个功能。 它首先由浏览器向服务端建立一个 HTTP 长连接，然后服务端不断地通过这个长连接将消息推送给浏览器。JS API 如下： // create SSE connection var source = new EventSource('/dates'); // 连接建立时，这些 API 和 WebSocket 的很相似 source.onopen = function(event) { // handle open event }; // 收到消息时（它只捕获未命名 event） source.onmessage = function(event) { var data = event.data; // 发送过来的实际数据（string） var origin = event.origin; // 服务器端URL的域名部分，即协议、域名和端口。 var lastEventId = event.lastEventId; // 数据的编号，由服务器端发送。如果没有编号，这个属性为空。 // handle message }; source.onerror = function(event) { // handle error event }; ","date":"2019-02-11","objectID":"/posts/websocket-http2-and-grpc/:2:1","tags":["WebSocket","gRPC","HTTP/2"],"title":"WebSocket、HTTP/2 与 gRPC","uri":"/posts/websocket-http2-and-grpc/"},{"categories":["技术"],"content":"具体的实现 在收到客户端的 SSE 请求（HTTP 协议）时，服务端返回的响应首部如下： Content-Type: text/event-stream Cache-Control: no-cache Connection: keep-alive 而 body 部分，SSE 定义了四种信息： data：数据栏 event：自定义数据类型 id ：数据 id retry：最大间隔时间，超时则重新连接 body 举例说明： : 这种格式的消息是注释，会被忽略\\n\\n : 通常服务器每隔一段时间就会发送一个注释，防止超时 retry\\n\\n : 下面这个是一个单行数据\\n\\n data: some text\\n\\n : 下面这个是多行数据，在客户端会重组成一个 data\\n\\n data: {\\n data: \"foo\": \"bar\",\\n data: \"baz\", 555\\n data: }\\n\\n : 这是一个命名 event，只会被事件名与之相同的 listener 捕获\\n\\n event: foo\\n data: a foo event\\n\\n : 未命名事件，会被 onmessage 捕获\\n\\n data: an unnamed event\\n\\n event: bar\\n data: a bar event\\n\\n : 这个 id 对应 event.lastEventId\\n\\n id: msg1\\n data: message\\n\\n ","date":"2019-02-11","objectID":"/posts/websocket-http2-and-grpc/:2:2","tags":["WebSocket","gRPC","HTTP/2"],"title":"WebSocket、HTTP/2 与 gRPC","uri":"/posts/websocket-http2-and-grpc/"},{"categories":["技术"],"content":"WebSocket、HTTP/2 与 SSE 的比较 加密与否： WebSocket 支持明文通信 ws:// 和加密 wss://， 而 HTTP/2 协议虽然没有规定必须加密，但是主流浏览器都只支持 HTTP/2 over TLS. SSE 是使用的 HTTP 协议通信，支持 http/https 消息推送： WebSocket是全双工通道，可以双向通信。而且消息是直接推送给 Web App. SSE 只能单向串行地从服务端将数据推送给 Web App. HTTP/2 虽然也支持 Server Push，但是服务器只能主动将资源推送到客户端缓存！并不允许将数据推送到客户端里跑的 Web App 本身。服务器推送只能由浏览器处理，不会在应用程序代码中弹出服务器数据，这意味着应用程序没有 API 来获取这些事件的通知。 为了接近实时地将数据推送给 Web App， HTTP/2 可以结合 SSE（Server-Sent Event）使用。 WebSocket 在需要接近实时双向通信的领域，很有用武之地。而 HTTP/2 + SSE 适合用于展示实时数据。 另外在客户端非浏览器的情况下，使用不加密的 HTTP/2 也是可能的。 ","date":"2019-02-11","objectID":"/posts/websocket-http2-and-grpc/:2:3","tags":["WebSocket","gRPC","HTTP/2"],"title":"WebSocket、HTTP/2 与 gRPC","uri":"/posts/websocket-http2-and-grpc/"},{"categories":["技术"],"content":"requests 查看 HTTP 协议版本号 可以通过 resp.raw.version 得到响应的 HTTP 版本号： \u003e\u003e\u003e import requests \u003e\u003e\u003e resp = requests.get(\"https://zhihu.com\") \u003e\u003e\u003e resp.raw.version 11 但是 requests 默认使用 HTTP/1.1，并且不支持 HTTP/2.（不过这也不是什么大问题，HTTP/2 只是做了性能优化，用 HTTP/1.1 也就是慢一点而已。） ","date":"2019-02-11","objectID":"/posts/websocket-http2-and-grpc/:2:4","tags":["WebSocket","gRPC","HTTP/2"],"title":"WebSocket、HTTP/2 与 gRPC","uri":"/posts/websocket-http2-and-grpc/"},{"categories":["技术"],"content":"三、gRPC 协议 gRPC 是一个远程过程调用框架，默认使用 protobuf3 进行数据的高效序列化与 service 定义，使用 HTTP/2 进行数据传输。 这里讨论的是 gRPC over HTTP/2 协议。 目前 gRPC 主要被用在微服务通信中，但是因为其优越的性能，它也很契合游戏、loT 等需要高性能低延迟的场景。 其实光从协议先进程度上讲，gRPC 基本全面超越 REST: 使用二进制进行数据序列化，比 json 更节约流量、序列化与反序列化也更快。 protobuf3 要求 api 被完全清晰的定义好，而 REST api 只能靠程序员自觉定义。 gRPC 官方就支持从 api 定义生成代码，而 REST api 需要借助 openapi-codegen 等第三方工具。 支持 4 种通信模式：一对一(unary)、客户端流、服务端流、双端流。更灵活 只是目前 gRPC 对 broswer 的支持仍然不是很好，如果你需要通过浏览器访问 api，那 gRPC 可能不是你的菜。 如果你的产品只打算面向 App 等可控的客户端，可以考虑上 gRPC。 对同时需要为浏览器和 APP 提供服务应用而言，也可以考虑 APP 使用 gRPC 协议，而浏览器使用 API 网关提供的 HTTP 接口，在 API 网关上进行 HTTP - gRPC 协议转换。 ","date":"2019-02-11","objectID":"/posts/websocket-http2-and-grpc/:3:0","tags":["WebSocket","gRPC","HTTP/2"],"title":"WebSocket、HTTP/2 与 gRPC","uri":"/posts/websocket-http2-and-grpc/"},{"categories":["技术"],"content":"gRPC over HTTP/2 定义 详细的定义参见官方文档 gRPC over HTTP/2. 这里是简要说明几点： gRPC 完全隐藏了 HTTP/2 本身的 method、headers、path 等语义，这些信息对用户而言完全不可见了。 请求统一使用 POST，响应状态统一为 200。只要响应是标准的 gRPC 格式，响应中的 HTTP 状态码将被完全忽略。 gRPC 定义了自己的 status 状态码、格式固定的 path、还有它自己的 headers。 ","date":"2019-02-11","objectID":"/posts/websocket-http2-and-grpc/:3:1","tags":["WebSocket","gRPC","HTTP/2"],"title":"WebSocket、HTTP/2 与 gRPC","uri":"/posts/websocket-http2-and-grpc/"},{"categories":["技术"],"content":"参考 深入探索 WebSockets 和 HTTP/2 HTTP/2 特性与抓包分析 SSE：服务器发送事件,使用长链接进行通讯 Using server-sent events - MDN Can I Use HTTP/2 on Browsers Python 3.x how to get http version (using requests library) WebSocket 是什么原理？ 原生模块打造一个简单的 WebSocket 服务器 Google Cloud - API design: Understanding gRPC, OpenAPI and REST and when to use them ","date":"2019-02-11","objectID":"/posts/websocket-http2-and-grpc/:4:0","tags":["WebSocket","gRPC","HTTP/2"],"title":"WebSocket、HTTP/2 与 gRPC","uri":"/posts/websocket-http2-and-grpc/"},{"categories":["随笔"],"content":" 高中考虑志向的时候，我最开始想选择电子信息工程，因为小时候就喜欢摆弄各种电子器件，这个方向硬件软件都能玩，就感觉很有趣，只是担心自己高三太放浪形骸考不上。 偶然想起在学校阅览室读杂志时，曾被科幻世界2013年12期里沖氏武彦的《在回声中重历》给打动——用耳朵“看见”世界实在是太奇妙了，我当时痴痴地幻想了好几天。 这样我开始考虑选择声学。 我从同桌推荐的《刀剑神域》开始接触日本的 ACG 文化，后来接触到初音未来和洛天依，就对歌声合成(singing synthesis)产生了很大的兴趣，仔细一想发现这也应该是声学的范畴，这使我坚定了我选择声学的想法。 从那时到现在的各种破事，实在不想多说，就略过不提了。总之我选择了声学，然后干得很失败。。。 为啥想写这篇文章呢？ 话还得从去年十一月份说起，当时要学 Matlab，就在 bilibili 上找了个教程：自制合成器演奏鸟之诗，讲用 Matlab 做吉他音合成，讲得特别棒，我跟着做出了 guitar-synthesizer 这个小玩意儿。 然后今天发现那个教程的作者就是做歌声合成的，而且从 2011 年 13 岁开始，因为想让初音唱中文，就开始写代码，一写就写到现在，从初中写到留学美国常春藤，从简单的时域拼接到现在的深度神经网络、隐马尔可夫模型。他在上个暑假到雅马哈（vocaloid 的制作公司）实习，并推出了自己第五次重制的歌声合成引擎 Synthesizer V。 这一系列的事迹，七年五次重制，从初中生到现在双修计算机科学和数学，简直让人叹为观止。 这位作者的名字叫华侃如（Kanru Hua），网络常用昵称 sleepwalking，他的个人网站 Kanru Hua’s Website - About Me. 通过他的博客，我了解到歌声合成需要两个方向的知识：信号处理和机器学习。 于是我想起了我的初心。 我小的时候特别喜欢拆各种电子设备，曾经用手机电池和坏手电做过手电筒，再加上个手机振动器用来吓人，还喜欢在家做各种实验。 可整个大学，我的各种手工课实验课弄得一塌糊涂，垫底的存在。这样说来，如果当初选了电子信息工程，可能会混得更差。 信号处理学得一团糟，一直想努力可在这方面总是半途而废。 说到底还是自控力太差，为何我就不能按部就班一回呢？到底是怎么搞的会养成这样的性格，这样的时候会很恨自己窝囊的性格。 如果我也能像室友一样按部就班的完成学业，只在空闲时间做自己的事…可惜没有如果。 经常会告诉自己做过的事已经不可逆转了，不需要后悔，所以也不会后悔。大概潜意识里还是会有怨念，所以才会在这样的时刻爆发出满满的恶意。 只是后悔却不做出改变是没有用的，但我完全不相信自己能改掉这样的性格，自暴自弃。纵欲一时爽，一直纵欲一直爽hhh… 说虽这样说，还是想继续挣扎下去，信号处理就像一道槛，不跨过去我无法面对自己。 “希望明年年底的总结中，我不要再这么丧”，这样大声说出来的话，言就能“灵”吧？我记得我运气一直挺好的（又窝囊到要靠运气。。） 更新： 今天二书介绍给我一篇文章我的编程经历，作者和我一样“不能兼顾兴趣与学业”，大一结束就退学了，现在在阿里巴巴工作。 他博客的观点很中肯： 我家人都是很传统的一代，不理解我学习编程最终能做出什么，他们主张先完成学业，再做这件事。但是我很清楚我自己的智商，不足以多线程处理不同的大领域，所以我顶着压力，努力地做出来他们能看出来的「成绩」，才能换来他们的理解。所以，如果你通过你的理性分析，坚持认为某件事情是对的，就努力的去做，不要放弃了以后看到另一个人做了你曾经想做的东西然后感慨当初应该怎么样怎么样。 –我的编程经历 只有在已经拥有解决问题的能力的时候，才有资格考虑退学这件事。 –你根本用不着退学 我仔细想了想，我不需要太在意自己“不能兼顾”、“自控力太弱”。如果真考虑清楚了得失，我只需要“拥有解决问题的能力”，然后去做我想做的就行了。 我自控力很弱，而且这么多年了一直改不了，即使再延期一年，也有可能还是拿不到毕业证，折磨自己而已。 而上班的话，就暑假实习的经验来看，工作都有进度条催着，反而活的更有朝气。 ","date":"2019-01-08","objectID":"/posts/relive-in-the-echo/:0:0","tags":["闲言碎语","初心","歌声合成"],"title":"在回声中重历","uri":"/posts/relive-in-the-echo/"},{"categories":["技术"],"content":" 本笔记整理自《SQL 基础教程》、《MySQL 必知必会》和网上资料。个人笔记不保证正确。 ","date":"2018-06-15","objectID":"/posts/sql-basic/:0:0","tags":["SQL","数据库"],"title":"SQL 基础笔记（一）","uri":"/posts/sql-basic/"},{"categories":["技术"],"content":"一、基础 SQL，即结构化查询语言，是为访问与操作关系数据库中的数据而设计的语言。 关系数据库以行(row)为单位读写数据 SQL 根据功能的不同，可分为三类（其中DML用得最多，增删查改嘛） DDL(Data Definition Language, 数据定义语言): CREATE/DROP/ALTER DML(Data Manipulation Language, 数据操作语言): SELECT/INSERT/UPDATE/DELETE DCL(Data Control Language, 数据控制语言): COMMIT/ROLLBACK/GRANT/REVOKE SQL 语句要以分号结尾。换行在 SQL 中不表示结束，而等同于空格。 SQL 不区分**关键字(Keyword)**的大小写，但是描述符就不一定了。 这里有个坑：MySQL 中，数据库和表其实就是数据目录下的目录和文件，因而，操作系统的敏感性决定数据库名和表名 是否大小写敏感。这就意味着数据库名和表名在 Windows 中是大小写不敏感的，而在大多数类型的 Unix/Linux 系统中是大小写敏感的。（注意仅指数据库名和表名）可通过修改配置文件的lower_case_table_names属性来统一这一行为。 而字段名、字段内容都是内部数据，是操作系统无关的。它们的大小写敏感性，由 MySQL 的的校对（COLLATE）规则来控制。该规则体现在 MySQL 的 校对字符集（COLLATION）的后缀上：比如 utf8字符集，utf8_general_ci表示不区分大小写，这个是 utf8 字符集默认的校对规则；utf8_general_cs 表示区分大小写，utf8_bin 表示二进制比较，同样也区分大小写 。 SQL 中的字符串和日期需要用单引号引用起来，日期有特定格式年-月-日 修改字符集：set names \u003c字符集名\u003e 记住在 MySQL 中，utf-8mb4 才是完全的 utf-8字符集。 ","date":"2018-06-15","objectID":"/posts/sql-basic/:1:0","tags":["SQL","数据库"],"title":"SQL 基础笔记（一）","uri":"/posts/sql-basic/"},{"categories":["技术"],"content":"二、DDL ","date":"2018-06-15","objectID":"/posts/sql-basic/:2:0","tags":["SQL","数据库"],"title":"SQL 基础笔记（一）","uri":"/posts/sql-basic/"},{"categories":["技术"],"content":"1. 数据库的创建和删除 创建数据库 CREATE DATABASE \u003c数据库名称\u003e; DROP DATABASE \u003c数据库名称\u003e; ","date":"2018-06-15","objectID":"/posts/sql-basic/:2:1","tags":["SQL","数据库"],"title":"SQL 基础笔记（一）","uri":"/posts/sql-basic/"},{"categories":["技术"],"content":"2. 创建表： 关系表的设计 关系表的设计，要确保把信息分解成多个表，一类信息一个表，各表通过某些常用的，基本不会改变的值（即关系表设计中的关系，也常称为外键）互相关联。尽量不要有冗余数据。 语句： CREATE TABLE \u003c表名\u003e ( \u003c列名1\u003e \u003c数据类型\u003e \u003c该列所需约束\u003e, \u003c列名2\u003e \u003c数据类型\u003e \u003c该列所需约束\u003e, \u003c列名3\u003e \u003c数据类型\u003e \u003c该列所需约束\u003e, ... \u003c该表的约束1\u003e, \u003c该表的约束2\u003e... ); 举例： CREATE TABLE `persons` ( `id` INT UNSIGNED NOT NULL AUTO_INCREMENT, `name` CHAR(20) NOT NULL, PRIMARY KEY (`id`) )ENGINE=InnoDB DEFAULT CHARSET=utf8mb4; 后面的是设置引擎和默认字符集。工作上，表的设计一定要深思熟虑，因为改起来很困难。 字段类型（MySQL） 有四类数据类型：字符串、数字、日期、二进制。它们又根据数据长度的区别，下分为多个类型。 字符串： 数字 日期 二进制 约束 SQL 约束是除了数据类型之外，对列中数据追加的限定条件。 类型约束：NOT NULL、AUTO_INCREMENT、UNSIGNED（这个只 MySQL 支持） 默认值：DEFAULT，举例 \u003c列名3\u003e VARCHAR(32) NOT NULL DEFAULT \"los angeles\" 表约束：PRIMARY KEY 主键约束（主键默认 UNIQUE 且 NOT NULL） 此外还有 FOREIGN KEY 和 CHECK 两个约束语句，在进阶笔记中介绍。 P.S. 字段约束也可以写成表约束（比如主键约束），而反过来很可能不行。 ","date":"2018-06-15","objectID":"/posts/sql-basic/:2:2","tags":["SQL","数据库"],"title":"SQL 基础笔记（一）","uri":"/posts/sql-basic/"},{"categories":["技术"],"content":"3. 删除表和更新表定义 删除表（危险操作） 删除整个表： DROP TABLE \u003c表名\u003e; - 只清空表内数据，但留下表： TRUNCATE \u003c表名\u003e; -- 非标准SQL语句，但是大部分DB都支持。（可能不能ROLLBACK） 更新表定义（麻烦的操作） 所以所创建表前要仔细想好格式了，更新表定义是不得已才能为之。 添加列定义： ALTER TABLE \u003c表名\u003e ADD COLUMN \u003c列名\u003e \u003c数据类型\u003e \u003c该列的约束\u003e; 删除列定义： ALTER TABLE \u003c表名\u003e DROP COLUMN \u003c列名\u003e; ","date":"2018-06-15","objectID":"/posts/sql-basic/:2:3","tags":["SQL","数据库"],"title":"SQL 基础笔记（一）","uri":"/posts/sql-basic/"},{"categories":["技术"],"content":"三、DML 万恶之源 NULL ","date":"2018-06-15","objectID":"/posts/sql-basic/:3:0","tags":["SQL","数据库"],"title":"SQL 基础笔记（一）","uri":"/posts/sql-basic/"},{"categories":["技术"],"content":"1. 查询（重点） 基本语句： SELECT \u003c字段1\u003e AS \u003c别名1\u003e, \u003c字段2\u003e AS \u003c别名2\u003e, ... FROM \u003c表名\u003e WHERE \u003c过滤条件\u003e; 可用 DISTINCT 修饰列名，使查询结果无重。例：SELECT DISTINCT \u003c列名\u003e FROM \u003c表名\u003e 过滤条件可使用比较运算(\u003c\u003e、=等)和逻辑运算(AND OR NOT). 过滤条件中，比较运算会永远忽略 NULL 值，如果需要对 NULL 值做操作，需要使用 IS NULL 或 IS NOT NULL（说忽略也许不太准确，NULL 既不为真也不为假，反正少用NULL。。） 包含NULL的四则运算，得到的结果总为NULL ","date":"2018-06-15","objectID":"/posts/sql-basic/:3:1","tags":["SQL","数据库"],"title":"SQL 基础笔记（一）","uri":"/posts/sql-basic/"},{"categories":["技术"],"content":"2. 聚合与排序（重点） 聚合函数 即对列进行统计分析的函数，主要有五个： COUNT：计算列的行数。（只有COUNT(*)会计算NULL行） SUM：求该列之和。 AVG：求该列的平均值。 MAX/MIN：求该列的 最大/最小 值 NOTE： 聚合函数计算时会排除所有NULL行。只有COUNT(*)例外，NULL行也会被它计数。 MAX/MIN 几乎适用于所有数据类型的列（对非数值型，以其二进制值来排序），而SUM/AVG只能用于数值类型的列。 聚合函数操作的列，也能用DISTINCT修饰。例：SELECT COUNT(DISTINCT \u003c列名\u003e) FROM \u003c表名\u003e 聚合函数只能用于SELECT子句和还没讲的HAVING子句（以及 ORDER BY 子句）中。 分组(GROUP BY) 分组以聚合键为分类标准，将数据分为多个逻辑组，从而能分别对每个组进行聚合运算。（分组是为了分类聚合） 若出现了 GROUP BY 子句，查询结果一定是每一组一行。 GROUP BY 会将 NULL 作为一组特定数据，显示为空。 聚合对SELECT子句的限制 首先要理解的是： 聚合函数的操作对象为某一列，而产生的结果只有一个值。 GROUP BY 的操作对象为一列或者多列，产生的结果呢，是每一组一个值。 因此为了避免歧义，只要使用了聚合函数或 GROUP BY 二者之一，SELECT 子句就只能包含： 常数 其他聚合函数（该聚合函数的操作对象可以为其他列） 如果使用了 GROUP BY 子句，还能包括该子句所指定的列名。（也就是聚合键）但是绝不能包含其他的列名，因为这会有歧义。 此外，还有一个问题是由 SQL 的执行顺序引起的。应该能很容易猜到，SELECT 语句的执行顺序和书写顺序是不一致的。 查询应该是从表开始，所以 FROM 语句一定先执行。然后应该要过滤(WHERE)，再是分组(GROUP BY)，最后才是 SELECT 语句。（就已经学到的子句而言，顺序是这样） 因此按理说，SELECT 语句 定义的别名，是不能在 GROUP BY 里使用的。（也有些DB支持该用法，但不通用） 对聚合结果进行过滤(HAVING) 从刚刚说过的SQL执行顺序可见，WHERE要比GROUP BY先执行，因此如果想过滤分组后的结果，不能用它。而应该使用 HAVING 子句。 HAVING 子句和 WHERE 子句都是用来过滤的，但是执行顺序的不同也就决定了它们的用途不同。 NOTE： 有时候，会发现某个过滤条件，不论是先执行（就是写在WHERE子句中）还是后执行（写在HAVING中）都没问题，这时候应该将它写在WHERE子句中，这样GROUP BY操作的数据会更少，处理更快。 HAVING 子句的元素，也存在和 SELECT 子句同样的限制。不能使用聚合键以外的列名。 排序(ORDER BY) ORDER BY 子句在 SELECT 子句之后执行，因此它能使用 SELECT 子句中定义的别名。（而 GROUP BY 之前已经说过不能用别名了） 格式： SELECT \u003c字段1\u003e AS \u003c别名1\u003e, \u003c字段2\u003e AS \u003c别名2\u003e, ... FROM \u003c表名\u003e WHERE \u003c过滤条件1\u003e GROUP BY \u003c列名1\u003e, \u003c列名2\u003e... HAVING \u003c过滤条件2\u003e ORDER BY \u003c列名/别名1\u003e, \u003c列名/别名2\u003e... ; 多排序键/列：指定多排序键时的排序规则为：优先使用左侧的列，如果该列存在相同值，再接着参考右侧的键，依此类推。（如果左侧键值不同，右侧的键就不会被使用了） NULL 值的顺序：排序键中出现了 NULL 值时，这类值会在结果的开头或结尾汇总，究竟是排在开头还是结尾，并没有特殊规定。 ORDER BY 子句只影响结果的先后顺序，因此排序键可以是结果集以外的东西，比如其他的列，或者使用了 GROUP BY 时，还能用聚合函数。 ","date":"2018-06-15","objectID":"/posts/sql-basic/:3:2","tags":["SQL","数据库"],"title":"SQL 基础笔记（一）","uri":"/posts/sql-basic/"},{"categories":["技术"],"content":"3. 数据的增、删、改 插入(INSERT INTO) 也算用的多了 语法： INSERT INTO \u003c表名\u003e (列名1, 列名2...) VALUES (值1, 值2...); 或者也可以使用 SELECT 语句来替代 VALUES 子句，达到将 SELECT 到的结果集插入某表的效果。（但是不要用ORDER BY，因为结果是集，没有顺序，排序是徒劳无功的） 插入时主键不能重复，否则会报错。（因此如果需要包含重复数据，一般都会定义一个自增的id字段） 删除(DELETE) 清空表（危险操作，而且效率不如 TRUNCATE）： DELETE FROM \u003c表名\u003e; 条件删除： DELETE FROM \u003c表名\u003e WHERE \u003c条件\u003e; 因此使用DELETE时，一定要记得带WHERE，不然就好玩了。。 更新(UPDATE) UPDATE \u003c表名\u003e SET \u003c列名1\u003e = \u003c算术表达式1\u003e, \u003c列名2\u003e = \u003c表达式2\u003e, ... WHERE \u003c条件\u003e; 同 DELETE 一样，不带 WHERE 子句的 UPDATE 是很危险的。 ","date":"2018-06-15","objectID":"/posts/sql-basic/:3:3","tags":["SQL","数据库"],"title":"SQL 基础笔记（一）","uri":"/posts/sql-basic/"},{"categories":["技术"],"content":"四、DCL - 事务处理(MySQL) 事务是一系列不可分割的数据库操作，也就是说，这一系列操作要么全部执行，要么全部不执行。如果执行过程中发生了问题（检查执行状态），可以通过执行 ROLLBACK 回滚到该事务执行前的状态。（注意并不会自动回滚） START TRANSACTION; -- do somthing COMMIT; START TRANSACTION: 标识事务的开始 COMMIT：提交事务。一旦提交，所执行过的操作就已成定论，恢复不了了。 ROLLBACK：事务回滚，**只能回滚未 COMMIT 的 DML 操作！**也就是说只能用在 START TRANSACTION 和 COMMIT 之间，并且只能回滚 INSERT/UPDATE/DELETE。（回滚 SELECT 没啥意义） SAVEPOINT \u003c保留点\u003e 和 ROLLBACK TO \u003c保留点\u003e：同样只能用在 START TRANSACTION 和 COMMIT 之间，其优势在于，ROLLBACK TO 可以指定回滚到某特定保留点，更灵活，而 ROLLBACK 只能回滚到事务开始前。 需要注意的有： COMMIT 和 ROLLBACK 语句也是事务的结束，因此如果执行了 ROLLBACK，那它与 COMMIT 之间的内容会被跳过。（在这一点上，它相当于大多数 PL 的 return） 如果事务执行出现问题，问题行后面的所有语句都不会被执行！包括 COMMIT 和 ROLLBACK！ 如果想用纯 SQL 实现事务原子性，必须使用存储过程检查执行状态！举例如下： CREATE PROCEDURE my_test() BEGIN DECLARE EXIT HANDLER FOR SQLEXCEPTION ROLLBACK -- 检测到 SQLEXCEPTION 则 rollback，然后 exit START TRANSACTION INSERT INTO table_test VALUES(1, 'A') INSERT INTO table_test VALUES(1, 'B') -- 这里主键冲突，会触发 SQLEXCEPTION COMMIT END CALL my_test() 或者在 PL 中通过异常处理执行 ROLLBACK。（事务虽然中止了，但并未结束！所以仍然可以 ROLLBACK 或者 COMMIT） ","date":"2018-06-15","objectID":"/posts/sql-basic/:4:0","tags":["SQL","数据库"],"title":"SQL 基础笔记（一）","uri":"/posts/sql-basic/"},{"categories":["技术"],"content":"数据何时被提交到数据库 显式提交：在事务中使用 COMMIT 提交数据操作被称为显式提交 隐式提交：非 DML 操作会被立即提交，也就是说这些语句本身就隐含了提交语义 自动提交： 如果 AUTOCOMMIT 被设置为 ON，当前 session 中的 DML 语句会在执行后被自动提交（START TRANSACTION 内部的 DML 除外，在它内部必须显式 COMMIT） 所有的 DML 语句都是要显式提交的，MySQL session 的 AUTOCOMMIT 默认为 ON，所以 DML 会被自动提交。 P.S. 许多语言的数据库 API 会定义自己的事务操作，不一定与这里一致。 ","date":"2018-06-15","objectID":"/posts/sql-basic/:4:1","tags":["SQL","数据库"],"title":"SQL 基础笔记（一）","uri":"/posts/sql-basic/"},{"categories":["随笔"],"content":"对我而言，学英语是一件挺痛苦的事。从初中开始学英文，可从来不觉得它有趣，主动性也就不强。 直到我开始学计算机，我开始认识到英文是不可避免的。于是尝试了很多方法。 最普遍的方式：背单词，可我从初中背到现在，背单词的计划从没哪次坚持超过一个月的。 后来听说看英文原版书有效，信心满满，结果也是看了一星期 Harry Potter，稍微看了点 Animal Farm，就没后续了。 又想练听力，开始听 ESLPod、EnglishPod，也就听了一个月的时间。。 练口语，听完了《赖世雄美语音标》，又看了点美国的口语纠正视频。这件事干得倒还算可以，口语的确标准了不少。不过也就花了十多天，就没然后了。 我学计算机的过程，和我的英文学习过程也有不少重合的地方。 印象中第一次读英文资料，是想学计算机图形学，被知乎上的高手们推荐看一个英文教程。死嗑了三天，坚持不下去放弃了。。 之后听了季索清学长的推荐，又见知乎上也都说 Python 好，就开始学 Python。看了 A Byte of Python, 印象中花了一天看完的，但是没在脑子里留下啥印象。 后来慢慢的开始熟悉 Python，在图书馆借了 Head First Python 英文版，可能是被厚度吓到了，看了几页就不了了之了。。 再后来用 Github，Pycharm IDEA 也是英文的，Python doc 和 Java doc 也全是英文的，标准库里的注释是英文的，Error 信息是英文的…… 虽然学啥都半途而废，但英文水平的确是慢慢地提升着。 慢慢地，能够不怎么吃力地看懂 Python 标准库了，有问题也可以看英文博客解决了。 最近看一个动漫看完不过瘾，转去看这部动漫的轻小说。轻小说中文翻译的太呆板，发现居然有英文的，直接啃起了英文。 换了好几部小说，现在在看 Grimgar of Fantasy and Ash、Tasogare-iro no Uta Tsukai(黄昏色の詠使い)，还找了个英文动漫网站，颇有以后看动漫也只看英文字幕的打算。希望能持续下去吧。 不过也有点苦恼，因为暑假就要找工作，现在却沉迷看英文小说。。本来这个月该学算法的，可半个月都过了，我的进度大概才认真的时候的五天的样子。。真不知道未来会是啥样。 ","date":"2018-05-16","objectID":"/posts/learning-english/:0:0","tags":["英语","语言学习"],"title":"学英语啊学英语","uri":"/posts/learning-english/"},{"categories":["书藉","影视"],"content":"看完了动画，也看了点小说。最敬佩、最喜欢、最向往的人物是珠晶，也就是供王。能感觉得到她是所有角色中，最自信、方向最明确的，而且她思考一直比较理性。身为富商之女，年仅十二，却能拥有超出所有国民的觉悟，“既然大人们没有勇气，那就由我去当王！”，并最终称王，不得不敬佩。她有句让人难以忘却的台词，“我之所以能过着比别人更好的生活，是因为我担负了相比更沉重的责任。如果没能完成相应的使命，我就会像峰王一样被砍掉脑袋。而祥琼没有认识到这一点，她不想担负责任，却觉得自己应该享受荣华富贵。” 其次就是“专职心理治疗”的乐俊小老鼠了，我简直有点怀疑存不存在这样可敬的老鼠(废话)。乐俊成熟得不适合当主角，几乎无可挑剔，大概也因此而戏份不多。 而花了大篇幅描写的庆东国的景王，还有一路走来的祥琼和铃、更夜还有泰麒，他们一度迷失掉了自我，虽然作为结果的他们实现了自我救赎，但是这个过程我喜欢不起来。大概因为我也是个偏激的人吧…… 珠晶遇到了顽丘，景王和祥琼被乐俊救赎，铃也有自己的贵人，更夜在斡由被杀时终于承认了自己的错误，泰麒也是被麒麟们合力救回来的。 谁都不可能只活在自己的世界，就能得到救赎。(这样就又得到了一个和刺猬的优雅类似的结论…) ","date":"2018-04-27","objectID":"/posts/the-twelve-kingdoms/:0:0","tags":["读后感","观后感","动漫"],"title":"《十二国记》","uri":"/posts/the-twelve-kingdoms/"},{"categories":["随笔"],"content":" 啊啊，还有十天就可以摆脱这个城市，回到那个令人安心的山林里了，一边期待着，一边焦躁着，想着为什么剩下的十天这么难熬这样的问题。 复习又是一塌糊涂，我也太懒了点。 这样懒散的我还做着码完几千行代码这样的春秋大梦，太不现实了。有点想认命了。 半夜一点多，寝室空调还是不习惯，过道阳台上的凉风倒是很舒服，这座城市此刻的静谧倒也有几分韵味。 不过不管怎么说，好想回家… ","date":"2017-06-27","objectID":"/posts/the-end-of-another-semester/:0:0","tags":[],"title":"又一个期末","uri":"/posts/the-end-of-another-semester/"},{"categories":["数学","技术"],"content":"很早就学过欧几里得算法，但是一直不知道它的原理。几乎每本算法书都会提到它，但是貌似只有数学书上才会见到它的原理。。。 前段时间粗粗看了点数论（《什么是数学》），惊讶于这个原理的奇妙。现在把它通俗地写下来，以免自己忘记。 欧几里得算法是求两个数的最大公约数(Greatest Common Divisor (GCD))的算法，我们首先假设有两个数 $a$ 和 $b$，其中 $a$ 是不小于 $b$ 的数， 记 $a$ 被 $b$ 除的余数为 $r$，那么 $a$ 可以写成这样的形式： $$a = bq + r$$ 其中 $q$ 是整数（我们不需要去管 $q$ 到底是多少，这和我们的目标无关）。 现在假设 $a$ 和 $b$ 的一个约数为 $u$，那么 $a$ 和 $b$ 都能被 $u$ 整除，即 $$a = su$$ $$b = tu$$ $s$ 和 $t$ 都是整数（同样的，我们只需要知道存在这样的整数 $s$ 和 $t$ 就行）。 这样可以得出 $$r = a - bq = su - (tu)q = (s - tq)u$$ 所以 $r$ 也能被 $u$ 整除，一般规律如下 $a$ 和 $b$ 的约数也整除它们的余数 $r$，所以 $a$ 和 $b$ 的任一约数同时也是 $b$ 和 $r$ 的约数。 —— 条件一 反过来可以得出 $b$ 和 $r$ 的任一约数同时也是 $a$ 和 $b$ 的约数。 ——条件二 这是因为对 $b$ 和 $r$ 每一个约数 $v$，有 $$b = kv$$ $$r = cv$$ 于是有 $$a = bq + r = (kv)q + cv = (kq + c)v$$ 由条件一和条件二可知 $a$ 和 $b$ 的约数的集合，全等于 $b$ 和 $r$ 的约数的集合。 于是 $a$ 和 $b$ 的最大公约数，就是 $b$ 和 $r$ 的最大公约数。 接下来用递推法， $a \\div b$ 余 $r$，现在设 $b \\div r$ 余 $r_1$ $r \\div r_1$ 余 $r_2$ …… $r_{n-3} \\div r_{n-2}$ 余 $r_{n-1}$ $r_{n-2} \\div r_{n-1}$ 余 $r_n=0$ 因为 $a \\ge b$，可以看出余数 $r_n$ 会越来越小，最终变成 $0$. 当 $r_{n-1} \\neq 0$ 且 $r_n = 0$ 时，可知 $r_{n-2}$ 可被 $r_{n-1}$ 整除（余数为 $0$ 嘛） 此时 $r_{n-2}$ 和 $r_{n-1}$ 的约数就只有：$r_{n-1}$ 和 $r_{n-1}$ 的因数，所以他们的最大公约数就是 $r_{n-1}$！ 所以 $r_{n-1}$ 就是 $a$ 和 $b$ 的最大公约数。（若 $r = 0$，则 $b$ 为最大公约数） 这个递推法写成c语言函数是这样的（比推导更简洁…）: unsigned int Gcd(unsigned int M,unsigned int N){ unsigned int Rem; while(N){ Rem = M % N; M = N; N = Rem; } return Rem; } 可以发现这里没有要求 M\u003e=N，这是因为如果那样，循环会自动交换它们的值。 P.S. 此外，还有最小公倍数(Least Common Multiple (LCM))算法，详见 GCD and LCM calculator ","date":"2017-05-26","objectID":"/posts/mathematics-in-euclidean-gcd/:0:0","tags":["算法"],"title":"欧几里得算法求最大公约数(GCD)的数学原理","uri":"/posts/mathematics-in-euclidean-gcd/"},{"categories":["随笔"],"content":"生活总是在给你希望之时，再埋点伏笔。本来我以为进了大学，就是一个全新的世界了，我可以重新开始，只要我很努力很努力，一切困难都将不堪一击。 显然那个时候，我还不知道，现实不同于想象。 高三在高压下全线崩溃，因此对大学寄予了过多期望。但这期望同时也带来了更大的压力。 我患上了阅读焦虑症。 从进入大学的那一刻起，就开始疯狂地制定阅读计划，泡图书馆，看各种学习方法、读书方法、记忆方法、速读术之类的书籍，恨不得一目十行。 但是很快的，我就发现自己出了问题：我太想提升自己了，因此翻开书的第一页，就期盼着翻到最后一页，读书的愉悦，被对看完一本书的渴望冲淡了。更多的时候，感觉到的是还没把这本书看完的焦虑。 而且因为长时间全神贯注，一本书看不到一半，耐心也渐渐失去，于是翻页速度越来越快，这个时候所谓的“阅读”已经名存实亡了。 这样的阅读的结果，只是在读书量上徒然添加几个数字，于自我提升而言，却是收效甚微。我很明白这一点，但是明白和作出改变之间，隔着一道鸿沟，我怎么也跨不过去。明明知道松弛有度效率会更高，但是心理上的焦虑让我无法说服自己放下书本哪怕一分钟，直到自己的耐心消耗殆尽…… 买了一大堆文学书放在柜头。可笑的是，大一整整一年，除了韩寒，我没看任何一本文学书超过半小时。“快速浏览”完十几本方法类书籍后，我开始阅读技术书籍。但是除了韩寒的书和几本技术书籍，阅读过程中的焦虑感从未远离我，这不仅降低了我的学习效率，更让我的倦怠期长了数倍(过度消耗精力)。其结果是，往往一本厚一点的书读上两三天，就有半个月会厌倦到不想碰它。 我能感觉到如果按着计划读书，我的成果绝不会差到现在这样。也想着有计划性一点，可是一看QQ，人家初三的小男孩已经学遍了高中数学、算法、初等数论、自然数学……网上认识的同龄人已经开始做神经网络了，知乎上一大群自学者也在努力攻克python/c/算法，我就停不下来，甚至平静下来做个计划都觉得浪费时间(实际上很明显这样带着焦虑阅读才是浪费时间)。 迫切的想要成为那个“自己想要成为的人”，因此连基本的理性都无法保持。 我想要的是从容、带着脑子的阅读，而不是这样走马观花，盲目追求量的阅读。 我又焦虑地打开知乎，不断搜索，然后写下这篇文章。 ","date":"2017-03-07","objectID":"/posts/reading-anxiety/:0:0","tags":[],"title":"我患上了阅读焦虑症","uri":"/posts/reading-anxiety/"},{"categories":["随笔"],"content":" 2017年2月的18号，清晨6点。天还只是朦朦亮，当空挂着半边弯月，一颗不知名的星星(大约是大角星)缀在月的旁边。 还没开学，学校几乎看不到人。 南食堂的一楼已亮起了灯，鸟儿们开始鸣叫个不停，可以听出有好几种鸟叫声。 易海仍是风平浪静。 我背着书包，拖着皮箱，耳边最清晰的声音便是皮箱轮胎与地面的摩擦声。 手机随便放起一首歌，恰好是《遥远的歌》。这首歌真是应景呢，逝去的时光遥远得无法触及，自己也离家千里，未来更是难以捉摸。 我还会记得吗？记得这个我印象中，最宁静安详的，安徽建筑大学。 ","date":"2017-02-18","objectID":"/posts/quiet-and-peaceful-campus/:0:0","tags":[],"title":"少有人迹的校园","uri":"/posts/quiet-and-peaceful-campus/"},{"categories":["随笔"],"content":" 上个暑假，刚刚从低谷爬出来，那时候整个人散发着一股子向上的气息，豪情万丈，甚至感染了周围的亲朋好友。那个时候，满以为以后的挫折都不能阻挡我的脚步。 可是，到底为什么，现在又变成了这个样子了呢？人生这样的东西，总是出人意料，以至于怎么也猜不透。 十二月十四，我度过了我的十九岁生日，现在应该正是那所谓的青春将逝未逝之时。而我的青春应该最辉煌的时候，我在干什么呢？ 我在翻山越岭。 上山时一路坎坷，累的要死不活的。陡然间萌生退意，心就在不断挣扎。就在却意战胜壮志之时，忽然间天地开阔，才发觉自己已然站在了大山之巅，于是一切痛苦尽皆远去，心也变得如这天地一般开阔。这个时候自然豪情万丈，看山山美，看水水秀。想当然的就觉得后面的山岭有再多的阻碍，也不能阻挡这个见过如此美景的登山人了。 可是事与愿违，山岭就像时间一样看不到边，翻过了一座又是一座，这又是一种更大的痛苦。这个登山人身心俱疲，只好万事随缘，继续一脚深，一脚浅的往那无尽山岭行去。 最近看了是枝裕和的电影《比海更深》，“我的人生到底出了什么差错？” 这样一个问句，道出了多少辛酸苦辣……想起了以前写过一篇文章，标题是《对不起，我没有成为你想成为的那个人》。 理想与现实之间仿佛总隔着一道鸿沟。 现在没有了万丈豪情，不再敢说“未来将是一片坦途”；也没有绝望到要写“我的人生到底出了什么差错？”这样的句子，那还是用我最喜欢的那个模棱两可的四字词作结吧。 且行且寻 ","date":"2017-02-06","objectID":"/posts/the-holiday-is-coming-to-an-end/:0:0","tags":[],"title":"忽而假末","uri":"/posts/the-holiday-is-coming-to-an-end/"}]