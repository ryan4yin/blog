<!doctype html><html lang=zh-CN><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=robots content="noodp"><title>Kubernetes 常见错误、原因及处理方法 - This Cute World</title><meta name=Description content="This Cute World"><meta property="og:url" content="https://thiscute.world/posts/kubernetes-common-errors-and-solutions/">
<meta property="og:site_name" content="This Cute World"><meta property="og:title" content="Kubernetes 常见错误、原因及处理方法"><meta property="og:description" content="Pod 常见错误 OOMKilled: Pod 的内存使用超出了 resources.limits 中的限制，被强制杀死。 SandboxChanged: Pod sandbox changed, it will be killed and re-created: 很可能是由于内存限制导致容器被 OOMKilled，或者其他资源不足"><meta property="og:locale" content="zh_CN"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2019-11-24T19:26:54+08:00"><meta property="article:modified_time" content="2019-11-24T19:26:54+08:00"><meta property="article:tag" content="Kubernetes"><meta property="og:image" content="https://thiscute.world/posts/kubernetes-common-errors-and-solutions/featured-image.webp"><meta property="og:see_also" content="https://thiscute.world/posts/finops-for-kubernetes/"><meta property="og:see_also" content="https://thiscute.world/posts/kubernetes-deployment-using-kubeadm/"><meta property="og:see_also" content="https://thiscute.world/posts/kubernetes-best-practices/"><meta property="og:see_also" content="https://thiscute.world/posts/experience-of-argo-workflows/"><meta property="og:see_also" content="https://thiscute.world/posts/experience-of-pulumi/"><meta property="og:see_also" content="https://thiscute.world/posts/use-istio-for-jwt-auth/"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://thiscute.world/posts/kubernetes-common-errors-and-solutions/featured-image.webp"><meta name=twitter:title content="Kubernetes 常见错误、原因及处理方法"><meta name=twitter:description content="Pod 常见错误 OOMKilled: Pod 的内存使用超出了 resources.limits 中的限制，被强制杀死。 SandboxChanged: Pod sandbox changed, it will be killed and re-created: 很可能是由于内存限制导致容器被 OOMKilled，或者其他资源不足"><meta name=twitter:site content="@ryan4yin"><meta name=application-name content="This Cute World"><meta name=apple-mobile-web-app-title content="This Cute World"><meta name=theme-color content="#f8f8f8"><meta name=twitter:creator content="@ryan4yin"><link rel="shortcut icon" type=image/x-icon href=/favicon.ico><link rel=icon type=image/png sizes=32x32 href=/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/favicon-16x16.png><link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png><link rel=canonical href=https://thiscute.world/posts/kubernetes-common-errors-and-solutions/><link rel=prev href=https://thiscute.world/posts/manjaro-instruction/><link rel=next href=https://thiscute.world/posts/2019-summary/><link rel=alternate href=/posts/kubernetes-common-errors-and-solutions/index.xml type=application/rss+xml title="This Cute World"><link rel=feed href=/posts/kubernetes-common-errors-and-solutions/index.xml type=application/rss+xml title="This Cute World"><link rel=stylesheet href=/css/main.css><link rel=stylesheet href=/lib/normalize/normalize.min.css><link rel=stylesheet href=/css/color.css><link rel=stylesheet href=/css/style.min.css><link rel=preload as=style onload='this.onload=null,this.rel="stylesheet"' href=/lib/fontawesome-free/all.min.css><noscript><link rel=stylesheet href=/lib/fontawesome-free/all.min.css></noscript><link rel=preload as=style onload='this.onload=null,this.rel="stylesheet"' href=/lib/animate/animate.min.css><noscript><link rel=stylesheet href=/lib/animate/animate.min.css></noscript><meta name=google-site-verification content="E8bpp1lVVlb9YnSJcUzPL1dLAG17Nl_sp5Ru9a8tUDQ"><meta name=baidu-site-verification content="code-ZZtDruAnX1"><script type=application/ld+json>{"@context":"http://schema.org","@type":"BlogPosting","headline":"Kubernetes 常见错误、原因及处理方法","inLanguage":"zh-CN","mainEntityOfPage":{"@type":"WebPage","@id":"https://thiscute.world/posts/kubernetes-common-errors-and-solutions/"},"image":[{"@type":"ImageObject","url":"https://thiscute.world/posts/kubernetes-common-errors-and-solutions/featured-image.webp","width":1920,"height":960}],"genre":"posts","keywords":"Kubernetes","wordcount":4577,"url":"https://thiscute.world/posts/kubernetes-common-errors-and-solutions/","datePublished":"2019-11-24T19:26:54+08:00","dateModified":"2019-11-24T19:26:54+08:00","publisher":{"@type":"Organization","name":"ryan4yin","logo":"https://thiscute.world/avatar/myself.png"},"author":{"@type":"Person","name":"ryan4yin"},"description":""}</script></head><body header-desktop=fixed header-mobile=auto><script type=text/javascript>function setTheme(e){document.body.setAttribute("theme",e),document.documentElement.style.setProperty("color-scheme",e==="light"?"light":"dark"),window.theme=e,window.isDark=window.theme!=="light"}function saveTheme(e){window.localStorage&&localStorage.setItem("theme",e)}function getMeta(e){const t=document.getElementsByTagName("meta");for(let n=0;n<t.length;n++)if(t[n].getAttribute("name")===e)return t[n];return""}if(window.localStorage&&localStorage.getItem("theme")){let e=localStorage.getItem("theme");e==="light"||e==="dark"||e==="black"?setTheme(e):setTheme(window.matchMedia&&window.matchMedia("(prefers-color-scheme: dark)").matches?"dark":"light")}else"auto"==="light"||"auto"==="dark"||"auto"==="black"?(setTheme("auto"),saveTheme("auto")):(saveTheme("auto"),setTheme(window.matchMedia&&window.matchMedia("(prefers-color-scheme: dark)").matches?"dark":"light"));let metaColors={light:"#f8f8f8",dark:"#252627",black:"#000000"};getMeta("theme-color").content=metaColors[document.body.getAttribute("theme")],window.switchThemeEventSet=new Set</script><div id=back-to-top></div><div id=mask></div><div class=wrapper><header class=desktop id=header-desktop><div class=header-wrapper><div class=header-title><a href=/ title="This Cute World"><span id=desktop-header-typeit class=typeit></span></a></div><div class=menu><div class=menu-inner><a class=menu-item href=/statistics/>阅读排行 </a><a class=menu-item href=/series/>系列 </a><a class=menu-item href=/categories/tech/>技术 </a><a class=menu-item href=/categories/life/>生活 </a><a class=menu-item href=/tags/>标签 </a><a class=menu-item href=/friends/>朋友们 </a><a class=menu-item href=/now/>此刻 </a><a class=menu-item href=/about/>关于 </a><span class="menu-item delimiter"></span><a href=javascript:void(0); class="menu-item language" title=选择语言>Simplified Chinese<i class="fas fa-chevron-right fa-fw"></i>
<select class=language-select title=选择语言 id=language-select-desktop onchange="location=this.value"><option value=/posts/kubernetes-common-errors-and-solutions/ selected>Simplified Chinese</option></select>
</a><span class="menu-item search" id=search-desktop><input type=text placeholder=搜索文章标题或内容... id=search-input-desktop>
<a href=javascript:void(0); class="search-button search-toggle" id=search-toggle-desktop title=搜索><i class="fas fa-search fa-fw"></i>
</a><a href=javascript:void(0); class="search-button search-clear" id=search-clear-desktop title=清空><i class="fas fa-times-circle fa-fw"></i>
</a><span class="search-button search-loading" id=search-loading-desktop><i class="fas fa-spinner fa-fw fa-spin"></i>
</span></span><a href=javascript:void(0); class="menu-item theme-switch" title=切换主题><i class="fas fa-adjust fa-fw"></i></a></div></div></div></header><header class=mobile id=header-mobile><div class=header-container><div class=header-wrapper><div class=header-title><a href=/ title="This Cute World"><span id=mobile-header-typeit class=typeit></span></a></div><div class=menu-toggle id=menu-toggle-mobile><span></span><span></span><span></span></div></div><div class=menu id=menu-mobile><div class=search-wrapper><div class="search mobile" id=search-mobile><input type=text placeholder=搜索文章标题或内容... id=search-input-mobile>
<a href=javascript:void(0); class="search-button search-toggle" id=search-toggle-mobile title=搜索><i class="fas fa-search fa-fw"></i>
</a><a href=javascript:void(0); class="search-button search-clear" id=search-clear-mobile title=清空><i class="fas fa-times-circle fa-fw"></i>
</a><span class="search-button search-loading" id=search-loading-mobile><i class="fas fa-spinner fa-fw fa-spin"></i></span></div><a href=javascript:void(0); class=search-cancel id=search-cancel-mobile>取消</a></div><a class=menu-item href=/statistics/ title>阅读排行</a><a class=menu-item href=/series/ title>系列</a><a class=menu-item href=/categories/tech/ title>技术</a><a class=menu-item href=/categories/life/ title>生活</a><a class=menu-item href=/tags/ title>标签</a><a class=menu-item href=/friends/ title>朋友们</a><a class=menu-item href=/now/ title>此刻</a><a class=menu-item href=/about/ title>关于</a><a href=javascript:void(0); class="menu-item theme-switch" title=切换主题>
<i class="fas fa-adjust fa-fw"></i>
</a><a href=javascript:void(0); class=menu-item title=选择语言>Simplified Chinese<i class="fas fa-chevron-right fa-fw"></i>
<select class=language-select title=选择语言 onchange="location=this.value"><option value=/posts/kubernetes-common-errors-and-solutions/ selected>Simplified Chinese</option></select></a></div></div></header><div class="search-dropdown desktop"><div id=search-dropdown-desktop></div></div><div class="search-dropdown mobile"><div id=search-dropdown-mobile></div></div><main class=main><div class=container><div class=toc id=toc-auto><h2 class=toc-title>目录</h2><div class=toc-content id=toc-content-auto><nav id=TableOfContents><ul><li><a href=#pod-常见错误>Pod 常见错误</a><ul><li><a href=#控制面故障可能会导致各类奇怪的异常现象>控制面故障可能会导致各类奇怪的异常现象</a></li><li><a href=#pod-无法删除>Pod 无法删除</a></li><li><a href=#initcontainers-不断-restart但是-containers-却都显示已-ready>initContainers 不断 restart，但是 Containers 却都显示已 ready</a></li></ul></li><li><a href=#节点常见错误>节点常见错误</a></li><li><a href=#网络常见错误>网络常见错误</a><ul><li><a href=#1-ingressistio-gateway-返回值>1. Ingress/Istio Gateway 返回值</a></li><li><a href=#2-上了-istio-sidecar-后应用程序偶尔间隔几天半个月会-redis-连接相关的错误>2. 上了 istio sidecar 后，应用程序偶尔（间隔几天半个月）会 redis 连接相关的错误</a></li></ul></li><li><a href=#名字空间常见错误>名字空间常见错误</a><ul><li><a href=#名字空间无法删除>名字空间无法删除</a></li></ul></li><li><a href=#kubectlistioctl-等客户端工具异常>kubectl/istioctl 等客户端工具异常</a></li><li><a href=#批量清理-evicted-记录>批量清理 Evicted 记录</a></li><li><a href=#容器镜像gcpod驱逐以及节点压力>容器镜像GC、Pod驱逐以及节点压力</a></li><li><a href=#监控hpa-常见错误>监控/HPA 常见错误</a><ul><li><a href=#服务设置了-hpa-阈值为-50-cpu所有业务容器在启动后不久就会-oomcpu-暴涨然后挂掉但是无法触发-cpu-扩缩容prometheus-监控指标也不对劲>服务设置了 HPA 阈值为 50% CPU，所有业务容器在启动后不久就会 OOM，CPU 暴涨然后挂掉。但是无法触发 CPU 扩缩容，Prometheus 监控指标也不对劲。</a></li></ul></li><li><a href=#其他问题>其他问题</a><ul><li><a href=#隔天-istio-等工具的-sidecar-自动注入莫名其妙失效了>隔天 Istio 等工具的 sidecar 自动注入莫名其妙失效了</a></li><li><a href=#如何重新运行一个-job>如何重新运行一个 Job？</a></li></ul></li><li><a href=#参考>参考</a></li></ul></nav></div></div><script>document.getElementsByTagName("main")[0].setAttribute("autoTOC","true")</script><article class="page single"><h1 class="single-title animate__animated animate__flipInX">Kubernetes 常见错误、原因及处理方法</h1><div class=post-meta><div class=post-meta-line><span class=post-author><span class="author fas fa-user-circle fa-fw"></span><a href=https://thiscute.world/ title=Author target=_blank rel="noopener noreferrer author" class=author>ryan4yin</a>
</span>&nbsp;<span class=post-category>收录于 </span>&nbsp;<span class=post-category>类别 <a href=/categories/tech/><i class="far fa-folder fa-fw"></i>Tech</a></span>&nbsp;<span class=post-category>和</span>&nbsp;<span class=post-series>系列 <a href=/series/%E4%BA%91%E5%8E%9F%E7%94%9F%E7%9B%B8%E5%85%B3/><i class="far fa-list-alt fa-fw"></i>云原生相关</a></span></div><div class=post-meta-line><i class="far fa-calendar-alt fa-fw"></i>&nbsp;<time datetime=2019-11-24>2019-11-24</time>&nbsp;<i class="far fa-edit fa-fw"></i>&nbsp;<time datetime=2019-11-24>2019-11-24</time>&nbsp;<i class="fas fa-pencil-alt fa-fw"></i>&nbsp;约 4577 字&nbsp;<i class="far fa-clock fa-fw"></i>&nbsp;预计阅读 10 分钟&nbsp;</div></div><div class=featured-image><img loading=eager src=/posts/kubernetes-common-errors-and-solutions/featured-image.webp srcset="/posts/kubernetes-common-errors-and-solutions/featured-image.webp, /posts/kubernetes-common-errors-and-solutions/featured-image.webp 1.5x, /posts/kubernetes-common-errors-and-solutions/featured-image.webp 2x" sizes=auto alt=/posts/kubernetes-common-errors-and-solutions/featured-image.webp title=/posts/kubernetes-common-errors-and-solutions/featured-image.webp height=960 width=1920></div><div class="details series-nav open"><div class="details-summary series-title"><span>系列 - </span><span><i class="details-icon fas fa-angle-right"></i></span></div><div class="details-content series-content"><nav><ul><li><a href=/posts/finops-for-kubernetes/>FinOps for Kubernetes - 如何拆分 Kubernetes 成本</a></li><li><a href=/posts/kubernetes-deployment-using-kubeadm/>部署一个 Kubernetes 集群</a></li><li><a href=/posts/kubernetes-best-practices/>Kubernetes 微服务最佳实践</a></li><li><a href=/posts/experience-of-argo-workflows/>云原生流水线 Argo Workflows 的安装、使用以及个人体验</a></li><li><a href=/posts/experience-of-pulumi/>Pulumi 使用体验 - 基础设施代码化</a></li><li><a href=/posts/use-istio-for-jwt-auth/>使用 Istio 进行 JWT 身份验证（充当 API 网关）</a></li><li><span class=active>Kubernetes 常见错误、原因及处理方法</span></li><li><a href=/posts/experience-of-vault/>secrets 管理工具 Vault 的介绍、安装及使用</a></li><li><a href=/posts/kubernetes-cert-management/>Kubernetes 中的证书管理工具 - cert-manager</a></li></ul></nav></div></div><div class="details toc" id=toc-static kept><div class="details-summary toc-title"><span>目录</span>
<span><i class="details-icon fas fa-angle-right"></i></span></div><div class="details-content toc-content" id=toc-content-static><nav id=TableOfContents><ul><li><a href=#pod-常见错误>Pod 常见错误</a><ul><li><a href=#控制面故障可能会导致各类奇怪的异常现象>控制面故障可能会导致各类奇怪的异常现象</a></li><li><a href=#pod-无法删除>Pod 无法删除</a></li><li><a href=#initcontainers-不断-restart但是-containers-却都显示已-ready>initContainers 不断 restart，但是 Containers 却都显示已 ready</a></li></ul></li><li><a href=#节点常见错误>节点常见错误</a></li><li><a href=#网络常见错误>网络常见错误</a><ul><li><a href=#1-ingressistio-gateway-返回值>1. Ingress/Istio Gateway 返回值</a></li><li><a href=#2-上了-istio-sidecar-后应用程序偶尔间隔几天半个月会-redis-连接相关的错误>2. 上了 istio sidecar 后，应用程序偶尔（间隔几天半个月）会 redis 连接相关的错误</a></li></ul></li><li><a href=#名字空间常见错误>名字空间常见错误</a><ul><li><a href=#名字空间无法删除>名字空间无法删除</a></li></ul></li><li><a href=#kubectlistioctl-等客户端工具异常>kubectl/istioctl 等客户端工具异常</a></li><li><a href=#批量清理-evicted-记录>批量清理 Evicted 记录</a></li><li><a href=#容器镜像gcpod驱逐以及节点压力>容器镜像GC、Pod驱逐以及节点压力</a></li><li><a href=#监控hpa-常见错误>监控/HPA 常见错误</a><ul><li><a href=#服务设置了-hpa-阈值为-50-cpu所有业务容器在启动后不久就会-oomcpu-暴涨然后挂掉但是无法触发-cpu-扩缩容prometheus-监控指标也不对劲>服务设置了 HPA 阈值为 50% CPU，所有业务容器在启动后不久就会 OOM，CPU 暴涨然后挂掉。但是无法触发 CPU 扩缩容，Prometheus 监控指标也不对劲。</a></li></ul></li><li><a href=#其他问题>其他问题</a><ul><li><a href=#隔天-istio-等工具的-sidecar-自动注入莫名其妙失效了>隔天 Istio 等工具的 sidecar 自动注入莫名其妙失效了</a></li><li><a href=#如何重新运行一个-job>如何重新运行一个 Job？</a></li></ul></li><li><a href=#参考>参考</a></li></ul></nav></div></div><div class=content id=content><h2 id=pod-常见错误 class=headerLink><a href=#pod-%e5%b8%b8%e8%a7%81%e9%94%99%e8%af%af class=header-mark></a>Pod 常见错误</h2><ol><li>OOMKilled: Pod 的内存使用超出了 resources.limits 中的限制，被强制杀死。</li><li><a href=https://cloud.tencent.com/developer/article/1411527 target=_blank rel="noopener noreferrer">SandboxChanged: Pod sandbox changed, it will be killed and re-created</a>:
很可能是由于内存限制导致容器被 OOMKilled，或者其他资源不足<ol><li>如果是 OOM，容器通常会被重启，<code>kubectl describe</code> 能看到容器上次被重启的原因<code>State.Last State.Reason = OOMKilled, Exit Code=137</code>.</li></ol></li><li>Pod 不断被重启，<code>kubectl describe</code> 显示重启原因<code>State.Last State.Reason = Error, Exit Code=137</code>，137 对应 SIGKILL(<code>kill -9</code>) 信号，说明容器被强制重启。可能的原因：<ol><li>最有可能的原因是，存活探针（livenessProbe）检查失败</li><li>节点资源不足，内核强制关闭了进程以释放资源，这种情况可以通过 <code>journalctl -k</code> 查看详细的系统日志。</li></ol></li><li>CrashLoopBackoff: Pod 进入 <strong>崩溃-重启</strong>循环，重启间隔时间从 10 20 40 80 一直翻倍到上限
300 秒，然后以 300 秒为间隔无限重启。</li><li>Pod 一直 Pending: 这说明没有任何节点能满足 Pod 的要求，容器无法被调度。比如端口被别的容器用 hostPort 占用，节点有污点等。</li><li><a href rel>FailedCreateSandBox: Failed create pod sandbox: rpc error: code = DeadlineExceeded desc = context deadline exceeded</a>：
很可能是 CNI 网络插件的问题（比如 ip 地址溢出），</li><li><a href=https://github.com/kubernetes/kubernetes/issues/55094 target=_blank rel="noopener noreferrer">FailedSync: error determining status: rpc error: code = DeadlineExceeded desc = context deadline exceeded</a>:
常和前两个错误先后出现，很可能是 CNI 网络插件的问题。</li><li>开发集群，一次性部署所有服务时，各 Pod 互相争抢资源，导致 Pod 生存探针失败，不断重启，
重启进一步加重资源使用。恶性循环。<ul><li><strong>需要给每个 Pod 加上 resources.requests，这样资源不足时，后续 Pod 会停止调度，直到资源恢复正常</strong>。</li></ul></li><li>Pod 出现大量的 Failed 记录，Deployment 一直重复建立 Pod: 通过<code>kubectl describe/edit pod &lt;pod-name></code> 查看 pod <code>Events</code> 和 <code>Status</code>，一般会看到失败信息，如节点异常导致 Pod 被驱逐。</li><li><a href=https://zhuanlan.zhihu.com/p/70031676 target=_blank rel="noopener noreferrer">Kubernetes 问题排查：Pod 状态一直 Terminating</a></li><li>创建了 Deployment 后，却没有自动创建 Pod: 缺少某些创建 Pod 必要的东西，比如设定的
ServiceAccount 不存在。</li><li>Pod 运行失败，状态为 MatchNodeSelector: 对主节点进行关机、迁移等操作，导致主调度器下线时，会在一段时间内导致 Pod 调度失败，调度失败会报这个错。</li><li>Pod 仍然存在，但是 <code>Service</code> 的 Endpoints 却为空，找不到对应的 Pod IPs: 遇到过一次，是因为时间跳变（从未来的时间改回了当前时间）导致的问题。</li><li>Pod 无法调度，报错 <code>x node(s) had volume node affinity conflict</code>: 说明该 pod 所绑定的
PV 有 nodeAffinity 无法满足，可以 check 对应的 PV yaml. 通常原因是 PV 所在的可用区，没有可用的节点，导致 Pod 无法调度。<ol><li>最简单的解决方法是，在对应的可用区补充节点</li><li>如果数据可以丢，也可以考虑直接删除重建 PV/PVC</li></ol></li></ol><h3 id=控制面故障可能会导致各类奇怪的异常现象 class=headerLink><a href=#%e6%8e%a7%e5%88%b6%e9%9d%a2%e6%95%85%e9%9a%9c%e5%8f%af%e8%83%bd%e4%bc%9a%e5%af%bc%e8%87%b4%e5%90%84%e7%b1%bb%e5%a5%87%e6%80%aa%e7%9a%84%e5%bc%82%e5%b8%b8%e7%8e%b0%e8%b1%a1 class=header-mark></a>控制面故障可能会导致各类奇怪的异常现象</h3><p>对于生产环境的集群，因为有高可用，通常我们比较少遇到控制面故障问题。但是一旦控制面发生故障，就可能会导致各类奇怪的异常现象。如果能在排查问题时，把控制面异常考虑进来，在这种情况下，就能节约大量的排查时间，快速定位到问题。</p><p>其中比较隐晦的就是 controller-manager 故障导致的异常：</p><ol><li>节点的服务器已经被终止，但是 Kubernetes 里还显示 node 为 Ready 状态，不会更新为
NotReady.</li><li>被删除的 Pods 可能会卡在 Terminating 状态，只有强制删除才能删除掉它们。并且确认 Pod 没有 <code>metadata.finalizers</code> 属性</li><li>HPA 的动态伸缩功能失效</li><li>&mldr;</li></ol><p>如果这些现象同时发生，就要怀疑是否是 kube-controller-manager 出问题了.</p><p>最简单的排查命令：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>$ kubectl get componentstatuses
</span></span><span class=line><span class=cl>Warning: v1 ComponentStatus is deprecated in v1.19+
</span></span><span class=line><span class=cl>NAME                 STATUS    MESSAGE   ERROR
</span></span><span class=line><span class=cl>scheduler            Healthy   ok
</span></span><span class=line><span class=cl>controller-manager   Healthy   ok
</span></span><span class=line><span class=cl>etcd-0               Healthy   ok
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 类似这种错误</span>
</span></span><span class=line><span class=cl>$ kubectl events --types<span class=o>=</span>Warning -n your-namespace
</span></span><span class=line><span class=cl>LAST SEEN   TYPE      REASON                     OBJECT                                   MESSAGE
</span></span><span class=line><span class=cl>56m         Warning   UpdateLoadBalancerFailed   Service/xxx-xxx   Error updating load balancer with new hosts <span class=o>[</span>xxx-4-3dsiuh &lt;<span class=m>1</span> more&gt;<span class=o>]</span>, error: failed to update load-balancer with ID xxx: xxx error
</span></span></code></pre></td></tr></table></div></div><p>如果三个组件任一个显示 Unhealthy，就能确定是控制面出现了问题。</p><blockquote><p>这个 API 已被废弃，有人建议使用 <code>kubectl get --raw='/readyz?verbose'</code> 来替代。但我实测发现即使这个命令返回全都 OK，但 controller-manager/scheduler 仍旧可能出问题。</p></blockquote><p>其他控制面异常的详细分析，参见<a href=https://github.com/ryan4yin/knowledge/blob/master/kubernetes/kubernetes%20%E6%8E%A7%E5%88%B6%E9%9D%A2%E6%95%85%E9%9A%9C%E7%8E%B0%E8%B1%A1%E5%8F%8A%E5%88%86%E6%9E%90.md target=_blank rel="noopener noreferrer">kubernetes 控制面故障现象及分析</a></p><h3 id=pod-无法删除 class=headerLink><a href=#pod-%e6%97%a0%e6%b3%95%e5%88%a0%e9%99%a4 class=header-mark></a>Pod 无法删除</h3><p>可能是某些资源无法被GC，这会导致容器已经 Exited 了，但是 Pod 一直处于 Terminating 状态。</p><p>这个问题在网上能搜到很多案例,但大都只是提供了如下的强制清理命令，未分析具体原因：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-shell data-lang=shell><span class=line><span class=cl>kubectl delete pods &lt;pod&gt; --grace-period<span class=o>=</span><span class=m>0</span> --force
</span></span></code></pre></td></tr></table></div></div><p>最近找到几篇详细的原因分析文章，值得一看：</p><ul><li><a href=https://cloud.tencent.com/developer/article/1680612 target=_blank rel="noopener noreferrer">腾讯云原生 -【Pod Terminating原因追踪系列】之 containerd 中被漏掉的 runc 错误信息</a></li><li><a href=https://cloud.tencent.com/developer/article/1680613 target=_blank rel="noopener noreferrer">腾讯云原生 -【Pod Terminating原因追踪系列之二】exec连接未关闭导致的事件阻塞</a></li><li><a href=https://cloud.tencent.com/developer/article/1689486 target=_blank rel="noopener noreferrer">腾讯云原生 -【Pod Terminating原因追踪系列之三】让docker事件处理罢工的cancel状态码</a></li><li><a href=https://www.likakuli.com/posts/docker-pod-terminating/ target=_blank rel="noopener noreferrer">Pod terminating - 问题排查 - KaKu Li</a></li></ul><p>大致总结一下，主要原因来自 docker 18.06 以及 kubernetes 的 docker-shim 运行时的底层逻辑，
已经在新版本被修复了。</p><h3 id=initcontainers-不断-restart但是-containers-却都显示已-ready class=headerLink><a href=#initcontainers-%e4%b8%8d%e6%96%ad-restart%e4%bd%86%e6%98%af-containers-%e5%8d%b4%e9%83%bd%e6%98%be%e7%a4%ba%e5%b7%b2-ready class=header-mark></a>initContainers 不断 restart，但是 Containers 却都显示已 ready</h3><p>Kubernetes 应该确保所有 initContainers 都 Completed，然后才能启动 Containers.</p><p>但是我们发现有一个节点上，所有包含 initContainers 的 Pod，状态全都是<code>Init:CrashLoopBackOff</code> 或者 <code>Init:Error</code>.</p><p>而且进一步 <code>kubectl describe po</code> 查看细节，发现 initContainer 的状态为:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>...
</span></span><span class=line><span class=cl>    State:          Waiting
</span></span><span class=line><span class=cl>      Reason:       CrashLoopBackOff
</span></span><span class=line><span class=cl>    Last State:     Terminated
</span></span><span class=line><span class=cl>      Reason:       Error
</span></span><span class=line><span class=cl>      Exit Code:    2
</span></span><span class=line><span class=cl>      Started:      Tue, 03 Aug 2021 06:02:42 +0000
</span></span><span class=line><span class=cl>      Finished:     Tue, 03 Aug 2021 06:02:42 +0000
</span></span><span class=line><span class=cl>    Ready:          False
</span></span><span class=line><span class=cl>    Restart Count:  67
</span></span><span class=line><span class=cl>...
</span></span></code></pre></td></tr></table></div></div><p>而 Containers 的状态居然是 ready:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span><span class=lnt>7
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>...
</span></span><span class=line><span class=cl>    Host Port:      0/TCP
</span></span><span class=line><span class=cl>    State:          Running
</span></span><span class=line><span class=cl>      Started:      Tue, 03 Aug 2021 00:35:30 +0000
</span></span><span class=line><span class=cl>    Ready:          True
</span></span><span class=line><span class=cl>    Restart Count:  0
</span></span><span class=line><span class=cl>...
</span></span></code></pre></td></tr></table></div></div><p>initContainers 还未运行成功，而 Containers 却 Ready 了，非常疑惑。</p><p>仔细想了下，早上因为磁盘余量告警，有手动运行过 <code>docker system prune</code> 命令，那么问题可能就是这条命令清理掉了已经 exited 的 initContainers 容器，导致 k8s 故障，不断尝试重启该容器。</p><p>网上一搜确实有相关的信息：</p><ul><li><a href=https://stackoverflow.com/questions/62333064/cant-delete-exited-init-container target=_blank rel="noopener noreferrer">https://stackoverflow.com/questions/62333064/cant-delete-exited-init-container</a></li><li><a href=https://github.com/kubernetes/kubernetes/issues/62362 target=_blank rel="noopener noreferrer">https://github.com/kubernetes/kubernetes/issues/62362</a></li></ul><p>结论：使用外部的垃圾清理命令可能导致 k8s 行为异常。</p><h2 id=节点常见错误 class=headerLink><a href=#%e8%8a%82%e7%82%b9%e5%b8%b8%e8%a7%81%e9%94%99%e8%af%af class=header-mark></a>节点常见错误</h2><ol><li><a href=https://kubernetes.io/docs/tasks/administer-cluster/out-of-resource/#node-conditions target=_blank rel="noopener noreferrer">DiskPressure</a>：
节点的可用空间不足。（通过<code>df -h</code> 查看，保证可用空间不小于 15%）</li><li>The node was low on resource: ephemeral-storage: 同上，节点的存储空间不够了。</li></ol><p>节点存储告警可能的原因：</p><ol><li>kubelet 的资源 GC 设置有问题，遗留的镜像等资源未及时 GC 导致告警</li><li>存在运行的 pod 使用了大量存储空间，在节点上通过 <code>docker ps -a --size | grep G</code> 可以查看到</li><li>如果使用的是 EKS，并且磁盘告警的挂载点为<code>/var/lib/kubelet/plugins/kubernetes.io/aws-ebs/mounts/aws/us-east-1b/vol-xxxxx</code><ol><li>显然是 EBS 存储卷快满了导致的</li><li>可通过 <code>kubectl get pv -A -o yaml | grep -C 30 vol-xxxxx</code> 来定位到具体的存储卷</li></ol></li></ol><h2 id=网络常见错误 class=headerLink><a href=#%e7%bd%91%e7%bb%9c%e5%b8%b8%e8%a7%81%e9%94%99%e8%af%af class=header-mark></a>网络常见错误</h2><h3 id=1-ingressistio-gateway-返回值 class=headerLink><a href=#1-ingressistio-gateway-%e8%bf%94%e5%9b%9e%e5%80%bc class=header-mark></a>1. Ingress/Istio Gateway 返回值</h3><ol><li>404：不存在该 Service/Istio Gateway，或者是服务自身返回 404</li><li>500：大概率是服务自身的错误导致 500，小概率是代理（Sidecar/Ingress 等）的错误</li><li>503：服务不可用，有如下几种可能的原因：<ol><li>Service 对应的 Pods 不存在，endpoints 为空</li><li>Service 对应的 Pods 全部都 NotReady，导致 endpoints 为空</li><li>也有可能是服务自身出错返回的 503</li><li>如果你使用了 envoy sidecar， 503 可能的原因就多了。基本上 sidecar 与主容器通信过程中的任何问题都会使 envoy 返回 503，使客户端重试。<ol><li>详见 <a href=https://blog.fleeto.us/post/istio-503-uc-debug/ target=_blank rel="noopener noreferrer">Istio：503、UC 和 TCP</a></li></ol></li></ol></li><li>502：Bad Gateway，通常是由于上游未返回正确的响应导致的，可能的根本原因：<ol><li>应用程序未正确处理 SIGTERM 信号，在请求未处理完毕时直接终止了进程。详见<a href=./%e6%9c%80%e4%bd%b3%e5%ae%9e%e8%b7%b5.md rel>优雅停止（Gracful Shutdown）与 502/504 报错 - K8s 最佳实践</a></li><li>网络插件 bug</li></ol></li><li>504：网关请求 upstream 超时，主要有两种可能<ol><li>考虑是不是 Ingress Controller 的 IP 列表未更新，将请求代理到了不存在的 ip，导致得不到响应</li><li>Service Endpoints 移除不够及时，在 Pod 已经被终止后，仍然有个别请求被路由到了该
Pod，得不到响应导致 504。详见<a href=./%e6%9c%80%e4%bd%b3%e5%ae%9e%e8%b7%b5.md rel>优雅停止（Gracful Shutdown）与 502/504 报错 - K8s 最佳实践</a></li><li>Pod 响应太慢，代码问题</li></ol></li></ol><p>再总结一下常见的几种错误：</p><ul><li>未设置优雅停止，导致 Pod 被重新终止时，有概率出现 502/504</li><li>服务的所有 Pods 的状态在「就绪」和「未就绪」之间摆动，导致间歇性地出现大量 503 错误</li><li>服务返回 5xx 错误导致客户端不断重试，请求流量被放大，导致服务一直起不来<ul><li>解决办法：限流、熔断（网关层直接返回固定的相应内容）</li></ul></li></ul><p>Ingress 相关网络问题的排查流程：</p><ol><li>Which ingress controller?</li><li>Timeout between client and ingress controller, or between ingress controller and
backend service/pod?</li><li>HTTP/504 generated by the ingress controller, proven by logs from the ingress
controller?</li><li>If you port-forward to skip the internet between client and ingress controller, does
the timeout still happen?</li></ol><h3 id=2-上了-istio-sidecar-后应用程序偶尔间隔几天半个月会-redis-连接相关的错误 class=headerLink><a href=#2-%e4%b8%8a%e4%ba%86-istio-sidecar-%e5%90%8e%e5%ba%94%e7%94%a8%e7%a8%8b%e5%ba%8f%e5%81%b6%e5%b0%94%e9%97%b4%e9%9a%94%e5%87%a0%e5%a4%a9%e5%8d%8a%e4%b8%aa%e6%9c%88%e4%bc%9a-redis-%e8%bf%9e%e6%8e%a5%e7%9b%b8%e5%85%b3%e7%9a%84%e9%94%99%e8%af%af class=header-mark></a>2. 上了 istio sidecar 后，应用程序偶尔（间隔几天半个月）会 redis 连接相关的错误</h3><p>考虑是否和 tcp 长时间使用有关，比如连接长时间空闲的话，可能会被 istio sidecar 断开。如果程序自身的重连机制有问题，就会导致这种现象。</p><p>确认方法：</p><ol><li>检查 istio 的 <code>idleTimeout</code> 时长（默认 1h）</li><li>创建三五个没流量的 Pod 放置 1h（与 istio idleTimeout 时长一致），看看是否会准时开始报
redis 的错。</li><li>对照组：创建三五个同样没流量的 Pod，但是不注入 istio sidecar，应该一直很正常</li></ol><p>这样就能确认问题，后续处理：</p><ol><li>抓包观察程序在出错后的 tcp 层行为</li><li>查阅 redis sdk 的相关 issue、代码，通过升级 SDK 应该能解决问题。</li></ol><h2 id=名字空间常见错误 class=headerLink><a href=#%e5%90%8d%e5%ad%97%e7%a9%ba%e9%97%b4%e5%b8%b8%e8%a7%81%e9%94%99%e8%af%af class=header-mark></a>名字空间常见错误</h2><h3 id=名字空间无法删除 class=headerLink><a href=#%e5%90%8d%e5%ad%97%e7%a9%ba%e9%97%b4%e6%97%a0%e6%b3%95%e5%88%a0%e9%99%a4 class=header-mark></a>名字空间无法删除</h3><p>这通常是某些资源如 CR(custom resources)/存储等资源无法释放导致的。比如常见的 monitoring 名字空间无法删除，应该就是 CR 无法 GC 导致的。</p><p>可手动删除 namespace 配置中的析构器（spec.finalizer，在名字空间生命周期结束前会生成的配置项），这样名字空间就会直接跳过 GC 步骤：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-shell data-lang=shell><span class=line><span class=cl><span class=c1># 编辑名字空间的配置</span>
</span></span><span class=line><span class=cl>kubectl edit namespace &lt;ns-name&gt;
</span></span><span class=line><span class=cl><span class=c1># 将 spec.finalizers 改成空列表 []</span>
</span></span></code></pre></td></tr></table></div></div><p>如果上述方法也无法删除名字空间，也找不到具体的问题，就只能直接从 etcd 中删除掉它了(有风险，谨慎操作！)。方法如下：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span><span class=lnt>7
</span><span class=lnt>8
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-shell data-lang=shell><span class=line><span class=cl><span class=c1># 登录到 etcd 容器中，执行如下命令：</span>
</span></span><span class=line><span class=cl><span class=nb>export</span> <span class=nv>ETCDCTL_API</span><span class=o>=</span><span class=m>3</span>
</span></span><span class=line><span class=cl><span class=nb>cd</span> /etc/kubernetes/pki/etcd/
</span></span><span class=line><span class=cl><span class=c1># 列出所有名字空间</span>
</span></span><span class=line><span class=cl>etcdctl --cacert ca.crt --cert peer.crt --key peer.key get /registry/namespaces --prefix --keys-only
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># （谨慎操作！！！）强制删除名字空间 `monitoring`。这可能导致相关资源无法被 GC！</span>
</span></span><span class=line><span class=cl>etcdctl --cacert ca.crt --cert peer.crt --key peer.key del /registry/namespaces/monitoring
</span></span></code></pre></td></tr></table></div></div><h2 id=kubectlistioctl-等客户端工具异常 class=headerLink><a href=#kubectlistioctl-%e7%ad%89%e5%ae%a2%e6%88%b7%e7%ab%af%e5%b7%a5%e5%85%b7%e5%bc%82%e5%b8%b8 class=header-mark></a>kubectl/istioctl 等客户端工具异常</h2><ol><li><code>socat not found</code>: kubectl 使用 <code>socat</code> 进行端口转发，集群的所有节点，以及本机都必须安装有 <code>socat</code> 工具。</li></ol><h2 id=批量清理-evicted-记录 class=headerLink><a href=#%e6%89%b9%e9%87%8f%e6%b8%85%e7%90%86-evicted-%e8%ae%b0%e5%bd%95 class=header-mark></a>批量清理 Evicted 记录</h2><p>有时候 Pod 因为节点选择器的问题，被不断调度到有问题的 Node 上，就会不断被 Evicted，导致出现大量的 Evicted Pods。排查完问题后，需要手动清理掉这些 Evicted Pods.</p><p>批量删除 Evicted 记录:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-shell data-lang=shell><span class=line><span class=cl>kubectl get pods <span class=p>|</span> grep Evicted <span class=p>|</span> awk <span class=s1>&#39;{print $1}&#39;</span> <span class=p>|</span> xargs kubectl delete pod
</span></span></code></pre></td></tr></table></div></div><h2 id=容器镜像gcpod驱逐以及节点压力 class=headerLink><a href=#%e5%ae%b9%e5%99%a8%e9%95%9c%e5%83%8fgcpod%e9%a9%b1%e9%80%90%e4%bb%a5%e5%8f%8a%e8%8a%82%e7%82%b9%e5%8e%8b%e5%8a%9b class=header-mark></a>容器镜像GC、Pod驱逐以及节点压力</h2><p>节点压力 DiskPressure 会导致 Pod 被驱逐，也会触发容器镜像的 GC。</p><p>根据官方文档<a href=https://kubernetes.io/zh/docs/tasks/administer-cluster/out-of-resource target=_blank rel="noopener noreferrer">配置资源不足时的处理方式</a>，Kubelet
提供如下用于配置容器 GC 及 Evicetion 的阈值：</p><ol><li><code>--eviction-hard</code> 和 <code>eviction-soft</code>: 对应旧参数 <code>--image-gc-high-threshold</code>，这两个参数配置镜像 GC 及驱逐的触发阈值。磁盘使用率的阈值默认为 85%<ol><li>区别在于 <code>eviction-hard</code> 是立即驱逐，而 <code>eviction-soft</code> 在超过<code>eviction-soft-grace-period</code> 之后才驱逐。</li></ol></li><li><code>--eviction-minimum-reclaim</code>: 对应旧参数 <code>--image-gc-low-threshold</code>。这是进行资源回收
（镜像GC、Pod驱逐等）后期望达到的磁盘使用率百分比。磁盘使用率的阈值默认值为 80%。</li></ol><p>问：能否为 ImageGC 设置一个比 DiskPressure 更低的阈值？因为我们希望能自动进行镜像 GC，但是不想立即触发 Pod 驱逐。</p><p>答：这应该可以通过设置 <code>eviction-soft</code> 和长一点的 <code>eviction-soft-grace-period</code> 来实现。另外 <code>--eviction-minimum-reclaim</code> 也可以设小一点，清理得更干净。示例如下：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-shell data-lang=shell><span class=line><span class=cl>--eviction-soft<span class=o>=</span>memory.available&lt;1Gi,nodefs.available&lt;2Gi,imagefs.available&lt;200Gi
</span></span><span class=line><span class=cl>--eviction-soft-grace-period<span class=o>=</span>3m
</span></span><span class=line><span class=cl>--eviction-minimum-reclaim<span class=o>=</span>memory.available<span class=o>=</span>0Mi,nodefs.available<span class=o>=</span>1Gi,imagefs.available<span class=o>=</span>2Gi
</span></span></code></pre></td></tr></table></div></div><h2 id=监控hpa-常见错误 class=headerLink><a href=#%e7%9b%91%e6%8e%a7hpa-%e5%b8%b8%e8%a7%81%e9%94%99%e8%af%af class=header-mark></a>监控/HPA 常见错误</h2><h3 id=服务设置了-hpa-阈值为-50-cpu所有业务容器在启动后不久就会-oomcpu-暴涨然后挂掉但是无法触发-cpu-扩缩容prometheus-监控指标也不对劲 class=headerLink><a href=#%e6%9c%8d%e5%8a%a1%e8%ae%be%e7%bd%ae%e4%ba%86-hpa-%e9%98%88%e5%80%bc%e4%b8%ba-50-cpu%e6%89%80%e6%9c%89%e4%b8%9a%e5%8a%a1%e5%ae%b9%e5%99%a8%e5%9c%a8%e5%90%af%e5%8a%a8%e5%90%8e%e4%b8%8d%e4%b9%85%e5%b0%b1%e4%bc%9a-oomcpu-%e6%9a%b4%e6%b6%a8%e7%84%b6%e5%90%8e%e6%8c%82%e6%8e%89%e4%bd%86%e6%98%af%e6%97%a0%e6%b3%95%e8%a7%a6%e5%8f%91-cpu-%e6%89%a9%e7%bc%a9%e5%ae%b9prometheus-%e7%9b%91%e6%8e%a7%e6%8c%87%e6%a0%87%e4%b9%9f%e4%b8%8d%e5%af%b9%e5%8a%b2 class=header-mark></a>服务设置了 HPA 阈值为 50% CPU，所有业务容器在启动后不久就会 OOM，CPU 暴涨然后挂掉。但是无法触发 CPU 扩缩容，Prometheus 监控指标也不对劲。</h3><p>根据<a href=https://github.com/kubernetes-sigs/metrics-server/blob/master/FAQ.md#how-often-metrics-are-scraped target=_blank rel="noopener noreferrer">metrics-sever - how-often-metrics-are-scraped</a>
描述，metrics-sever 默认情况下每 60s 采集一次指标，而 Prometheus 的采集间隔通常会配置为
15s/30s。</p><p>这说明如果业务容器每次重启后，都坚持不过 60s 就会挂掉，就很可能导致 metrics-sever 采集不到足够的指标，HPA 查询到的平均 CPU 将会是 0%，无法触发扩容操作。</p><p>Prometheus 也是一样的逻辑，如果容器每次启动都坚持不过 30s，那就会导致 prometheus 经常抓不到指标，监控图表或者告警就会出问题。</p><h2 id=其他问题 class=headerLink><a href=#%e5%85%b6%e4%bb%96%e9%97%ae%e9%a2%98 class=header-mark></a>其他问题</h2><h3 id=隔天-istio-等工具的-sidecar-自动注入莫名其妙失效了 class=headerLink><a href=#%e9%9a%94%e5%a4%a9-istio-%e7%ad%89%e5%b7%a5%e5%85%b7%e7%9a%84-sidecar-%e8%87%aa%e5%8a%a8%e6%b3%a8%e5%85%a5%e8%8e%ab%e5%90%8d%e5%85%b6%e5%a6%99%e5%a4%b1%e6%95%88%e4%ba%86 class=header-mark></a>隔天 Istio 等工具的 sidecar 自动注入莫名其妙失效了</h3><p>如果服务器晚上会关机，可能导致第二天网络插件出问题，导致 sidecar 注入器无法观察到 pod 的创建，也就无法完成 sidecar 注入。</p><h3 id=如何重新运行一个-job class=headerLink><a href=#%e5%a6%82%e4%bd%95%e9%87%8d%e6%96%b0%e8%bf%90%e8%a1%8c%e4%b8%80%e4%b8%aa-job class=header-mark></a>如何重新运行一个 Job？</h3><p>我们有一个 Job 因为外部原因运行失败了，修复好后就需要重新运行它。</p><p>方法是：删除旧的 Job，再使用同一份配置重建 Job.</p><p>如果你使用的是 fluxcd 这类 GitOps 工具，就只需要手工删除旧 Pod，fluxcd 会定时自动 apply 所有配置，这就完成了 Job 的重建。</p><h2 id=参考 class=headerLink><a href=#%e5%8f%82%e8%80%83 class=header-mark></a>参考</h2><ul><li><a href="https://yq.aliyun.com/articles/703971?type=2" target=_blank rel="noopener noreferrer">Kubernetes管理经验</a></li><li><a href=https://www.reddit.com/r/kubernetes/comments/ced0py/504_gateway_timeout_when_accessing_workload_via/ target=_blank rel="noopener noreferrer">504 Gateway Timeout when accessing workload via ingress</a></li><li><a href=https://k8s.af/ target=_blank rel="noopener noreferrer">Kubernetes Failure Stories</a></li><li><a href=https://blog.fleeto.us/post/istio-503-uc-debug/ target=_blank rel="noopener noreferrer">Istio：503、UC 和 TCP</a></li><li><a href=https://imroc.cc/istio/ target=_blank rel="noopener noreferrer">istio 实践指南 - imroc.cc</a></li><li><a href=https://imroc.cc/kubernetes/ target=_blank rel="noopener noreferrer">Kubernetes 实践指南 - imroc.cc</a></li></ul></div><h2>相关内容</h2><div class=related-container><div class=related-item-container><div class=related-image><a href=/posts/kubernetes-cert-management/><img loading=lazy src=/posts/kubernetes-cert-management/cert-manager.webp srcset="/posts/kubernetes-cert-management/cert-manager.webp, /posts/kubernetes-cert-management/cert-manager.webp 1.5x, /posts/kubernetes-cert-management/cert-manager.webp 2x" sizes=auto alt=/posts/kubernetes-cert-management/cert-manager.webp title=/posts/kubernetes-cert-management/cert-manager.webp height=200 width=400></a></div><h2 class=related-title><a href=/posts/kubernetes-cert-management/>Kubernetes 中的证书管理工具 - cert-manager</a></h2></div><div class=related-item-container><div class=related-image><a href=/posts/finops-for-kubernetes/><img loading=lazy src=/posts/finops-for-kubernetes/finops-for-kubernetes.webp srcset="/posts/finops-for-kubernetes/finops-for-kubernetes.webp, /posts/finops-for-kubernetes/finops-for-kubernetes.webp 1.5x, /posts/finops-for-kubernetes/finops-for-kubernetes.webp 2x" sizes=auto alt=/posts/finops-for-kubernetes/finops-for-kubernetes.webp title=/posts/finops-for-kubernetes/finops-for-kubernetes.webp height=200 width=400></a></div><h2 class=related-title><a href=/posts/finops-for-kubernetes/>FinOps for Kubernetes - 如何拆分 Kubernetes 成本</a></h2></div><div class=related-item-container><div class=related-image><a href=/posts/kubernetes-deployment-using-kubeadm/><img loading=lazy src=/posts/kubernetes-deployment-using-kubeadm/adopt-kubernetes.webp srcset="/posts/kubernetes-deployment-using-kubeadm/adopt-kubernetes.webp, /posts/kubernetes-deployment-using-kubeadm/adopt-kubernetes.webp 1.5x, /posts/kubernetes-deployment-using-kubeadm/adopt-kubernetes.webp 2x" sizes=auto alt=/posts/kubernetes-deployment-using-kubeadm/adopt-kubernetes.webp title=/posts/kubernetes-deployment-using-kubeadm/adopt-kubernetes.webp height=200 width=400></a></div><h2 class=related-title><a href=/posts/kubernetes-deployment-using-kubeadm/>部署一个 Kubernetes 集群</a></h2></div></div><div class=sponsor><div class=sponsor-avatar><img loading=lazy src=https://thiscute.world/avatar/myself.webp srcset="https://thiscute.world/avatar/myself.webp, https://thiscute.world/avatar/myself.webp 1.5x, https://thiscute.world/avatar/myself.webp 2x" sizes=auto alt=https://thiscute.world/avatar/myself.webp title=https://thiscute.world/avatar/myself.webp></div><p class=sponsor-bio><em>如果你觉得这篇文章对你有所帮助，欢迎评论、分享、打赏~</em></p><a href=https://afdian.com/a/ryan4yin title=Sponsor target=_blank class=sponsor-button rel="noopener noreferrer"><i class="far fa-heart fa-fw icon" style=color:#ec6cb9></i>
<span>赞赏</span></a></div><div class=post-footer id=post-footer><div class=post-info><div class=post-info-line><div class=post-info-mod><span>更新于 2019-11-24</span></div><div class=post-info-license></div></div><div class=post-info-line><div class=post-info-md></div><div class=post-info-share><button title="分享到 Twitter" data-sharer=twitter data-url=https://thiscute.world/posts/kubernetes-common-errors-and-solutions/ data-title="Kubernetes 常见错误、原因及处理方法" data-via=ryan4yin data-hashtags=Kubernetes><span class="fab fa-twitter fa-fw"></span></button><button title="分享到 Hacker News" data-sharer=hackernews data-url=https://thiscute.world/posts/kubernetes-common-errors-and-solutions/ data-title="Kubernetes 常见错误、原因及处理方法"><span class="fab fa-hacker-news fa-fw"></span></button><button title="分享到 Reddit" data-sharer=reddit data-url=https://thiscute.world/posts/kubernetes-common-errors-and-solutions/><span class="fab fa-reddit fa-fw"></span></button><script>function shareOnMastodon(e,t){const o="share_mastodon_domain",i=localStorage.getItem(o)??"mastodon.social",n=prompt("Enter your Mastodon domain",i);if(n===null)return;localStorage.setItem(o,n);const a=e+`

`+t,s=new URL("https://"+n);s.pathname="share",s.searchParams.append("text",a),window.open(s,"_blank","width=500,height=500,left=500,toolbar=0,status=0")}</script>
<button title="分享到 Mastodon" onclick='shareOnMastodon("Kubernetes 常见错误、原因及处理方法","https://thiscute.world/posts/kubernetes-common-errors-and-solutions/")'><span class="fab fa-mastodon fa-fw"></span></button></div></div></div><div class=post-info-more><section class=post-tags><i class="fas fa-tags fa-fw"></i>&nbsp;<a href=/tags/kubernetes/>Kubernetes</a></section><section><span><a href=javascript:void(0); onclick=window.history.back()>返回</a></span>&nbsp;|&nbsp;<span><a href=/>主页</a></span></section></div><div class=post-nav><a href=/posts/manjaro-instruction/ class=prev rel=prev title="Manjaro 使用指南"><i class="fas fa-angle-left fa-fw"></i>Manjaro 使用指南</a>
<a href=/posts/2019-summary/ class=next rel=next title="2019 年年终总结">2019 年年终总结<i class="fas fa-angle-right fa-fw"></i></a></div></div><div id=comments><div id=utterances></div><noscript>Please enable JavaScript to view the comments powered by <a href=https://utteranc.es/>Utterances</a>.</noscript></div></article></div></main><footer class=footer><div class=footer-container><div class=footer-line><a href=https://www.foreverblog.cn/ target=_blank><img src=https://img.foreverblog.cn/logo_en_default.png alt style=display:inline-block;width:auto;height:20px>
</a><a href=https://www.foreverblog.cn/go.html target=_blank><img src=https://img.foreverblog.cn/wormhole_3.gif alt style=display:inline-block;width:auto;height:20px title=穿梭虫洞-随机访问十年之约友链博客></a></div><div class=footer-line>由 <a href=https://gohugo.io/ target=_blank rel="noopener noreferrer" title="Hugo 0.126.1">Hugo</a> 强力驱动&nbsp;|&nbsp;托管在 <a title=Vercel href=https://vercel.com/ target=_blank rel="noopener noreffer">Vercel</a> 上&nbsp;|&nbsp;主题 - <a href=https://github.com/HEIGE-PCloud/DoIt target=_blank rel="noopener noreferrer" title="DoIt 0.4.0"><i class="far fa-edit fa-fw"></i> DoIt</a></div><div class=footer-line><i class="far fa-copyright fa-fw"></i><span itemprop=copyrightYear>2021 - 2024</span><span class=author itemprop=copyrightHolder>&nbsp;<a href=https://thiscute.world/ target=_blank rel="noopener noreferrer">ryan4yin</a></span>&nbsp;|&nbsp;<span class=license> <a rel="license external nofollow noopener noreffer" href=https://creativecommons.org/licenses/by-nc/4.0/ target=_blank>CC BY-NC 4.0</a></span></div><div class=footer-line></div><div class=footer-line></div></div></footer></div><div id=fixed-buttons><a href=#back-to-top id=back-to-top-button class=fixed-button title=回到顶部><i class="fas fa-arrow-up fa-fw"></i>
</a><a href=# id=view-comments class=fixed-button title=查看评论><i class="fas fa-comment fa-fw"></i></a></div><div class=assets><link rel=stylesheet href=/lib/katex/katex.min.css><link rel=preload as=style onload='this.onload=null,this.rel="stylesheet"' href=/lib/katex/copy-tex.min.css><noscript><link rel=stylesheet href=/lib/katex/copy-tex.min.css></noscript><script type=text/javascript>window.config={code:{copyTitle:"复制到剪贴板",maxShownLines:30},comment:{utterances:{darkTheme:"github-dark",issueTerm:"pathname",label:"",lightTheme:"github-light",repo:"ryan4yin/thiscute.world"}},data:{"desktop-header-typeit":"This Cute World","mobile-header-typeit":"This Cute World"},math:{delimiters:[{display:!0,left:"$$",right:"$$"},{display:!0,left:"\\[",right:"\\]"},{display:!1,left:"$",right:"$"},{display:!1,left:"\\(",right:"\\)"}],strict:!1},search:{algoliaAppID:"747LJ10EI7",algoliaIndex:"ryan-space",algoliaSearchKey:"658db5f2bf056f83458cacf5dd58ec80",highlightTag:"em",maxResultLength:10,noResultsFound:"没有找到结果",snippetLength:50,type:"algolia"},sharerjs:!0,typeit:{cursorChar:null,cursorSpeed:null,data:{"desktop-header-typeit":["desktop-header-typeit"],"mobile-header-typeit":["mobile-header-typeit"]},duration:null,speed:null}}</script><script type=text/javascript src=/lib/clipboard/clipboard.min.js></script><script type=text/javascript src=/lib/sharer/sharer.min.js></script><script type=text/javascript src=/lib/typeit/typeit.min.js></script><script type=text/javascript src=/lib/katex/katex.min.js defer></script><script type=text/javascript src=/lib/katex/auto-render.min.js defer></script><script type=text/javascript src=/lib/katex/copy-tex.min.js defer></script><script type=text/javascript src=/lib/katex/mhchem.min.js defer></script><script type=text/javascript src=/js/katex.min.js defer></script><script type=text/javascript src=/js/theme.min.js defer></script><script type=text/javascript src=/js/utterances.min.js defer></script><script type=text/javascript>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-4V93QVSNFW",{anonymize_ip:!0})</script><script type=text/javascript src="https://www.googletagmanager.com/gtag/js?id=G-4V93QVSNFW" async></script></div></body></html>