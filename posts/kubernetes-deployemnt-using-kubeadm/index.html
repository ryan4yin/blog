<!doctype html><html lang=zh-cn>
<head>
<meta charset=utf-8>
<meta name=viewport content="width=device-width,initial-scale=1">
<meta name=robots content="noodp">
<meta http-equiv=x-ua-compatible content="IE=edge, chrome=1">
<title>部署一个 Kubernetes 集群 - Ryan4Yin's Space</title><meta name=description content="Ryan4Yin's Space"><meta property="og:title" content="部署一个 Kubernetes 集群">
<meta property="og:description" content="本文由个人笔记 ryan4yin/knowledge 整理而来，不保证正确 本地 Kubernetes 集群安装工具 云上的 Kubernetes 集群，基本上各云厂商都支持一键部署。这里主要关注本地部署，或者叫做裸机(bar">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ryan4yin.space/posts/kubernetes-deployemnt-using-kubeadm/"><meta property="og:image" content="https://ryan4yin.space/posts/kubernetes-deployemnt-using-kubeadm/adopt-kubernetes.png"><meta property="article:section" content="posts">
<meta property="article:published_time" content="2022-01-25T01:37:00+08:00">
<meta property="article:modified_time" content="2022-01-25T01:37:00+08:00">
<meta name=twitter:card content="summary_large_image">
<meta name=twitter:image content="https://ryan4yin.space/posts/kubernetes-deployemnt-using-kubeadm/adopt-kubernetes.png">
<meta name=twitter:title content="部署一个 Kubernetes 集群">
<meta name=twitter:description content="本文由个人笔记 ryan4yin/knowledge 整理而来，不保证正确 本地 Kubernetes 集群安装工具 云上的 Kubernetes 集群，基本上各云厂商都支持一键部署。这里主要关注本地部署，或者叫做裸机(bar">
<meta name=application-name content="Ryan4Yin's Space">
<meta name=apple-mobile-web-app-title content="Ryan4Yin's Space"><link rel="shortcut icon" type=image/x-icon href=/favicon.ico>
<link rel=icon type=image/png sizes=32x32 href=/favicon-32x32.png>
<link rel=icon type=image/png sizes=16x16 href=/favicon-16x16.png><link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png><link rel=manifest href=/site.webmanifest><link rel=canonical href=https://ryan4yin.space/posts/kubernetes-deployemnt-using-kubeadm/><link rel=prev href=https://ryan4yin.space/posts/kubernetes-best-practices/><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/normalize.css@8.0.1/normalize.min.css><link rel=stylesheet href=/css/style.min.css><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.13.0/css/all.min.css><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/animate.css@3.7.2/animate.min.css><meta name=google-site-verification content="E8bpp1lVVlb9YnSJcUzPL1dLAG17Nl_sp5Ru9a8tUDQ"><meta name=baidu-site-verification content="code-ZZtDruAnX1"><script type=application/ld+json>{"@context":"http://schema.org","@type":"BlogPosting","headline":"部署一个 Kubernetes 集群","inLanguage":"zh-CN","mainEntityOfPage":{"@type":"WebPage","@id":"https:\/\/ryan4yin.space\/posts\/kubernetes-deployemnt-using-kubeadm\/"},"image":[{"@type":"ImageObject","url":"https:\/\/ryan4yin.space\/posts\/kubernetes-deployemnt-using-kubeadm\/adopt-kubernetes.png","width":1920,"height":960}],"genre":"posts","keywords":"Kubernetes, 云原生","wordcount":6073,"url":"https:\/\/ryan4yin.space\/posts\/kubernetes-deployemnt-using-kubeadm\/","datePublished":"2022-01-25T01:37:00+08:00","dateModified":"2022-01-25T01:37:00+08:00","publisher":{"@type":"Organization","name":"ryan4yin","logo":"https:\/\/ryan4yin.space\/avatar\/myself.png"},"author":{"@type":"Person","name":"ryan4yin"},"description":""}</script></head>
<body header-desktop=fixed header-mobile=auto><script type=text/javascript>(''==='true'&&window.localStorage&&localStorage.getItem('theme')?localStorage.getItem('theme')==='dark':'auto'==='auto'?window.matchMedia('(prefers-color-scheme: dark)').matches:'auto'==='dark')&&document.body.setAttribute('theme','dark')</script>
<div id=mask></div><div class=wrapper><header class=desktop id=header-desktop>
<div class=header-wrapper>
<div class=header-title>
<a href=/ title="Ryan4Yin's Space"><span id=id-1 class=typeit></span></a>
</div>
<div class=menu>
<div class=menu-inner><a class=menu-item href=/categories/> 分类 </a><a class=menu-item href=/posts/> 文章 </a><a class=menu-item href=/tags/> 标签 </a><a class=menu-item href=/now/> 此刻 </a><a class=menu-item href=/friends/> 伙伴们 </a><a class=menu-item href=/about/> 关于 </a><span class="menu-item delimiter"></span><a href=javascript:void(0); class="menu-item language" title=选择语言>Simplified Chinese<i class="fas fa-chevron-right fa-fw"></i>
<select class=language-select id=language-select-desktop onchange="location=this.value"><option value=/posts/kubernetes-deployemnt-using-kubeadm/ selected>Simplified Chinese</option></select>
</a><span class="menu-item search" id=search-desktop>
<input type=text placeholder=搜索文章标题或内容... id=search-input-desktop>
<a href=javascript:void(0); class="search-button search-toggle" id=search-toggle-desktop title=搜索>
<i class="fas fa-search fa-fw"></i>
</a>
<a href=javascript:void(0); class="search-button search-clear" id=search-clear-desktop title=清空>
<i class="fas fa-times-circle fa-fw"></i>
</a>
<span class="search-button search-loading" id=search-loading-desktop>
<i class="fas fa-spinner fa-fw fa-spin"></i>
</span>
</span><a href=javascript:void(0); class="menu-item theme-switch" title=切换主题>
<i class="fas fa-adjust fa-fw"></i>
</a>
</div>
</div>
</div>
</header><header class=mobile id=header-mobile>
<div class=header-container>
<div class=header-wrapper>
<div class=header-title>
<a href=/ title="Ryan4Yin's Space"><span id=id-2 class=typeit></span></a>
</div>
<div class=menu-toggle id=menu-toggle-mobile>
<span></span><span></span><span></span>
</div>
</div>
<div class=menu id=menu-mobile><div class=search-wrapper>
<div class="search mobile" id=search-mobile>
<input type=text placeholder=搜索文章标题或内容... id=search-input-mobile>
<a href=javascript:void(0); class="search-button search-toggle" id=search-toggle-mobile title=搜索>
<i class="fas fa-search fa-fw"></i>
</a>
<a href=javascript:void(0); class="search-button search-clear" id=search-clear-mobile title=清空>
<i class="fas fa-times-circle fa-fw"></i>
</a>
<span class="search-button search-loading" id=search-loading-mobile>
<i class="fas fa-spinner fa-fw fa-spin"></i>
</span>
</div>
<a href=javascript:void(0); class=search-cancel id=search-cancel-mobile>
取消
</a>
</div><a class=menu-item href=/categories/ title>分类</a><a class=menu-item href=/posts/ title>文章</a><a class=menu-item href=/tags/ title>标签</a><a class=menu-item href=/now/ title>此刻</a><a class=menu-item href=/friends/ title>伙伴们</a><a class=menu-item href=/about/ title>关于</a><a href=javascript:void(0); class="menu-item theme-switch" title=切换主题>
<i class="fas fa-adjust fa-fw"></i>
</a><a href=javascript:void(0); class=menu-item title=选择语言>Simplified Chinese<i class="fas fa-chevron-right fa-fw"></i>
<select class=language-select onchange="location=this.value"><option value=/posts/kubernetes-deployemnt-using-kubeadm/ selected>Simplified Chinese</option></select>
</a></div>
</div>
</header>
<div class="search-dropdown desktop">
<div id=search-dropdown-desktop></div>
</div>
<div class="search-dropdown mobile">
<div id=search-dropdown-mobile></div>
</div>
<main class=main>
<div class=container><div class=toc id=toc-auto>
<h2 class=toc-title>目录</h2>
<div class=toc-content id=toc-content-auto></div>
</div><article class="page single"><h1 class="single-title animated flipInX">部署一个 Kubernetes 集群</h1><div class=post-meta>
<div class=post-meta-line><span class=post-author><a href=https://ryan4yin.space title=Author target=_blank rel="noopener noreferrer author" class=author><i class="fas fa-user-circle fa-fw"></i>ryan4yin</a></span>&nbsp;<span class=post-category>收录于 <a href=/categories/%E6%8A%80%E6%9C%AF/><i class="far fa-folder fa-fw"></i>技术</a></span></div>
<div class=post-meta-line><i class="far fa-calendar-alt fa-fw"></i>&nbsp;<time datetime=2022-01-25>2022-01-25</time>&nbsp;<i class="fas fa-pencil-alt fa-fw"></i>&nbsp;约 6073 字&nbsp;
<i class="far fa-clock fa-fw"></i>&nbsp;预计阅读 13 分钟&nbsp;</div>
</div><div class=featured-image><img class=lazyload src=/svg/loading.min.svg data-src=/posts/kubernetes-deployemnt-using-kubeadm/adopt-kubernetes.png data-srcset="/posts/kubernetes-deployemnt-using-kubeadm/adopt-kubernetes.png, /posts/kubernetes-deployemnt-using-kubeadm/adopt-kubernetes.png 1.5x, /posts/kubernetes-deployemnt-using-kubeadm/adopt-kubernetes.png 2x" data-sizes=auto alt=/posts/kubernetes-deployemnt-using-kubeadm/adopt-kubernetes.png title=/posts/kubernetes-deployemnt-using-kubeadm/adopt-kubernetes.png></div><div class="details toc" id=toc-static kept>
<div class="details-summary toc-title">
<span>目录</span>
<span><i class="details-icon fas fa-angle-right"></i></span>
</div>
<div class="details-content toc-content" id=toc-content-static><nav id=TableOfContents>
<ul>
<li><a href=#本地-kubernetes-集群安装工具>本地 Kubernetes 集群安装工具</a></li>
<li><a href=#1-节点的环境准备>1. 节点的环境准备</a>
<ul>
<li><a href=#11-iptables-设置>1.1 iptables 设置</a></li>
<li><a href=#12-开放节点端口>1.2 开放节点端口</a></li>
</ul>
</li>
<li><a href=#2-安装-containerd>2. 安装 containerd</a></li>
<li><a href=#3-安装-kubeletkubeadmkubectl>3. 安装 kubelet/kubeadm/kubectl</a></li>
<li><a href=#4-为-master-的-kube-apiserver-创建负载均衡实现高可用>4. 为 master 的 kube-apiserver 创建负载均衡实现高可用</a></li>
<li><a href=#5-使用-kubeadm-创建集群>5. 使用 kubeadm 创建集群</a>
<ul>
<li><a href=#51-常见问题>5.1 常见问题</a>
<ul>
<li><a href=#511-使用国内镜像源>5.1.1 使用国内镜像源</a></li>
<li><a href=#512-重置集群配置>5.1.2 重置集群配置</a></li>
</ul>
</li>
</ul>
</li>
<li><a href=#6-验证集群的高可用性>6. 验证集群的高可用性</a></li>
<li><a href=#7-安装网络插件>7. 安装网络插件</a>
<ul>
<li><a href=#71-安装-cilium>7.1 安装 Cilium</a></li>
<li><a href=#72-安装-calico>7.2 安装 Calico</a></li>
</ul>
</li>
<li><a href=#8-查看集群状态>8. 查看集群状态</a></li>
<li><a href=#9-安装-metrics-server>9. 安装 metrics-server</a></li>
<li><a href=#10-为-etcd-添加定期备份能力>10. 为 etcd 添加定期备份能力</a></li>
<li><a href=#11-安装-volume-provisioner>11. 安装 Volume Provisioner</a></li>
</ul>
</nav></div>
</div><div class=content id=content><blockquote>
<p>本文由个人笔记 <a href=https://github.com/ryan4yin/knowledge/tree/master/kubernetes target=_blank rel="noopener noreferrer">ryan4yin/knowledge</a> 整理而来，不保证正确</p>
</blockquote>
<h2 id=本地-kubernetes-集群安装工具>本地 Kubernetes 集群安装工具</h2>
<blockquote>
<p>云上的 Kubernetes 集群，基本上各云厂商都支持一键部署。这里主要关注本地部署，或者叫做裸机(baremetal)部署</p>
</blockquote>
<blockquote>
<p>本文介绍的方法适合开发测试使用，安全性、稳定性、长期可用性等方案都可能还有问题。</p>
</blockquote>
<blockquote>
<p>本文未考虑国内网络环境，建议在路由器上整个科学代理，或者自行调整文中的部分命令。</p>
</blockquote>
<p>kubernetes 是一个组件化的系统，安装过程有很大的灵活性，很多组件都有多种实现，这些实现各有特点，让初学者眼花缭乱。</p>
<p>而且要把这些组件一个个安装配置好并且能协同工作，也是很不容易的。</p>
<p>因此社区出现了各种各样的安装方案，下面介绍下几种支持裸机（Baremetal）部署的工具：</p>
<ol>
<li><a href=https://kuboard.cn/install/install-k8s.html target=_blank rel="noopener noreferrer">kubeadm</a>: 社区的集群安装工具，目前已经很成熟了。
<ol>
<li>使用难度：简单</li>
</ol>
</li>
<li><a href=https://github.com/k3s-io/k3s target=_blank rel="noopener noreferrer">k3s</a>: 轻量级 kubernetes，资源需求小，部署非常简单，适合开发测试用或者边缘环境
<ol>
<li>支持 airgap 离线部署</li>
<li>使用难度：超级简单</li>
</ol>
</li>
<li><a href=https://github.com/alibaba/sealer target=_blank rel="noopener noreferrer">alibaba/sealer</a>: 支持将整个 kubernetes 打包成一个镜像进行交付，而且部署也非常简单。
<ol>
<li>使用难度：超级简单</li>
<li>这个项目目前还在发展中，不过貌似已经有很多 toB 的公司在使用它进行 k8s 应用的交付了。</li>
</ol>
</li>
<li><a href=https://github.com/kubernetes-sigs/kubespray target=_blank rel="noopener noreferrer">kubespray</a>: 适合自建生产级别的集群，是一个大而全的 kubernetes 安装方案，自动安装容器运行时、k8s、网络插件等组件，而且各组件都有很多方案可选，但是感觉有点复杂。
<ol>
<li>使用难度：中等</li>
<li>支持 airgap 离线部署，但是以前我试用过是有坑，现在不知道咋样了</li>
<li>底层使用了 kubeadm 部署集群</li>
</ol>
</li>
</ol>
<p>笔者为了学习 Kubernetes，下面采用官方的 kubeadm 进行部署（不要问为啥不二进制部署，问就是懒），容器运行时使用 containerd，网络插件则使用目前最潮的基于 eBPF 的 Cilium.</p>
<p>kubernetes 官方介绍了两种高可用集群的拓扑结构：「Stacked etcd topology」和「External etcd topology」，简单起见，本文使用第一种「堆叠 Etcd 拓扑」结构，创建一个三 master 的高可用集群。</p>
<p>参考：</p>
<ul>
<li><a href=https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/install-kubeadm/ target=_blank rel="noopener noreferrer">Kubernetes Docs - Installing kubeadm</a></li>
<li><a href=https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/high-availability/ target=_blank rel="noopener noreferrer">Kubernetes Docs - Creating Highly Available clusters with kubeadm</a></li>
</ul>
<h2 id=1-节点的环境准备>1. 节点的环境准备</h2>
<p>首先准备三台 Linux 虚拟机，系统按需选择，然后调整这三台机器的设置：</p>
<ul>
<li>节点配置：
<ul>
<li>master：不低于 2c/3g，硬盘 20G
<ul>
<li>主节点性能也受集群 Pods 个数的影响，上述配置应该可以支撑到每个 Worker 节点跑 100 个 Pod.</li>
</ul>
</li>
<li>worker：看需求，建议不低于 2c/4g，硬盘不小于 20G，资源充分的话建议 40G.</li>
</ul>
</li>
<li>处于同一网络内并可互通（通常是同一局域网）</li>
<li>各主机的 hostname 和 mac/ip 地址以及 <code>/sys/class/dmi/id/product_uuid</code>，都必须唯一
<ul>
<li>这里最容易出问题的，通常是 hostname 冲突！</li>
</ul>
</li>
<li><strong>必须</strong>关闭 swap，kubelet 才能正常工作！</li>
</ul>
<p>方便起见，我直接使用 <a href=https://github.com/ryan4yin/pulumi-libvirt#examples target=_blank rel="noopener noreferrer">ryan4yin/pulumi-libvirt</a> 自动创建了五个虚拟机，并设置好了 ip/hostname.</p>
<p>本文使用了 opensuse leap 15.3 的 KVM cloud image 进行安装测试。</p>
<h3 id=11-iptables-设置>1.1 iptables 设置</h3>
<p>目前 kubernetes 的容器网络，默认使用的是 bridge 模式，这种模式下，需要使 <code>iptables</code> 能够接管 bridge 上的流量。</p>
<p>配置如下：</p>
<div class=highlight><div class=chroma>
<table class=lntable><tr><td class=lntd>
<pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span></code></pre></td>
<td class=lntd>
<pre tabindex=0 class=chroma><code class=language-shell data-lang=shell>sudo modprobe br_netfilter
cat <span class=s>&lt;&lt;EOF | sudo tee /etc/modules-load.d/k8s.conf
</span><span class=s>br_netfilter
</span><span class=s>EOF</span>

cat <span class=s>&lt;&lt;EOF | sudo tee /etc/sysctl.d/k8s.conf
</span><span class=s>net.bridge.bridge-nf-call-ip6tables = 1
</span><span class=s>net.bridge.bridge-nf-call-iptables = 1
</span><span class=s>EOF</span>
sudo sysctl --system
</code></pre></td></tr></table>
</div>
</div><h3 id=12-开放节点端口>1.2 开放节点端口</h3>
<blockquote>
<p>局域网环境的话，建议直接关闭防火墙。这样所有端口都可用，方便快捷。</p>
</blockquote>
<blockquote>
<p>通常我们的云上集群，也是关闭防火墙的，只是会通过云服务提供的「安全组」来限制客户端 ip</p>
</blockquote>
<p>Control-plane 节点，也就是 master，需要开放如下端口：</p>
<table>
<thead>
<tr>
<th>Protocol</th>
<th>Direction</th>
<th>Port Range</th>
<th>Purpose</th>
<th>Used By</th>
</tr>
</thead>
<tbody>
<tr>
<td>TCP</td>
<td>Inbound</td>
<td>6443*</td>
<td>Kubernetes API server</td>
<td>All</td>
</tr>
<tr>
<td>TCP</td>
<td>Inbound</td>
<td>2379-2380</td>
<td>etcd server client API</td>
<td>kube-apiserver, etcd</td>
</tr>
<tr>
<td>TCP</td>
<td>Inbound</td>
<td>10250</td>
<td>kubelet API</td>
<td>Self, Control plane</td>
</tr>
<tr>
<td>TCP</td>
<td>Inbound</td>
<td>10251</td>
<td>kube-scheduler</td>
<td>Self</td>
</tr>
<tr>
<td>TCP</td>
<td>Inbound</td>
<td>10252</td>
<td>kube-controller-manager</td>
<td>Self</td>
</tr>
</tbody>
</table>
<p>Worker 节点需要开发如下端口：</p>
<table>
<thead>
<tr>
<th>Protocol</th>
<th>Direction</th>
<th>Port Range</th>
<th>Purpose</th>
<th>Used By</th>
</tr>
</thead>
<tbody>
<tr>
<td>TCP</td>
<td>Inbound</td>
<td>10250</td>
<td>kubelet API</td>
<td>Self, Control plane</td>
</tr>
<tr>
<td>TCP</td>
<td>Inbound</td>
<td>30000-32767</td>
<td>NodePort Services†</td>
<td>All</td>
</tr>
</tbody>
</table>
<p>另外通常我们本地测试的时候，可能更想直接在 <code>80</code> <code>443</code> <code>8080</code> 等端口上使用 <code>NodePort</code>，
就需要修改 kube-apiserver 的 <code>--service-node-port-range</code> 参数来自定义 NodePort 的端口范围，相应的 Worker 节点也得开放这些端口。</p>
<h2 id=2-安装-containerd>2. 安装 containerd</h2>
<p>首先是环境配置：</p>
<div class=highlight><div class=chroma>
<table class=lntable><tr><td class=lntd>
<pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span></code></pre></td>
<td class=lntd>
<pre tabindex=0 class=chroma><code class=language-shell data-lang=shell>cat <span class=s>&lt;&lt;EOF | sudo tee /etc/modules-load.d/containerd.conf
</span><span class=s>overlay
</span><span class=s>br_netfilter
</span><span class=s>nf_conntrack
</span><span class=s>EOF</span>

sudo modprobe overlay
sudo modprobe br_netfilter
sudo modprobe nf_conntrack

<span class=c1># Setup required sysctl params, these persist across reboots.</span>
cat <span class=s>&lt;&lt;EOF | sudo tee /etc/sysctl.d/99-kubernetes-cri.conf
</span><span class=s>net.bridge.bridge-nf-call-iptables  = 1
</span><span class=s>net.ipv4.ip_forward                 = 1
</span><span class=s>net.bridge.bridge-nf-call-ip6tables = 1
</span><span class=s>EOF</span>

<span class=c1># Apply sysctl params without reboot</span>
sudo sysctl --system
</code></pre></td></tr></table>
</div>
</div><p>安装 containerd+nerdctl:</p>
<div class=highlight><div class=chroma>
<table class=lntable><tr><td class=lntd>
<pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span><span class=lnt>7
</span><span class=lnt>8
</span></code></pre></td>
<td class=lntd>
<pre tabindex=0 class=chroma><code class=language-shell data-lang=shell>wget https://github.com/containerd/nerdctl/releases/download/v0.11.1/nerdctl-full-0.11.1-linux-amd64.tar.gz
tar -axvf nerdctl-full-0.11.1-linux-amd64.tar.gz
<span class=c1># 这里简单起见，rootless 相关的东西也一起装进去了，测试嘛就无所谓了...</span>
mv bin/* /usr/local/bin/
mv lib/systemd/system/containerd.service /usr/lib/systemd/system/

systemctl <span class=nb>enable</span> containerd
systemctl start containerd
</code></pre></td></tr></table>
</div>
</div><p><code>nerdctl</code> 是一个 containerd 的命令行工具，但是它的容器、镜像与 Kubernetes 的容器、镜像是完全隔离的，不能互通！</p>
<p>目前只能通过 <code>crictl</code> 来查看、拉取 Kubernetes 的容器、镜像，下一节会介绍 crictl 的安装。</p>
<h2 id=3-安装-kubeletkubeadmkubectl>3. 安装 kubelet/kubeadm/kubectl</h2>
<div class=highlight><div class=chroma>
<table class=lntable><tr><td class=lntd>
<pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span><span class=lnt>24
</span><span class=lnt>25
</span><span class=lnt>26
</span><span class=lnt>27
</span><span class=lnt>28
</span><span class=lnt>29
</span><span class=lnt>30
</span><span class=lnt>31
</span><span class=lnt>32
</span><span class=lnt>33
</span><span class=lnt>34
</span></code></pre></td>
<td class=lntd>
<pre tabindex=0 class=chroma><code class=language-shell data-lang=shell><span class=c1># 一些全局都需要用的变量</span>
<span class=nv>CNI_VERSION</span><span class=o>=</span><span class=s2>&#34;v0.8.2&#34;</span>
<span class=nv>CRICTL_VERSION</span><span class=o>=</span><span class=s2>&#34;v1.17.0&#34;</span>
<span class=c1># kubernetes 的版本号</span>
<span class=c1># RELEASE=&#34;$(curl -sSL https://dl.k8s.io/release/stable.txt)&#34;</span>
<span class=nv>RELEASE</span><span class=o>=</span><span class=s2>&#34;1.22.1&#34;</span>
<span class=c1># kubelet 配置文件的版本号</span>
<span class=nv>RELEASE_VERSION</span><span class=o>=</span><span class=s2>&#34;v0.4.0&#34;</span>
<span class=c1># 架构</span>
<span class=nv>ARCH</span><span class=o>=</span><span class=s2>&#34;amd64&#34;</span>
<span class=c1>#　安装目录</span>
<span class=nv>DOWNLOAD_DIR</span><span class=o>=</span>/usr/local/bin


<span class=c1># CNI 插件</span>
sudo mkdir -p /opt/cni/bin
curl -L <span class=s2>&#34;https://github.com/containernetworking/plugins/releases/download/</span><span class=si>${</span><span class=nv>CNI_VERSION</span><span class=si>}</span><span class=s2>/cni-plugins-linux-</span><span class=si>${</span><span class=nv>ARCH</span><span class=si>}</span><span class=s2>-</span><span class=si>${</span><span class=nv>CNI_VERSION</span><span class=si>}</span><span class=s2>.tgz&#34;</span> <span class=p>|</span> sudo tar -C /opt/cni/bin -xz

<span class=c1># crictl 相关工具</span>
curl -L <span class=s2>&#34;https://github.com/kubernetes-sigs/cri-tools/releases/download/</span><span class=si>${</span><span class=nv>CRICTL_VERSION</span><span class=si>}</span><span class=s2>/crictl-</span><span class=si>${</span><span class=nv>CRICTL_VERSION</span><span class=si>}</span><span class=s2>-linux-</span><span class=si>${</span><span class=nv>ARCH</span><span class=si>}</span><span class=s2>.tar.gz&#34;</span> <span class=p>|</span> sudo tar -C <span class=nv>$DOWNLOAD_DIR</span> -xz

<span class=c1># kubelet/kubeadm/kubectl</span>
<span class=nb>cd</span> <span class=nv>$DOWNLOAD_DIR</span>
sudo curl -L --remote-name-all https://storage.googleapis.com/kubernetes-release/release/<span class=si>${</span><span class=nv>RELEASE</span><span class=si>}</span>/bin/linux/<span class=si>${</span><span class=nv>ARCH</span><span class=si>}</span>/<span class=o>{</span>kubeadm,kubelet,kubectl<span class=o>}</span>
sudo chmod +x <span class=o>{</span>kubeadm,kubelet,kubectl<span class=o>}</span>

<span class=c1># kubelet/kubeadm 配置</span>
curl -sSL <span class=s2>&#34;https://raw.githubusercontent.com/kubernetes/release/</span><span class=si>${</span><span class=nv>RELEASE_VERSION</span><span class=si>}</span><span class=s2>/cmd/kubepkg/templates/latest/deb/kubelet/lib/systemd/system/kubelet.service&#34;</span> <span class=p>|</span> sed <span class=s2>&#34;s:/usr/bin:</span><span class=si>${</span><span class=nv>DOWNLOAD_DIR</span><span class=si>}</span><span class=s2>:g&#34;</span> <span class=p>|</span> sudo tee /etc/systemd/system/kubelet.service
sudo mkdir -p /etc/systemd/system/kubelet.service.d
curl -sSL <span class=s2>&#34;https://raw.githubusercontent.com/kubernetes/release/</span><span class=si>${</span><span class=nv>RELEASE_VERSION</span><span class=si>}</span><span class=s2>/cmd/kubepkg/templates/latest/deb/kubeadm/10-kubeadm.conf&#34;</span> <span class=p>|</span> sed <span class=s2>&#34;s:/usr/bin:</span><span class=si>${</span><span class=nv>DOWNLOAD_DIR</span><span class=si>}</span><span class=s2>:g&#34;</span> <span class=p>|</span> sudo tee /etc/systemd/system/kubelet.service.d/10-kubeadm.conf

systemctl <span class=nb>enable</span> --now kubelet
<span class=c1># 验证 kubelet 启动起来了，但是目前还没有初始化配置，过一阵就会重启一次</span>
systemctl status kubelet
</code></pre></td></tr></table>
</div>
</div><p>试用 crictl:</p>
<div class=highlight><div class=chroma>
<table class=lntable><tr><td class=lntd>
<pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span></code></pre></td>
<td class=lntd>
<pre tabindex=0 class=chroma><code class=language-shell data-lang=shell><span class=nb>export</span> <span class=nv>CONTAINER_RUNTIME_ENDPOINT</span><span class=o>=</span><span class=s1>&#39;unix:///var/run/containerd/containerd.sock&#39;</span>
<span class=c1># 列出所有 pods，现在应该啥也没</span>
crictl  pods

<span class=c1># 列出所有镜像</span>
crictl images
</code></pre></td></tr></table>
</div>
</div><h2 id=4-为-master-的-kube-apiserver-创建负载均衡实现高可用>4. 为 master 的 kube-apiserver 创建负载均衡实现高可用</h2>
<p>根据 kubeadm 官方文档 <a href=https://github.com/kubernetes/kubeadm/blob/master/docs/ha-considerations.md#kube-vip target=_blank rel="noopener noreferrer">Kubeadm Docs - High Availability Considerations</a> 介绍，要实现 kube-apiserver 的高可用，目前最知名的负载均衡方式是 keepalived+haproxy，另外也可以考虑使用 kube-vip 等更简单的工具。</p>
<p>简单起见，我们直接用 kube-vip 吧，参考了 kube-vip 的官方文档：<a href=https://kube-vip.io/install_static/ target=_blank rel="noopener noreferrer">Kube-vip as a Static Pod with Kubelet</a>.</p>
<blockquote>
<p>P.S. 我也见过有的安装工具会直接抛弃 keepalived，直接在每个节点上跑一个 nginx 做负载均衡，配置里写死了所有 master 的地址&mldr;</p>
</blockquote>
<p>首先使用如下命令生成 kube-vip 的配置文件，以 ARP 为例（生产环境建议换成 BGP）：</p>
<div class=highlight><div class=chroma>
<table class=lntable><tr><td class=lntd>
<pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span></code></pre></td>
<td class=lntd>
<pre tabindex=0 class=chroma><code class=language-shell data-lang=shell>cat <span class=s>&lt;&lt;EOF | sudo tee add-kube-vip.sh
</span><span class=s># 你的虚拟机网卡，opensuse/centos 等都是 eth0，但是 ubuntu 可能是 ens3
</span><span class=s>export INTERFACE=eth0
</span><span class=s>
</span><span class=s># 用于实现高可用的 vip，需要和前面的网络接口在同一网段内，否则就无法路由了。
</span><span class=s>export VIP=192.168.122.200
</span><span class=s>
</span><span class=s># 生成 static-pod 的配置文件
</span><span class=s>mkdir -p /etc/kubernetes/manifests
</span><span class=s>nerdctl run --rm --network=host --entrypoint=/kube-vip ghcr.io/kube-vip/kube-vip:v0.3.8 \
</span><span class=s>  manifest pod \
</span><span class=s>  --interface $INTERFACE \
</span><span class=s>  --vip $VIP \
</span><span class=s>  --controlplane \
</span><span class=s>  --services \
</span><span class=s>  --arp \
</span><span class=s>  --leaderElection | tee  /etc/kubernetes/manifests/kube-vip.yaml
</span><span class=s>EOF</span>

bash add-kube-vip.sh
</code></pre></td></tr></table>
</div>
</div><p>三个 master 节点都需要跑下上面的命令（worker 不需要），创建好 kube-vip 的 static-pod 配置文件。
在完成 kubeadm 初始化后，kubelet 会自动把它们拉起为 static pod.</p>
<h2 id=5-使用-kubeadm-创建集群>5. 使用 kubeadm 创建集群</h2>
<p>其实需要运行的就是这条命令：</p>
<div class=highlight><div class=chroma>
<table class=lntable><tr><td class=lntd>
<pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span><span class=lnt>24
</span><span class=lnt>25
</span><span class=lnt>26
</span><span class=lnt>27
</span><span class=lnt>28
</span><span class=lnt>29
</span><span class=lnt>30
</span><span class=lnt>31
</span><span class=lnt>32
</span><span class=lnt>33
</span><span class=lnt>34
</span><span class=lnt>35
</span><span class=lnt>36
</span><span class=lnt>37
</span></code></pre></td>
<td class=lntd>
<pre tabindex=0 class=chroma><code class=language-shell data-lang=shell><span class=c1># 极简配置：</span>
cat <span class=s>&lt;&lt;EOF | sudo tee kubeadm-config.yaml
</span><span class=s>apiVersion: kubeadm.k8s.io/v1beta3
</span><span class=s>kind: InitConfiguration
</span><span class=s>nodeRegistration:
</span><span class=s>  criSocket: &#34;/var/run/containerd/containerd.sock&#34;
</span><span class=s>  imagePullPolicy: IfNotPresent
</span><span class=s>---
</span><span class=s>kind: ClusterConfiguration
</span><span class=s>apiVersion: kubeadm.k8s.io/v1beta3
</span><span class=s>kubernetesVersion: v1.22.1
</span><span class=s>clusterName: kubernetes
</span><span class=s>certificatesDir: /etc/kubernetes/pki
</span><span class=s>imageRepository: k8s.gcr.io
</span><span class=s>controlPlaneEndpoint: &#34;192.168.122.200:6443&#34;  # 填 apiserver 的 vip 地址，或者整个域名也行，但是就得加 /etc/hosts 或者内网 DNS 解析
</span><span class=s>networking:
</span><span class=s>  serviceSubnet: &#34;10.96.0.0/16&#34;
</span><span class=s>  podSubnet: &#34;10.244.0.0/16&#34;
</span><span class=s>etcd:
</span><span class=s>  local:
</span><span class=s>    dataDir: /var/lib/etcd
</span><span class=s>---
</span><span class=s>apiVersion: kubelet.config.k8s.io/v1beta1
</span><span class=s>kind: KubeletConfiguration
</span><span class=s>cgroupDriver: systemd
</span><span class=s># 让 kubelet 从 certificates.k8s.io 申请由集群 CA Root 签名的 tls 证书，而非直接使用自签名证书
</span><span class=s># 如果不启用这个， 安装 metrics-server 时就会遇到证书报错，后面会详细介绍。
</span><span class=s>serverTLSBootstrap: true
</span><span class=s>EOF</span>

<span class=c1># 查看 kubeadm 默认的完整配置，供参考</span>
kubeadm config print init-defaults &gt; init.default.yaml

<span class=c1># 执行集群的初始化，这会直接将当前节点创建为 master</span>
<span class=c1># 成功运行的前提：前面该装的东西都装好了，而且 kubelet 已经在后台运行了</span>
<span class=c1># `--upload-certs` 会将生成的集群证书上传到 kubeadm 服务器，在两小时内加入集群的 master 节点会自动拉证书，主要是方便集群创建。</span>
kubeadm init --config kubeadm-config.yaml --upload-certs
</code></pre></td></tr></table>
</div>
</div><p>kubeadm 应该会报错，提示你有些依赖不存在，下面先安装好依赖项。</p>
<div class=highlight><div class=chroma>
<table class=lntable><tr><td class=lntd>
<pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td>
<td class=lntd>
<pre tabindex=0 class=chroma><code class=language-shell data-lang=shell>sudo zypper in -y socat ebtables conntrack-tools
</code></pre></td></tr></table>
</div>
</div><p>再重新运行前面的 kubeadm 命令，应该就能正常执行了，它做的操作有：</p>
<ul>
<li>拉取控制面的容器镜像</li>
<li>生成 ca 根证书</li>
<li>使用根证书为 etcd/apiserver 等一票工具生成 tls 证书</li>
<li>为控制面的各个组件生成 kubeconfig 配置</li>
<li>生成 static pod 配置，kubelet 会根据这些配置自动拉起 kube-proxy 以及其他所有的 k8s master 组件</li>
</ul>
<p>运行完会给出三部分命令：</p>
<ul>
<li>将 <code>kubeconfig</code> 放到 <code>$HOME/.kube/config</code> 下，<code>kubectl</code> 需要使用该配置文件连接 kube-apiserver</li>
<li>control-plane 节点加入集群的命令:
<ul>
<li>这里由于我们提前添加了 kube-vip 的 static-pod 配置，这里的 preflight-check 会报错，需要添加此参数忽略该报错 - <code>--ignore-preflight-errors=DirAvailable--etc-kubernetes-manifests</code></li>
</ul>
<div class=highlight><div class=chroma>
<table class=lntable><tr><td class=lntd>
<pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span></code></pre></td>
<td class=lntd>
<pre tabindex=0 class=chroma><code class=language-shell data-lang=shell>kubeadm join 192.168.122.200:6443 --token &lt;token&gt; <span class=se>\
</span><span class=se></span>  --discovery-token-ca-cert-hash sha256:&lt;hash&gt; <span class=se>\
</span><span class=se></span>  --control-plane --certificate-key &lt;key&gt; <span class=se>\
</span><span class=se></span>  --ignore-preflight-errors<span class=o>=</span>DirAvailable--etc-kubernetes-manifests
</code></pre></td></tr></table>
</div>
</div></li>
<li>worker 节点加入集群的命令:
<div class=highlight><div class=chroma>
<table class=lntable><tr><td class=lntd>
<pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span></code></pre></td>
<td class=lntd>
<pre tabindex=0 class=chroma><code class=language-shell data-lang=shell>kubeadm join 192.168.122.200:6443 --token &lt;token&gt; <span class=se>\
</span><span class=se></span>      --discovery-token-ca-cert-hash sha256:&lt;hash&gt; 
</code></pre></td></tr></table>
</div>
</div></li>
</ul>
<p>跑完第一部分 <code>kubeconfig</code> 的处理命令后，就可以使用 kubectl 查看集群状况了：</p>
<div class=highlight><div class=chroma>
<table class=lntable><tr><td class=lntd>
<pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span></code></pre></td>
<td class=lntd>
<pre tabindex=0 class=chroma><code class=language-shell data-lang=shell>k8s-master-0:~/kubeadm <span class=c1># kubectl get no</span>
NAME           STATUS     ROLES                  AGE   VERSION
k8s-master-0   NotReady   control-plane,master   79s   v1.22.1
k8s-master-0:~/kubeadm <span class=c1># kubectl get po --all-namespaces</span>
NAMESPACE     NAME                                   READY   STATUS    RESTARTS   AGE
kube-system   coredns-78fcd69978-6tlnw               0/1     Pending   <span class=m>0</span>          83s
kube-system   coredns-78fcd69978-hxtvs               0/1     Pending   <span class=m>0</span>          83s
kube-system   etcd-k8s-master-0                      1/1     Running   <span class=m>6</span>          90s
kube-system   kube-apiserver-k8s-master-0            1/1     Running   <span class=m>4</span>          90s
kube-system   kube-controller-manager-k8s-master-0   1/1     Running   <span class=m>4</span>          90s
kube-system   kube-proxy-6w2bx                       1/1     Running   <span class=m>0</span>          83s
kube-system   kube-scheduler-k8s-master-0            1/1     Running   <span class=m>7</span>          97s
</code></pre></td></tr></table>
</div>
</div><p>现在在其他节点运行前面打印出的加入集群的命令，就可以搭建好一个高可用的集群了。</p>
<p>所有节点都加入集群后，通过 kubectl 查看，应该是三个控制面 master，两个 worker：</p>
<div class=highlight><div class=chroma>
<table class=lntable><tr><td class=lntd>
<pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span><span class=lnt>7
</span></code></pre></td>
<td class=lntd>
<pre tabindex=0 class=chroma><code class=language-shell data-lang=shell>k8s-master-0:~/kubeadm <span class=c1># kubectl get node</span>
NAME           STATUS     ROLES                  AGE     VERSION
k8s-master-0   NotReady   control-plane,master   26m     v1.22.1
k8s-master-1   NotReady   control-plane,master   7m2s    v1.22.1
k8s-master-2   NotReady   control-plane,master   2m10s   v1.22.1
k8s-worker-0   NotReady   &lt;none&gt;                 97s     v1.22.1
k8s-worker-1   NotReady   &lt;none&gt;                 86s     v1.22.1
</code></pre></td></tr></table>
</div>
</div><p>现在它们都还处于 NotReady 状态，需要等到我们把网络插件安装好，才会 Ready.</p>
<p>现在再看下集群的证书签发状态：</p>
<div class=highlight><div class=chroma>
<table class=lntable><tr><td class=lntd>
<pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span></code></pre></td>
<td class=lntd>
<pre tabindex=0 class=chroma><code class=language-shell data-lang=shell>❯ kubectl get csr --sort-by<span class=o>=</span><span class=s1>&#39;{.spec.username}&#39;</span>
NAME        AGE     SIGNERNAME                                    REQUESTOR                  REQUESTEDDURATION   CONDITION
csr-95hll   6m58s   kubernetes.io/kube-apiserver-client-kubelet   system:bootstrap:q8ivnz    &lt;none&gt;              Approved,Issued
csr-tklnr   7m5s    kubernetes.io/kube-apiserver-client-kubelet   system:bootstrap:q8ivnz    &lt;none&gt;              Approved,Issued
csr-w92jv   9m15s   kubernetes.io/kube-apiserver-client-kubelet   system:bootstrap:q8ivnz    &lt;none&gt;              Approved,Issued
csr-rv7sj   8m11s   kubernetes.io/kube-apiserver-client-kubelet   system:bootstrap:q8ivnz    &lt;none&gt;              Approved,Issued
csr-nxkgx   10m     kubernetes.io/kube-apiserver-client-kubelet   system:node:k8s-master-0   &lt;none&gt;              Approved,Issued
csr-cd22c   10m     kubernetes.io/kubelet-serving                 system:node:k8s-master-0   &lt;none&gt;              Pending
csr-wjrnr   9m53s   kubernetes.io/kubelet-serving                 system:node:k8s-master-0   &lt;none&gt;              Pending
csr-sjq42   9m8s    kubernetes.io/kubelet-serving                 system:node:k8s-master-1   &lt;none&gt;              Pending
csr-xtv8f   8m56s   kubernetes.io/kubelet-serving                 system:node:k8s-master-1   &lt;none&gt;              Pending
csr-f2dsf   8m3s    kubernetes.io/kubelet-serving                 system:node:k8s-master-2   &lt;none&gt;              Pending
csr-xl8dg   6m58s   kubernetes.io/kubelet-serving                 system:node:k8s-worker-0   &lt;none&gt;              Pending
csr-p9g24   6m52s   kubernetes.io/kubelet-serving                 system:node:k8s-worker-1   &lt;none&gt;              Pending
</code></pre></td></tr></table>
</div>
</div><p>能看到有好几个 <code>kubernetes.io/kubelet-serving</code> 的证书还处于 pending 状态，
这是因为我们在 kubeadm 配置文件中，设置了 <code>serverTLSBootstrap: true</code>，让 Kubelet 从集群中申请 CA 签名证书，而不是自签名导致的。</p>
<p>设置这个参数的主要目的，是为了让 metrics-server 等组件能使用 https 协议与 kubelet 通信，避免为 metrics-server 添加参数 <code>--kubelet-insecure-tls</code>.</p>
<p>目前 kubeadm 不支持自动批准 kubelet 申请的证书，需要我们手动批准一下：</p>
<div class=highlight><div class=chroma>
<table class=lntable><tr><td class=lntd>
<pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span></code></pre></td>
<td class=lntd>
<pre tabindex=0 class=chroma><code class=language-shell data-lang=shell><span class=c1># 批准 Kubelet 申请的所有证书</span>
kubectl certificate approve csr-cd22c csr-wjrnr csr-sjq42 csr-xtv8f csr-f2dsf csr-xl8dg csr-p9g24
</code></pre></td></tr></table>
</div>
</div><p>在未批准这些证书之前，所有需要调用 kubelet api 的功能都将无法使用，比如：</p>
<ul>
<li>查看 pod 日志</li>
<li>获取节点 metrics</li>
<li>等等</li>
</ul>
<h3 id=51-常见问题>5.1 常见问题</h3>
<h4 id=511-使用国内镜像源>5.1.1 使用国内镜像源</h4>
<p>如果你没有科学环境，kubeadm 默认的镜像仓库在国内是拉不了的。
如果对可靠性要求高，最好是自建私有镜像仓库，把镜像推送到私有仓库。</p>
<p>可以通过如下命令列出所有需要用到的镜像地址：</p>
<div class=highlight><div class=chroma>
<table class=lntable><tr><td class=lntd>
<pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span><span class=lnt>7
</span><span class=lnt>8
</span></code></pre></td>
<td class=lntd>
<pre tabindex=0 class=chroma><code class=language-shell data-lang=shell>❯ kubeadm config images list --kubernetes-version v1.22.1
k8s.gcr.io/kube-apiserver:v1.22.1
k8s.gcr.io/kube-controller-manager:v1.22.1
k8s.gcr.io/kube-scheduler:v1.22.1
k8s.gcr.io/kube-proxy:v1.22.1
k8s.gcr.io/pause:3.5
k8s.gcr.io/etcd:3.5.0-0
k8s.gcr.io/coredns/coredns:v1.8.4
</code></pre></td></tr></table>
</div>
</div><p>使用 <code>skopeo</code> 等工具或脚本将上述镜像拷贝到你的私有仓库，或者图方便（测试环境）也可以考虑网上找找别人同步好的镜像地址。将镜像地址添加到 <code>kubeadm-config.yaml</code> 中再部署。</p>
<h4 id=512-重置集群配置>5.1.2 重置集群配置</h4>
<p>创建集群的过程中出现任何问题，都可以通过在所有节点上运行 <code>kubeadm reset</code> 来还原配置，然后重新走 kubeadm 的集群创建流程。</p>
<p>但是要注意几点：</p>
<ul>
<li><code>kubeadm reset</code> 会清除包含 kube-vip 配置在内的所有 static-pod 配置文件，所以 master 节点需要重新跑下前面给的 kube-vip 命令，生成下 kube-vip 配置。</li>
<li><code>kubeadm reset</code> 不会重置网络接口的配置，master 节点需要手动清理下 kube-vip 添加的 vip: <code>ip addr del 192.168.122.200/32 dev eth0</code>.</li>
<li>如果你在安装了网络插件之后希望重装集群，顺序如下：
<ul>
<li>通过 <code>kubectl delete -f xxx.yaml</code>/<code>helm uninstall</code> 删除所有除网络之外的其他应用配置</li>
<li>删除网络插件</li>
<li>先重启一遍所有节点，或者手动重置所有节点的网络配置
<ul>
<li>建议重启，因为我不知道该怎么手动重置&mldr; 试了 <code>systemctl restart network</code> 并不会清理所有虚拟网络接口。</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>如此操作后，再重新执行集群安装，应该就没啥毛病了。</p>
<h2 id=6-验证集群的高可用性>6. 验证集群的高可用性</h2>
<p>虽然网络插件还没装导致集群所有节点都还没 ready，但是我们已经可以通过 kubectl 命令来简单验证集群的高可用性了。</p>
<p>首先，我们将前面放置在 k8s-master-0 的认证文件 <code>$HOME/.kube/config</code> 以及 kunbectl 安装在另一台机器上，比如我直接放我的宿主机。</p>
<p>然后在宿主机上跑 <code>kubectl get node</code> 命令验证集群的高可用性：</p>
<ul>
<li>三个主节点都正常运行时，kubectl 命令也正常</li>
<li>pause 或者 stop 其中一个 master，kubectl 命令仍然能正常运行</li>
<li>再 pause 第二个 master，kubectl 命令应该就会卡住，并且超时，无法使用了</li>
<li>resume 恢复停掉的两个 master 之一，会发现 kubectl 命令又能正常运行了</li>
</ul>
<p>到这里 kubeadm 的工作就完成了，接下来再安装网络插件，集群就可用了。</p>
<h2 id=7-安装网络插件>7. 安装网络插件</h2>
<p>社区有很多种网络插件可选，比较知名且性能也不错的，应该是 Calico 和 Cilium，其中 Cilium 主打基于 eBPF 的高性能与高可观测性。</p>
<p>下面分别介绍这两个插件的安装方法。（注意只能安装其中一个网络插件，不能重复安装。）</p>
<p>需要提前在本机安装好 helm，我这里使用宿主机，因此只需要在宿主机安装:</p>
<div class=highlight><div class=chroma>
<table class=lntable><tr><td class=lntd>
<pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span></code></pre></td>
<td class=lntd>
<pre tabindex=0 class=chroma><code class=language-shell data-lang=shell><span class=c1># 一行命令安装，也可以自己手动下载安装包，都行</span>
curl https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3 <span class=p>|</span> bash

<span class=c1># 或者 opensuse 直接用包管理器安装</span>
sudo zypper in helm
</code></pre></td></tr></table>
</div>
</div><h3 id=71-安装-cilium>7.1 安装 Cilium</h3>
<blockquote>
<p>官方文档：https://docs.cilium.io/en/v1.10/gettingstarted/k8s-install-kubeadm/</p>
</blockquote>
<p>cilium 通过 eBPF 提供了高性能与高可观测的 k8s 集群网络，
另外 cilium 还提供了比 kube-proxy 更高效的实现，可以完全替代 kube-proxy.</p>
<p>这里我们还是先使用 kube-proxy 模式，先熟悉下 cilium 的使用：</p>
<div class=highlight><div class=chroma>
<table class=lntable><tr><td class=lntd>
<pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span></code></pre></td>
<td class=lntd>
<pre tabindex=0 class=chroma><code class=language-shell data-lang=shell>helm repo add cilium https://helm.cilium.io/
helm search repo cilium/cilium -l <span class=p>|</span> head

helm install cilium cilium/cilium --version 1.10.4 --namespace kube-system
</code></pre></td></tr></table>
</div>
</div><p>可以通过 <code>kubectl get pod -A</code> 查看 cilium 的安装进度，当所有 pod 都 ready 后，集群就 ready 了~</p>
<p>cilium 也提供了专用的客户端：</p>
<div class=highlight><div class=chroma>
<table class=lntable><tr><td class=lntd>
<pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span></code></pre></td>
<td class=lntd>
<pre tabindex=0 class=chroma><code class=language-shell data-lang=shell>curl -L --remote-name-all https://github.com/cilium/cilium-cli/releases/latest/download/cilium-linux-amd64.tar.gz<span class=o>{</span>,.sha256sum<span class=o>}</span>
sha256sum --check cilium-linux-amd64.tar.gz.sha256sum
sudo tar xzvfC cilium-linux-amd64.tar.gz /usr/local/bin
rm cilium-linux-amd64.tar.gz<span class=o>{</span>,.sha256sum<span class=o>}</span>
</code></pre></td></tr></table>
</div>
</div><p>然后使用 cilium 客户端检查网络插件的状态：</p>
<div class=highlight><div class=chroma>
<table class=lntable><tr><td class=lntd>
<pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span></code></pre></td>
<td class=lntd>
<pre tabindex=0 class=chroma><code class=language-shell data-lang=shell> $ cilium status --wait
    /¯¯<span class=se>\
</span><span class=se></span> /¯¯<span class=se>\_</span>_/¯¯<span class=se>\ </span>   Cilium:         OK
 <span class=se>\_</span>_/¯¯<span class=se>\_</span>_/    Operator:       OK
 /¯¯<span class=se>\_</span>_/¯¯<span class=se>\ </span>   Hubble:         disabled
 <span class=se>\_</span>_/¯¯<span class=se>\_</span>_/    ClusterMesh:    disabled
    <span class=se>\_</span>_/

DaemonSet         cilium             Desired: 5, Ready: 5/5, Available: 5/5
Deployment        cilium-operator    Desired: 2, Ready: 2/2, Available: 2/2
Containers:       cilium             Running: <span class=m>5</span>
                  cilium-operator    Running: <span class=m>2</span>
Cluster Pods:     2/2 managed by Cilium
Image versions    cilium             quay.io/cilium/cilium:v1.10.4@sha256:7d354052ccf2a7445101d78cebd14444c7c40129ce7889f2f04b89374dbf8a1d: <span class=m>5</span>
                  cilium-operator    quay.io/cilium/operator-generic:v1.10.4@sha256:c49a14e34634ff1a494c84b718641f27267fb3a0291ce3d74352b44f8a8d2f93: <span class=m>2</span>
</code></pre></td></tr></table>
</div>
</div><p>cilium 还提供了命令，自动创建 pod 进行集群网络的连接性测试：</p>
<div class=highlight><div class=chroma>
<table class=lntable><tr><td class=lntd>
<pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span></code></pre></td>
<td class=lntd>
<pre tabindex=0 class=chroma><code class=language-shell data-lang=shell>❯ cilium connectivity <span class=nb>test</span>
ℹ️  Monitor aggregation detected, will skip some flow validation steps
✨ <span class=o>[</span>kubernetes<span class=o>]</span> Creating namespace <span class=k>for</span> connectivity check...
✨ <span class=o>[</span>kubernetes<span class=o>]</span> Deploying echo-same-node service...
✨ <span class=o>[</span>kubernetes<span class=o>]</span> Deploying same-node deployment...
✨ <span class=o>[</span>kubernetes<span class=o>]</span> Deploying client deployment...
✨ <span class=o>[</span>kubernetes<span class=o>]</span> Deploying client2 deployment...
✨ <span class=o>[</span>kubernetes<span class=o>]</span> Deploying echo-other-node service...
✨ <span class=o>[</span>kubernetes<span class=o>]</span> Deploying other-node deployment...
...
ℹ️  Expose Relay locally with:
   cilium hubble <span class=nb>enable</span>
   cilium status --wait
   cilium hubble port-forward<span class=p>&amp;</span>
🏃 Running tests...
...
---------------------------------------------------------------------------------------------------------------------
✅ All <span class=m>11</span> tests <span class=o>(</span><span class=m>134</span> actions<span class=o>)</span> successful, <span class=m>0</span> tests skipped, <span class=m>0</span> scenarios skipped.
</code></pre></td></tr></table>
</div>
</div><p>通过 <code>kubectl get po -A</code> 能观察到，这个测试命令会自动创建一个 <code>cilium-test</code> 名字空间，并在启动创建若干 pod 进行详细的测试。</p>
<p>整个测试流程大概会持续 5 分多钟，测试完成后，相关 Pod 不会自动删除，使用如下命令手动删除：</p>
<div class=highlight><div class=chroma>
<table class=lntable><tr><td class=lntd>
<pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td>
<td class=lntd>
<pre tabindex=0 class=chroma><code class=language-shell data-lang=shell>kubectl delete namespace cilium-test
</code></pre></td></tr></table>
</div>
</div><h3 id=72-安装-calico>7.2 安装 Calico</h3>
<blockquote>
<p>官方文档：https://docs.projectcalico.org/getting-started/kubernetes/self-managed-onprem/onpremises</p>
</blockquote>
<p>也就两三行命令。安装确实特别简单，懒得介绍了，看官方文档吧。</p>
<p>但是实际上 calico 的细节还蛮多的，建议通读下它的官方文档，了解下 calico 的架构。</p>
<h2 id=8-查看集群状态>8. 查看集群状态</h2>
<p>官方的 dashboard 个人感觉不太好用，建议直接在本地装个 k9s 用，特别爽。</p>
<div class=highlight><div class=chroma>
<table class=lntable><tr><td class=lntd>
<pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td>
<td class=lntd>
<pre tabindex=0 class=chroma><code class=language-shell data-lang=shell>sudo zypper in k9s
</code></pre></td></tr></table>
</div>
</div><p>然后就可以愉快地玩耍了。</p>
<h2 id=9-安装-metrics-server>9. 安装 metrics-server</h2>
<blockquote>
<p>这一步可能遇到的问题：<a href=https://kubernetes.io/docs/tasks/administer-cluster/kubeadm/kubeadm-certs/#kubelet-serving-certs target=_blank rel="noopener noreferrer">Enabling signed kubelet serving certificates</a></p>
</blockquote>
<p>如果需要使用 HPA 以及简单的集群监控，那么 metrics-server 是必须安装的，现在我们安装一下它。</p>
<p>首先，跑 kubectl 的监控命令应该会报错：</p>
<div class=highlight><div class=chroma>
<table class=lntable><tr><td class=lntd>
<pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span></code></pre></td>
<td class=lntd>
<pre tabindex=0 class=chroma><code class=language-shell data-lang=shell>❯ kubectl top node
error: Metrics API not available
</code></pre></td></tr></table>
</div>
</div><p>k9s 里面应该也看不到任何监控指标。</p>
<p>现在通过 helm 安装它：</p>
<div class=highlight><div class=chroma>
<table class=lntable><tr><td class=lntd>
<pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span></code></pre></td>
<td class=lntd>
<pre tabindex=0 class=chroma><code class=language-shell data-lang=shell>helm repo add metrics-server https://kubernetes-sigs.github.io/metrics-server/
helm search repo metrics-server/metrics-server -l <span class=p>|</span> head

helm upgrade --install metrics-server metrics-server/metrics-server --version 3.5.0 --namespace kube-system
</code></pre></td></tr></table>
</div>
</div><blockquote>
<p>metrics-server 默认只会部署一个实例，如果希望高可用，请参考官方配置：<a href=https://github.com/kubernetes-sigs/metrics-server/tree/master/manifests/high-availability target=_blank rel="noopener noreferrer">metrics-server - high-availability manifests</a></p>
</blockquote>
<p>等 metrics-server 启动好后，就可以使用 <code>kubectl top</code> 命令啦：</p>
<div class=highlight><div class=chroma>
<table class=lntable><tr><td class=lntd>
<pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span><span class=lnt>24
</span><span class=lnt>25
</span><span class=lnt>26
</span><span class=lnt>27
</span><span class=lnt>28
</span><span class=lnt>29
</span><span class=lnt>30
</span><span class=lnt>31
</span><span class=lnt>32
</span><span class=lnt>33
</span><span class=lnt>34
</span><span class=lnt>35
</span><span class=lnt>36
</span><span class=lnt>37
</span><span class=lnt>38
</span><span class=lnt>39
</span><span class=lnt>40
</span><span class=lnt>41
</span><span class=lnt>42
</span><span class=lnt>43
</span></code></pre></td>
<td class=lntd>
<pre tabindex=0 class=chroma><code class=language-shell data-lang=shell>❯ kubectl top node
NAME           CPU<span class=o>(</span>cores<span class=o>)</span>   CPU%   MEMORY<span class=o>(</span>bytes<span class=o>)</span>   MEMORY%   
k8s-master-0   327m         16%    1465Mi          50%       
k8s-master-1   263m         13%    1279Mi          44%       
k8s-master-2   289m         14%    1282Mi          44%       
k8s-worker-0   62m          3%     518Mi           13%       
k8s-worker-1   115m         2%     659Mi           8%        

❯ kubectl top pod
No resources found in default namespace.

❯ kubectl top pod -A
NAMESPACE     NAME                                   CPU<span class=o>(</span>cores<span class=o>)</span>   MEMORY<span class=o>(</span>bytes<span class=o>)</span>   
kube-system   cilium-45nw4                           9m           135Mi           
kube-system   cilium-5x7jf                           6m           154Mi           
kube-system   cilium-84sr2                           7m           160Mi           
kube-system   cilium-operator-78f45675-dp4b6         2m           30Mi            
kube-system   cilium-operator-78f45675-fpm5g         1m           30Mi            
kube-system   cilium-tkhl4                           6m           141Mi           
kube-system   cilium-zxbvm                           5m           138Mi           
kube-system   coredns-78fcd69978-dpxxk               3m           16Mi            
kube-system   coredns-78fcd69978-ptd9p               1m           18Mi            
kube-system   etcd-k8s-master-0                      61m          88Mi            
kube-system   etcd-k8s-master-1                      50m          85Mi            
kube-system   etcd-k8s-master-2                      55m          83Mi            
kube-system   kube-apiserver-k8s-master-0            98m          462Mi           
kube-system   kube-apiserver-k8s-master-1            85m          468Mi           
kube-system   kube-apiserver-k8s-master-2            85m          423Mi           
kube-system   kube-controller-manager-k8s-master-0   22m          57Mi            
kube-system   kube-controller-manager-k8s-master-1   2m           23Mi            
kube-system   kube-controller-manager-k8s-master-2   2m           23Mi            
kube-system   kube-proxy-j2s76                       1m           24Mi            
kube-system   kube-proxy-k6d6z                       1m           18Mi            
kube-system   kube-proxy-k85rx                       1m           23Mi            
kube-system   kube-proxy-pknsc                       1m           20Mi            
kube-system   kube-proxy-xsq4m                       1m           15Mi            
kube-system   kube-scheduler-k8s-master-0            3m           25Mi            
kube-system   kube-scheduler-k8s-master-1            4m           21Mi            
kube-system   kube-scheduler-k8s-master-2            5m           21Mi            
kube-system   kube-vip-k8s-master-0                  4m           17Mi            
kube-system   kube-vip-k8s-master-1                  2m           16Mi            
kube-system   kube-vip-k8s-master-2                  2m           17Mi            
kube-system   metrics-server-559f85484-5b6xf         7m           27Mi    
</code></pre></td></tr></table>
</div>
</div><h2 id=10-为-etcd-添加定期备份能力>10. 为 etcd 添加定期备份能力</h2>
<p>请移步 <a href=https://github.com/ryan4yin/knowledge/blob/master/datastore/etcd/etcd%20%E7%9A%84%E5%A4%87%E4%BB%BD%E4%B8%8E%E6%81%A2%E5%A4%8D.md target=_blank rel="noopener noreferrer">etcd 的备份与恢复</a></p>
<h2 id=11-安装-volume-provisioner>11. 安装 Volume Provisioner</h2>
<p>在我们学习使用 Prometheus/MinIO/Tekton 等有状态应用时，它们默认情况下会通过 PVC 声明需要的数据卷。</p>
<p>为了支持这个能力，我们需要在集群中部署一个 Volume Provisioner.</p>
<p>对于云上环境，直接接入云服务商提供的 Volume Provisioner 就 OK 了，方便省事而且足够可靠。</p>
<p>而对于 bare-metal 环境，比较有名的应该是 rook-ceph，但是这个玩意部署复杂，维护难度又高，不适合用来测试学习，也不适合生产环境。</p>
<p>对于开发、测试环境，或者个人集群，建议使用：</p>
<ul>
<li>local 数据卷，适合数据可丢失，且不要求分布式的场景，如开发测试环境
<ul>
<li><a href=https://github.com/kubernetes-sigs/sig-storage-local-static-provisioner target=_blank rel="noopener noreferrer">https://github.com/kubernetes-sigs/sig-storage-local-static-provisioner</a></li>
<li><a href=https://github.com/rancher/local-path-provisioner target=_blank rel="noopener noreferrer">https://github.com/rancher/local-path-provisioner</a></li>
</ul>
</li>
<li>NFS 数据卷，适合数据可丢失，对性能要求不高，并且要求分布式的场景。比如开发测试环境、或者线上没啥压力的应用
<ul>
<li><a href=https://github.com/kubernetes-sigs/nfs-subdir-external-provisioner target=_blank rel="noopener noreferrer">https://github.com/kubernetes-sigs/nfs-subdir-external-provisioner</a></li>
<li><a href=https://github.com/kubernetes-csi/csi-driver-nfs target=_blank rel="noopener noreferrer">https://github.com/kubernetes-csi/csi-driver-nfs</a></li>
<li>NFS 数据的可靠性依赖于外部 NFS 服务器，企业通常使用群晖等 NAS 来做 NFS 服务器</li>
<li>如果外部 NFS 服务器出问题，应用就会崩。</li>
</ul>
</li>
<li>直接使用云上的对象存储，适合希望数据不丢失、对性能要求不高的场景。
<ul>
<li>直接使用 <a href=https://github.com/rclone/rclone target=_blank rel="noopener noreferrer">https://github.com/rclone/rclone</a> mount 模式来保存数据，或者直接同步文件夹数据到云端（可能会有一定数据丢失）。</li>
</ul>
</li>
</ul>
</div><div class=post-footer id=post-footer>
<div class=post-info>
<div class=post-info-line>
<div class=post-info-mod>
<span>更新于 2022-01-25</span>
</div>
<div class=post-info-license></div>
</div>
<div class=post-info-line>
<div class=post-info-md></div>
<div class=post-info-share>
<span><a href=javascript:void(0); title="分享到 Twitter" data-sharer=twitter data-url=https://ryan4yin.space/posts/kubernetes-deployemnt-using-kubeadm/ data-title="部署一个 Kubernetes 集群" data-via=ryan4yin data-hashtags=Kubernetes,云原生><i class="fab fa-twitter fa-fw"></i></a><a href=javascript:void(0); title="分享到 Facebook" data-sharer=facebook data-url=https://ryan4yin.space/posts/kubernetes-deployemnt-using-kubeadm/ data-hashtag=Kubernetes><i class="fab fa-facebook-square fa-fw"></i></a><a href=javascript:void(0); title="分享到 Reddit" data-sharer=reddit data-url=https://ryan4yin.space/posts/kubernetes-deployemnt-using-kubeadm/><i class="fab fa-reddit fa-fw"></i></a></span>
</div>
</div>
</div>
<div class=post-info-more>
<section class=post-tags><i class="fas fa-tags fa-fw"></i>&nbsp;<a href=/tags/kubernetes/>Kubernetes</a>,&nbsp;<a href=/tags/%E4%BA%91%E5%8E%9F%E7%94%9F/>云原生</a></section>
<section>
<span><a href=javascript:void(0); onclick=window.history.back()>返回</a></span>&nbsp;|&nbsp;<span><a href=/>主页</a></span>
</section>
</div>
<div class=post-nav><a href=/posts/kubernetes-best-practices/ class=prev rel=prev title="Kubernetes 微服务最佳实践"><i class="fas fa-angle-left fa-fw"></i>Kubernetes 微服务最佳实践</a></div>
</div>
<div id=comments><div id=utterances></div><noscript>
Please enable JavaScript to view the comments powered by <a href=https://utteranc.es/>Utterances</a>.
</noscript></div></article></div>
</main><footer class=footer>
<div class=footer-container><div class=footer-line>由 <a href=https://gohugo.io/ target=_blank rel="noopener noreferrer" title="Hugo 0.91.2">Hugo</a> 强力驱动 | 主题 - <a href=https://github.com/sunt-programator/CodeIT target=_blank rel="noopener noreferrer" title="CodeIT 0.2.10"><i class="fas fa-laptop-code fa-fw"></i> CodeIT</a>
</div><div class=footer-line><i class="far fa-copyright fa-fw"></i><span itemprop=copyrightYear>2021 - 2022</span><span class=author itemprop=copyrightHolder>&nbsp;<a href=https://ryan4yin.space target=_blank rel="noopener noreferrer">ryan4yin</a></span>&nbsp;|&nbsp;<span class=license><a rel="license external nofollow noopener noreffer" href=https://creativecommons.org/licenses/by-nc/4.0/ target=_blank>CC BY-NC 4.0</a></span></div>
</div>
</footer></div>
<div id=fixed-buttons><a href=# id=back-to-top class=fixed-button title=回到顶部>
<i class="fas fa-arrow-up fa-fw"></i>
</a><a href=# id=view-comments class=fixed-button title=查看评论>
<i class="fas fa-comment fa-fw"></i>
</a>
</div><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.13.11/dist/katex.min.css><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.13.11/dist/contrib/copy-tex.min.css><script type=text/javascript src=https://cdn.jsdelivr.net/npm/smooth-scroll@16.1.3/dist/smooth-scroll.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/autocomplete.js@0.37.1/dist/autocomplete.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/algoliasearch@4.2.0/dist/algoliasearch-lite.umd.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/lazysizes@5.2.2/lazysizes.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/clipboard@2.0.6/dist/clipboard.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/sharer.js@0.4.0/sharer.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/typeit@7.0.4/dist/typeit.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/katex@0.13.11/dist/katex.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/katex@0.13.11/dist/contrib/auto-render.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/katex@0.13.11/dist/contrib/copy-tex.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/katex@0.13.11/dist/contrib/mhchem.min.js></script><script type=text/javascript>window.config={code:{copyTitle:"复制到剪贴板",maxShownLines:30},comment:{utterances:{darkTheme:"github-dark",issueTerm:"pathname",label:"",lightTheme:"github-light",repo:"ryan4yin/ryan4yin.space"}},data:{"id-1":"Ryan4Yin's Space","id-2":"Ryan4Yin's Space"},math:{delimiters:[{display:!0,left:"$$",right:"$$"},{display:!0,left:"\\[",right:"\\]"},{display:!1,left:"$",right:"$"},{display:!1,left:"\\(",right:"\\)"}],strict:!1},search:{algoliaAppID:"747LJ10EI7",algoliaIndex:"ryan-space",algoliaSearchKey:"658db5f2bf056f83458cacf5dd58ec80",highlightTag:"em",maxResultLength:10,noResultsFound:"没有找到结果",snippetLength:50,type:"algolia"},typeit:{cursorChar:null,cursorSpeed:null,data:{"id-1":["id-1"],"id-2":["id-2"]},duration:null,speed:null}}</script><script type=text/javascript src=/js/theme.min.js></script><script type=text/javascript>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag('js',new Date),gtag('config','G-4V93QVSNFW',{anonymize_ip:!0})</script><script type=text/javascript src="https://www.googletagmanager.com/gtag/js?id=G-4V93QVSNFW" async></script></body>
</html>