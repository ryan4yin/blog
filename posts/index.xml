<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:dc="http://purl.org/dc/elements/1.1/" version="2.0"><channel><title>所有文章 - Ryan4Yin's Space</title><link>https://ryan4yin.space/posts/</link><description>所有文章 | Ryan4Yin's Space</description><generator>Hugo -- gohugo.io</generator><language>zh-CN</language><managingEditor>xiaoyin_c@qq.com (ryan4yin)</managingEditor><webMaster>xiaoyin_c@qq.com (ryan4yin)</webMaster><lastBuildDate>Tue, 25 Jan 2022 01:37:00 +0800</lastBuildDate><atom:link href="https://ryan4yin.space/posts/" rel="self" type="application/rss+xml"/><item><title>部署一个 Kubernetes 集群</title><link>https://ryan4yin.space/posts/kubernetes-deployemnt-using-kubeadm/</link><pubDate>Tue, 25 Jan 2022 01:37:00 +0800</pubDate><author>xiaoyin_c@qq.com</author><dc:creator>ryan4yin</dc:creator><guid>https://ryan4yin.space/posts/kubernetes-deployemnt-using-kubeadm/</guid><description><![CDATA[<blockquote>
<p>本文由个人笔记 <a href="https://github.com/ryan4yin/knowledge/tree/master/kubernetes" target="_blank" rel="noopener noreferrer">ryan4yin/knowledge</a> 整理而来，不保证正确</p>
</blockquote>
<h2 id="本地-kubernetes-集群安装工具">本地 Kubernetes 集群安装工具</h2>
<blockquote>
<p>云上的 Kubernetes 集群，基本上各云厂商都支持一键部署。这里主要关注本地部署，或者叫做裸机(baremetal)部署</p>
</blockquote>
<blockquote>
<p>本文介绍的方法适合开发测试使用，安全性、稳定性、长期可用性等方案都可能还有问题。</p>
</blockquote>
<blockquote>
<p>本文未考虑国内网络环境，建议在路由器上整个科学代理，或者自行调整文中的部分命令。</p>
</blockquote>
<p>kubernetes 是一个组件化的系统，安装过程有很大的灵活性，很多组件都有多种实现，这些实现各有特点，让初学者眼花缭乱。</p>
<p>而且要把这些组件一个个安装配置好并且能协同工作，也是很不容易的。</p>
<p>因此社区出现了各种各样的安装方案，下面介绍下几种支持裸机（Baremetal）部署的工具：</p>
<ol>
<li><a href="https://kuboard.cn/install/install-k8s.html" target="_blank" rel="noopener noreferrer">kubeadm</a>: 社区的集群安装工具，目前已经很成熟了。
<ol>
<li>使用难度：简单</li>
</ol>
</li>
<li><a href="https://github.com/k3s-io/k3s" target="_blank" rel="noopener noreferrer">k3s</a>: 轻量级 kubernetes，资源需求小，部署非常简单，适合开发测试用或者边缘环境
<ol>
<li>支持 airgap 离线部署</li>
<li>使用难度：超级简单</li>
</ol>
</li>
<li><a href="https://github.com/alibaba/sealer" target="_blank" rel="noopener noreferrer">alibaba/sealer</a>: 支持将整个 kubernetes 打包成一个镜像进行交付，而且部署也非常简单。
<ol>
<li>使用难度：超级简单</li>
<li>这个项目目前还在发展中，不过貌似已经有很多 toB 的公司在使用它进行 k8s 应用的交付了。</li>
</ol>
</li>
<li><a href="https://github.com/kubernetes-sigs/kubespray" target="_blank" rel="noopener noreferrer">kubespray</a>: 适合自建生产级别的集群，是一个大而全的 kubernetes 安装方案，自动安装容器运行时、k8s、网络插件等组件，而且各组件都有很多方案可选，但是感觉有点复杂。
<ol>
<li>使用难度：中等</li>
<li>支持 airgap 离线部署，但是以前我试用过是有坑，现在不知道咋样了</li>
<li>底层使用了 kubeadm 部署集群</li>
</ol>
</li>
</ol>
<p>笔者为了学习 Kuberntes，下面采用官方的 kubeadm 进行部署（不要问为啥不二进制部署，问就是懒），容器运行时使用 containerd，网络插件则使用目前最潮的基于 eBPF 的 Cilium.</p>
<p>kubernetes 官方介绍了两种高可用集群的拓扑结构：「Stacked etcd topology」和「External etcd topology」，简单起见，本文使用第一种「堆叠 Etcd 拓扑」结构，创建一个三 master 的高可用集群。</p>
<p>参考：</p>
<ul>
<li><a href="https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/install-kubeadm/" target="_blank" rel="noopener noreferrer">Kubernetes Docs - Installing kubeadm</a></li>
<li><a href="https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/high-availability/" target="_blank" rel="noopener noreferrer">Kubernetes Docs - Creating Highly Available clusters with kubeadm</a></li>
</ul>
<h2 id="1-节点的环境准备">1. 节点的环境准备</h2>
<p>首先准备三台 Linux 虚拟机，系统按需选择，然后调整这三台机器的设置：</p>
<ul>
<li>节点配置：
<ul>
<li>master：不低于 2c/3g，硬盘 20G
<ul>
<li>主节点性能也受集群 Pods 个数的影响，上述配置应该可以支撑到每个 Worker 节点跑 100 个 Pod.</li>
</ul>
</li>
<li>worker：看需求，建议不低于 2c/4g，硬盘不小于 20G，资源充分的话建议 40G.</li>
</ul>
</li>
<li>处于同一网络内并可互通（通常是同一局域网）</li>
<li>各主机的 hostname 和 mac/ip 地址以及 <code>/sys/class/dmi/id/product_uuid</code>，都必须唯一
<ul>
<li>这里最容易出问题的，通常是 hostname 冲突！</li>
</ul>
</li>
<li><strong>必须</strong>关闭 swap，kubelet 才能正常工作！</li>
</ul>
<p>方便起见，我直接使用 <a href="https://github.com/ryan4yin/pulumi-libvirt#examples" target="_blank" rel="noopener noreferrer">ryan4yin/pulumi-libvirt</a> 自动创建了五个虚拟机，并设置好了 ip/hostname.</p>
<p>本文使用了 opensuse leap 15.3 的 KVM cloud image 进行安装测试。</p>
<h3 id="11-iptables-设置">1.1 iptables 设置</h3>
<p>目前 kubernetes 的容器网络，默认使用的是 bridge 模式，这种模式下，需要使 <code>iptables</code> 能够接管 bridge 上的流量。</p>
<p>配置如下：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-shell" data-lang="shell">sudo modprobe br_netfilter
cat <span class="s">&lt;&lt;EOF | sudo tee /etc/modules-load.d/k8s.conf
</span><span class="s">br_netfilter
</span><span class="s">EOF</span>

cat <span class="s">&lt;&lt;EOF | sudo tee /etc/sysctl.d/k8s.conf
</span><span class="s">net.bridge.bridge-nf-call-ip6tables = 1
</span><span class="s">net.bridge.bridge-nf-call-iptables = 1
</span><span class="s">EOF</span>
sudo sysctl --system
</code></pre></td></tr></table>
</div>
</div><h3 id="12-开放节点端口">1.2 开放节点端口</h3>
<blockquote>
<p>局域网环境的话，建议直接关闭防火墙。这样所有端口都可用，方便快捷。</p>
</blockquote>
<blockquote>
<p>通常我们的云上集群，也是关闭防火墙的，只是会通过云服务提供的「安全组」来限制客户端 ip</p>
</blockquote>
<p>Control-plane 节点，也就是 master，需要开放如下端口：</p>
<table>
<thead>
<tr>
<th>Protocol</th>
<th>Direction</th>
<th>Port Range</th>
<th>Purpose</th>
<th>Used By</th>
</tr>
</thead>
<tbody>
<tr>
<td>TCP</td>
<td>Inbound</td>
<td>6443*</td>
<td>Kubernetes API server</td>
<td>All</td>
</tr>
<tr>
<td>TCP</td>
<td>Inbound</td>
<td>2379-2380</td>
<td>etcd server client API</td>
<td>kube-apiserver, etcd</td>
</tr>
<tr>
<td>TCP</td>
<td>Inbound</td>
<td>10250</td>
<td>kubelet API</td>
<td>Self, Control plane</td>
</tr>
<tr>
<td>TCP</td>
<td>Inbound</td>
<td>10251</td>
<td>kube-scheduler</td>
<td>Self</td>
</tr>
<tr>
<td>TCP</td>
<td>Inbound</td>
<td>10252</td>
<td>kube-controller-manager</td>
<td>Self</td>
</tr>
</tbody>
</table>
<p>Worker 节点需要开发如下端口：</p>
<table>
<thead>
<tr>
<th>Protocol</th>
<th>Direction</th>
<th>Port Range</th>
<th>Purpose</th>
<th>Used By</th>
</tr>
</thead>
<tbody>
<tr>
<td>TCP</td>
<td>Inbound</td>
<td>10250</td>
<td>kubelet API</td>
<td>Self, Control plane</td>
</tr>
<tr>
<td>TCP</td>
<td>Inbound</td>
<td>30000-32767</td>
<td>NodePort Services†</td>
<td>All</td>
</tr>
</tbody>
</table>
<p>另外通常我们本地测试的时候，可能更想直接在 <code>80</code> <code>443</code> <code>8080</code> 等端口上使用 <code>NodePort</code>，
就需要修改 kube-apiserver 的 <code>--service-node-port-range</code> 参数来自定义 NodePort 的端口范围，相应的 Worker 节点也得开放这些端口。</p>
<h2 id="2-安装-containerd">2. 安装 containerd</h2>
<p>首先是环境配置：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-shell" data-lang="shell">cat <span class="s">&lt;&lt;EOF | sudo tee /etc/modules-load.d/containerd.conf
</span><span class="s">overlay
</span><span class="s">br_netfilter
</span><span class="s">nf_conntrack
</span><span class="s">EOF</span>

sudo modprobe overlay
sudo modprobe br_netfilter
sudo modprobe nf_conntrack

<span class="c1"># Setup required sysctl params, these persist across reboots.</span>
cat <span class="s">&lt;&lt;EOF | sudo tee /etc/sysctl.d/99-kubernetes-cri.conf
</span><span class="s">net.bridge.bridge-nf-call-iptables  = 1
</span><span class="s">net.ipv4.ip_forward                 = 1
</span><span class="s">net.bridge.bridge-nf-call-ip6tables = 1
</span><span class="s">EOF</span>

<span class="c1"># Apply sysctl params without reboot</span>
sudo sysctl --system
</code></pre></td></tr></table>
</div>
</div><p>安装 containerd+nerdctl:</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-shell" data-lang="shell">wget https://github.com/containerd/nerdctl/releases/download/v0.11.1/nerdctl-full-0.11.1-linux-amd64.tar.gz
tar -axvf nerdctl-full-0.11.1-linux-amd64.tar.gz
<span class="c1"># 这里简单起见，rootless 相关的东西也一起装进去了，测试嘛就无所谓了...</span>
mv bin/* /usr/local/bin/
mv lib/systemd/system/containerd.service /usr/lib/systemd/system/

systemctl <span class="nb">enable</span> containerd
systemctl start containerd
</code></pre></td></tr></table>
</div>
</div><p><code>nerdctl</code> 是一个 containerd 的命令行工具，但是它的容器、镜像与 Kubernetes 的容器、镜像是完全隔离的，不能互通！</p>
<p>目前只能通过 <code>crictl</code> 来查看、拉取 Kubernetes 的容器、镜像，下一节会介绍 crictl 的安装。</p>
<h2 id="3-安装-kubeletkubeadmkubectl">3. 安装 kubelet/kubeadm/kubectl</h2>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-shell" data-lang="shell"><span class="c1"># 一些全局都需要用的变量</span>
<span class="nv">CNI_VERSION</span><span class="o">=</span><span class="s2">&#34;v0.8.2&#34;</span>
<span class="nv">CRICTL_VERSION</span><span class="o">=</span><span class="s2">&#34;v1.17.0&#34;</span>
<span class="c1"># kubernetes 的版本号</span>
<span class="c1"># RELEASE=&#34;$(curl -sSL https://dl.k8s.io/release/stable.txt)&#34;</span>
<span class="nv">RELEASE</span><span class="o">=</span><span class="s2">&#34;1.22.1&#34;</span>
<span class="c1"># kubelet 配置文件的版本号</span>
<span class="nv">RELEASE_VERSION</span><span class="o">=</span><span class="s2">&#34;v0.4.0&#34;</span>
<span class="c1"># 架构</span>
<span class="nv">ARCH</span><span class="o">=</span><span class="s2">&#34;amd64&#34;</span>
<span class="c1">#　安装目录</span>
<span class="nv">DOWNLOAD_DIR</span><span class="o">=</span>/usr/local/bin


<span class="c1"># CNI 插件</span>
sudo mkdir -p /opt/cni/bin
curl -L <span class="s2">&#34;https://github.com/containernetworking/plugins/releases/download/</span><span class="si">${</span><span class="nv">CNI_VERSION</span><span class="si">}</span><span class="s2">/cni-plugins-linux-</span><span class="si">${</span><span class="nv">ARCH</span><span class="si">}</span><span class="s2">-</span><span class="si">${</span><span class="nv">CNI_VERSION</span><span class="si">}</span><span class="s2">.tgz&#34;</span> <span class="p">|</span> sudo tar -C /opt/cni/bin -xz

<span class="c1"># crictl 相关工具</span>
curl -L <span class="s2">&#34;https://github.com/kubernetes-sigs/cri-tools/releases/download/</span><span class="si">${</span><span class="nv">CRICTL_VERSION</span><span class="si">}</span><span class="s2">/crictl-</span><span class="si">${</span><span class="nv">CRICTL_VERSION</span><span class="si">}</span><span class="s2">-linux-</span><span class="si">${</span><span class="nv">ARCH</span><span class="si">}</span><span class="s2">.tar.gz&#34;</span> <span class="p">|</span> sudo tar -C <span class="nv">$DOWNLOAD_DIR</span> -xz

<span class="c1"># kubelet/kubeadm/kubectl</span>
<span class="nb">cd</span> <span class="nv">$DOWNLOAD_DIR</span>
sudo curl -L --remote-name-all https://storage.googleapis.com/kubernetes-release/release/<span class="si">${</span><span class="nv">RELEASE</span><span class="si">}</span>/bin/linux/<span class="si">${</span><span class="nv">ARCH</span><span class="si">}</span>/<span class="o">{</span>kubeadm,kubelet,kubectl<span class="o">}</span>
sudo chmod +x <span class="o">{</span>kubeadm,kubelet,kubectl<span class="o">}</span>

<span class="c1"># kubelet/kubeadm 配置</span>
curl -sSL <span class="s2">&#34;https://raw.githubusercontent.com/kubernetes/release/</span><span class="si">${</span><span class="nv">RELEASE_VERSION</span><span class="si">}</span><span class="s2">/cmd/kubepkg/templates/latest/deb/kubelet/lib/systemd/system/kubelet.service&#34;</span> <span class="p">|</span> sed <span class="s2">&#34;s:/usr/bin:</span><span class="si">${</span><span class="nv">DOWNLOAD_DIR</span><span class="si">}</span><span class="s2">:g&#34;</span> <span class="p">|</span> sudo tee /etc/systemd/system/kubelet.service
sudo mkdir -p /etc/systemd/system/kubelet.service.d
curl -sSL <span class="s2">&#34;https://raw.githubusercontent.com/kubernetes/release/</span><span class="si">${</span><span class="nv">RELEASE_VERSION</span><span class="si">}</span><span class="s2">/cmd/kubepkg/templates/latest/deb/kubeadm/10-kubeadm.conf&#34;</span> <span class="p">|</span> sed <span class="s2">&#34;s:/usr/bin:</span><span class="si">${</span><span class="nv">DOWNLOAD_DIR</span><span class="si">}</span><span class="s2">:g&#34;</span> <span class="p">|</span> sudo tee /etc/systemd/system/kubelet.service.d/10-kubeadm.conf

systemctl <span class="nb">enable</span> --now kubelet
<span class="c1"># 验证 kubelet 启动起来了，但是目前还没有初始化配置，过一阵就会重启一次</span>
systemctl status kubelet
</code></pre></td></tr></table>
</div>
</div><p>试用 crictl:</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-shell" data-lang="shell"><span class="nb">export</span> <span class="nv">CONTAINER_RUNTIME_ENDPOINT</span><span class="o">=</span><span class="s1">&#39;unix:///var/run/containerd/containerd.sock&#39;</span>
<span class="c1"># 列出所有 pods，现在应该啥也没</span>
crictl  pods

<span class="c1"># 列出所有镜像</span>
crictl images
</code></pre></td></tr></table>
</div>
</div><h2 id="4-为-master-的-kube-apiserver-创建负载均衡实现高可用">4. 为 master 的 kube-apiserver 创建负载均衡实现高可用</h2>
<p>根据 kubeadm 官方文档 <a href="https://github.com/kubernetes/kubeadm/blob/master/docs/ha-considerations.md#kube-vip" target="_blank" rel="noopener noreferrer">Kubeadm Docs - High Availability Considerations</a> 介绍，要实现 kube-apiserver 的高可用，目前最知名的负载均衡方式是 keepalived+haproxy，另外也可以考虑使用 kube-vip 等更简单的工具。</p>
<p>简单起见，我们直接用 kube-vip 吧，参考了 kube-vip 的官方文档：<a href="https://kube-vip.io/install_static/" target="_blank" rel="noopener noreferrer">Kube-vip as a Static Pod with Kubelet</a>.</p>
<blockquote>
<p>P.S. 我也见过有的安装工具会直接抛弃 keepalived，直接在每个节点上跑一个 nginx 做负载均衡，配置里写死了所有 master 的地址&hellip;</p>
</blockquote>
<p>首先使用如下命令生成 kube-vip 的配置文件，以 ARP 为例（生产环境建议换成 BGP）：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-shell" data-lang="shell">cat <span class="s">&lt;&lt;EOF | sudo tee add-kube-vip.sh
</span><span class="s"># 你的虚拟机网卡，opensuse/centos 等都是 eth0，但是 ubuntu 可能是 ens3
</span><span class="s">export INTERFACE=eth0
</span><span class="s">
</span><span class="s"># 用于实现高可用的 vip，需要和前面的网络接口在同一网段内，否则就无法路由了。
</span><span class="s">export VIP=192.168.122.200
</span><span class="s">
</span><span class="s"># 生成 static-pod 的配置文件
</span><span class="s">mkdir -p /etc/kubernetes/manifests
</span><span class="s">nerdctl run --rm --network=host --entrypoint=/kube-vip ghcr.io/kube-vip/kube-vip:v0.3.8 \
</span><span class="s">  manifest pod \
</span><span class="s">  --interface $INTERFACE \
</span><span class="s">  --vip $VIP \
</span><span class="s">  --controlplane \
</span><span class="s">  --services \
</span><span class="s">  --arp \
</span><span class="s">  --leaderElection | tee  /etc/kubernetes/manifests/kube-vip.yaml
</span><span class="s">EOF</span>

bash add-kube-vip.sh
</code></pre></td></tr></table>
</div>
</div><p>三个 master 节点都需要跑下上面的命令（worker 不需要），创建好 kube-vip 的 static-pod 配置文件。
在完成 kubeadm 初始化后，kubelet 会自动把它们拉起为 static pod.</p>
<h2 id="5-使用-kubeadm-创建集群">5. 使用 kubeadm 创建集群</h2>
<p>其实需要运行的就是这条命令：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-shell" data-lang="shell"><span class="c1"># 极简配置：</span>
cat <span class="s">&lt;&lt;EOF | sudo tee kubeadm-config.yaml
</span><span class="s">apiVersion: kubeadm.k8s.io/v1beta3
</span><span class="s">kind: InitConfiguration
</span><span class="s">nodeRegistration:
</span><span class="s">  criSocket: &#34;/var/run/containerd/containerd.sock&#34;
</span><span class="s">  imagePullPolicy: IfNotPresent
</span><span class="s">---
</span><span class="s">kind: ClusterConfiguration
</span><span class="s">apiVersion: kubeadm.k8s.io/v1beta3
</span><span class="s">kubernetesVersion: v1.22.1
</span><span class="s">clusterName: kubernetes
</span><span class="s">certificatesDir: /etc/kubernetes/pki
</span><span class="s">imageRepository: k8s.gcr.io
</span><span class="s">controlPlaneEndpoint: &#34;192.168.122.200:6443&#34;  # 填 apiserver 的 vip 地址，或者整个域名也行，但是就得加 /etc/hosts 或者内网 DNS 解析
</span><span class="s">networking:
</span><span class="s">  serviceSubnet: &#34;10.96.0.0/16&#34;
</span><span class="s">  podSubnet: &#34;10.244.0.0/16&#34;
</span><span class="s">etcd:
</span><span class="s">  local:
</span><span class="s">    dataDir: /var/lib/etcd
</span><span class="s">---
</span><span class="s">apiVersion: kubelet.config.k8s.io/v1beta1
</span><span class="s">kind: KubeletConfiguration
</span><span class="s">cgroupDriver: systemd
</span><span class="s"># 让 kubelet 从 certificates.k8s.io 申请由集群 CA Root 签名的 tls 证书，而非直接使用自签名证书
</span><span class="s"># 如果不启用这个， 安装 metrics-server 时就会遇到证书报错，后面会详细介绍。
</span><span class="s">serverTLSBootstrap: true
</span><span class="s">EOF</span>

<span class="c1"># 查看 kubeadm 默认的完整配置，供参考</span>
kubeadm config print init-defaults &gt; init.default.yaml

<span class="c1"># 执行集群的初始化，这会直接将当前节点创建为 master</span>
<span class="c1"># 成功运行的前提：前面该装的东西都装好了，而且 kubelet 已经在后台运行了</span>
<span class="c1"># `--upload-certs` 会将生成的集群证书上传到 kubeadm 服务器，在两小时内加入集群的 master 节点会自动拉证书，主要是方便集群创建。</span>
kubeadm init --config kubeadm-config.yaml --upload-certs
</code></pre></td></tr></table>
</div>
</div><p>kubeadm 应该会报错，提示你有些依赖不存在，下面先安装好依赖项。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-shell" data-lang="shell">sudo zypper in -y socat ebtables conntrack-tools
</code></pre></td></tr></table>
</div>
</div><p>再重新运行前面的 kubeadm 命令，应该就能正常执行了，它做的操作有：</p>
<ul>
<li>拉取控制面的容器镜像</li>
<li>生成 ca 根证书</li>
<li>使用根证书为 etcd/apiserver 等一票工具生成 tls 证书</li>
<li>为控制面的各个组件生成 kubeconfig 配置</li>
<li>生成 static pod 配置，kubelet 会根据这些配置自动拉起 kube-proxy 以及其他所有的 k8s master 组件</li>
</ul>
<p>运行完会给出三部分命令：</p>
<ul>
<li>将 <code>kubeconfig</code> 放到 <code>$HOME/.kube/config</code> 下，<code>kubectl</code> 需要使用该配置文件连接 kube-apiserver</li>
<li>control-plane 节点加入集群的命令:
<ul>
<li>这里由于我们提前添加了 kube-vip 的 static-pod 配置，这里的 preflight-check 会报错，需要添加此参数忽略该报错 - <code>--ignore-preflight-errors=DirAvailable--etc-kubernetes-manifests</code></li>
</ul>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-shell" data-lang="shell">kubeadm join 192.168.122.200:6443 --token &lt;token&gt; <span class="se">\
</span><span class="se"></span>  --discovery-token-ca-cert-hash sha256:&lt;hash&gt; <span class="se">\
</span><span class="se"></span>  --control-plane --certificate-key &lt;key&gt; <span class="se">\
</span><span class="se"></span>  --ignore-preflight-errors<span class="o">=</span>DirAvailable--etc-kubernetes-manifests
</code></pre></td></tr></table>
</div>
</div></li>
<li>worker 节点加入集群的命令:
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-shell" data-lang="shell">kubeadm join 192.168.122.200:6443 --token &lt;token&gt; <span class="se">\
</span><span class="se"></span>      --discovery-token-ca-cert-hash sha256:&lt;hash&gt; 
</code></pre></td></tr></table>
</div>
</div></li>
</ul>
<p>跑完第一部分 <code>kubeconfig</code> 的处理命令后，就可以使用 kubectl 查看集群状况了：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-shell" data-lang="shell">k8s-master-0:~/kubeadm <span class="c1"># kubectl get no</span>
NAME           STATUS     ROLES                  AGE   VERSION
k8s-master-0   NotReady   control-plane,master   79s   v1.22.1
k8s-master-0:~/kubeadm <span class="c1"># kubectl get po --all-namespaces</span>
NAMESPACE     NAME                                   READY   STATUS    RESTARTS   AGE
kube-system   coredns-78fcd69978-6tlnw               0/1     Pending   <span class="m">0</span>          83s
kube-system   coredns-78fcd69978-hxtvs               0/1     Pending   <span class="m">0</span>          83s
kube-system   etcd-k8s-master-0                      1/1     Running   <span class="m">6</span>          90s
kube-system   kube-apiserver-k8s-master-0            1/1     Running   <span class="m">4</span>          90s
kube-system   kube-controller-manager-k8s-master-0   1/1     Running   <span class="m">4</span>          90s
kube-system   kube-proxy-6w2bx                       1/1     Running   <span class="m">0</span>          83s
kube-system   kube-scheduler-k8s-master-0            1/1     Running   <span class="m">7</span>          97s
</code></pre></td></tr></table>
</div>
</div><p>现在在其他节点运行前面打印出的加入集群的命令，就可以搭建好一个高可用的集群了。</p>
<p>所有节点都加入集群后，通过 kubectl 查看，应该是三个控制面 master，两个 worker：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-shell" data-lang="shell">k8s-master-0:~/kubeadm <span class="c1"># kubectl get node</span>
NAME           STATUS     ROLES                  AGE     VERSION
k8s-master-0   NotReady   control-plane,master   26m     v1.22.1
k8s-master-1   NotReady   control-plane,master   7m2s    v1.22.1
k8s-master-2   NotReady   control-plane,master   2m10s   v1.22.1
k8s-worker-0   NotReady   &lt;none&gt;                 97s     v1.22.1
k8s-worker-1   NotReady   &lt;none&gt;                 86s     v1.22.1
</code></pre></td></tr></table>
</div>
</div><p>现在它们都还处于 NotReady 状态，需要等到我们把网络插件安装好，才会 Ready.</p>
<p>现在再看下集群的证书签发状态：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-shell" data-lang="shell">❯ kubectl get csr --sort-by<span class="o">=</span><span class="s1">&#39;{.spec.username}&#39;</span>
NAME        AGE     SIGNERNAME                                    REQUESTOR                  REQUESTEDDURATION   CONDITION
csr-95hll   6m58s   kubernetes.io/kube-apiserver-client-kubelet   system:bootstrap:q8ivnz    &lt;none&gt;              Approved,Issued
csr-tklnr   7m5s    kubernetes.io/kube-apiserver-client-kubelet   system:bootstrap:q8ivnz    &lt;none&gt;              Approved,Issued
csr-w92jv   9m15s   kubernetes.io/kube-apiserver-client-kubelet   system:bootstrap:q8ivnz    &lt;none&gt;              Approved,Issued
csr-rv7sj   8m11s   kubernetes.io/kube-apiserver-client-kubelet   system:bootstrap:q8ivnz    &lt;none&gt;              Approved,Issued
csr-nxkgx   10m     kubernetes.io/kube-apiserver-client-kubelet   system:node:k8s-master-0   &lt;none&gt;              Approved,Issued
csr-cd22c   10m     kubernetes.io/kubelet-serving                 system:node:k8s-master-0   &lt;none&gt;              Pending
csr-wjrnr   9m53s   kubernetes.io/kubelet-serving                 system:node:k8s-master-0   &lt;none&gt;              Pending
csr-sjq42   9m8s    kubernetes.io/kubelet-serving                 system:node:k8s-master-1   &lt;none&gt;              Pending
csr-xtv8f   8m56s   kubernetes.io/kubelet-serving                 system:node:k8s-master-1   &lt;none&gt;              Pending
csr-f2dsf   8m3s    kubernetes.io/kubelet-serving                 system:node:k8s-master-2   &lt;none&gt;              Pending
csr-xl8dg   6m58s   kubernetes.io/kubelet-serving                 system:node:k8s-worker-0   &lt;none&gt;              Pending
csr-p9g24   6m52s   kubernetes.io/kubelet-serving                 system:node:k8s-worker-1   &lt;none&gt;              Pending
</code></pre></td></tr></table>
</div>
</div><p>能看到有好几个 <code>kubernetes.io/kubelet-serving</code> 的证书还处于 pending 状态，
这是因为我们在 kubeadm 配置文件中，设置了 <code>serverTLSBootstrap: true</code>，让 Kubelet 从集群中申请 CA 签名证书，而不是自签名导致的。</p>
<p>设置这个参数的主要目的，是为了让 metrics-server 等组件能使用 https 协议与 kubelet 通信，避免为 metrics-server 添加参数 <code>--kubelet-insecure-tls</code>.</p>
<p>目前 kubeadm 不支持自动批准 kubelet 申请的证书，需要我们手动批准一下：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-shell" data-lang="shell"><span class="c1"># 批准 Kubelet 申请的所有证书</span>
kubectl certificate approve csr-cd22c csr-wjrnr csr-sjq42 csr-xtv8f csr-f2dsf csr-xl8dg csr-p9g24
</code></pre></td></tr></table>
</div>
</div><p>在未批准这些证书之前，所有需要调用 kubelet api 的功能都将无法使用，比如：</p>
<ul>
<li>查看 pod 日志</li>
<li>获取节点 metrics</li>
<li>等等</li>
</ul>
<h3 id="51-常见问题">5.1 常见问题</h3>
<h4 id="511-使用国内镜像源">5.1.1 使用国内镜像源</h4>
<p>如果你没有科学环境，kubeadm 默认的镜像仓库在国内是拉不了的。
如果对可靠性要求高，最好是自建私有镜像仓库，把镜像推送到私有仓库。</p>
<p>可以通过如下命令列出所有需要用到的镜像地址：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-shell" data-lang="shell">❯ kubeadm config images list --kubernetes-version v1.22.1
k8s.gcr.io/kube-apiserver:v1.22.1
k8s.gcr.io/kube-controller-manager:v1.22.1
k8s.gcr.io/kube-scheduler:v1.22.1
k8s.gcr.io/kube-proxy:v1.22.1
k8s.gcr.io/pause:3.5
k8s.gcr.io/etcd:3.5.0-0
k8s.gcr.io/coredns/coredns:v1.8.4
</code></pre></td></tr></table>
</div>
</div><p>使用 <code>skopeo</code> 等工具或脚本将上述镜像拷贝到你的私有仓库，或者图方便（测试环境）也可以考虑网上找找别人同步好的镜像地址。将镜像地址添加到 <code>kubeadm-config.yaml</code> 中再部署。</p>
<h4 id="512-重置集群配置">5.1.2 重置集群配置</h4>
<p>创建集群的过程中出现任何问题，都可以通过在所有节点上运行 <code>kubeadm reset</code> 来还原配置，然后重新走 kubeadm 的集群创建流程。</p>
<p>但是要注意几点：</p>
<ul>
<li><code>kubeadm reset</code> 会清除包含 kube-vip 配置在内的所有 static-pod 配置文件，所以 master 节点需要重新跑下前面给的 kube-vip 命令，生成下 kube-vip 配置。</li>
<li><code>kubeadm reset</code> 不会重置网络接口的配置，master 节点需要手动清理下 kube-vip 添加的 vip: <code>ip addr del 192.168.122.200/32 dev eth0</code>.</li>
<li>如果你在安装了网络插件之后希望重装集群，顺序如下：
<ul>
<li>通过 <code>kubectl delete -f xxx.yaml</code>/<code>helm uninstall</code> 删除所有除网络之外的其他应用配置</li>
<li>删除网络插件</li>
<li>先重启一遍所有节点，或者手动重置所有节点的网络配置
<ul>
<li>建议重启，因为我不知道该怎么手动重置&hellip; 试了 <code>systemctl restart network</code> 并不会清理所有虚拟网络接口。</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>如此操作后，再重新执行集群安装，应该就没啥毛病了。</p>
<h2 id="6-验证集群的高可用性">6. 验证集群的高可用性</h2>
<p>虽然网络插件还没装导致集群所有节点都还没 ready，但是我们已经可以通过 kubectl 命令来简单验证集群的高可用性了。</p>
<p>首先，我们将前面放置在 k8s-master-0 的认证文件 <code>$HOME/.kube/config</code> 以及 kunbectl 安装在另一台机器上，比如我直接放我的宿主机。</p>
<p>然后在宿主机上跑 <code>kubectl get node</code> 命令验证集群的高可用性：</p>
<ul>
<li>三个主节点都正常运行时，kubectl 命令也正常</li>
<li>pause 或者 stop 其中一个 master，kubectl 命令仍然能正常运行</li>
<li>再 pause 第二个 master，kubectl 命令应该就会卡住，并且超时，无法使用了</li>
<li>resume 恢复停掉的两个 master 之一，会发现 kubectl 命令又能正常运行了</li>
</ul>
<p>到这里 kubeadm 的工作就完成了，接下来再安装网络插件，集群就可用了。</p>
<h2 id="7-安装网络插件">7. 安装网络插件</h2>
<p>社区有很多种网络插件可选，比较知名且性能也不错的，应该是 Calico 和 Cilium，其中 Cilium 主打基于 eBPF 的高性能与高可观测性。</p>
<p>下面分别介绍这两个插件的安装方法。（注意只能安装其中一个网络插件，不能重复安装。）</p>
<p>需要提前在本机安装好 helm，我这里使用宿主机，因此只需要在宿主机安装:</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-shell" data-lang="shell"><span class="c1"># 一行命令安装，也可以自己手动下载安装包，都行</span>
curl https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3 <span class="p">|</span> bash

<span class="c1"># 或者 opensuse 直接用包管理器安装</span>
sudo zypper in helm
</code></pre></td></tr></table>
</div>
</div><h3 id="71-安装-cilium">7.1 安装 Cilium</h3>
<blockquote>
<p>官方文档：https://docs.cilium.io/en/v1.10/gettingstarted/k8s-install-kubeadm/</p>
</blockquote>
<p>cilium 通过 eBPF 提供了高性能与高可观测的 k8s 集群网络，
另外 cilium 还提供了比 kube-proxy 更高效的实现，可以完全替代 kube-proxy.</p>
<p>这里我们还是先使用 kube-proxy 模式，先熟悉下 cilium 的使用：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-shell" data-lang="shell">helm repo add cilium https://helm.cilium.io/
helm search repo cilium/cilium -l <span class="p">|</span> head

helm install cilium cilium/cilium --version 1.10.4 --namespace kube-system
</code></pre></td></tr></table>
</div>
</div><p>可以通过 <code>kubectl get pod -A</code> 查看 cilium 的安装进度，当所有 pod 都 ready 后，集群就 ready 了~</p>
<p>cilium 也提供了专用的客户端：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-shell" data-lang="shell">curl -L --remote-name-all https://github.com/cilium/cilium-cli/releases/latest/download/cilium-linux-amd64.tar.gz<span class="o">{</span>,.sha256sum<span class="o">}</span>
sha256sum --check cilium-linux-amd64.tar.gz.sha256sum
sudo tar xzvfC cilium-linux-amd64.tar.gz /usr/local/bin
rm cilium-linux-amd64.tar.gz<span class="o">{</span>,.sha256sum<span class="o">}</span>
</code></pre></td></tr></table>
</div>
</div><p>然后使用 cilium 客户端检查网络插件的状态：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-shell" data-lang="shell"> $ cilium status --wait
    /¯¯<span class="se">\
</span><span class="se"></span> /¯¯<span class="se">\_</span>_/¯¯<span class="se">\ </span>   Cilium:         OK
 <span class="se">\_</span>_/¯¯<span class="se">\_</span>_/    Operator:       OK
 /¯¯<span class="se">\_</span>_/¯¯<span class="se">\ </span>   Hubble:         disabled
 <span class="se">\_</span>_/¯¯<span class="se">\_</span>_/    ClusterMesh:    disabled
    <span class="se">\_</span>_/

DaemonSet         cilium             Desired: 5, Ready: 5/5, Available: 5/5
Deployment        cilium-operator    Desired: 2, Ready: 2/2, Available: 2/2
Containers:       cilium             Running: <span class="m">5</span>
                  cilium-operator    Running: <span class="m">2</span>
Cluster Pods:     2/2 managed by Cilium
Image versions    cilium             quay.io/cilium/cilium:v1.10.4@sha256:7d354052ccf2a7445101d78cebd14444c7c40129ce7889f2f04b89374dbf8a1d: <span class="m">5</span>
                  cilium-operator    quay.io/cilium/operator-generic:v1.10.4@sha256:c49a14e34634ff1a494c84b718641f27267fb3a0291ce3d74352b44f8a8d2f93: <span class="m">2</span>
</code></pre></td></tr></table>
</div>
</div><p>cilium 还提供了命令，自动创建 pod 进行集群网络的连接性测试：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-shell" data-lang="shell">❯ cilium connectivity <span class="nb">test</span>
ℹ️  Monitor aggregation detected, will skip some flow validation steps
✨ <span class="o">[</span>kubernetes<span class="o">]</span> Creating namespace <span class="k">for</span> connectivity check...
✨ <span class="o">[</span>kubernetes<span class="o">]</span> Deploying echo-same-node service...
✨ <span class="o">[</span>kubernetes<span class="o">]</span> Deploying same-node deployment...
✨ <span class="o">[</span>kubernetes<span class="o">]</span> Deploying client deployment...
✨ <span class="o">[</span>kubernetes<span class="o">]</span> Deploying client2 deployment...
✨ <span class="o">[</span>kubernetes<span class="o">]</span> Deploying echo-other-node service...
✨ <span class="o">[</span>kubernetes<span class="o">]</span> Deploying other-node deployment...
...
ℹ️  Expose Relay locally with:
   cilium hubble <span class="nb">enable</span>
   cilium status --wait
   cilium hubble port-forward<span class="p">&amp;</span>
🏃 Running tests...
...
---------------------------------------------------------------------------------------------------------------------
✅ All <span class="m">11</span> tests <span class="o">(</span><span class="m">134</span> actions<span class="o">)</span> successful, <span class="m">0</span> tests skipped, <span class="m">0</span> scenarios skipped.
</code></pre></td></tr></table>
</div>
</div><p>通过 <code>kubectl get po -A</code> 能观察到，这个测试命令会自动创建一个 <code>cilium-test</code> 名字空间，并在启动创建若干 pod 进行详细的测试。</p>
<p>整个测试流程大概会持续 5 分多钟，测试完成后，相关 Pod 不会自动删除，使用如下命令手动删除：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-shell" data-lang="shell">kubectl delete namespace cilium-test
</code></pre></td></tr></table>
</div>
</div><h3 id="72-安装-calico">7.2 安装 Calico</h3>
<blockquote>
<p>官方文档：https://docs.projectcalico.org/getting-started/kubernetes/self-managed-onprem/onpremises</p>
</blockquote>
<p>也就两三行命令。安装确实特别简单，懒得介绍了，看官方文档吧。</p>
<p>但是实际上 calico 的细节还蛮多的，建议通读下它的官方文档，了解下 calico 的架构。</p>
<h2 id="8-查看集群状态">8. 查看集群状态</h2>
<p>官方的 dashboard 个人感觉不太好用，建议直接在本地装个 k9s 用，特别爽。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-shell" data-lang="shell">sudo zypper in k9s
</code></pre></td></tr></table>
</div>
</div><p>然后就可以愉快地玩耍了。</p>
<h2 id="9-安装-metrics-server">9. 安装 metrics-server</h2>
<blockquote>
<p>这一步可能遇到的问题：<a href="https://kubernetes.io/docs/tasks/administer-cluster/kubeadm/kubeadm-certs/#kubelet-serving-certs" target="_blank" rel="noopener noreferrer">Enabling signed kubelet serving certificates</a></p>
</blockquote>
<p>如果需要使用 HPA 以及简单的集群监控，那么 metrics-server 是必须安装的，现在我们安装一下它。</p>
<p>首先，跑 kubectl 的监控命令应该会报错：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-shell" data-lang="shell">❯ kubectl top node
error: Metrics API not available
</code></pre></td></tr></table>
</div>
</div><p>k9s 里面应该也看不到任何监控指标。</p>
<p>现在通过 helm 安装它：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-shell" data-lang="shell">helm repo add metrics-server https://kubernetes-sigs.github.io/metrics-server/
helm search repo metrics-server/metrics-server -l <span class="p">|</span> head

helm upgrade --install metrics-server metrics-server/metrics-server --version 3.5.0 --namespace kube-system
</code></pre></td></tr></table>
</div>
</div><blockquote>
<p>metrics-server 默认只会部署一个实例，如果希望高可用，请参考官方配置：<a href="https://github.com/kubernetes-sigs/metrics-server/tree/master/manifests/high-availability" target="_blank" rel="noopener noreferrer">metrics-server - high-availability manifests</a></p>
</blockquote>
<p>等 metrics-server 启动好后，就可以使用 <code>kubectl top</code> 命令啦：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span><span class="lnt">43
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-shell" data-lang="shell">❯ kubectl top node
NAME           CPU<span class="o">(</span>cores<span class="o">)</span>   CPU%   MEMORY<span class="o">(</span>bytes<span class="o">)</span>   MEMORY%   
k8s-master-0   327m         16%    1465Mi          50%       
k8s-master-1   263m         13%    1279Mi          44%       
k8s-master-2   289m         14%    1282Mi          44%       
k8s-worker-0   62m          3%     518Mi           13%       
k8s-worker-1   115m         2%     659Mi           8%        

❯ kubectl top pod
No resources found in default namespace.

❯ kubectl top pod -A
NAMESPACE     NAME                                   CPU<span class="o">(</span>cores<span class="o">)</span>   MEMORY<span class="o">(</span>bytes<span class="o">)</span>   
kube-system   cilium-45nw4                           9m           135Mi           
kube-system   cilium-5x7jf                           6m           154Mi           
kube-system   cilium-84sr2                           7m           160Mi           
kube-system   cilium-operator-78f45675-dp4b6         2m           30Mi            
kube-system   cilium-operator-78f45675-fpm5g         1m           30Mi            
kube-system   cilium-tkhl4                           6m           141Mi           
kube-system   cilium-zxbvm                           5m           138Mi           
kube-system   coredns-78fcd69978-dpxxk               3m           16Mi            
kube-system   coredns-78fcd69978-ptd9p               1m           18Mi            
kube-system   etcd-k8s-master-0                      61m          88Mi            
kube-system   etcd-k8s-master-1                      50m          85Mi            
kube-system   etcd-k8s-master-2                      55m          83Mi            
kube-system   kube-apiserver-k8s-master-0            98m          462Mi           
kube-system   kube-apiserver-k8s-master-1            85m          468Mi           
kube-system   kube-apiserver-k8s-master-2            85m          423Mi           
kube-system   kube-controller-manager-k8s-master-0   22m          57Mi            
kube-system   kube-controller-manager-k8s-master-1   2m           23Mi            
kube-system   kube-controller-manager-k8s-master-2   2m           23Mi            
kube-system   kube-proxy-j2s76                       1m           24Mi            
kube-system   kube-proxy-k6d6z                       1m           18Mi            
kube-system   kube-proxy-k85rx                       1m           23Mi            
kube-system   kube-proxy-pknsc                       1m           20Mi            
kube-system   kube-proxy-xsq4m                       1m           15Mi            
kube-system   kube-scheduler-k8s-master-0            3m           25Mi            
kube-system   kube-scheduler-k8s-master-1            4m           21Mi            
kube-system   kube-scheduler-k8s-master-2            5m           21Mi            
kube-system   kube-vip-k8s-master-0                  4m           17Mi            
kube-system   kube-vip-k8s-master-1                  2m           16Mi            
kube-system   kube-vip-k8s-master-2                  2m           17Mi            
kube-system   metrics-server-559f85484-5b6xf         7m           27Mi    
</code></pre></td></tr></table>
</div>
</div><h2 id="10-为-etcd-添加定期备份能力">10. 为 etcd 添加定期备份能力</h2>
<p>请移步 <a href="https://github.com/ryan4yin/knowledge/blob/master/datastore/etcd/etcd%20%E7%9A%84%E5%A4%87%E4%BB%BD%E4%B8%8E%E6%81%A2%E5%A4%8D.md" target="_blank" rel="noopener noreferrer">etcd 的备份与恢复</a></p>
<h2 id="11-安装-volume-provisioner">11. 安装 Volume Provisioner</h2>
<p>在我们学习使用 Prometheus/MinIO/Tekton 等有状态应用时，它们默认情况下会通过 PVC 声明需要的数据卷。</p>
<p>为了支持这个能力，我们需要在集群中部署一个 Volume Provisioner.</p>
<p>对于云上环境，直接接入云服务商提供的 Volume Provisioner 就 OK 了，方便省事而且足够可靠。</p>
<p>而对于 bare-metal 环境，比较有名的应该是 rook-ceph，但是这个玩意部署复杂，维护难度又高，不适合用来测试学习，也不适合生产环境。</p>
<p>对于开发、测试环境，或者个人集群，建议使用：</p>
<ul>
<li>local 数据卷，适合数据可丢失，且不要求分布式的场景，如开发测试环境
<ul>
<li><a href="https://github.com/kubernetes-sigs/sig-storage-local-static-provisioner">https://github.com/kubernetes-sigs/sig-storage-local-static-provisioner</a></li>
<li><a href="https://github.com/rancher/local-path-provisioner">https://github.com/rancher/local-path-provisioner</a></li>
</ul>
</li>
<li>NFS 数据卷，适合数据可丢失，对性能要求不高，并且要求分布式的场景。比如开发测试环境、或者线上没啥压力的应用
<ul>
<li><a href="https://github.com/kubernetes-sigs/nfs-subdir-external-provisioner">https://github.com/kubernetes-sigs/nfs-subdir-external-provisioner</a></li>
<li><a href="https://github.com/kubernetes-csi/csi-driver-nfs">https://github.com/kubernetes-csi/csi-driver-nfs</a></li>
<li>NFS 数据的可靠性依赖于外部 NFS 服务器，企业通常使用群晖等 NAS 来做 NFS 服务器</li>
<li>如果外部 NFS 服务器出问题，应用就会崩。</li>
</ul>
</li>
<li>直接使用云上的对象存储，适合希望数据不丢失、对性能要求不高的场景。
<ul>
<li>直接使用 <a href="https://github.com/rclone/rclone">https://github.com/rclone/rclone</a> mount 模式来保存数据，或者直接同步文件夹数据到云端（可能会有一定数据丢失）。</li>
</ul>
</li>
</ul>
]]></description></item><item><title>Kubernetes 微服务最佳实践</title><link>https://ryan4yin.space/posts/kubernetes-best-practices/</link><pubDate>Tue, 25 Jan 2022 00:13:00 +0800</pubDate><author>xiaoyin_c@qq.com</author><dc:creator>ryan4yin</dc:creator><guid>https://ryan4yin.space/posts/kubernetes-best-practices/</guid><description><![CDATA[<blockquote>
<p>本文由个人笔记 <a href="https://github.com/ryan4yin/knowledge/tree/master/kubernetes" target="_blank" rel="noopener noreferrer">ryan4yin/knowledge</a> 整理而来</p>
</blockquote>
<p>本文主要介绍下我个人在使用 Kubernetes 的过程中，总结出的一套「Kubernetes 配置」，是我个人的「最佳实践」。
其中大部分内容都经历过线上环境的考验，但是也有少部分还只在我脑子里模拟过，请谨慎参考。</p>
<p>阅读前的几个注意事项：</p>
<ul>
<li>这份文档比较长，囊括了很多内容，建议当成参考手册使用，先参照目录简单读一读，有需要再细读相关内容。</li>
<li>这份文档需要一定的 Kubernetes 基础才能理解，而且如果没有过实践经验的话，看上去可能会比较枯燥。
<ul>
<li>而有过实践经验的大佬，可能会跟我有不同的见解，欢迎各路大佬评论~</li>
</ul>
</li>
</ul>
<p>我会视情况不定期更新这份文档。</p>
<h2 id="零示例">零、示例</h2>
<p>首先，这里给出一些本文遵守的前提，这些前提只是契合我遇到的场景，可灵活变通：</p>
<ul>
<li>这里只讨论无状态服务，有状态服务不在讨论范围内</li>
<li>我们不使用 Deployment 的滚动更新能力，而是为每个服务的每个版本，都创建不同的 Deployment + HPA + PodDisruptionBudget，这是为了方便做金丝雀/灰度发布</li>
<li>我们的服务可能会使用 IngressController / Service Mesh 来进行服务的负载均衡、流量切分</li>
</ul>
<p>下面先给出一个 Deployment + HPA + PodDisruptionBudget 的 demo，后面再拆开详细说下：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">  1
</span><span class="lnt">  2
</span><span class="lnt">  3
</span><span class="lnt">  4
</span><span class="lnt">  5
</span><span class="lnt">  6
</span><span class="lnt">  7
</span><span class="lnt">  8
</span><span class="lnt">  9
</span><span class="lnt"> 10
</span><span class="lnt"> 11
</span><span class="lnt"> 12
</span><span class="lnt"> 13
</span><span class="lnt"> 14
</span><span class="lnt"> 15
</span><span class="lnt"> 16
</span><span class="lnt"> 17
</span><span class="lnt"> 18
</span><span class="lnt"> 19
</span><span class="lnt"> 20
</span><span class="lnt"> 21
</span><span class="lnt"> 22
</span><span class="lnt"> 23
</span><span class="lnt"> 24
</span><span class="lnt"> 25
</span><span class="lnt"> 26
</span><span class="lnt"> 27
</span><span class="lnt"> 28
</span><span class="lnt"> 29
</span><span class="lnt"> 30
</span><span class="lnt"> 31
</span><span class="lnt"> 32
</span><span class="lnt"> 33
</span><span class="lnt"> 34
</span><span class="lnt"> 35
</span><span class="lnt"> 36
</span><span class="lnt"> 37
</span><span class="lnt"> 38
</span><span class="lnt"> 39
</span><span class="lnt"> 40
</span><span class="lnt"> 41
</span><span class="lnt"> 42
</span><span class="lnt"> 43
</span><span class="lnt"> 44
</span><span class="lnt"> 45
</span><span class="lnt"> 46
</span><span class="lnt"> 47
</span><span class="lnt"> 48
</span><span class="lnt"> 49
</span><span class="lnt"> 50
</span><span class="lnt"> 51
</span><span class="lnt"> 52
</span><span class="lnt"> 53
</span><span class="lnt"> 54
</span><span class="lnt"> 55
</span><span class="lnt"> 56
</span><span class="lnt"> 57
</span><span class="lnt"> 58
</span><span class="lnt"> 59
</span><span class="lnt"> 60
</span><span class="lnt"> 61
</span><span class="lnt"> 62
</span><span class="lnt"> 63
</span><span class="lnt"> 64
</span><span class="lnt"> 65
</span><span class="lnt"> 66
</span><span class="lnt"> 67
</span><span class="lnt"> 68
</span><span class="lnt"> 69
</span><span class="lnt"> 70
</span><span class="lnt"> 71
</span><span class="lnt"> 72
</span><span class="lnt"> 73
</span><span class="lnt"> 74
</span><span class="lnt"> 75
</span><span class="lnt"> 76
</span><span class="lnt"> 77
</span><span class="lnt"> 78
</span><span class="lnt"> 79
</span><span class="lnt"> 80
</span><span class="lnt"> 81
</span><span class="lnt"> 82
</span><span class="lnt"> 83
</span><span class="lnt"> 84
</span><span class="lnt"> 85
</span><span class="lnt"> 86
</span><span class="lnt"> 87
</span><span class="lnt"> 88
</span><span class="lnt"> 89
</span><span class="lnt"> 90
</span><span class="lnt"> 91
</span><span class="lnt"> 92
</span><span class="lnt"> 93
</span><span class="lnt"> 94
</span><span class="lnt"> 95
</span><span class="lnt"> 96
</span><span class="lnt"> 97
</span><span class="lnt"> 98
</span><span class="lnt"> 99
</span><span class="lnt">100
</span><span class="lnt">101
</span><span class="lnt">102
</span><span class="lnt">103
</span><span class="lnt">104
</span><span class="lnt">105
</span><span class="lnt">106
</span><span class="lnt">107
</span><span class="lnt">108
</span><span class="lnt">109
</span><span class="lnt">110
</span><span class="lnt">111
</span><span class="lnt">112
</span><span class="lnt">113
</span><span class="lnt">114
</span><span class="lnt">115
</span><span class="lnt">116
</span><span class="lnt">117
</span><span class="lnt">118
</span><span class="lnt">119
</span><span class="lnt">120
</span><span class="lnt">121
</span><span class="lnt">122
</span><span class="lnt">123
</span><span class="lnt">124
</span><span class="lnt">125
</span><span class="lnt">126
</span><span class="lnt">127
</span><span class="lnt">128
</span><span class="lnt">129
</span><span class="lnt">130
</span><span class="lnt">131
</span><span class="lnt">132
</span><span class="lnt">133
</span><span class="lnt">134
</span><span class="lnt">135
</span><span class="lnt">136
</span><span class="lnt">137
</span><span class="lnt">138
</span><span class="lnt">139
</span><span class="lnt">140
</span><span class="lnt">141
</span><span class="lnt">142
</span><span class="lnt">143
</span><span class="lnt">144
</span><span class="lnt">145
</span><span class="lnt">146
</span><span class="lnt">147
</span><span class="lnt">148
</span><span class="lnt">149
</span><span class="lnt">150
</span><span class="lnt">151
</span><span class="lnt">152
</span><span class="lnt">153
</span><span class="lnt">154
</span><span class="lnt">155
</span><span class="lnt">156
</span><span class="lnt">157
</span><span class="lnt">158
</span><span class="lnt">159
</span><span class="lnt">160
</span><span class="lnt">161
</span><span class="lnt">162
</span><span class="lnt">163
</span><span class="lnt">164
</span><span class="lnt">165
</span><span class="lnt">166
</span><span class="lnt">167
</span><span class="lnt">168
</span><span class="lnt">169
</span><span class="lnt">170
</span><span class="lnt">171
</span><span class="lnt">172
</span><span class="lnt">173
</span><span class="lnt">174
</span><span class="lnt">175
</span><span class="lnt">176
</span><span class="lnt">177
</span><span class="lnt">178
</span><span class="lnt">179
</span><span class="lnt">180
</span><span class="lnt">181
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-yaml" data-lang="yaml"><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l">apps/v1</span><span class="w">
</span><span class="w"></span><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l">Deployment</span><span class="w">
</span><span class="w"></span><span class="nt">metadata</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">my-app-v3</span><span class="w">
</span><span class="w">  </span><span class="nt">namespace</span><span class="p">:</span><span class="w"> </span><span class="l">prod</span><span class="w">
</span><span class="w">  </span><span class="nt">labels</span><span class="p">:</span><span class="w">
</span><span class="w">    </span><span class="nt">app</span><span class="p">:</span><span class="w"> </span><span class="l">my-app</span><span class="w">
</span><span class="w"></span><span class="nt">spec</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">replicas</span><span class="p">:</span><span class="w"> </span><span class="m">3</span><span class="w">
</span><span class="w">  </span><span class="nt">strategy</span><span class="p">:</span><span class="w">
</span><span class="w">    </span><span class="nt">type</span><span class="p">:</span><span class="w"> </span><span class="l">RollingUpdate</span><span class="w">
</span><span class="w">    </span><span class="c"># 因为服务的每个版本都使用各自的 Deployment，服务更新时其实是用不上这里的滚动更新策略的</span><span class="w">
</span><span class="w">    </span><span class="c"># 这个配置应该只在 SRE 手动修改 Deployment 配置时才会生效（通常不应该发生这种事）</span><span class="w">
</span><span class="w">    </span><span class="nt">rollingUpdate</span><span class="p">:</span><span class="w">
</span><span class="w">      </span><span class="nt">maxSurge</span><span class="p">:</span><span class="w"> </span><span class="m">10</span><span class="l">% </span><span class="w"> </span><span class="c"># 滚动更新时，每次最多更新 10% 的 Pods</span><span class="w">
</span><span class="w">      </span><span class="nt">maxUnavailable</span><span class="p">:</span><span class="w"> </span><span class="m">0</span><span class="w">  </span><span class="c"># 滚动更新时，不允许出现不可用的 Pods，也就是说始终要维持 3 个可用副本</span><span class="w">
</span><span class="w">  </span><span class="nt">selector</span><span class="p">:</span><span class="w">
</span><span class="w">    </span><span class="nt">matchLabels</span><span class="p">:</span><span class="w">
</span><span class="w">      </span><span class="nt">app</span><span class="p">:</span><span class="w"> </span><span class="l">my-app</span><span class="w">
</span><span class="w">      </span><span class="nt">version</span><span class="p">:</span><span class="w"> </span><span class="l">v3</span><span class="w">
</span><span class="w">  </span><span class="nt">template</span><span class="p">:</span><span class="w">
</span><span class="w">    </span><span class="nt">metadata</span><span class="p">:</span><span class="w">
</span><span class="w">      </span><span class="nt">labels</span><span class="p">:</span><span class="w">
</span><span class="w">        </span><span class="nt">app</span><span class="p">:</span><span class="w"> </span><span class="l">my-app</span><span class="w">
</span><span class="w">        </span><span class="nt">version</span><span class="p">:</span><span class="w"> </span><span class="l">v3</span><span class="w">
</span><span class="w">    </span><span class="nt">spec</span><span class="p">:</span><span class="w">
</span><span class="w">      </span><span class="nt">affinity</span><span class="p">:</span><span class="w">
</span><span class="w">        </span><span class="nt">podAffinity</span><span class="p">:</span><span class="w">
</span><span class="w">          </span><span class="nt">preferredDuringSchedulingIgnoredDuringExecution</span><span class="p">:</span><span class="w"> </span><span class="c"># 非强制性条件</span><span class="w">
</span><span class="w">          </span>- <span class="nt">weight</span><span class="p">:</span><span class="w"> </span><span class="m">100</span><span class="w">  </span><span class="c"># weight 用于为节点评分，会优先选择评分最高的节点（只有一条规则的情况下，这个值没啥意义）</span><span class="w">
</span><span class="w">            </span><span class="nt">podAffinityTerm</span><span class="p">:</span><span class="w">
</span><span class="w">              </span><span class="nt">labelSelector</span><span class="p">:</span><span class="w">
</span><span class="w">                </span><span class="nt">matchExpressions</span><span class="p">:</span><span class="w">
</span><span class="w">                </span>- <span class="nt">key</span><span class="p">:</span><span class="w"> </span><span class="l">app</span><span class="w">
</span><span class="w">                  </span><span class="nt">operator</span><span class="p">:</span><span class="w"> </span><span class="l">In</span><span class="w">
</span><span class="w">                  </span><span class="nt">values</span><span class="p">:</span><span class="w">
</span><span class="w">                  </span>- <span class="l">my-app</span><span class="w">
</span><span class="w">                </span>- <span class="nt">key</span><span class="p">:</span><span class="w"> </span><span class="l">version</span><span class="w">
</span><span class="w">                  </span><span class="nt">operator</span><span class="p">:</span><span class="w"> </span><span class="l">In</span><span class="w">
</span><span class="w">                  </span><span class="nt">values</span><span class="p">:</span><span class="w">
</span><span class="w">                  </span>- <span class="l">v3</span><span class="w">
</span><span class="w">              </span><span class="c"># pod 尽量使用同一种节点类型，也就是尽量保证节点的性能一致</span><span class="w">
</span><span class="w">              </span><span class="nt">topologyKey</span><span class="p">:</span><span class="w"> </span><span class="l">node.kubernetes.io/instance-type</span><span class="w">
</span><span class="w">        </span><span class="nt">podAntiAffinity</span><span class="p">:</span><span class="w">
</span><span class="w">          </span><span class="nt">preferredDuringSchedulingIgnoredDuringExecution</span><span class="p">:</span><span class="w"> </span><span class="c"># 非强制性条件</span><span class="w">
</span><span class="w">          </span>- <span class="nt">weight</span><span class="p">:</span><span class="w"> </span><span class="m">100</span><span class="w">  </span><span class="c"># weight 用于为节点评分，会优先选择评分最高的节点（只有一条规则的情况下，这个值没啥意义）</span><span class="w">
</span><span class="w">            </span><span class="nt">podAffinityTerm</span><span class="p">:</span><span class="w">
</span><span class="w">              </span><span class="nt">labelSelector</span><span class="p">:</span><span class="w">
</span><span class="w">                </span><span class="nt">matchExpressions</span><span class="p">:</span><span class="w">
</span><span class="w">                </span>- <span class="nt">key</span><span class="p">:</span><span class="w"> </span><span class="l">app</span><span class="w">
</span><span class="w">                  </span><span class="nt">operator</span><span class="p">:</span><span class="w"> </span><span class="l">In</span><span class="w">
</span><span class="w">                  </span><span class="nt">values</span><span class="p">:</span><span class="w">
</span><span class="w">                  </span>- <span class="l">my-app</span><span class="w">
</span><span class="w">                </span>- <span class="nt">key</span><span class="p">:</span><span class="w"> </span><span class="l">version</span><span class="w">
</span><span class="w">                  </span><span class="nt">operator</span><span class="p">:</span><span class="w"> </span><span class="l">In</span><span class="w">
</span><span class="w">                  </span><span class="nt">values</span><span class="p">:</span><span class="w">
</span><span class="w">                  </span>- <span class="l">v3</span><span class="w">
</span><span class="w">              </span><span class="c"># 将 pod 尽量打散在多个可用区</span><span class="w">
</span><span class="w">              </span><span class="nt">topologyKey</span><span class="p">:</span><span class="w"> </span><span class="l">topology.kubernetes.io/zone</span><span class="w">
</span><span class="w">          </span><span class="nt">requiredDuringSchedulingIgnoredDuringExecution</span><span class="p">:</span><span class="w">  </span><span class="c"># 强制性要求（这个建议按需添加）</span><span class="w">
</span><span class="w">          </span><span class="c"># 注意这个没有 weights，必须满足列表中的所有条件</span><span class="w">
</span><span class="w">          </span>- <span class="nt">labelSelector</span><span class="p">:</span><span class="w">
</span><span class="w">              </span><span class="nt">matchExpressions</span><span class="p">:</span><span class="w">
</span><span class="w">              </span>- <span class="nt">key</span><span class="p">:</span><span class="w"> </span><span class="l">app</span><span class="w">
</span><span class="w">                </span><span class="nt">operator</span><span class="p">:</span><span class="w"> </span><span class="l">In</span><span class="w">
</span><span class="w">                </span><span class="nt">values</span><span class="p">:</span><span class="w">
</span><span class="w">                </span>- <span class="l">my-app</span><span class="w">
</span><span class="w">              </span>- <span class="nt">key</span><span class="p">:</span><span class="w"> </span><span class="l">version</span><span class="w">
</span><span class="w">                </span><span class="nt">operator</span><span class="p">:</span><span class="w"> </span><span class="l">In</span><span class="w">
</span><span class="w">                </span><span class="nt">values</span><span class="p">:</span><span class="w">
</span><span class="w">                </span>- <span class="l">v3</span><span class="w">
</span><span class="w">            </span><span class="c"># Pod 必须运行在不同的节点上</span><span class="w">
</span><span class="w">            </span><span class="nt">topologyKey</span><span class="p">:</span><span class="w"> </span><span class="l">kubernetes.io/hostname</span><span class="w">
</span><span class="w">      </span><span class="nt">securityContext</span><span class="p">:</span><span class="w">
</span><span class="w">        </span><span class="c"># runAsUser: 1000  # 设定用户</span><span class="w">
</span><span class="w">        </span><span class="c"># runAsGroup: 1000  # 设定用户组</span><span class="w">
</span><span class="w">        </span><span class="nt">runAsNonRoot</span><span class="p">:</span><span class="w"> </span><span class="kc">true</span><span class="w">  </span><span class="c"># Pod 必须以非 root 用户运行</span><span class="w">
</span><span class="w">        </span><span class="nt">seccompProfile</span><span class="p">:</span><span class="w">  </span><span class="c"># security compute mode</span><span class="w">
</span><span class="w">          </span><span class="nt">type</span><span class="p">:</span><span class="w"> </span><span class="l">RuntimeDefault</span><span class="w">
</span><span class="w">      </span><span class="nt">nodeSelector</span><span class="p">:</span><span class="w">
</span><span class="w">        </span><span class="nt">eks.amazonaws.com/nodegroup</span><span class="p">:</span><span class="w"> </span><span class="l">common </span><span class="w"> </span><span class="c"># 使用专用节点组，如果希望使用多个节点组，可改用节点亲和性</span><span class="w">
</span><span class="w">      </span><span class="nt">volumes</span><span class="p">:</span><span class="w">
</span><span class="w">      </span>- <span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">tmp-dir</span><span class="w">
</span><span class="w">        </span><span class="nt">emptyDir</span><span class="p">:</span><span class="w"> </span>{}<span class="w">
</span><span class="w">      </span><span class="nt">containers</span><span class="p">:</span><span class="w">
</span><span class="w">      </span>- <span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">my-app-v3</span><span class="w">
</span><span class="w">        </span><span class="nt">image</span><span class="p">:</span><span class="w"> </span><span class="l">my-app:v3 </span><span class="w"> </span><span class="c"># 建议使用私有镜像仓库，规避 docker.io 的镜像拉取限制</span><span class="w">
</span><span class="w">        </span><span class="nt">imagePullPolicy</span><span class="p">:</span><span class="w"> </span><span class="l">IfNotPresent</span><span class="w">
</span><span class="w">        </span><span class="nt">volumeMounts</span><span class="p">:</span><span class="w">
</span><span class="w">        </span>- <span class="nt">mountPath</span><span class="p">:</span><span class="w"> </span><span class="l">/tmp</span><span class="w">
</span><span class="w">          </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">tmp-dir</span><span class="w">
</span><span class="w">        </span><span class="nt">lifecycle</span><span class="p">:</span><span class="w">
</span><span class="w">          </span><span class="nt">preStop</span><span class="p">:</span><span class="w">
</span><span class="w">            </span><span class="nt">exec</span><span class="p">:</span><span class="w">
</span><span class="w">              </span><span class="nt">command</span><span class="p">:</span><span class="w">
</span><span class="w">              </span>- <span class="l">/bin/sh</span><span class="w">
</span><span class="w">              </span>- -<span class="l">c</span><span class="w">
</span><span class="w">              </span>- <span class="s2">&#34;while [ $(netstat -plunt | grep tcp | wc -l | xargs) -ne 0 ]; do sleep 1; done&#34;</span><span class="w">
</span><span class="w">        </span><span class="nt">resources</span><span class="p">:</span><span class="w">  </span><span class="c"># 资源请求与限制</span><span class="w">
</span><span class="w">          </span><span class="c"># 对于核心服务，建议设置 requests = limits，避免资源竞争</span><span class="w">
</span><span class="w">          </span><span class="nt">requests</span><span class="p">:</span><span class="w">
</span><span class="w">            </span><span class="c"># HPA 会使用 requests 计算资源利用率</span><span class="w">
</span><span class="w">            </span><span class="c"># 建议将 requests 设为服务正常状态下的 CPU 使用率，HPA 的目前指标设为 80%</span><span class="w">
</span><span class="w">            </span><span class="c"># 所有容器的 requests 总量不建议为 2c/4G 4c/8G 等常见值，因为节点通常也是这个配置，这会导致 Pod 只能调度到更大的节点上，适当调小 requests 等扩充可用的节点类型，从而扩充节点池。 </span><span class="w">
</span><span class="w">            </span><span class="nt">cpu</span><span class="p">:</span><span class="w"> </span><span class="l">1000m</span><span class="w">
</span><span class="w">            </span><span class="nt">memory</span><span class="p">:</span><span class="w"> </span><span class="l">1Gi</span><span class="w">
</span><span class="w">          </span><span class="nt">limits</span><span class="p">:</span><span class="w">
</span><span class="w">            </span><span class="c"># limits - requests 为允许超卖的资源量，建议为 requests 的 1 到 2 倍，酌情配置。</span><span class="w">
</span><span class="w">            </span><span class="nt">cpu</span><span class="p">:</span><span class="w"> </span><span class="l">1000m</span><span class="w">
</span><span class="w">            </span><span class="nt">memory</span><span class="p">:</span><span class="w"> </span><span class="l">1Gi</span><span class="w">
</span><span class="w">        </span><span class="nt">securityContext</span><span class="p">:</span><span class="w">
</span><span class="w">          </span><span class="c"># 将容器层设为只读，防止容器文件被篡改</span><span class="w">
</span><span class="w">          </span><span class="c">## 如果需要写入临时文件，建议额外挂载 emptyDir 来提供可读写的数据卷</span><span class="w">
</span><span class="w">          </span><span class="nt">readOnlyRootFilesystem</span><span class="p">:</span><span class="w"> </span><span class="kc">true</span><span class="w">
</span><span class="w">          </span><span class="c"># 禁止 Pod 做任何权限提升</span><span class="w">
</span><span class="w">          </span><span class="nt">allowPrivilegeEscalation</span><span class="p">:</span><span class="w"> </span><span class="kc">false</span><span class="w">
</span><span class="w">          </span><span class="nt">capabilities</span><span class="p">:</span><span class="w">
</span><span class="w">            </span><span class="c"># drop ALL 的权限比较严格，可按需修改</span><span class="w">
</span><span class="w">            </span><span class="nt">drop</span><span class="p">:</span><span class="w">
</span><span class="w">            </span>- <span class="l">ALL</span><span class="w">
</span><span class="w">        </span><span class="nt">startupProbe</span><span class="p">:</span><span class="w">  </span><span class="c"># 要求 kubernetes 1.18+</span><span class="w">
</span><span class="w">          </span><span class="nt">httpGet</span><span class="p">:</span><span class="w">
</span><span class="w">            </span><span class="nt">path</span><span class="p">:</span><span class="w"> </span><span class="l">/actuator/health </span><span class="w"> </span><span class="c"># 直接使用健康检查接口即可</span><span class="w">
</span><span class="w">            </span><span class="nt">port</span><span class="p">:</span><span class="w"> </span><span class="m">8080</span><span class="w">
</span><span class="w">          </span><span class="nt">periodSeconds</span><span class="p">:</span><span class="w"> </span><span class="m">5</span><span class="w">
</span><span class="w">          </span><span class="nt">timeoutSeconds</span><span class="p">:</span><span class="w"> </span><span class="m">1</span><span class="w">
</span><span class="w">          </span><span class="nt">failureThreshold</span><span class="p">:</span><span class="w"> </span><span class="m">20</span><span class="w">  </span><span class="c"># 最多提供给服务 5s * 20 的启动时间</span><span class="w">
</span><span class="w">          </span><span class="nt">successThreshold</span><span class="p">:</span><span class="w"> </span><span class="m">1</span><span class="w">
</span><span class="w">        </span><span class="nt">livenessProbe</span><span class="p">:</span><span class="w">
</span><span class="w">          </span><span class="nt">httpGet</span><span class="p">:</span><span class="w">
</span><span class="w">            </span><span class="nt">path</span><span class="p">:</span><span class="w"> </span><span class="l">/actuator/health </span><span class="w"> </span><span class="c"># spring 的通用健康检查路径</span><span class="w">
</span><span class="w">            </span><span class="nt">port</span><span class="p">:</span><span class="w"> </span><span class="m">8080</span><span class="w">
</span><span class="w">          </span><span class="nt">periodSeconds</span><span class="p">:</span><span class="w"> </span><span class="m">5</span><span class="w">
</span><span class="w">          </span><span class="nt">timeoutSeconds</span><span class="p">:</span><span class="w"> </span><span class="m">1</span><span class="w">
</span><span class="w">          </span><span class="nt">failureThreshold</span><span class="p">:</span><span class="w"> </span><span class="m">5</span><span class="w">
</span><span class="w">          </span><span class="nt">successThreshold</span><span class="p">:</span><span class="w"> </span><span class="m">1</span><span class="w">
</span><span class="w">        </span><span class="c"># Readiness probes are very important for a RollingUpdate to work properly,</span><span class="w">
</span><span class="w">        </span><span class="nt">readinessProbe</span><span class="p">:</span><span class="w">
</span><span class="w">          </span><span class="nt">httpGet</span><span class="p">:</span><span class="w">
</span><span class="w">            </span><span class="nt">path</span><span class="p">:</span><span class="w"> </span><span class="l">/actuator/health </span><span class="w"> </span><span class="c"># 简单起见可直接使用 livenessProbe 相同的接口，当然也可额外定义</span><span class="w">
</span><span class="w">            </span><span class="nt">port</span><span class="p">:</span><span class="w"> </span><span class="m">8080</span><span class="w">
</span><span class="w">          </span><span class="nt">periodSeconds</span><span class="p">:</span><span class="w"> </span><span class="m">5</span><span class="w">
</span><span class="w">          </span><span class="nt">timeoutSeconds</span><span class="p">:</span><span class="w"> </span><span class="m">1</span><span class="w">
</span><span class="w">          </span><span class="nt">failureThreshold</span><span class="p">:</span><span class="w"> </span><span class="m">5</span><span class="w">
</span><span class="w">          </span><span class="nt">successThreshold</span><span class="p">:</span><span class="w"> </span><span class="m">1</span><span class="w">
</span><span class="w"></span><span class="nn">---</span><span class="w">
</span><span class="w"></span><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l">autoscaling/v2beta2</span><span class="w">
</span><span class="w"></span><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l">HorizontalPodAutoscaler</span><span class="w">
</span><span class="w"></span><span class="nt">metadata</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">labels</span><span class="p">:</span><span class="w">
</span><span class="w">    </span><span class="nt">app</span><span class="p">:</span><span class="w"> </span><span class="l">my-app</span><span class="w">
</span><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">my-app-v3</span><span class="w">
</span><span class="w">  </span><span class="nt">namespace</span><span class="p">:</span><span class="w"> </span><span class="l">prod</span><span class="w">
</span><span class="w"></span><span class="nt">spec</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">scaleTargetRef</span><span class="p">:</span><span class="w">
</span><span class="w">    </span><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l">apps/v1</span><span class="w">
</span><span class="w">    </span><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l">Deployment</span><span class="w">
</span><span class="w">    </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">my-app-v3</span><span class="w">
</span><span class="w">  </span><span class="nt">maxReplicas</span><span class="p">:</span><span class="w"> </span><span class="m">50</span><span class="w">
</span><span class="w">  </span><span class="nt">minReplicas</span><span class="p">:</span><span class="w"> </span><span class="m">3</span><span class="w">
</span><span class="w">  </span><span class="nt">metrics</span><span class="p">:</span><span class="w">
</span><span class="w">  </span>- <span class="nt">type</span><span class="p">:</span><span class="w"> </span><span class="l">Resource</span><span class="w">
</span><span class="w">    </span><span class="nt">resource</span><span class="p">:</span><span class="w">
</span><span class="w">      </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">cpu</span><span class="w">
</span><span class="w">      </span><span class="nt">target</span><span class="p">:</span><span class="w">
</span><span class="w">        </span><span class="nt">type</span><span class="p">:</span><span class="w"> </span><span class="l">Utilization</span><span class="w">
</span><span class="w">        </span><span class="nt">averageUtilization</span><span class="p">:</span><span class="w"> </span><span class="m">70</span><span class="w">
</span><span class="w"></span><span class="nn">---</span><span class="w">
</span><span class="w"></span><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l">policy/v1</span><span class="w">
</span><span class="w"></span><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l">PodDisruptionBudget</span><span class="w">
</span><span class="w"></span><span class="nt">metadata</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">my-app-v3</span><span class="w">
</span><span class="w">  </span><span class="nt">namespace</span><span class="p">:</span><span class="w"> </span><span class="l">prod</span><span class="w">
</span><span class="w">  </span><span class="nt">labels</span><span class="p">:</span><span class="w">
</span><span class="w">    </span><span class="nt">app</span><span class="p">:</span><span class="w"> </span><span class="l">my-app</span><span class="w">
</span><span class="w"></span><span class="nt">spec</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">minAvailable</span><span class="p">:</span><span class="w"> </span><span class="m">75</span><span class="l">%</span><span class="w">
</span><span class="w">  </span><span class="nt">selector</span><span class="p">:</span><span class="w">
</span><span class="w">    </span><span class="nt">matchLabels</span><span class="p">:</span><span class="w">
</span><span class="w">      </span><span class="nt">app</span><span class="p">:</span><span class="w"> </span><span class="l">my-app</span><span class="w">
</span><span class="w">      </span><span class="nt">version</span><span class="p">:</span><span class="w"> </span><span class="l">v3</span><span class="w">
</span></code></pre></td></tr></table>
</div>
</div><h2 id="一优雅停止gracful-shutdown与-502504-报错">一、优雅停止（Gracful Shutdown）与 502/504 报错</h2>
<p>如果 Pod 正在处理大量请求（比如 1000 QPS+）时，因为节点故障或「竞价节点」被回收等原因被重新调度，
你可能会观察到在容器被 terminate 的一段时间内出现少量 502/504。</p>
<p>为了搞清楚这个问题，需要先理解清楚 terminate 一个 Pod 的流程：</p>
<ol>
<li>Pod 的状态被设为「Terminating」，（几乎）同时该 Pod 被从所有关联的 Service Endpoints 中移除</li>
<li><code>preStop</code> 钩子被执行，它可以是一个命令，或者一个对 Pod 中容器的 http 调用
<ol>
<li>如果你的程序在收到 SIGTERM 信号时，无法优雅退出，就可以考虑使用 <code>preStop</code></li>
<li>如果让程序本身支持优雅退出比较麻烦的话，用 <code>preStop</code> 实现优雅退出是一个非常好的方式</li>
</ol>
</li>
<li>将 SIGTERM 发送给 Pod 中的所有容器</li>
<li>继续等待，直到超过 <code>spec.terminationGracePeriodSeconds</code> 设定好的时间，这个值默认为 30s
<ol>
<li>需要注意的是，这个优雅退出的等待计时是与 <code>preStop</code> 同步开始的！而且它也不会等待 <code>preStop</code> 结束！</li>
</ol>
</li>
<li>如果超过了 <code>spec.terminationGracePeriodSeconds</code> 容器仍然没有停止，k8s 将会发送 SIGKILL 信号给容器</li>
<li>进程全部终止后，整个 Pod 完全被清理掉</li>
</ol>
<p><strong>注意</strong>：1 和 2 两个工作是异步发生的，所以可能会出现「Pod 还在 Service Endpoints 中，但是 <code>preStop</code> 已经执行了」的情况，我们需要考虑到这种状况的发生。</p>
<p>了解了上面的流程后，我们就能分析出两种错误码出现的原因：</p>
<ul>
<li>502：应用程序在收到 SIGTERM 信号后直接终止了运行，导致部分还没有被处理完的请求直接中断，代理层返回 502 表示这种情况</li>
<li>504：Service Endpoints 移除不够及时，在 Pod 已经被终止后，仍然有个别请求被路由到了该 Pod，得不到响应导致 504</li>
</ul>
<p>通常的解决方案是，在 Pod 的 <code>preStop</code> 步骤加一个 15s 的等待时间。
其原理是：在 Pod 处理 terminating 状态的时候，就会被从 Service Endpoints 中移除，也就不会再有新的请求过来了。
在 <code>preStop</code> 等待 15s，基本就能保证所有的请求都在容器死掉之前被处理完成（一般来说，绝大部分请求的处理时间都在 300ms 以内吧）。</p>
<p>一个简单的示例如下，它使 Pod 被终止时，总是先等待 15s，再发送 SIGTERM 信号给容器：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span><span class="lnt">9
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-yaml" data-lang="yaml"><span class="w">    </span><span class="nt">containers</span><span class="p">:</span><span class="w">
</span><span class="w">    </span>- <span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">my-app</span><span class="w">
</span><span class="w">      </span><span class="c"># 添加下面这部分</span><span class="w">
</span><span class="w">      </span><span class="nt">lifecycle</span><span class="p">:</span><span class="w">
</span><span class="w">        </span><span class="nt">preStop</span><span class="p">:</span><span class="w">
</span><span class="w">          </span><span class="nt">exec</span><span class="p">:</span><span class="w">
</span><span class="w">            </span><span class="nt">command</span><span class="p">:</span><span class="w">
</span><span class="w">            </span>- <span class="l">/bin/sleep</span><span class="w">
</span><span class="w">            </span>- <span class="s2">&#34;15&#34;</span><span class="w">
</span></code></pre></td></tr></table>
</div>
</div><p>更好的解决办法，是直接等待所有 tcp 连接都关闭（需要镜像中有 netstat）：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-yaml" data-lang="yaml"><span class="w">    </span><span class="nt">containers</span><span class="p">:</span><span class="w">
</span><span class="w">    </span>- <span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">my-app</span><span class="w">
</span><span class="w">      </span><span class="c"># 添加下面这部分</span><span class="w">
</span><span class="w">      </span><span class="nt">lifecycle</span><span class="p">:</span><span class="w">
</span><span class="w">      </span><span class="nt">preStop</span><span class="p">:</span><span class="w">
</span><span class="w">          </span><span class="nt">exec</span><span class="p">:</span><span class="w">
</span><span class="w">            </span><span class="nt">command</span><span class="p">:</span><span class="w">
</span><span class="w">            </span>- <span class="l">/bin/sh</span><span class="w">
</span><span class="w">            </span>- -<span class="l">c</span><span class="w">
</span><span class="w">            </span>- <span class="s2">&#34;while [ $(netstat -plunt | grep tcp | wc -l | xargs) -ne 0 ]; do sleep 1; done&#34;</span><span class="w">
</span></code></pre></td></tr></table>
</div>
</div><h3 id="k8s-istio-pod-prestop">如果我的服务还使用了 Sidecar 代理网络请求，该怎么处理？</h3>
<p>以服务网格 Istio 为例，在 Envoy 代理了 Pod 流量的情况下，502/504 的问题会变得更复杂一点——还需要考虑 Sidecar 与主容器的关闭顺序：</p>
<ul>
<li>如果在 Envoy 已关闭后，有新的请求再进来，将会导致 504（没人响应这个请求了）
<ul>
<li>所以 Envoy 最好在 Terminating 至少 3s 后才能关，确保 Istio 网格配置已完全更新</li>
</ul>
</li>
<li>如果在 Envoy 还没停止时，主容器先关闭，然后又有新的请求再进来，Envoy 将因为无法连接到 upstream 导致 503
<ul>
<li>所以主容器也最好在 Terminating 至少 3s 后，才能关闭。</li>
</ul>
</li>
<li>如果主容器处理还未处理完遗留请求时，Envoy 或者主容器的其中一个停止了，会因为 tcp 连接直接断开连接导致 502
<ul>
<li>因此 Envoy 必须在主容器处理完遗留请求后（即没有 tcp 连接时），才能关闭</li>
</ul>
</li>
</ul>
<p>所以总结下：Envoy 及主容器的 <code>preStop</code> 都至少得设成 3s，并且在「没有 tcp 连接」时，才能关闭，避免出现 502/503/504.</p>
<p>主容器的修改方法在前文中已经写过了，下面介绍下 Envoy 的修改方法。</p>
<p>和主容器一样，Envoy 也能直接加 <code>preStop</code>，修改 <code>istio-sidecar-injector</code> 这个 <code>configmap</code>，在 sidecar 里添加 preStop sleep 命令:</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-yaml" data-lang="yaml"><span class="w">    </span><span class="nt">containers</span><span class="p">:</span><span class="w">
</span><span class="w">    </span>- <span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">istio-proxy</span><span class="w">
</span><span class="w">      </span><span class="c"># 添加下面这部分</span><span class="w">
</span><span class="w">      </span><span class="nt">lifecycle</span><span class="p">:</span><span class="w">
</span><span class="w">      </span><span class="nt">preStop</span><span class="p">:</span><span class="w">
</span><span class="w">          </span><span class="nt">exec</span><span class="p">:</span><span class="w">
</span><span class="w">            </span><span class="nt">command</span><span class="p">:</span><span class="w">
</span><span class="w">            </span>- <span class="l">/bin/sh</span><span class="w">
</span><span class="w">            </span>- -<span class="l">c</span><span class="w">
</span><span class="w">            </span>- <span class="s2">&#34;while [ $(netstat -plunt | grep tcp | grep -v envoy | wc -l | xargs) -ne 0 ]; do sleep 1; done&#34;</span><span class="w">
</span></code></pre></td></tr></table>
</div>
</div><h3 id="参考">参考</h3>
<ul>
<li><a href="https://cloud.google.com/blog/products/containers-kubernetes/kubernetes-best-practices-terminating-with-grace" target="_blank" rel="noopener noreferrer">Kubernetes best practices: terminating with grace</a></li>
<li><a href="https://medium.com/flant-com/kubernetes-graceful-shutdown-nginx-php-fpm-d5ab266963c2" target="_blank" rel="noopener noreferrer">Graceful shutdown in Kubernetes is not always trivial</a></li>
</ul>
<h2 id="k8s-hpa">二、服务的伸缩配置 - HPA</h2>
<p>Kubernetes 官方主要支持基于 Pod CPU 的伸缩，这是应用最为广泛的伸缩指标，需要部署 <a href="https://github.com/kubernetes-sigs/metrics-server" target="_blank" rel="noopener noreferrer">metrics-server</a> 才可使用。</p>
<p>先回顾下前面给出的示例：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-yaml" data-lang="yaml"><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l">autoscaling/v2beta2 </span><span class="w"> </span><span class="c"># k8s 1.23+ 此 API 已经 GA</span><span class="w">
</span><span class="w"></span><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l">HorizontalPodAutoscaler</span><span class="w">
</span><span class="w"></span><span class="nt">metadata</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">labels</span><span class="p">:</span><span class="w">
</span><span class="w">    </span><span class="nt">app</span><span class="p">:</span><span class="w"> </span><span class="l">my-app</span><span class="w">
</span><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">my-app-v3</span><span class="w">
</span><span class="w">  </span><span class="nt">namespace</span><span class="p">:</span><span class="w"> </span><span class="l">prod</span><span class="w">
</span><span class="w"></span><span class="nt">spec</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">scaleTargetRef</span><span class="p">:</span><span class="w">
</span><span class="w">    </span><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l">apps/v1</span><span class="w">
</span><span class="w">    </span><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l">Deployment</span><span class="w">
</span><span class="w">    </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">my-app-v3</span><span class="w">
</span><span class="w">  </span><span class="nt">maxReplicas</span><span class="p">:</span><span class="w"> </span><span class="m">50</span><span class="w">
</span><span class="w">  </span><span class="nt">minReplicas</span><span class="p">:</span><span class="w"> </span><span class="m">3</span><span class="w">
</span><span class="w">  </span><span class="nt">metrics</span><span class="p">:</span><span class="w">
</span><span class="w">  </span>- <span class="nt">type</span><span class="p">:</span><span class="w"> </span><span class="l">Resource</span><span class="w">
</span><span class="w">    </span><span class="nt">resource</span><span class="p">:</span><span class="w">
</span><span class="w">      </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">cpu</span><span class="w">
</span><span class="w">      </span><span class="nt">target</span><span class="p">:</span><span class="w">
</span><span class="w">        </span><span class="nt">type</span><span class="p">:</span><span class="w"> </span><span class="l">Utilization</span><span class="w">
</span><span class="w">        </span><span class="nt">averageUtilization</span><span class="p">:</span><span class="w"> </span><span class="m">70</span><span class="w">
</span></code></pre></td></tr></table>
</div>
</div><h3 id="1-当前指标值的计算方式">1. 当前指标值的计算方式</h3>
<p>HPA 默认使用 Pod 的当前指标进行计算，以 CPU 为例，其计算公式为：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback">「Pod 的 CPU 利用率」= 100% * 「所有 Container 的 CPU 用量之和」/「所有 Container 的 CPU requests 之和」
</code></pre></td></tr></table>
</div>
</div><p>注意分母是总的 requests 量，而不是 limits.</p>
<h4 id="11-存在的问题与解决方法">1.1 存在的问题与解决方法</h4>
<p>在 Pod 只有一个容器时这没啥问题，但是当 Pod 注入了 envoy 等 sidecar 时，这就会有问题了。</p>
<p>因为 Istio 的 Sidecar requests 默认为 <code>100m</code> 也就是 0.1 核。
在未 tuning 的情况下，服务负载一高，sidecar 的实际用量很容易就能涨到 0.2-0.4 核。
把这两个值代入前面的公式，会发现 <strong>对于 QPS 较高的服务，添加 Sidecar 后，「Pod 的 CPU 利用率」可能会高于「应用容器的 CPU 利用率」</strong>，造成不必要的扩容。</p>
<p>解决方法：</p>
<ul>
<li>方法一：针对每个服务的 CPU 使用情况，为 sidecar 设置不同的 requests/limits</li>
<li>方法二：使用 KEDA 等第三方组件，获取到应用程序的 CPU 利用率（排除掉 Sidecar），使用它进行扩缩容</li>
<li>方法三：使用 k8s 1.20 提供的 alpha 特性：<a href="https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/#container-resource-metrics" target="_blank" rel="noopener noreferrer">Container Resourse Metrics</a>.</li>
</ul>
<h4 id="2-hpa-的扩缩容算法">2. HPA 的扩缩容算法</h4>
<p>HPA 什么时候会扩容，这一点是很好理解的。但是 HPA 的缩容策略，会有些迷惑，下面简单分析下。</p>
<ol>
<li>HPA 的「目标指标」可以使用两种形式：绝对度量指标和资源利用率。
<ul>
<li>绝对度量指标：比如 CPU，就是设定绝对核数。</li>
<li>资源利用率（资源使用量/资源请求 * 100%）：在 Pod 设置了资源请求时，可以使用资源利用率进行 Pod 伸缩。</li>
</ul>
</li>
<li>HPA 的「当前指标」是一段时间内所有 Pods 的平均值，不是峰值。Pod 的指标是其中所有容器指标之和。</li>
</ol>
<p>HPA 的扩缩容算法为：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback">期望副本数 = ceil[当前副本数 * ( 当前指标 / 目标指标 )]
</code></pre></td></tr></table>
</div>
</div><p>从上面的参数可以看到：</p>
<ol>
<li>只要「当前指标」超过了目标指标，就一定会发生扩容。</li>
<li><code>当前指标 / 目标指标</code>要小到一定的程度，才会触发缩容。
<ol>
<li>比如双副本的情况下，上述比值要小于等于 1/2，才会缩容到单副本。</li>
<li>三副本的情况下，上述比值的临界点是 2/3。</li>
<li>五副本时临界值是 4/5，100副本时临界值是 99/100，依此类推。</li>
<li>如果 <code>当前指标 / 目标指标</code> 从 1 降到 0.5，副本的数量将会减半。（虽然说副本数越多，发生这么大变化的可能性就越小。）</li>
</ol>
</li>
<li><code>当前副本数 / 目标指标</code>的值越大，「当前指标」的波动对「期望副本数」的影响就越大。</li>
</ol>
<p>为了防止扩缩容过于敏感，它还有几个延时相关的参数：</p>
<ol>
<li>HPA Loop 延时：默认 15 秒，每 15 秒钟进行一次 HPA 扫描。</li>
<li><code>--horizontal-pod-autoscaler-cpu-initialization-period</code>:</li>
<li>缩容冷却时间：默认 5 分钟。</li>
</ol>
<h3 id="3-hpa-的期望值设成多少合适">3. HPA 的期望值设成多少合适</h3>
<p>这个需要针对每个服务的具体情况，具体分析。</p>
<p>以最常用的按 CPU 值伸缩为例，</p>
<ul>
<li>核心服务
<ul>
<li>requests/limits 值: 建议设成相等的，保证<a href="https://kubernetes.io/docs/tasks/configure-pod-container/quality-service-pod/" target="_blank" rel="noopener noreferrer">服务质量等级</a>为 Guaranteed
<ul>
<li>需要注意 CPU 跟 Memory 的 limits 限制策略是不同的，CPU 是真正地限制了上限，而 Memory 是用超了就干掉容器（OOMKilled）</li>
<li>k8s 一直使用 cgroups v1 (<code>cpu_shares</code>/<code>memory.limit_in_bytes</code>)来限制 cpu/memory，但是对于 <code>Guaranteed</code> 的 Pods 而言，内存并不能完全预留，资源竞争总是有可能发生的。1.22 有 alpha 特性改用 cgroups v2，可以关注下。</li>
</ul>
</li>
<li>HPA: 一般来说，期望值设为 60% 到 70% 可能是比较合适的，最小副本数建议设为 2 - 5. （仅供参考）</li>
<li>PodDisruptionBudget: 建议按服务的健壮性与 HPA 期望值，来设置 PDB，后面会详细介绍，这里就先略过了</li>
</ul>
</li>
<li>非核心服务
<ul>
<li>requests/limits 值: 建议 requests 设为 limits 的 0.6 - 0.9 倍（仅供参考），对应的服务质量等级为 Burstable
<ul>
<li>也就是超卖了资源，这样做主要的考量点是，很多非核心服务负载都很低，根本跑不到 limits 这么高，降低 requests 可以提高集群资源利用率，也不会损害服务稳定性。</li>
</ul>
</li>
<li>HPA: 因为 requests 降低了，我们可以提高 HPA 到期望值，比如 80% ~ 90%，最小副本数建议设为 1 - 3. （仅供参考）</li>
<li>PodDisruptionBudget: 非核心服务嘛，保证最少副本数为 1 就行了。</li>
</ul>
</li>
</ul>
<h3 id="4-hpa-的常见问题">4. HPA 的常见问题</h3>
<h4 id="41-pod-扩容---预热陷阱">4.1. Pod 扩容 - 预热陷阱</h4>
<blockquote>
<p>预热：Java/C# 这类运行在虚拟机上的语言，第一次使用到某些功能时，往往需要初始化一些资源，例如「JIT 即时编译」。
如果代码里还应用了动态类加载之类的功能，就很可能导致微服务某些 API 第一次被调用时，响应特别慢（要动态编译 class）。
因此 Pod 在提供服务前，需要提前「预热（slow_start）」一次这些接口，将需要用到的资源提前初始化好。</p>
</blockquote>
<p>在负载很高的情况下，HPA 会自动扩容。
但是如果扩容的 Pod 需要预热，就可能会遇到「预热陷阱」。</p>
<p>在有大量用户访问的时候，不论使用何种负载均衡策略，只要请求被转发到新建的 Pod 上，这个请求就会「卡住」。
如果请求速度太快，Pod 启动的瞬间「卡住」的请求就越多，这将会导致新建 Pod 因为压力过大而垮掉。
然后 Pod 一重启就被压垮，进入 CrashLoopBackoff 循环。</p>
<p>如果是在使用多线程做负载测试时，效果更明显：50 个线程在不间断地请求，
别的 Pod 响应时间是「毫秒级」，而新建的 Pod 的首次响应是「秒级」。几乎是一瞬间，50 个线程就会全部陷在新建的 Pod 这里。
而新建的 Pod 在启动的瞬间可能特别脆弱，瞬间的 50 个并发请求就可以将它压垮。
然后 Pod 一重启就被压垮，进入 CrashLoopBackoff 循环。</p>
<p><strong>解决方法</strong>：</p>
<p>可以在「应用层面」解决：</p>
<ol>
<li>在启动探针 API 的后端控制器里面，依次调用所有需要预热的接口或者其他方式，提前初始化好所有资源。
<ol>
<li>启动探针的控制器中，可以通过 <code>localhost</code> 回环地址调用它自身的接口。</li>
</ol>
</li>
<li>使用「AOT 预编译」技术：预热，通常都是因为「JIT 即时编译」导致的问题，在需要用到时它才编译。而 AOT 是预先编译，在使用前完成编译，因此 AOT 能解决预热的问题。</li>
</ol>
<p>也可以在「基础设施层面」解决：</p>
<ol>
<li>像 AWS ALB TargetGroup 以及其他云服务商的 ALB 服务，通常都可以设置 <code>slow_start</code> 时长，即对新加入的实例，使用一定时间慢慢地把流量切过去，最终达到预期的负载均衡状态。这个可以解决服务预热问题。</li>
<li>Envoy 也已经支持 <code>slow_start</code> 模式，支持在一个设置好的时间窗口内，把流量慢慢负载到新加入的实例上，达成预热效果。</li>
</ol>
<h4 id="42-hpa-扩缩容过于敏感导致-pod-数量震荡">4.2. HPA 扩缩容过于敏感，导致 Pod 数量震荡</h4>
<p>通常来讲，EKS 上绝大部分负载都应该选择使用 CPU 进行扩缩容。因为 CPU 通常能很好的反映服务的负载情况</p>
<p>但是有些服务会存在其他影响 CPU 使用率的因素，导致使用 CPU 扩缩容变得不那么可靠，比如：</p>
<ul>
<li>有些 Java 服务堆内存设得很大，GC pause 也设得比较长，因此内存 GC 会造成 CPU 间歇性飙升，CPU 监控会有大量的尖峰。</li>
<li>有些服务有定时任务，定时任务一运行 CPU 就涨，但是这跟服务的 QPS 是无关的</li>
<li>有些服务可能一运行 CPU 就会立即处于一个高位状态，它可能希望使用别的业务侧指标来进行扩容，而不是 CPU.</li>
</ul>
<p>因为上述问题存在，使用 CPU 扩缩容，就可能会造成服务频繁的扩容然后缩容，或者无限扩容。
而有些服务（如我们的「推荐服务」），对「扩容」和「缩容」都是比较敏感的，每次扩缩都会造成服务可用率抖动。</p>
<p>对这类服务而言，HPA 有这几种调整策略：</p>
<ul>
<li>选择使用 <strong>QPS</strong> 等相对比较平滑，没有 GC 这类干扰的指标来进行扩缩容，这需要借助 KEDA 等社区组件。</li>
<li>对 kubernetes 1.18+，可以直接使用 HPA 的 <code>behavior.scaleDown</code> 和 <code>behavior.scaleUp</code> 两个参数，控制每次扩缩容的最多 pod 数量或者比例。 示例如下：</li>
</ul>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-yaml" data-lang="yaml"><span class="nn">---</span><span class="w">
</span><span class="w"></span><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l">autoscaling/v2beta2</span><span class="w">
</span><span class="w"></span><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l">HorizontalPodAutoscaler</span><span class="w">
</span><span class="w"></span><span class="nt">metadata</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">podinfo</span><span class="w">
</span><span class="w">  </span><span class="nt">namespace</span><span class="p">:</span><span class="w"> </span><span class="l">default</span><span class="w">
</span><span class="w"></span><span class="nt">spec</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">scaleTargetRef</span><span class="p">:</span><span class="w">
</span><span class="w">    </span><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l">apps/v1</span><span class="w">
</span><span class="w">    </span><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l">Deployment</span><span class="w">
</span><span class="w">    </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">podinfo</span><span class="w">
</span><span class="w">  </span><span class="nt">minReplicas</span><span class="p">:</span><span class="w"> </span><span class="m">3</span><span class="w">
</span><span class="w">  </span><span class="nt">maxReplicas</span><span class="p">:</span><span class="w"> </span><span class="m">50</span><span class="w">
</span><span class="w">  </span><span class="nt">metrics</span><span class="p">:</span><span class="w">
</span><span class="w">  </span>- <span class="nt">type</span><span class="p">:</span><span class="w"> </span><span class="l">Resource</span><span class="w">
</span><span class="w">    </span><span class="nt">resource</span><span class="p">:</span><span class="w">
</span><span class="w">      </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">cpu</span><span class="w">
</span><span class="w">      </span><span class="nt">target</span><span class="p">:</span><span class="w">
</span><span class="w">        </span><span class="nt">type</span><span class="p">:</span><span class="w"> </span><span class="l">Utilization</span><span class="w">
</span><span class="w">        </span><span class="nt">averageUtilization</span><span class="p">:</span><span class="w"> </span><span class="m">50</span><span class="w">  </span><span class="c"># 期望的 CPU 平均值</span><span class="w">
</span><span class="w">  </span><span class="nt">behavior</span><span class="p">:</span><span class="w">
</span><span class="w">    </span><span class="nt">scaleUp</span><span class="p">:</span><span class="w">
</span><span class="w">      </span><span class="nt">stabilizationWindowSeconds</span><span class="p">:</span><span class="w"> </span><span class="m">0</span><span class="w">  </span><span class="c"># 默认为 0，只使用当前值进行扩缩容</span><span class="w">
</span><span class="w">      </span><span class="nt">policies</span><span class="p">:</span><span class="w">
</span><span class="w">      </span>- <span class="nt">periodSeconds</span><span class="p">:</span><span class="w"> </span><span class="m">180</span><span class="w">  </span><span class="c"># 每 3 分钟最多扩容 5% 的 Pods</span><span class="w">
</span><span class="w">        </span><span class="nt">type</span><span class="p">:</span><span class="w"> </span><span class="l">Percent</span><span class="w">
</span><span class="w">        </span><span class="nt">value</span><span class="p">:</span><span class="w"> </span><span class="m">5</span><span class="w">
</span><span class="w">      </span>- <span class="nt">periodSeconds</span><span class="p">:</span><span class="w"> </span><span class="m">60</span><span class="w">  </span><span class="c"># 每分钟最多扩容 1 个 Pod，扩的慢一点主要是为了一个个地预热，避免一次扩容太多未预热的 Pods 导致服务可用率剧烈抖动</span><span class="w">
</span><span class="w">        </span><span class="nt">type</span><span class="p">:</span><span class="w"> </span><span class="l">Pods</span><span class="w">
</span><span class="w">        </span><span class="nt">value</span><span class="p">:</span><span class="w"> </span><span class="m">1</span><span class="w">
</span><span class="w">      </span><span class="nt">selectPolicy</span><span class="p">:</span><span class="w"> </span><span class="l">Min </span><span class="w"> </span><span class="c"># 选择最小的策略</span><span class="w">
</span><span class="w">    </span><span class="c"># 以下的一切配置，都是为了更平滑地缩容</span><span class="w">
</span><span class="w">    </span><span class="nt">scaleDown</span><span class="p">:</span><span class="w">
</span><span class="w">      </span><span class="nt">stabilizationWindowSeconds</span><span class="p">:</span><span class="w"> </span><span class="m">600</span><span class="w">  </span><span class="c"># 使用过去 10 mins 的最大 cpu 值进行缩容计算，避免过快缩容</span><span class="w">
</span><span class="w">      </span><span class="nt">policies</span><span class="p">:</span><span class="w">
</span><span class="w">      </span>- <span class="nt">type</span><span class="p">:</span><span class="w"> </span><span class="l">Percent </span><span class="w"> </span><span class="c"># 每 3 mins 最多缩容 `ceil[当前副本数 * 5%]` 个 pod（20 个 pod 以内，一次只缩容 1 个 pod）</span><span class="w">
</span><span class="w">        </span><span class="nt">value</span><span class="p">:</span><span class="w"> </span><span class="m">5</span><span class="w">
</span><span class="w">        </span><span class="nt">periodSeconds</span><span class="p">:</span><span class="w"> </span><span class="m">180</span><span class="w">
</span><span class="w">      </span>- <span class="nt">type</span><span class="p">:</span><span class="w"> </span><span class="l">Pods </span><span class="w"> </span><span class="c"># 每 1 mins 最多缩容 1 个 pod</span><span class="w">
</span><span class="w">        </span><span class="nt">value</span><span class="p">:</span><span class="w"> </span><span class="m">1</span><span class="w">
</span><span class="w">        </span><span class="nt">periodSeconds</span><span class="p">:</span><span class="w"> </span><span class="m">60</span><span class="w">
</span><span class="w">      </span><span class="nt">selectPolicy</span><span class="p">:</span><span class="w"> </span><span class="l">Min </span><span class="w"> </span><span class="c"># 上面的 policies 列表，只生效其中最小的值作为缩容限制（保证平滑缩容）</span><span class="w">
</span></code></pre></td></tr></table>
</div>
</div><p>而对于扩容不够平滑这个问题，可以考虑提供类似 AWS ALB TargetGroup <code>slow_start</code> 的功能，在扩容时缓慢将流量切到新 Pod 上，以实现预热服务（JVM 预热以及本地缓存预热），这样就能达到比较好的平滑扩容效果。</p>
<h3 id="5-hpa-注意事项">5. HPA 注意事项</h3>
<p>注意 kubectl 1.23 以下的版本，默认使用 <code>hpa.v1.autoscaling</code> 来查询 HPA 配置，<code>v2beta2</code> 相关的参数会被编码到 <code>metadata.annotations</code> 中。</p>
<p>比如 <code>behavior</code> 就会被编码到 <code>autoscaling.alpha.kubernetes.io/behavior</code> 这个 key 所对应的值中。</p>
<p>因此如果使用了 v2beta2 的 HPA，一定要明确指定使用 <code>v2beta2</code> 版本的 HPA：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-shell" data-lang="shell">kubectl get hpa.v2beta2.autoscaling
</code></pre></td></tr></table>
</div>
</div><p>否则不小心动到 <code>annotations</code> 中编码的某些参数，可能会产生意料之外的效果，甚至直接把控制面搞崩&hellip;
比如这个 issue: <a href="https://github.com/kubernetes/kubernetes/issues/107038" target="_blank" rel="noopener noreferrer">Nil pointer dereference in KCM after v1 HPA patch request</a></p>
<h3 id="6-参考">6. 参考</h3>
<ul>
<li><a href="https://kubernetes.io/zh/docs/tasks/run-application/horizontal-pod-autoscale/" target="_blank" rel="noopener noreferrer">Pod 水平自动伸缩 - Kubernetes Docs</a></li>
<li><a href="https://kubernetes.io/zh/docs/tasks/run-application/horizontal-pod-autoscale-walkthrough/" target="_blank" rel="noopener noreferrer">Horizontal Pod Autoscaler演练 - Kubernetes Docs</a></li>
</ul>
<h2 id="k8s-PodDistruptionBuget">三、<a href="https://kubernetes.io/zh/docs/tasks/run-application/configure-pdb/" target="_blank" rel="noopener noreferrer">节点维护与Pod干扰预算</a></h2>
<p>在我们通过 <code>kubectl drain</code> 将某个节点上的容器驱逐走的时候，
kubernetes 会依据 Pod 的「PodDistruptionBuget」来进行 Pod 的驱逐。</p>
<p>如果不设置任何明确的 PodDistruptionBuget，Pod 将会被直接杀死，然后在别的节点重新调度，<strong>这可能导致服务中断！</strong></p>
<p>PDB 是一个单独的 CR 自定义资源，示例如下：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-yaml" data-lang="yaml"><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l">policy/v1beta1</span><span class="w">
</span><span class="w"></span><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l">PodDisruptionBudget</span><span class="w">
</span><span class="w"></span><span class="nt">metadata</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">podinfo-pdb</span><span class="w">
</span><span class="w"></span><span class="nt">spec</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="c"># 如果不满足 PDB，Pod 驱逐将会失败！</span><span class="w">
</span><span class="w">  </span><span class="nt">minAvailable</span><span class="p">:</span><span class="w"> </span><span class="m">1</span><span class="w">      </span><span class="c"># 最少也要维持一个 Pod 可用</span><span class="w">
</span><span class="w"></span><span class="c">#   maxUnavailable: 1  # 最大不可用的 Pod 数，与 minAvailable 不能同时配置！二选一</span><span class="w">
</span><span class="w">  </span><span class="nt">selector</span><span class="p">:</span><span class="w">
</span><span class="w">    </span><span class="nt">matchLabels</span><span class="p">:</span><span class="w">
</span><span class="w">      </span><span class="nt">app</span><span class="p">:</span><span class="w"> </span><span class="l">podinfo</span><span class="w">
</span></code></pre></td></tr></table>
</div>
</div><p>如果在进行节点维护时(kubectl drain)，Pod 不满足 PDB，drain 将会失败，示例：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-shell" data-lang="shell">&gt; kubectl drain node-205 --ignore-daemonsets --delete-local-data
node/node-205 cordoned
WARNING: ignoring DaemonSet-managed Pods: kube-system/calico-node-nfhj7, kube-system/kube-proxy-94dz5
evicting pod default/podinfo-7c84d8c94d-h9brq
evicting pod default/podinfo-7c84d8c94d-gw6qf
error when evicting pod <span class="s2">&#34;podinfo-7c84d8c94d-h9brq&#34;</span> <span class="o">(</span>will retry after 5s<span class="o">)</span>: Cannot evict pod as it would violate the pod<span class="s1">&#39;s disruption budget.
</span><span class="s1">evicting pod default/podinfo-7c84d8c94d-h9brq
</span><span class="s1">error when evicting pod &#34;podinfo-7c84d8c94d-h9brq&#34; (will retry after 5s): Cannot evict pod as it would violate the pod&#39;</span>s disruption budget.
evicting pod default/podinfo-7c84d8c94d-h9brq
error when evicting pod <span class="s2">&#34;podinfo-7c84d8c94d-h9brq&#34;</span> <span class="o">(</span>will retry after 5s<span class="o">)</span>: Cannot evict pod as it would violate the pod<span class="err">&#39;</span>s disruption budget.
evicting pod default/podinfo-7c84d8c94d-h9brq
pod/podinfo-7c84d8c94d-gw6qf evicted
pod/podinfo-7c84d8c94d-h9brq evicted
node/node-205 evicted
</code></pre></td></tr></table>
</div>
</div><p>上面的示例中，podinfo 一共有两个副本，都运行在 node-205 上面。我给它设置了干扰预算 PDB <code>minAvailable: 1</code>。</p>
<p>然后使用 <code>kubectl drain</code> 驱逐 Pod 时，其中一个 Pod 被立即驱逐走了，而另一个 Pod 大概在 15 秒内一直驱逐失败。
因为第一个 Pod 还没有在新的节点上启动完成，它不满足干扰预算 PDB <code>minAvailable: 1</code> 这个条件。</p>
<p>大约 15 秒后，最先被驱逐走的 Pod 在新节点上启动完成了，另一个 Pod 满足了 PDB 所以终于也被驱逐了。这才完成了一个节点的 drain 操作。</p>
<blockquote>
<p>ClusterAutoscaler 等集群节点伸缩组件，在缩容节点时也会考虑 PodDisruptionBudget. 如果你的集群使用了 ClusterAutoscaler 等动态扩缩容节点的组件，强烈建议设置为所有服务设置 PodDisruptionBudget.</p>
</blockquote>
<h4 id="在-pdb-中使用百分比的注意事项">在 PDB 中使用百分比的注意事项</h4>
<p>在使用百分比时，计算出的实例数都会被向上取整，这会造成两个现象：</p>
<ul>
<li>如果使用 <code>minAvailable</code>，实例数较少的情况下，可能会导致 ALLOWED DISRUPTIONS 为 0</li>
<li>如果使用 <code>maxUnavailable</code>，因为是向上取整，ALLOWED DISRUPTIONS 的值一定不会低于 1</li>
</ul>
<p>因此从便于驱逐的角度看，如果你的服务至少有 2-3 个实例，建议在 PDB 中使用百分比配置 <code>maxUnavailable</code>，而不是 <code>minAvailable</code>.</p>
<h3 id="最佳实践-deployment--hpa--poddisruptionbudget">最佳实践 Deployment + HPA + PodDisruptionBudget</h3>
<p>一般而言，一个服务的每个版本，都应该包含如下三个资源：</p>
<ul>
<li>Deployment: 管理服务自身的 Pods 嘛</li>
<li>HPA: 负责 Pods 的扩缩容，通常使用 CPU 指标进行扩缩容</li>
<li>PodDisruptionBudget(PDB): 建议按照 HPA 的目标值，来设置 PDB.
<ul>
<li>比如 HPA CPU 目标值为 60%，就可以考虑设置 PDB <code>minAvailable=65%</code>，保证至少有 65% 的 Pod 可用。这样理论上极限情况下 QPS 均摊到剩下 65% 的 Pods 上也不会造成雪崩（这里假设 QPS 和 CPU 是完全的线性关系）</li>
</ul>
</li>
</ul>
<h2 id="k8s-affinity">四、节点亲和性与节点组</h2>
<p>我们一个集群，通常会使用不同的标签为节点组进行分类，比如 kubernetes 自动生成的一些节点标签：</p>
<ul>
<li><code>kubernetes.io/os</code>: 通常都用 <code>linux</code></li>
<li><code>kubernetes.io/arch</code>: <code>amd64</code>, <code>arm64</code></li>
<li><code>topology.kubernetes.io/region</code> 和 <code>topology.kubernetes.io/zone</code>: 云服务的区域及可用区</li>
</ul>
<p>我们使用得比较多的，是「节点亲和性」以及「Pod 反亲和性」，另外两个策略视情况使用。</p>
<h3 id="1-节点亲和性">1. 节点亲和性</h3>
<p>如果你使用的是 aws，那 aws 有一些自定义的节点标签：</p>
<ul>
<li><code>eks.amazonaws.com/nodegroup</code>: aws eks 节点组的名称，同一个节点组使用同样的 aws ec2 实例模板
<ul>
<li>比如 arm64 节点组、amd64/x64 节点组</li>
<li>内存比例高的节点组如 m 系实例，计算性能高的节点组如 c 系列</li>
<li>竞价实例节点组：这个省钱啊，但是动态性很高，随时可能被回收</li>
<li>按量付费节点组：这类实例贵，但是稳定。</li>
</ul>
</li>
</ul>
<p>假设你希望优先选择竞价实例跑你的 Pod，如果竞价实例暂时跑满了，就选择按量付费实例。
那 <code>nodeSelector</code> 就满足不了你的需求了，你需要使用 <code>nodeAffinity</code>，示例如下:</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span><span class="lnt">43
</span><span class="lnt">44
</span><span class="lnt">45
</span><span class="lnt">46
</span><span class="lnt">47
</span><span class="lnt">48
</span><span class="lnt">49
</span><span class="lnt">50
</span><span class="lnt">51
</span><span class="lnt">52
</span><span class="lnt">53
</span><span class="lnt">54
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-yaml" data-lang="yaml"><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l">apps/v1</span><span class="w">
</span><span class="w"></span><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l">Deployment</span><span class="w">
</span><span class="w"></span><span class="nt">metadata</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">xxx</span><span class="w">
</span><span class="w">  </span><span class="nt">namespace</span><span class="p">:</span><span class="w"> </span><span class="l">xxx</span><span class="w">
</span><span class="w"></span><span class="nt">spec</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="c"># ...</span><span class="w">
</span><span class="w">  </span><span class="nt">template</span><span class="p">:</span><span class="w">
</span><span class="w">    </span><span class="c"># ...</span><span class="w">
</span><span class="w">    </span><span class="nt">spec</span><span class="p">:</span><span class="w">
</span><span class="w">      </span><span class="nt">affinity</span><span class="p">:</span><span class="w">
</span><span class="w">        </span><span class="nt">nodeAffinity</span><span class="p">:</span><span class="w">
</span><span class="w">          </span><span class="c"># 优先选择 spot-group-c 的节点</span><span class="w">
</span><span class="w">          </span><span class="nt">preferredDuringSchedulingIgnoredDuringExecution</span><span class="p">:</span><span class="w">
</span><span class="w">          </span>- <span class="nt">preference</span><span class="p">:</span><span class="w">
</span><span class="w">              </span><span class="nt">matchExpressions</span><span class="p">:</span><span class="w">
</span><span class="w">              </span>- <span class="nt">key</span><span class="p">:</span><span class="w"> </span><span class="l">eks.amazonaws.com/nodegroup</span><span class="w">
</span><span class="w">                </span><span class="nt">operator</span><span class="p">:</span><span class="w"> </span><span class="l">In</span><span class="w">
</span><span class="w">                </span><span class="nt">values</span><span class="p">:</span><span class="w">
</span><span class="w">                </span>- <span class="l">spot-group-c</span><span class="w">
</span><span class="w">            </span><span class="nt">weight</span><span class="p">:</span><span class="w"> </span><span class="m">80</span><span class="w">  </span><span class="c"># weight 用于为节点评分，会优先选择评分最高的节点</span><span class="w">
</span><span class="w">          </span>- <span class="nt">preference</span><span class="p">:</span><span class="w">
</span><span class="w">              </span><span class="nt">matchExpressions</span><span class="p">:</span><span class="w">
</span><span class="w">              </span><span class="c"># 优先选择 aws c6i 的机器</span><span class="w">
</span><span class="w">              </span>- <span class="nt">key</span><span class="p">:</span><span class="w"> </span><span class="l">node.kubernetes.io/instance-type</span><span class="w">
</span><span class="w">                </span><span class="nt">operator</span><span class="p">:</span><span class="w"> </span><span class="l">In</span><span class="w">
</span><span class="w">                </span><span class="nt">values</span><span class="p">:</span><span class="w">
</span><span class="w">                </span>- <span class="s2">&#34;c6i.xlarge&#34;</span><span class="w">
</span><span class="w">                </span>- <span class="s2">&#34;c6i.2xlarge&#34;</span><span class="w">
</span><span class="w">                </span>- <span class="s2">&#34;c6i.4xlarge&#34;</span><span class="w">
</span><span class="w">                </span>- <span class="s2">&#34;c6i.8xlarge&#34;</span><span class="w">
</span><span class="w">            </span><span class="nt">weight</span><span class="p">:</span><span class="w"> </span><span class="m">70</span><span class="w">
</span><span class="w">          </span>- <span class="nt">preference</span><span class="p">:</span><span class="w">
</span><span class="w">              </span><span class="nt">matchExpressions</span><span class="p">:</span><span class="w">
</span><span class="w">              </span><span class="c"># 其次选择 aws c5 的机器</span><span class="w">
</span><span class="w">              </span>- <span class="nt">key</span><span class="p">:</span><span class="w"> </span><span class="l">node.kubernetes.io/instance-type</span><span class="w">
</span><span class="w">                </span><span class="nt">operator</span><span class="p">:</span><span class="w"> </span><span class="l">In</span><span class="w">
</span><span class="w">                </span><span class="nt">values</span><span class="p">:</span><span class="w">
</span><span class="w">                </span>- <span class="s2">&#34;c5.xlarge&#34;</span><span class="w">
</span><span class="w">                </span>- <span class="s2">&#34;c5.2xlarge&#34;</span><span class="w">
</span><span class="w">                </span>- <span class="s2">&#34;c5.4xlarge&#34;</span><span class="w">
</span><span class="w">                </span>- <span class="s2">&#34;c5.9xlarge&#34;</span><span class="w">
</span><span class="w">            </span><span class="nt">weight</span><span class="p">:</span><span class="w"> </span><span class="m">60</span><span class="w">
</span><span class="w">         </span><span class="c"># 如果没 spot-group-c 可用，也可选择 ondemand-group-c 的节点跑</span><span class="w">
</span><span class="w">          </span><span class="nt">requiredDuringSchedulingIgnoredDuringExecution</span><span class="p">:</span><span class="w">
</span><span class="w">            </span><span class="nt">nodeSelectorTerms</span><span class="p">:</span><span class="w">
</span><span class="w">            </span>- <span class="nt">matchExpressions</span><span class="p">:</span><span class="w">
</span><span class="w">              </span>- <span class="nt">key</span><span class="p">:</span><span class="w"> </span><span class="l">eks.amazonaws.com/nodegroup</span><span class="w">
</span><span class="w">                </span><span class="nt">operator</span><span class="p">:</span><span class="w"> </span><span class="l">In</span><span class="w">
</span><span class="w">                </span><span class="nt">values</span><span class="p">:</span><span class="w">
</span><span class="w">                </span>- <span class="l">spot-group-c</span><span class="w">
</span><span class="w">                </span>- <span class="l">ondemand-group-c</span><span class="w">
</span><span class="w">      </span><span class="nt">containers</span><span class="p">:</span><span class="w">
</span><span class="w">        </span><span class="c"># ...</span><span class="w">
</span></code></pre></td></tr></table>
</div>
</div><h3 id="2-pod-反亲和性">2. Pod 反亲和性</h3>
<p>通常建议为每个 Deployment 的 template 配置 Pod 反亲和性，把 Pods 打散在所有节点上：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-yaml" data-lang="yaml"><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l">apps/v1</span><span class="w">
</span><span class="w"></span><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l">Deployment</span><span class="w">
</span><span class="w"></span><span class="nt">metadata</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">xxx</span><span class="w">
</span><span class="w">  </span><span class="nt">namespace</span><span class="p">:</span><span class="w"> </span><span class="l">xxx</span><span class="w">
</span><span class="w"></span><span class="nt">spec</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="c"># ...</span><span class="w">
</span><span class="w">  </span><span class="nt">template</span><span class="p">:</span><span class="w">
</span><span class="w">    </span><span class="c"># ...</span><span class="w">
</span><span class="w">    </span><span class="nt">spec</span><span class="p">:</span><span class="w">
</span><span class="w">      </span><span class="nt">replicas</span><span class="p">:</span><span class="w"> </span><span class="m">3</span><span class="w">
</span><span class="w">      </span><span class="nt">affinity</span><span class="p">:</span><span class="w">
</span><span class="w">        </span><span class="nt">podAntiAffinity</span><span class="p">:</span><span class="w">
</span><span class="w">          </span><span class="nt">preferredDuringSchedulingIgnoredDuringExecution</span><span class="p">:</span><span class="w"> </span><span class="c"># 非强制性条件</span><span class="w">
</span><span class="w">          </span>- <span class="nt">weight</span><span class="p">:</span><span class="w"> </span><span class="m">100</span><span class="w">  </span><span class="c"># weight 用于为节点评分，会优先选择评分最高的节点</span><span class="w">
</span><span class="w">            </span><span class="nt">podAffinityTerm</span><span class="p">:</span><span class="w">
</span><span class="w">              </span><span class="nt">labelSelector</span><span class="p">:</span><span class="w">
</span><span class="w">                </span><span class="nt">matchExpressions</span><span class="p">:</span><span class="w">
</span><span class="w">                </span>- <span class="nt">key</span><span class="p">:</span><span class="w"> </span><span class="l">app</span><span class="w">
</span><span class="w">                  </span><span class="nt">operator</span><span class="p">:</span><span class="w"> </span><span class="l">In</span><span class="w">
</span><span class="w">                  </span><span class="nt">values</span><span class="p">:</span><span class="w">
</span><span class="w">                  </span>- <span class="l">xxx</span><span class="w">
</span><span class="w">                </span>- <span class="nt">key</span><span class="p">:</span><span class="w"> </span><span class="l">version</span><span class="w">
</span><span class="w">                  </span><span class="nt">operator</span><span class="p">:</span><span class="w"> </span><span class="l">In</span><span class="w">
</span><span class="w">                  </span><span class="nt">values</span><span class="p">:</span><span class="w">
</span><span class="w">                  </span>- <span class="l">v12</span><span class="w">
</span><span class="w">              </span><span class="c"># 将 pod 尽量打散在多个可用区</span><span class="w">
</span><span class="w">              </span><span class="nt">topologyKey</span><span class="p">:</span><span class="w"> </span><span class="l">topology.kubernetes.io/zone</span><span class="w">
</span><span class="w">          </span><span class="nt">requiredDuringSchedulingIgnoredDuringExecution</span><span class="p">:</span><span class="w">  </span><span class="c"># 强制性要求</span><span class="w">
</span><span class="w">          </span><span class="c"># 注意这个没有 weights，必须满足列表中的所有条件</span><span class="w">
</span><span class="w">          </span>- <span class="nt">labelSelector</span><span class="p">:</span><span class="w">
</span><span class="w">              </span><span class="nt">matchExpressions</span><span class="p">:</span><span class="w">
</span><span class="w">              </span>- <span class="nt">key</span><span class="p">:</span><span class="w"> </span><span class="l">app</span><span class="w">
</span><span class="w">                </span><span class="nt">operator</span><span class="p">:</span><span class="w"> </span><span class="l">In</span><span class="w">
</span><span class="w">                </span><span class="nt">values</span><span class="p">:</span><span class="w">
</span><span class="w">                </span>- <span class="l">xxx</span><span class="w">
</span><span class="w">              </span>- <span class="nt">key</span><span class="p">:</span><span class="w"> </span><span class="l">version</span><span class="w">
</span><span class="w">                </span><span class="nt">operator</span><span class="p">:</span><span class="w"> </span><span class="l">In</span><span class="w">
</span><span class="w">                </span><span class="nt">values</span><span class="p">:</span><span class="w">
</span><span class="w">                </span>- <span class="l">v12</span><span class="w">
</span><span class="w">            </span><span class="c"># Pod 必须运行在不同的节点上</span><span class="w">
</span><span class="w">            </span><span class="nt">topologyKey</span><span class="p">:</span><span class="w"> </span><span class="l">kubernetes.io/hostname</span><span class="w">
</span></code></pre></td></tr></table>
</div>
</div><h2 id="k8s-container-probe">五、Pod 的就绪探针、存活探针与启动探针</h2>
<p>Pod 提供如下三种探针，均支持使用 Command、HTTP API、TCP Socket 这三种手段来进行服务可用性探测。</p>
<ul>
<li><code>startupProbe</code> 启动探针（Kubernetes v1.18 [beta]）: 此探针通过后，「就绪探针」与「存活探针」才会进行存活性与就绪检查
<ul>
<li>用于对慢启动容器进行存活性检测，避免它们在启动运行之前就被杀掉
<ul>
<li>startupProbe 显然比 livenessProbe 的 initialDelaySeconds 参数更灵活。</li>
<li>同时它也能延迟 readinessProbe 的生效时间，这主要是为了避免无意义的探测。容器都还没 startUp，显然是不可能就绪的。</li>
</ul>
</li>
<li>程序将最多有 <code>failureThreshold * periodSeconds</code> 的时间用于启动，比如设置 <code>failureThreshold=20</code>、<code>periodSeconds=5</code>，程序启动时间最长就为 100s，如果超过 100s 仍然未通过「启动探测」，容器会被杀死。</li>
</ul>
</li>
<li><code>readinessProbe</code> 就绪探针:
<ul>
<li>就绪探针失败次数超过 <code>failureThreshold</code> 限制（默认三次），服务将被暂时从 Service 的 Endpoints 中踢出，直到服务再次满足 <code>successThreshold</code>.</li>
</ul>
</li>
<li><code>livenessProbe</code> 存活探针: 检测服务是否存活，它可以捕捉到死锁等情况，及时杀死这种容器。
<ul>
<li>存活探针失败可能的原因：
<ul>
<li>服务发生死锁，对所有请求均无响应</li>
<li>服务线程全部卡在对外部 redis/mysql 等外部依赖的等待中，导致请求无响应</li>
</ul>
</li>
<li>存活探针失败次数超过 <code>failureThreshold</code> 限制（默认三次），容器将被杀死，随后根据重启策略执行重启。
<ul>
<li><code>kubectl describe pod</code> 会显示重启原因为 <code>State.Last State.Reason = Error, Exit Code=137</code>，同时 Events 中会有 <code>Liveness probe failed: ...</code> 这样的描述。</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>上述三类探测器的参数都是通用的，五个时间相关的参数列举如下：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-yaml" data-lang="yaml"><span class="c"># 下面的值就是 k8s 的默认值</span><span class="w">
</span><span class="w"></span><span class="nt">initialDelaySeconds</span><span class="p">:</span><span class="w"> </span><span class="m">0</span><span class="w">  </span><span class="c"># 默认没有 delay 时间</span><span class="w">
</span><span class="w"></span><span class="nt">periodSeconds</span><span class="p">:</span><span class="w"> </span><span class="m">10</span><span class="w">
</span><span class="w"></span><span class="nt">timeoutSeconds</span><span class="p">:</span><span class="w"> </span><span class="m">1</span><span class="w">
</span><span class="w"></span><span class="nt">failureThreshold</span><span class="p">:</span><span class="w"> </span><span class="m">3</span><span class="w">
</span><span class="w"></span><span class="nt">successThreshold</span><span class="p">:</span><span class="w"> </span><span class="m">1</span><span class="w">
</span></code></pre></td></tr></table>
</div>
</div><p>示例：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-yaml" data-lang="yaml"><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l">apps/v1</span><span class="w">
</span><span class="w"></span><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l">Deployment</span><span class="w">
</span><span class="w"></span><span class="nt">metadata</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">my-app-v3</span><span class="w">
</span><span class="w"></span><span class="nt">spec</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="c"># ...</span><span class="w">
</span><span class="w">  </span><span class="nt">template</span><span class="p">:</span><span class="w">
</span><span class="w">    </span><span class="c">#  ...</span><span class="w">
</span><span class="w">    </span><span class="nt">spec</span><span class="p">:</span><span class="w">
</span><span class="w">      </span><span class="nt">containers</span><span class="p">:</span><span class="w">
</span><span class="w">      </span>- <span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">my-app-v3</span><span class="w">
</span><span class="w">        </span><span class="nt">image</span><span class="p">:</span><span class="w"> </span><span class="l">xxx.com/app/my-app:v3</span><span class="w">
</span><span class="w">        </span><span class="nt">imagePullPolicy</span><span class="p">:</span><span class="w"> </span><span class="l">IfNotPresent </span><span class="w">
</span><span class="w">        </span><span class="c"># ... 省略若干配置</span><span class="w">
</span><span class="w">        </span><span class="nt">startupProbe</span><span class="p">:</span><span class="w">
</span><span class="w">          </span><span class="nt">httpGet</span><span class="p">:</span><span class="w">
</span><span class="w">            </span><span class="nt">path</span><span class="p">:</span><span class="w"> </span><span class="l">/actuator/health </span><span class="w"> </span><span class="c"># 直接使用健康检查接口即可</span><span class="w">
</span><span class="w">            </span><span class="nt">port</span><span class="p">:</span><span class="w"> </span><span class="m">8080</span><span class="w">
</span><span class="w">          </span><span class="nt">periodSeconds</span><span class="p">:</span><span class="w"> </span><span class="m">5</span><span class="w">
</span><span class="w">          </span><span class="nt">timeoutSeconds</span><span class="p">:</span><span class="w"> </span><span class="m">1</span><span class="w">
</span><span class="w">          </span><span class="nt">failureThreshold</span><span class="p">:</span><span class="w"> </span><span class="m">20</span><span class="w">  </span><span class="c"># 最多提供给服务 5s * 20 的启动时间</span><span class="w">
</span><span class="w">          </span><span class="nt">successThreshold</span><span class="p">:</span><span class="w"> </span><span class="m">1</span><span class="w">
</span><span class="w">        </span><span class="nt">livenessProbe</span><span class="p">:</span><span class="w">
</span><span class="w">          </span><span class="nt">httpGet</span><span class="p">:</span><span class="w">
</span><span class="w">            </span><span class="nt">path</span><span class="p">:</span><span class="w"> </span><span class="l">/actuator/health </span><span class="w"> </span><span class="c"># spring 的通用健康检查路径</span><span class="w">
</span><span class="w">            </span><span class="nt">port</span><span class="p">:</span><span class="w"> </span><span class="m">8080</span><span class="w">
</span><span class="w">          </span><span class="nt">periodSeconds</span><span class="p">:</span><span class="w"> </span><span class="m">5</span><span class="w">
</span><span class="w">          </span><span class="nt">timeoutSeconds</span><span class="p">:</span><span class="w"> </span><span class="m">1</span><span class="w">
</span><span class="w">          </span><span class="nt">failureThreshold</span><span class="p">:</span><span class="w"> </span><span class="m">5</span><span class="w">
</span><span class="w">          </span><span class="nt">successThreshold</span><span class="p">:</span><span class="w"> </span><span class="m">1</span><span class="w">
</span><span class="w">        </span><span class="c"># Readiness probes are very important for a RollingUpdate to work properly,</span><span class="w">
</span><span class="w">        </span><span class="nt">readinessProbe</span><span class="p">:</span><span class="w">
</span><span class="w">          </span><span class="nt">httpGet</span><span class="p">:</span><span class="w">
</span><span class="w">            </span><span class="nt">path</span><span class="p">:</span><span class="w"> </span><span class="l">/actuator/health </span><span class="w"> </span><span class="c"># 简单起见可直接使用 livenessProbe 相同的接口，当然也可额外定义</span><span class="w">
</span><span class="w">            </span><span class="nt">port</span><span class="p">:</span><span class="w"> </span><span class="m">8080</span><span class="w">
</span><span class="w">          </span><span class="nt">periodSeconds</span><span class="p">:</span><span class="w"> </span><span class="m">5</span><span class="w">
</span><span class="w">          </span><span class="nt">timeoutSeconds</span><span class="p">:</span><span class="w"> </span><span class="m">1</span><span class="w">
</span><span class="w">          </span><span class="nt">failureThreshold</span><span class="p">:</span><span class="w"> </span><span class="m">5</span><span class="w">
</span><span class="w">          </span><span class="nt">successThreshold</span><span class="p">:</span><span class="w"> </span><span class="m">1</span><span class="w">
</span></code></pre></td></tr></table>
</div>
</div><p>在 Kubernetes 1.18 之前，通用的手段是为「就绪探针」添加较长的 <code>initialDelaySeconds</code> 来实现类似「启动探针」的功能动，避免容器因为启动太慢，存活探针失败导致容器被重启。示例如下：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-yaml" data-lang="yaml"><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l">apps/v1</span><span class="w">
</span><span class="w"></span><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l">Deployment</span><span class="w">
</span><span class="w"></span><span class="nt">metadata</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">my-app-v3</span><span class="w">
</span><span class="w"></span><span class="nt">spec</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="c"># ...</span><span class="w">
</span><span class="w">  </span><span class="nt">template</span><span class="p">:</span><span class="w">
</span><span class="w">    </span><span class="c">#  ...</span><span class="w">
</span><span class="w">    </span><span class="nt">spec</span><span class="p">:</span><span class="w">
</span><span class="w">      </span><span class="nt">containers</span><span class="p">:</span><span class="w">
</span><span class="w">      </span>- <span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">my-app-v3</span><span class="w">
</span><span class="w">        </span><span class="nt">image</span><span class="p">:</span><span class="w"> </span><span class="l">xxx.com/app/my-app:v3</span><span class="w">
</span><span class="w">        </span><span class="nt">imagePullPolicy</span><span class="p">:</span><span class="w"> </span><span class="l">IfNotPresent </span><span class="w">
</span><span class="w">        </span><span class="c"># ... 省略若干配置</span><span class="w">
</span><span class="w">        </span><span class="nt">livenessProbe</span><span class="p">:</span><span class="w">
</span><span class="w">          </span><span class="nt">httpGet</span><span class="p">:</span><span class="w">
</span><span class="w">            </span><span class="nt">path</span><span class="p">:</span><span class="w"> </span><span class="l">/actuator/health </span><span class="w"> </span><span class="c"># spring 的通用健康检查路径</span><span class="w">
</span><span class="w">            </span><span class="nt">port</span><span class="p">:</span><span class="w"> </span><span class="m">8080</span><span class="w">
</span><span class="w">          </span><span class="nt">initialDelaySeconds</span><span class="p">:</span><span class="w"> </span><span class="m">120</span><span class="w">  </span><span class="c"># 前两分钟，都假设服务健康，避免 livenessProbe 失败导致服务重启</span><span class="w">
</span><span class="w">          </span><span class="nt">periodSeconds</span><span class="p">:</span><span class="w"> </span><span class="m">5</span><span class="w">
</span><span class="w">          </span><span class="nt">timeoutSeconds</span><span class="p">:</span><span class="w"> </span><span class="m">1</span><span class="w">
</span><span class="w">          </span><span class="nt">failureThreshold</span><span class="p">:</span><span class="w"> </span><span class="m">5</span><span class="w">
</span><span class="w">          </span><span class="nt">successThreshold</span><span class="p">:</span><span class="w"> </span><span class="m">1</span><span class="w">
</span><span class="w">        </span><span class="c"># 容器一启动，Readiness probes 就会不断进行检测</span><span class="w">
</span><span class="w">        </span><span class="nt">readinessProbe</span><span class="p">:</span><span class="w">
</span><span class="w">          </span><span class="nt">httpGet</span><span class="p">:</span><span class="w">
</span><span class="w">            </span><span class="nt">path</span><span class="p">:</span><span class="w"> </span><span class="l">/actuator/health</span><span class="w">
</span><span class="w">            </span><span class="nt">port</span><span class="p">:</span><span class="w"> </span><span class="m">8080</span><span class="w">
</span><span class="w">          </span><span class="nt">initialDelaySeconds</span><span class="p">:</span><span class="w"> </span><span class="m">3</span><span class="w">  </span><span class="c"># readiness probe 不需要设太长时间，使 Pod 尽快加入到 Endpoints.</span><span class="w">
</span><span class="w">          </span><span class="nt">periodSeconds</span><span class="p">:</span><span class="w"> </span><span class="m">5</span><span class="w">
</span><span class="w">          </span><span class="nt">timeoutSeconds</span><span class="p">:</span><span class="w"> </span><span class="m">1</span><span class="w">
</span><span class="w">          </span><span class="nt">failureThreshold</span><span class="p">:</span><span class="w"> </span><span class="m">5</span><span class="w">
</span><span class="w">          </span><span class="nt">successThreshold</span><span class="p">:</span><span class="w"> </span><span class="m">1</span><span class="w">
</span></code></pre></td></tr></table>
</div>
</div><h2 id="k8s-pod-security">六、Pod 安全</h2>
<p>这里只介绍 Pod 中安全相关的参数，其他诸如集群全局的安全策略，不在这里讨论。</p>
<h3 id="1-pod-securitycontexthttpskubernetesiodocstasksconfigure-pod-containersecurity-context">1. <a href="https://kubernetes.io/docs/tasks/configure-pod-container/security-context/" target="_blank" rel="noopener noreferrer">Pod SecurityContext</a></h3>
<p>通过设置 Pod 的 SecurityContext，可以为每个 Pod 设置特定的安全策略。</p>
<p>SecurityContext 有两种类型：</p>
<ol>
<li><code>spec.securityContext</code>: 这是一个 <a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.20/#podsecuritycontext-v1-core" target="_blank" rel="noopener noreferrer">PodSecurityContext</a> 对象
<ul>
<li>顾名思义，它对 Pod 中的所有 contaienrs 都有效。</li>
</ul>
</li>
<li><code>spec.containers[*].securityContext</code>: 这是一个 <a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.20/#securitycontext-v1-core" target="_blank" rel="noopener noreferrer">SecurityContext</a> 对象
<ul>
<li>container 私有的 SecurityContext</li>
</ul>
</li>
</ol>
<p>这两个 SecurityContext 的参数只有部分重叠，重叠的部分 <code>spec.containers[*].securityContext</code> 优先级更高。</p>
<p>我们比较常遇到的一些<strong>提升权限</strong>的安全策略：</p>
<ol>
<li>特权容器：<code>spec.containers[*].securityContext.privileged</code></li>
<li>添加（Capabilities）可选的系统级能力: <code>spec.containers[*].securityContext.capabilities.add</code>
<ol>
<li>只有 ntp 同步服务等少数容器，可以开启这项功能。请注意这非常危险。</li>
</ol>
</li>
<li>Sysctls: 系统参数: <code>spec.securityContext.sysctls</code></li>
</ol>
<p><strong>权限限制</strong>相关的安全策略有（<strong>强烈建议在所有 Pod 上按需配置如下安全策略！</strong>）：</p>
<ol>
<li><code>spec.volumes</code>: 所有的数据卷都可以设定读写权限</li>
<li><code>spec.securityContext.runAsNonRoot: true</code> Pod 必须以非 root 用户运行</li>
<li><code>spec.containers[*].securityContext.readOnlyRootFileSystem:true</code> <strong>将容器层设为只读，防止容器文件被篡改。</strong>
<ol>
<li>如果微服务需要读写文件，建议额外挂载 <code>emptydir</code> 类型的数据卷。</li>
</ol>
</li>
<li><code>spec.containers[*].securityContext.allowPrivilegeEscalation: false</code> 不允许 Pod 做任何权限提升！</li>
<li><code>spec.containers[*].securityContext.capabilities.drop</code>: 移除（Capabilities）可选的系统级能力</li>
</ol>
<p>还有其他诸如指定容器的运行用户(user)/用户组(group)等功能未列出，请自行查阅 Kubernetes 相关文档。</p>
<p>一个无状态的微服务 Pod 配置举例：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-yaml" data-lang="yaml"><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l">v1</span><span class="w">
</span><span class="w"></span><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l">Pod</span><span class="w">
</span><span class="w"></span><span class="nt">metadata</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">&lt;Pod name&gt;</span><span class="w">
</span><span class="w"></span><span class="nt">spec</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">containers</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">- name</span><span class="p">:</span><span class="w"> </span><span class="l">&lt;container name&gt;</span><span class="w">
</span><span class="w">    </span><span class="nt">image</span><span class="p">:</span><span class="w"> </span><span class="l">&lt;image&gt;</span><span class="w">
</span><span class="w">    </span><span class="nt">imagePullPolicy</span><span class="p">:</span><span class="w"> </span><span class="l">IfNotPresent </span><span class="w">
</span><span class="w">    </span><span class="c"># ......此处省略 500 字</span><span class="w">
</span><span class="w">    </span><span class="nt">securityContext</span><span class="p">:</span><span class="w">
</span><span class="w">      </span><span class="nt">readOnlyRootFilesystem</span><span class="p">:</span><span class="w"> </span><span class="kc">true</span><span class="w">  </span><span class="c"># 将容器层设为只读，防止容器文件被篡改。</span><span class="w">
</span><span class="w">      </span><span class="nt">allowPrivilegeEscalation</span><span class="p">:</span><span class="w"> </span><span class="kc">false</span><span class="w">  </span><span class="c"># 禁止 Pod 做任何权限提升</span><span class="w">
</span><span class="w">      </span><span class="nt">capabilities</span><span class="p">:</span><span class="w">
</span><span class="w">        </span><span class="nt">drop</span><span class="p">:</span><span class="w">
</span><span class="w">        </span><span class="c"># 禁止容器使用 raw 套接字，通常只有 hacker 才会用到 raw 套接字。</span><span class="w">
</span><span class="w">        </span><span class="c"># raw_socket 可自定义网络层数据，避开 tcp/udp 协议栈，直接操作底层的 ip/icmp 数据包。可实现 ip 伪装、自定义协议等功能。</span><span class="w">
</span><span class="w">        </span><span class="c"># 去掉 net_raw 会导致 tcpdump 无法使用，无法进行容器内抓包。需要抓包时可临时去除这项配置</span><span class="w">
</span><span class="w">        </span>- <span class="l">NET_RAW</span><span class="w">
</span><span class="w">        </span><span class="c"># 更好的选择：直接禁用所有 capabilities</span><span class="w">
</span><span class="w">        </span><span class="c"># - ALL</span><span class="w">
</span><span class="w">  </span><span class="nt">securityContext</span><span class="p">:</span><span class="w">
</span><span class="w">    </span><span class="c"># runAsUser: 1000  # 设定用户</span><span class="w">
</span><span class="w">    </span><span class="c"># runAsGroup: 1000  # 设定用户组</span><span class="w">
</span><span class="w">    </span><span class="nt">runAsNonRoot</span><span class="p">:</span><span class="w"> </span><span class="kc">true</span><span class="w">  </span><span class="c"># Pod 必须以非 root 用户运行</span><span class="w">
</span><span class="w">    </span><span class="nt">seccompProfile</span><span class="p">:</span><span class="w">  </span><span class="c"># security compute mode</span><span class="w">
</span><span class="w">      </span><span class="nt">type</span><span class="p">:</span><span class="w"> </span><span class="l">RuntimeDefault</span><span class="w">
</span></code></pre></td></tr></table>
</div>
</div><h3 id="2-seccomp-security-compute-mode">2. seccomp: security compute mode</h3>
<p>seccomp 和 seccomp-bpf 允许对系统调用进行过滤，可以防止用户的二进制文对主机操作系统件执行通常情况下并不需要的危险操作。它和 Falco 有些类似，不过 Seccomp 没有为容器提供特别的支持。</p>
<p>视频:</p>
<ul>
<li><a href="https://www.youtube.com/watch?v=Ro4QRx7VPsY&amp;list=PLj6h78yzYM2Pn8RxfLh2qrXBDftr6Qjut$index=22" target="_blank" rel="noopener noreferrer">Seccomp: What Can It Do For You? - Justin Cormack, Docker</a></li>
</ul>
<h2 id="其他问题">其他问题</h2>
<ul>
<li>不同节点类型的性能有差距，导致 QPS 均衡的情况下，CPU 负载不均衡
<ul>
<li>解决办法（未验证）：
<ul>
<li>尽量使用性能相同的实例类型：通过 <code>podAffinity</code> 及 <code>nodeAffinity</code> 添加节点类型的亲和性</li>
</ul>
</li>
</ul>
</li>
</ul>
]]></description></item><item><title>2021 年年终总结</title><link>https://ryan4yin.space/posts/2021-summary/</link><pubDate>Mon, 03 Jan 2022 14:50:00 +0800</pubDate><author>xiaoyin_c@qq.com</author><dc:creator>ryan4yin</dc:creator><guid>https://ryan4yin.space/posts/2021-summary/</guid><description><![CDATA[<blockquote>
<p>更新：2022/1/22</p>
</blockquote>
<h2 id="闲言碎语">闲言碎语</h2>
<p>一晃一年又是过去了，这个新年，全球疫情再创新高，圣诞节后美国单日新增更是直接突破 <del>50 万直逼 60 万大关❌</del> <strong>100 万✅</strong>，国内也有西安管理不力导致民众忍饥挨饿。</p>
<p>新冠已经两年多了啊。</p>
<p>言归正传，我今年年初从 W 公司离职后，非常幸运地进了现在的公司——大宇无限，在融入大宇的过程中也是五味杂陈。
不过总体结果还是挺满意的，目前工作已经步入正轨，也发现了非常多的机会，大宇的基础设施领域仍然大有可为。</p>
<p>一些重要事情还是没怎么想通，不过毕竟风口上的猪都能飞，今年小小努力了一把，大部分时间仍然随波逐流，却也渐入佳境。</p>
<h2 id="生活">生活</h2>
<ul>
<li>1 月的时候从博客园迁移到这个独立博客，还认识了 <a href="https://chee5e.space/" target="_blank" rel="noopener noreferrer">@芝士</a>，芝士帮我调整好了博客「友链」页面的样式，超级感谢~</li>
<li>2 月的时候从 W 公司离职，然后怎么说呢，瞬间感觉海阔天空，心态 180 度转变，好得不得了，但是其实也很担心自己各方面的不足。总之心里有好多的想法，跟 <a href="https://sanshiliuxiao.top/" target="_blank" rel="noopener noreferrer">@是格子啊</a>、<a href="https://chee5e.space/" target="_blank" rel="noopener noreferrer">@芝士</a> 以及前同事聊了好多，非常感谢这几位朋友跟同事帮我梳理思路，给我肯定。也是这个时间点，我被 <a href="https://chee5e.space/" target="_blank" rel="noopener noreferrer">@芝士</a> 拉进了中文 twitter 的圈子。</li>
<li>过年响应号召没回家（其实是嫌核酸检测麻烦，家里也建议先别回），每天爬爬山看看风景，买了个吊床去公园午睡，练习口琴竹笛，就这样玩了一个月。
<ul>
<li></li>
</ul>
</li>
<li>到了 3 月份的时候我开始找工作，面了几家公司后，非常幸运地进入了大宇无限，成为了一名 SRE 萌新。在大宇一年的感受，就放在后面的「工作」部分写了，这里先略过。
<ul>
<li>加入大宇后，全年都有定期的团建，跟 SRE 小伙伴公款吃喝，我 2021 年下馆子次数估计是上一年的七八倍</li>
</ul>
</li>
<li>3 月底，看了电影——《寻龙传说》（2021 年看的唯一一部电影），片尾曲超好听。</li>
<li>4 月份，各种巧合下，意外发现初中同学住得离我 1km 不到，在他家吃了顿家乡菜，还有杨梅酒，味道非常棒！还有回甘强烈的城步青钱柳茶，让我念念不忘。</li>
<li>8 月份，堂弟来深圳暑期实习，跟两个堂弟一起穿越深圳东西冲海岸线，风景非常棒，不过路上也是又热又渴</li>
<li>10 月份
<ul>
<li>加入了大宇的冲浪小分队，第一次冲浪、海边烧烤</li>
<li>买了双轮滑鞋，学会了倒滑、压步转向，复习了大学时学过的若干基础技巧</li>
</ul>
</li>
<li>12 月，买了台云米泉先净饮机后，有了随时随地的矿物质热水，就想起了 4 月份在初中同学家喝过的青钱柳，然后就喝茶上瘾了，一桌子的滇红、祁门红茶、安吉白茶、黄山毛峰、青钱柳、莓茶、梅子菁&hellip;目前感觉滇红跟祁门红茶最好喝，安吉白茶跟黄山毛峰都非常清香，青钱柳回甘最强烈，莓茶怎么说呢味道感觉不太好（也可能是泡的手法不对？）
<ul>
<li><figure><a class="lightgallery" href="./yunmi-ro-filterred-water-dispenser.jpg" title="./yunmi-ro-filterred-water-dispenser.jpg" data-thumbnail="./yunmi-ro-filterred-water-dispenser.jpg" data-sub-html="<h2>我的云米净饮机</h2>">
        
    </a><figcaption class="image-caption">我的云米净饮机</figcaption>
    </figure></li>
<li><figure><a class="lightgallery" href="./my-tea.jpg" title="./my-tea.jpg" data-thumbnail="./my-tea.jpg" data-sub-html="<h2>桌面上的各种茶叶</h2>">
        
    </a><figcaption class="image-caption">桌面上的各种茶叶</figcaption>
    </figure></li>
</ul>
</li>
<li>2022 年 1 月，第一次买动漫手办，妆点后感觉房间都增色不少~
<ul>
<li><figure><a class="lightgallery" href="./Posts-and-Garage-Kit.jpg" title="./Posts-and-Garage-Kit.jpg" data-thumbnail="./Posts-and-Garage-Kit.jpg" data-sub-html="<h2>我的房间-挂画-手办</h2>">
        
    </a><figcaption class="image-caption">我的房间-挂画-手办</figcaption>
    </figure></li>
</ul>
</li>
</ul>
<h2 id="读书">读书</h2>
<ul>
<li>年初辞职后游山玩水，心思稍微安定了些，看了大半本《走出荒野》。</li>
<li>6 月份社区组织打新冠疫苗时，在等候室看了本《青春驿站——深圳打工妹写真》，讲述八九十年代打工妹的生活。很真实，感情很细腻。</li>
<li>年末二爷爷去世，参加完葬礼后，心态有些变化，看完了大一时买下的《月宫 Moon Palace》，讲述主角的悲剧人生。</li>
<li>其余大部分业余时间，无聊，又不想学点东西，也不想运动，于是看了非常多的网络小说打发时间。</li>
</ul>
<h2 id="音乐">音乐</h2>
<p>年初辞职后，练了一段时间的竹笛跟蓝调口琴，但后来找到工作后就基本沉寂了。</p>
<p>总的来说还是原地踏步吧。</p>
<p></p>
<h2 id="工作---我在大宇无限的这一年">工作 - 我在大宇无限的这一年</h2>
<p>3 月份刚进大宇的我充满好奇，但也小心谨慎，甚至有点不敢相信自己能进到一家这么棒的公司，感觉自己运气爆棚。
毕竟大宇无论是同事水平还是工作氛围，亦或是用户体量，相比我上家公司都是质的差别。</p>
<p><figure><a class="lightgallery" href="./workstation-1.jpg" title="./workstation-1.jpg" data-thumbnail="./workstation-1.jpg" data-sub-html="<h2>我在大宇的第一个工位</h2>">
        
    </a><figcaption class="image-caption">我在大宇的第一个工位</figcaption>
    </figure></p>
<p>之后慢慢熟悉工作的内容与方法，leader 尽力把最匹配我兴趣的工作安排给我，帮我排疑解难，同时又给我极大的自主性，真的是棒极了。</p>
<p>然而自主性高带来的也是更高的工作难度，遇到困难时也曾手忙脚乱、迷茫、甚至自我怀疑，很担心是不是隔天就得跑路了&hellip;（是的我抗压能力有点弱）
但好在我终究还是能调节好心态，负起责任，一步步把工作完成。
中间有几次工作有延误时，leader 还陪我加班，事情干完后又带我去吃大餐犒劳自己，真的超级感谢他的帮助与支持。</p>
<p><figure><a class="lightgallery" href="./workstation-2.jpg" title="./workstation-2.jpg" data-thumbnail="./workstation-2.jpg" data-sub-html="<h2>换座位后的新工位，落地窗风景很棒</h2>">
        
    </a><figcaption class="image-caption">换座位后的新工位，落地窗风景很棒</figcaption>
    </figure></p>
<p>这样经历了几个项目的洗礼后，现在我终于能说自己是脚踏实地了，心态从「明天是不是得提桶跑路」转变成了「哇还有这个可以搞，那个 ROI 也很高，有好多有趣的事可以做啊」，我终于能说自己真正融入了大宇无限这家公司，成为了它的一员。</p>
<p>回看下了 2020 年的总结与展望，今年实际的进步，跟去年期望的差别很大。最初的目标大概只实现了 10%，但是接触到了许多意料之外的东西，总体还是满意的：</p>
<ul>
<li>熟悉了新公司的文化与工作方式，这感觉是个很大的收获，我的工作方式有了很大的改善</li>
<li>接触并且熟悉了新公司的 AWS 线上环境
<ul>
<li>负责维护线上 Kubernetes 管理平台，第一次接触到的线上集群峰值 QPS 就有好几万。从一开始的小心翼翼，到现在也转变成了老手，这算是意义重大吧</li>
<li>使用 python 写了几个 Kubernetes 管理平台的服务，这也是我第一次写线上服务，很有些成就感</li>
<li>下半年在 AWS 成本的分析与管控上花了很多精力，也有了一些不错的成果，受益匪浅</li>
<li>学会了 Nginx 的简单使用，刚好够用于维护公司先有的 Nginx 代理配置</li>
</ul>
</li>
<li>主导完成了「新建 K8s 集群，将服务迁移到新集群」。虽然并不是一件很难的事，但这应该算是我 2021 年最大的成就了。
<ul>
<li>升级过程中也是遇到了各种问题，第一次升级迁移时我准备了好久，慌的不行，结果升级时部分服务还是出了问题，当时脑子真的是个懵的，跟 leader 搞到半夜 1 点多后还是没解决，回退到了旧集群，升级失败。之后通过测试确认到是某个服务扩缩容震荡导致可用率无法恢复，尝试通过 HPA 的 behavior 来控制扩缩容速率，又意外触发了 K8s HPA 的 bug 把集群控制面搞崩了&hellip; 再之后把问题都确认了，第二次尝试升级，又是有个别服务可用率抖动，调试了好几天。那几天神经一直紧绷，每天早上都是被服务可用率的告警吵醒的。跨年的那天晚上业务量上涨，我就在观察服务可用率的过程中跨年了。这样才终于完成了 K8s 集群的升级，期间各位同事也有参与帮忙分析排查各种问题，非常感谢他们，还有努力的我自己。</li>
</ul>
</li>
<li>随便写了几个 go 的 demo，基本没啥进步</li>
<li>学了一个星期的 rust 语言，快速看完了 the book，用 rust 重写了个 video2chars</li>
<li>学习了 Linux 容器的底层原理：cgroups/namespace 技术，并且用 go/rust 实现了个 demo</li>
<li>学习了 Linux 的各种网络接口、Iptables</li>
<li>熟悉了 PromQL/Grafana，现在也能拷贝些 PromQL 查各种数据了</li>
</ul>
<p>如果要给自己打分的话，那就是「良好」吧。因为并没有很强的进取心，所以出来的结果也并不能称之为「优秀」。</p>
<p>顺便公司的新办公区真的超赞，详情见我的 twitter：</p>
<blockquote class="twitter-tweet"><p lang="zh" dir="ltr">新办公区真好呐～<br><br>值此良辰美景，好想整个榻榻米坐垫，坐在角落的落地窗边工作🤣<br>那种使用公共设施工（mo）作（yu）的乐趣，以及平常工位见不到的景色交相辉映，是不太好表述的奇妙体验 <a href="https://t.co/FASffzw8N3">pic.twitter.com/FASffzw8N3</a></p>&mdash; ryan4yin | 於清樂 (@ryan4yin) <a href="https://twitter.com/ryan4yin/status/1482891448731070466?ref_src=twsrc%5Etfw">January 17, 2022</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script> 
<h2 id="技术方面的感受">技术方面的感受</h2>
<ul>
<li>Istio 服务网格：体会到了它有点重，而且它的发展跟我们的需求不一定匹配
<ul>
<li>Sidecar 模式的成本比较高，在未调优的情况下，它会给服务带来 1/3 到 1/4 的成本提升，以及延迟上升</li>
<li>比如切量权重固定为 100（新版本将会放宽限制），不支持 pod 的 warm up（社区已经有 PR，持续观望吧）</li>
<li>而它重点发展的虚拟机支持我们却完全不需要</li>
<li>一直在思考是持续往 Istio 投入，还是换其他的方案</li>
</ul>
</li>
<li>服务网格仍然在快速发展，未来的趋势应该是 eBPF + Envoy + WASM
<ul>
<li>Cilium 推出的基于 eBPF 的 Service Mesh 是一个新趋势（它使用高级特性时会退化成 Per Node Proxy 模式），成本、延迟方面都有望吊打 Sidecar 模式的其他服务网格，是今年服务网格领域的大新闻。</li>
<li>我们曾尝试使用中心化网关来替代 Sidecar 以降低成本。但是跨区流量成本、HTTP/gRPC 多协议共存，这些都是挑战。而且这也并不是社区的最佳实践，现在我觉得维持 Sidecar 其实反而能提升资源利用率，我们的集群资源利用率目前很低。如果能把控好，这部分成本或许是可以接受的。</li>
</ul>
</li>
<li>K8s 集群的日志方面，我们目前是使用自研的基于 gelf 协议的系统，但是问题挺多的
<ul>
<li>从提升系统的可维护性、易用性等角度来说，loki 是值得探索下的</li>
</ul>
</li>
<li>K8s 集群管理方面，觉得集群的升级迭代，可以做得更自动化、更可靠。明年可以在多集群管理这个方向上多探索下。</li>
<li><a href="https://kubernetes.io/docs/tasks/configure-pod-container/quality-service-pod/" target="_blank" rel="noopener noreferrer">Pod 服务质量</a>：对非核心服务，可以适当调低 requests 的资源量，而不是完全预留(<code>Guaranteed</code>)，以提升资源利用率。</li>
<li>官方的 HPA 能力是不够用的，业务侧可能会需要基于 QPS/Queue 或者业务侧的其他参数来进行扩缩容，需要持续关注 <a href="https://github.com/kedacore/keda" target="_blank" rel="noopener noreferrer">KEDA</a> 这个项目。</li>
<li>成本控制方面，体会到了 ARM 架构以及 Spot 竞价实例的好处</li>
<li>跨区流量成本有很大的潜在优化空间
<ul>
<li>跨区流量成本是在两边都会收费，而且不仅涉及 Kubernetes 集群内服务间的调用，还会涉及对 RDS/ES/ElastiCache/EC2 等其他资源的调用。</li>
</ul>
</li>
<li>今年各云厂商故障频发，没有<strong>跨 region 的服务迁移</strong>就会很难受，需要持续关注下 <a href="https://github.com/karmada-io/karmada" target="_blank" rel="noopener noreferrer">karmada</a> 这类多集群管理方案。
<ul>
<li>Google 账号系统宕机</li>
<li>Fastly CDN 故障</li>
<li>Facebook 故障</li>
<li>AWS 更是各种可用区故障，12/7 的故障导致 AWS 大部分服务都崩了。因此我们 SRE 今年经常是救各种大火小火&hellip;</li>
</ul>
</li>
<li>Rust/Go/WASM 蓬勃发展，未来可期。</li>
<li>AI 落地到各个领域，影响到了我们日常使用的语音导航、歌声合成、语音合成等多个领域，当然也包括与 SRE 工作相关的场景：AIOps</li>
</ul>
<h2 id="2022-年的展望">2022 年的展望</h2>
<h3 id="技术侧">技术侧</h3>
<p>今年的展望写得更聚焦一些，争取能实现 50%，就是很大的突破了。</p>
<ol>
<li>熟练掌握 Go/Rust 语言，并分别用于至少两个项目中
<ol>
<li>打铁还需自身硬，编码能力是基础中的基础。</li>
</ol>
</li>
<li>深入学习如下技术
<ol>
<li>Kubernetes 源码</li>
</ol>
</li>
<li>网络技术
<ol>
<li>服务网格 Istio</li>
<li>代理工具 Envoy/APISIX</li>
<li>网络插件 Cilium + eBPF</li>
</ol>
</li>
<li>AWS K8s 成本与服务稳定性优化
<ol>
<li>节约跨可用区/跨域的流量成本</li>
<li>K8s 新特性：<a href="https://kubernetes.io/docs/concepts/services-networking/topology-aware-hints/" target="_blank" rel="noopener noreferrer">Topology Aware Hints</a></li>
<li>Istio: <a href="https://istio.io/latest/docs/tasks/traffic-management/locality-load-balancing/" target="_blank" rel="noopener noreferrer">Locality Load Balancing</a></li>
<li>实例类型优化：
<ol>
<li>使用更合适的实例类型</li>
<li>使用 ARM 架构机型，降本增效</li>
</ol>
</li>
<li>推广 GRPC 协议</li>
</ol>
</li>
<li>打通本地开发环境与云上的运行环境：
<ol>
<li><a href="https://github.com/nocalhost/nocalhost" target="_blank" rel="noopener noreferrer">nocalhost</a></li>
</ol>
</li>
<li>探索新技术与可能性（优先级低）
<ol>
<li>基于 Kubernetes 的服务平台，未来的发展方向
<ol>
<li>kubevela</li>
<li>buildpack</li>
<li>是否应该推进 gitops</li>
<li>openkruise</li>
</ol>
</li>
<li>Serverless 平台的进展
<ol>
<li>Knative</li>
<li>OpenFunction</li>
</ol>
</li>
<li>跨集群的应用部署、容灾
<ol>
<li>karmada</li>
</ol>
</li>
<li>机器学习、深度学习技术：想尝试下将 AI 应用在音乐、语音、SRE 等我感兴趣的领域，即使是调包也行啊，总之想出点成果&hellip;</li>
</ol>
</li>
</ol>
<p>可以预料到明年 SRE 团队有超多的机会，这其中我具体能负责哪些部分，又能做出怎样的成果，真的相当期待~</p>
<h2 id="生活侧">生活侧</h2>
<ul>
<li>运动：
<ul>
<li>把轮滑练好，学会点花样吧，每个月至少两次。</li>
<li>进行三次以上的次短途旅行，东西冲穿越可以再来一次。</li>
</ul>
</li>
<li>音乐：
<ul>
<li>再一次学习乐理&hellip;</li>
<li>midi 键盘买了一直吃灰，多多练习吧</li>
<li>买了个 Synthesizer V  Stduio Pro + 「青溯 AI」，新的一年想学下调教，翻唱些自己喜欢的歌。</li>
</ul>
</li>
<li>阅读：清单如下，一个月至少读完其中一本。
<ul>
<li>文学类：
<ul>
<li>《人间失格》：久仰大名的一本书，曾经有同学力荐，但是一直没看。</li>
<li>《生命最后的读书会》：或许曾经看过，但是一点印象都没了</li>
<li>《百年孤独》：高中的时候读过一遍，但是都忘差不多了</li>
<li>《霍乱时期的爱情》</li>
<li>《苏菲的世界》：据说是哲学启蒙读物，曾经看过，但是对内容完全没印象了。</li>
<li>《你一生的故事》：我也曾是个科幻迷</li>
<li>《沈从文的后半生》</li>
<li>《我与地坛》</li>
<li>《将饮茶》</li>
<li>《吾国与吾民 - 林语堂》</li>
<li>《房思琪的初恋乐园》</li>
</ul>
</li>
<li>人文社科
<ul>
<li>《在生命的尽头拥抱你-临终关怀医生手记》：今年想更多地了解下「死亡」</li>
<li>《怎样征服美丽少女》：哈哈</li>
<li>《爱的艺术》</li>
<li>《社会心理学》</li>
<li>《被讨厌的勇气》</li>
<li>《人体简史》</li>
<li>《科学革命的结构》</li>
<li>《邓小平时代》</li>
<li>《论中国》</li>
<li>《刘擎西方现代思想讲义》</li>
<li>《时间的秩序》</li>
<li>《极简宇宙史》</li>
<li>《圆圈正义-作为自由前提的信念》</li>
<li>《人生脚本》</li>
</ul>
</li>
<li>技术类
<ul>
<li>《复杂》</li>
<li>《SRE - Google 运维解密》</li>
<li>《凤凰项目：一个 IT 运维的传奇故事》</li>
<li>《人月神话》</li>
<li>《绩效使能：超越 OKR》</li>
<li>《奈飞文化手册》</li>
<li>《幕后产品-打造突破式思维》</li>
<li>《深入 Linux 内核架构》</li>
<li>《Linux/UNIX 系统编程手册》</li>
<li>《重构 - 改善既有代码的设计》</li>
<li>《网络是怎样连接的》：曾经学习过《计算机网络：自顶向下方法》，不过只学到网络层。就从这本书开始重新学习吧。</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="结语">结语</h2>
<p>2021 年初朋友与我给自己的期许是「拆破玉笼飞彩凤，顿开金锁走蛟龙」，感觉确实应验了。</p>
<p>今年我希望不论是在生活上还是在工作上，都能「更上层楼」~</p>
<blockquote>
<p>更多有趣的、有深度的 2021 年度总结：<a href="https://github.com/saveweb/review-2021">https://github.com/saveweb/review-2021</a></p>
</blockquote>
]]></description></item><item><title>此岸弃草，彼岸繁花</title><link>https://ryan4yin.space/posts/weeds-on-this-side-flowers-on-the-other/</link><pubDate>Sat, 28 Aug 2021 12:24:20 +0800</pubDate><author>xiaoyin_c@qq.com</author><dc:creator>ryan4yin</dc:creator><guid>https://ryan4yin.space/posts/weeds-on-this-side-flowers-on-the-other/</guid><description><![CDATA[<blockquote>
<p>「此岸弃草，彼岸繁花。」取自前永动机主唱「河津樱/白金」的个人简介</p>
</blockquote>
<p>今天想推几首歌 emmmm</p>
<!-- 庭前鹤高歌 - 河津樱 -->
<meting-js server="netease" type="song" id="1466346187" theme="#448aff"></meting-js>
<!-- 明日见黄花 - 永动机 -->
<meting-js server="netease" type="song" id="519225198" theme="#448aff"></meting-js>
<!-- 1400424646 - She Her Her Hers -->
<meting-js server="netease" type="song" id="1400424646" theme="#448aff"></meting-js>
]]></description></item><item><title>iptables 及 docker 容器网络分析</title><link>https://ryan4yin.space/posts/iptables-and-container-networks/</link><pubDate>Sun, 15 Aug 2021 19:11:29 +0800</pubDate><author>xiaoyin_c@qq.com</author><dc:creator>ryan4yin</dc:creator><guid>https://ryan4yin.space/posts/iptables-and-container-networks/</guid><description><![CDATA[<blockquote>
<p>本文仅针对 ipv4 网络</p>
</blockquote>
<p>本文先介绍 iptables 的基本概念及常用命令，然后分析 docker/podman 是如何利用 iptables 和 Linux 虚拟网络接口实现的单机容器网络。</p>
<h1 id="一iptables">一、iptables</h1>
<p><a href="https://www.netfilter.org/projects/iptables/index.html" target="_blank" rel="noopener noreferrer">iptables</a> 提供了包过滤、NAT 以及其他的包处理能力，iptables 应用最多的两个场景是 firewall 和 NAT</p>
<p>iptables 及新的 nftables 都是基于 netfilter 开发的，是 netfilter 的子项目。</p>
<p>但是 eBPF 社区目前正在开发旨在取代 netfilter 的新项目 bpfilter，他们的目标之一是兼容 iptables/nftables 规则，让我们拭目以待吧。</p>
<h2 id="iptables-基础概念---四表五链">iptables 基础概念 - 四表五链</h2>
<blockquote>
<p>实际上还有张 SELinux 相关的 security 表（应该是较新的内核新增的，但是不清楚是哪个版本加的），但是我基本没接触过，就略过了。</p>
</blockquote>
<p>详细的说明参见 <a href="https://www.zsythink.net/archives/1199" target="_blank" rel="noopener noreferrer">iptables详解（1）：iptables概念 - 朱双印</a>，这篇文章写得非常棒！把 iptables 讲清楚了。</p>
<p>默认情况下，iptables 提供了四张表（不考虑 security 的话）和五条链，数据在这四表五链中的处理流程如下图所示：</p>
<blockquote>
<p>在这里的介绍中，可以先忽略掉图中 link layer 层的链路，它属于 ebtables 的范畴。另外 <code>conntrack</code> 也暂时忽略，在下一小节会详细介绍 conntrack 的功能。</p>
</blockquote>
<p><figure><a class="lightgallery" href="/images/netfilter/netfilter-packet-flow.png" title="/images/netfilter/netfilter-packet-flow.png" data-thumbnail="/images/netfilter/netfilter-packet-flow.png" data-sub-html="<h2>netfilter 数据包处理流程，来自 wikipedia</h2>">
        
    </a><figcaption class="image-caption">netfilter 数据包处理流程，来自 wikipedia</figcaption>
    </figure></p>
<p>对照上图，对于发送到某个用户层程序的数据而言，流量顺序如下：</p>
<ul>
<li>首先进入 PREROUTING 链，依次经过这三个表： raw -&gt; mangle -&gt; nat</li>
<li>然后进入 INPUT 链，这个链上也有三个表，处理顺序是：mangle -&gt; nat -&gt; filter</li>
<li>过了 INPUT 链后，数据才会进入内核协议栈，最终到达用户层程序。</li>
</ul>
<p>用户层程序发出的报文，则依次经过这几个表：OUTPUT -&gt; POSTROUTING</p>
<p>从图中也很容易看出，如果数据 dst ip 不是本机任一接口的 ip，那它通过的几个链依次是：PREROUTEING -&gt; FORWARD -&gt; POSTROUTING</p>
<p>五链的功能和名称完全一致，应该很容易理解。下面按优先级分别介绍下链中的四个表：</p>
<ul>
<li>raw: 对收到的数据包在连接跟踪前进行处理。一般用不到，可以忽略
<ul>
<li>一旦用户使用了 RAW 表，RAW 表处理完后，将跳过 NAT 表和 ip_conntrack 处理，即不再做地址转换和数据包的链接跟踪处理了</li>
</ul>
</li>
<li>mangle: 用于修改报文、给报文打标签</li>
<li>nat: 主要用于做网络地址转换，SNAT 或者 DNAT</li>
<li>filter: 主要用于过滤数据包</li>
</ul>
<p>数据在按优先级经过四个表的处理时，一旦在某个表中匹配到一条规则 A,下一条处理规则就由规则 A 的 target 参数指定，<strong>后续的所有表</strong>都会被忽略。target 有如下几种类型：</p>
<ul>
<li>ACCEPT: 直接允许数据包通过</li>
<li>DROP: 直接丢弃数据包，对程序而言就是 100% 丢包</li>
<li>REJECT: 丢弃数据包，但是会给程序返回  RESET。这个对程序更友好，但是存在安全隐患，通常不使用。</li>
<li>MASQUERADE: （伪装）将 src ip 改写为网卡 ip，和 SNAT 的区别是它会自动读取网卡 ip。路由设备必备。</li>
<li>SNAT/DNAT: 顾名思义，做网络地址转换</li>
<li>REDIRECT: 在本机做端口映射</li>
<li>LOG: 在/var/log/messages文件中记录日志信息，然后将数据包传递给下一条规则，也就是说除了记录以外不对数据包做任何其他操作，仍然让下一条规则去匹配。
<ul>
<li>只有这个 target 特殊一些，匹配它的数据仍然可以匹配后续规则，不会直接跳过。</li>
</ul>
</li>
<li>其他类型，可以用到的时候再查</li>
</ul>
<p>理解了上面这张图，以及四个表的用途，就很容易理解 iptables 的命令了。</p>
<h2 id="常用命令">常用命令</h2>
<blockquote>
<p><strong>注意</strong>: 下面提供的 iptables 命令做的修改是未持久化的，重启就会丢失！在下一节会简单介绍持久化配置的方法。</p>
</blockquote>
<p>命令格式：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-shell" data-lang="shell">iptables <span class="o">[</span>-t table<span class="o">]</span> <span class="o">{</span>-A<span class="p">|</span>-C<span class="p">|</span>-D<span class="o">}</span> chain <span class="o">[</span>-m matchname <span class="o">[</span>per-match-options<span class="o">]]</span> -j targetname <span class="o">[</span>per-target-options<span class="o">]</span>
</code></pre></td></tr></table>
</div>
</div><p>其中 table 默认为 <code>filter</code> 表，但是感觉系统管理员实际使用最多的是 INPUT 表，用于设置防火墙。</p>
<p>以下简单介绍在 INPUT 表上添加、修改规则，来设置防火墙：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-shell" data-lang="shell"><span class="c1"># --add 允许 80 端口通过</span>
iptables -A INPUT -p tcp --dport <span class="m">80</span> -j ACCEPT

<span class="c1"># --list-rules 查看所有规则</span>
iptables -S

<span class="c1"># --list-rules 查看 INPUT 表中的所有规则</span>
iptables -S INPUT
<span class="c1"># 查看 iptables 中的所有规则（比 -L 更详细）</span>

<span class="c1"># ---delete 通过编号删除规则</span>
iptables -D <span class="m">1</span>
<span class="c1"># 或者通过完整的规则参数来删除规则</span>
iptables -D INPUT -p tcp --dport <span class="m">80</span> -j ACCEPT

<span class="c1"># --replace 通过编号来替换规则内容</span>
iptables -R INPUT <span class="m">1</span> -s 192.168.0.1 -j DROP

<span class="c1"># --insert 在指定的位置插入规则，可类比链表的插入</span>
iptables -I INPUT <span class="m">1</span> -p tcp --dport <span class="m">80</span> -j ACCEPT

<span class="c1"># 在匹配条件前面使用感叹号表示取反</span>
<span class="c1"># 如下规则表示接受所有来自 docker0，但是目标接口不是 docker0 的流量</span>
iptables -A FORWARD -i docker0 ! -o docker0 -j ACCEPT

<span class="c1"># --policy 设置某个链的默认规则</span>
<span class="c1"># 很多系统管理员会习惯将连接公网的服务器，默认规则设为 DROP，提升安全性，避免错误地开放了端口。</span>
<span class="c1"># 但是也要注意，默认规则设为 DROP 前，一定要先把允许 ssh 端口的规则加上，否则就尴尬了。</span>
iptables -P INPUT DROP

<span class="c1"># --flush 清空 INPUT 表上的所有规则</span>
iptables -F INPUT
</code></pre></td></tr></table>
</div>
</div><hr>
<blockquote>
<p>本文后续分析时，假设用户已经清楚 linux bridge、veth 等虚拟网络接口相关知识。
如果你还缺少这些前置知识，请先阅读文章 <a href="https://ryan4yin.space/posts/linux-virtual-network-interfaces/" target="_blank" rel="noopener noreferrer">Linux 中的虚拟网络接口</a>。</p>
</blockquote>
<h2 id="conntrack-连接跟踪与-nat">conntrack 连接跟踪与 NAT</h2>
<p>在讲 conntrack 之间，我们再回顾下前面给出过的 netfilter 数据处理流程图：</p>
<p><figure><a class="lightgallery" href="/images/netfilter/netfilter-packet-flow.png" title="/images/netfilter/netfilter-packet-flow.png" data-thumbnail="/images/netfilter/netfilter-packet-flow.png" data-sub-html="<h2>netfilter 数据包处理流程，来自 wikipedia</h2>">
        
    </a><figcaption class="image-caption">netfilter 数据包处理流程，来自 wikipedia</figcaption>
    </figure></p>
<p>上一节中我们忽略了图中的 conntrack，它就是本节的主角——netfilter 的连接跟踪（connection tracking）模块。</p>
<p>netfilter/conntrack 是 iptables 实现 SNAT/DNAT/MASQUERADE 的前提条件，上面的流程图显示， conntrack 在 PREROUTEING 和 OUTPUT 表的 raw 链之后生效。</p>
<p>下面以 docker 默认的 bridge 网络为例详细介绍下 conntrack 的功能。</p>
<p>首先，这是我在「Linux 的虚拟网络接口」文中给出过的 docker0 网络架构图:</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback">+-----------------------------------------------+-----------------------------------+-----------------------------------+
|                      Host                     |           Container A             |           Container B             |
|                                               |                                   |                                   |
|   +---------------------------------------+   |    +-------------------------+    |    +-------------------------+    |
|   |       Network Protocol Stack          |   |    |  Network Protocol Stack |    |    |  Network Protocol Stack |    |
|   +----+-------------+--------------------+   |    +-----------+-------------+    |    +------------+------------+    |
|        ^             ^                        |                ^                  |                 ^                 |
|........|.............|........................|................|..................|.................|.................|
|        v             v  ↓                     |                v                  |                 v                 |
|   +----+----+  +-----+------+                 |          +-----+-------+          |           +-----+-------+         |
|   | .31.101 |  | 172.17.0.1 |      +------+   |          | 172.17.0.2  |          |           |  172.17.0.3 |         |
|   +---------+  +-------------&lt;----&gt;+ veth |   |          +-------------+          |           +-------------+         |
|   |  eth0   |  |   docker0  |      +--+---+   |          | eth0(veth)  |          |           | eth0(veth)  |         |
|   +----+----+  +-----+------+         ^       |          +-----+-------+          |           +-----+-------+         |
|        ^             ^                |       |                ^                  |                 ^                 |
|        |             |                +------------------------+                  |                 |                 |
|        |             v                        |                                   |                 |                 |
|        |          +--+---+                    |                                   |                 |                 |
|        |          | veth |                    |                                   |                 |                 |
|        |          +--+---+                    |                                   |                 |                 |
|        |             ^                        |                                   |                 |                 |
|        |             +------------------------------------------------------------------------------+                 |
|        |                                      |                                   |                                   |
|        |                                      |                                   |                                   |
+-----------------------------------------------+-----------------------------------+-----------------------------------+
         v
    Physical Network  (192.168.31.0/24)
</code></pre></td></tr></table>
</div>
</div><p>docker 会在 iptables 中为 docker0 网桥添加如下规则：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-shell" data-lang="shell">-t nat -A POSTROUTING -s 172.17.0.0/16 ! -o docker0 -j MASQUERADE

-t filter -P DROP
-t filter -A FORWARD -o docker0 -m conntrack --ctstate RELATED,ESTABLISHED -j ACCEPT
</code></pre></td></tr></table>
</div>
</div><p>这几行规则使 docker 容器能正常访问外部网络。<code>MASQUERADE</code> 在请求出网时，会自动做 <code>SNAT</code>，将 src ip 替换成出口网卡的 ip.
这样数据包能正常出网，而且对端返回的数据包现在也能正常回到出口网卡。</p>
<p>现在问题就来了：<strong>出口网卡收到返回的数据包后，还能否将数据包转发到数据的初始来源端——某个 docker 容器</strong>？难道 docker 还额外添加了与 MASQUERADE 对应的 dst ip 反向转换规则？</p>
<p>实际上这一步依赖的是本节的主角——iptables 提供的 conntrack 连接跟踪功能（在「参考」中有一篇文章详细介绍了此功能）。</p>
<p>连接跟踪对 NAT 的贡献是：在做 NAT 转换时，无需手动添加额外的规则来执行<strong>反向转换</strong>以实现数据的双向传输。netfilter/conntrack 系统会记录 NAT 的连接状态，NAT 地址的反向转换是根据这个状态自动完成的。</p>
<p>比如上图中的 <code>Container A</code> 通过 bridge 网络向 baidu.com 发起了 N 个连接，这时数据的处理流程如下：</p>
<ul>
<li>首先 <code>Container A</code> 发出的数据包被 MASQUERADE 规则处理，将 src ip 替换成 eth0 的 ip，然后发送到物理网络 <code>192..168.31.0/24</code>。
<ul>
<li>conntrack 系统记录此连接被 NAT 处理前后的状态信息，并将其状态设置为 NEW，表示这是新发起的一个连接</li>
</ul>
</li>
<li>对端 baidu.com 返回数据包后，会首先到达 eth0 网卡</li>
<li>conntrack 查表，发现返回数据包的连接已经记录在表中并且状态为 NEW，于是它将连接的状态修改为 ESTABLISHED，并且将 dst_ip 改为 <code>172.17.0.2</code> 然后发送出去
<ul>
<li>注意，这个和 tcp 的 ESTABLISHED 没任何关系</li>
</ul>
</li>
<li>经过路由匹配，数据包会进入到 docker0，然后匹配上 iptables 规则：<code>-t filter -A FORWARD -o docker0 -m conntrack --ctstate RELATED,ESTABLISHED -j ACCEPT</code>，数据直接被放行</li>
<li>数据经过 veth 后，最终进入到 <code>Container A</code> 中，交由容器的内核协议栈处理。</li>
<li>数据被 <code>Container A</code> 的内核协议栈发送到「发起连接的应用程序」。</li>
</ul>
<h3 id="实际测试-conntrack">实际测试 conntrack</h3>
<p>现在我们来实际测试一下，看看是不是这么回事：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-shell" data-lang="shell"><span class="c1"># 使用 tcpdump 分别在出口网卡 wlp4s0 （相当于 eth0）和 dcoker0 网桥上抓包，后面会用来分析</span>
❯ sudo tcpdump -i wlp4s0 -n &gt; wlp4s0.dump   <span class="c1"># 窗口一，抓 wlp4s0 的包</span>
❯ sudo tcpdump -i docker0 -n &gt; docker0.dump  <span class="c1"># 窗口二，抓 docker0 的包</span>
</code></pre></td></tr></table>
</div>
</div><p>现在新建窗口三，启动一个容器，通过 curl 命令低速下载一个视频文件：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback">❯ docker run --rm --name curl -it curlimages/curl &#34;https://media.w3.org/2010/05/sintel/trailer.mp4&#34; -o /tmp/video.mp4 --limit-rate 100k
</code></pre></td></tr></table>
</div>
</div><p>然后新建窗口四，在宿主机查看 conntrack 状态</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-shell" data-lang="shell">❯ sudo zypper in conntrack-tools  <span class="c1"># 这个记得先提前安装好</span>
❯ sudo conntrack -L <span class="p">|</span> grep 172.17
<span class="c1"># curl 通过 NAT 网络发起了一个 dns 查询请求，DNS 服务器是网关上的 192.168.31.1</span>
udp      <span class="m">17</span> <span class="m">22</span> <span class="nv">src</span><span class="o">=</span>172.17.0.4 <span class="nv">dst</span><span class="o">=</span>192.168.31.1 <span class="nv">sport</span><span class="o">=</span><span class="m">59423</span> <span class="nv">dport</span><span class="o">=</span><span class="m">53</span> <span class="nv">src</span><span class="o">=</span>192.168.31.1 <span class="nv">dst</span><span class="o">=</span>192.168.31.228 <span class="nv">sport</span><span class="o">=</span><span class="m">53</span> <span class="nv">dport</span><span class="o">=</span><span class="m">59423</span> <span class="o">[</span>ASSURED<span class="o">]</span> <span class="nv">mark</span><span class="o">=</span><span class="m">0</span> <span class="nv">use</span><span class="o">=</span><span class="m">1</span>
<span class="c1"># curl 通过 NAT 网络向 media.w3.org 发起了 tcp 连接</span>
tcp      <span class="m">6</span> <span class="m">298</span> ESTABLISHED <span class="nv">src</span><span class="o">=</span>172.17.0.4 <span class="nv">dst</span><span class="o">=</span>198.18.5.130 <span class="nv">sport</span><span class="o">=</span><span class="m">54636</span> <span class="nv">dport</span><span class="o">=</span><span class="m">443</span> <span class="nv">src</span><span class="o">=</span>198.18.5.130 <span class="nv">dst</span><span class="o">=</span>192.168.31.228 <span class="nv">sport</span><span class="o">=</span><span class="m">443</span> <span class="nv">dport</span><span class="o">=</span><span class="m">54636</span> <span class="o">[</span>ASSURED<span class="o">]</span> <span class="nv">mark</span><span class="o">=</span><span class="m">0</span> <span class="nv">use</span><span class="o">=</span><span class="m">1</span>
</code></pre></td></tr></table>
</div>
</div><p>等 curl 命令跑个十来秒，然后关闭所有窗口及应用程序，接下来进行数据分析：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-shell" data-lang="shell"><span class="c1"># 前面查到的，本地发起请求的端口是 54636，下面以此为过滤条件查询数据</span>

<span class="c1"># 首先查询 wlp4s0/eth0 进来的数据，可以看到本机的 dst_ip 为 192.168.31.228.54636</span>
❯ cat wlp4s0.dump <span class="p">|</span> grep <span class="m">54636</span> <span class="p">|</span> head -n <span class="m">15</span>
18:28:28.349321 IP 192.168.31.228.54636 &gt; 198.18.5.130.443: Flags <span class="o">[</span>S<span class="o">]</span>, seq 750859357, win 64240, options <span class="o">[</span>mss 1460,sackOK,TS val <span class="m">3365688110</span> ecr 0,nop,wscale 7<span class="o">]</span>, length <span class="m">0</span>
18:28:28.350757 IP 198.18.5.130.443 &gt; 192.168.31.228.54636: Flags <span class="o">[</span>S.<span class="o">]</span>, seq 2381759932, ack 750859358, win 28960, options <span class="o">[</span>mss 1460,sackOK,TS val <span class="m">22099541</span> ecr 3365688110,nop,wscale 5<span class="o">]</span>, length <span class="m">0</span>
18:28:28.350814 IP 192.168.31.228.54636 &gt; 198.18.5.130.443: Flags <span class="o">[</span>.<span class="o">]</span>, ack 1, win 502, options <span class="o">[</span>nop,nop,TS val <span class="m">3365688111</span> ecr 22099541<span class="o">]</span>, length <span class="m">0</span>
18:28:28.357345 IP 192.168.31.228.54636 &gt; 198.18.5.130.443: Flags <span class="o">[</span>P.<span class="o">]</span>, seq 1:518, ack 1, win 502, options <span class="o">[</span>nop,nop,TS val <span class="m">3365688118</span> ecr 22099541<span class="o">]</span>, length <span class="m">517</span>
18:28:28.359253 IP 198.18.5.130.443 &gt; 192.168.31.228.54636: Flags <span class="o">[</span>.<span class="o">]</span>, ack 518, win 939, options <span class="o">[</span>nop,nop,TS val <span class="m">22099542</span> ecr 3365688118<span class="o">]</span>, length <span class="m">0</span>
18:28:28.726544 IP 198.18.5.130.443 &gt; 192.168.31.228.54636: Flags <span class="o">[</span>P.<span class="o">]</span>, seq 1:2622, ack 518, win 939, options <span class="o">[</span>nop,nop,TS val <span class="m">22099579</span> ecr 3365688118<span class="o">]</span>, length <span class="m">2621</span>
18:28:28.726616 IP 192.168.31.228.54636 &gt; 198.18.5.130.443: Flags <span class="o">[</span>.<span class="o">]</span>, ack 2622, win 482, options <span class="o">[</span>nop,nop,TS val <span class="m">3365688487</span> ecr 22099579<span class="o">]</span>, length <span class="m">0</span>
18:28:28.727652 IP 192.168.31.228.54636 &gt; 198.18.5.130.443: Flags <span class="o">[</span>P.<span class="o">]</span>, seq 518:598, ack 2622, win 501, options <span class="o">[</span>nop,nop,TS val <span class="m">3365688488</span> ecr 22099579<span class="o">]</span>, length <span class="m">80</span>
18:28:28.727803 IP 192.168.31.228.54636 &gt; 198.18.5.130.443: Flags <span class="o">[</span>P.<span class="o">]</span>, seq 598:644, ack 2622, win 501, options <span class="o">[</span>nop,nop,TS val <span class="m">3365688488</span> ecr 22099579<span class="o">]</span>, length <span class="m">46</span>
18:28:28.727828 IP 192.168.31.228.54636 &gt; 198.18.5.130.443: Flags <span class="o">[</span>P.<span class="o">]</span>, seq 644:693, ack 2622, win 501, options <span class="o">[</span>nop,nop,TS val <span class="m">3365688488</span> ecr 22099579<span class="o">]</span>, length <span class="m">49</span>
18:28:28.727850 IP 192.168.31.228.54636 &gt; 198.18.5.130.443: Flags <span class="o">[</span>P.<span class="o">]</span>, seq 693:728, ack 2622, win 501, options <span class="o">[</span>nop,nop,TS val <span class="m">3365688488</span> ecr 22099579<span class="o">]</span>, length <span class="m">35</span>
18:28:28.727875 IP 192.168.31.228.54636 &gt; 198.18.5.130.443: Flags <span class="o">[</span>P.<span class="o">]</span>, seq 728:812, ack 2622, win 501, options <span class="o">[</span>nop,nop,TS val <span class="m">3365688488</span> ecr 22099579<span class="o">]</span>, length <span class="m">84</span>
18:28:28.729241 IP 198.18.5.130.443 &gt; 192.168.31.228.54636: Flags <span class="o">[</span>.<span class="o">]</span>, ack 598, win 939, options <span class="o">[</span>nop,nop,TS val <span class="m">22099579</span> ecr 3365688488<span class="o">]</span>, length <span class="m">0</span>
18:28:28.729245 IP 198.18.5.130.443 &gt; 192.168.31.228.54636: Flags <span class="o">[</span>.<span class="o">]</span>, ack 644, win 939, options <span class="o">[</span>nop,nop,TS val <span class="m">22099579</span> ecr 3365688488<span class="o">]</span>, length <span class="m">0</span>
18:28:28.729247 IP 198.18.5.130.443 &gt; 192.168.31.228.54636: Flags <span class="o">[</span>.<span class="o">]</span>, ack 693, win 939, options <span class="o">[</span>nop,nop,TS val <span class="m">22099579</span> ecr 3365688488<span class="o">]</span>, length <span class="m">0</span>


<span class="c1"># 然后再查询 docker0 上的数据，能发现本地的地址为 172.17.0.4.54636</span>
❯ cat docker0.dump <span class="p">|</span> grep <span class="m">54636</span> <span class="p">|</span> head -n <span class="m">20</span>
18:28:28.349299 IP 172.17.0.4.54636 &gt; 198.18.5.130.443: Flags <span class="o">[</span>S<span class="o">]</span>, seq 750859357, win 64240, options <span class="o">[</span>mss 1460,sackOK,TS val <span class="m">3365688110</span> ecr 0,nop,wscale 7<span class="o">]</span>, length <span class="m">0</span>
18:28:28.350780 IP 198.18.5.130.443 &gt; 172.17.0.4.54636: Flags <span class="o">[</span>S.<span class="o">]</span>, seq 2381759932, ack 750859358, win 28960, options <span class="o">[</span>mss 1460,sackOK,TS val <span class="m">22099541</span> ecr 3365688110,nop,wscale 5<span class="o">]</span>, length <span class="m">0</span>
18:28:28.350812 IP 172.17.0.4.54636 &gt; 198.18.5.130.443: Flags <span class="o">[</span>.<span class="o">]</span>, ack 1, win 502, options <span class="o">[</span>nop,nop,TS val <span class="m">3365688111</span> ecr 22099541<span class="o">]</span>, length <span class="m">0</span>
18:28:28.357328 IP 172.17.0.4.54636 &gt; 198.18.5.130.443: Flags <span class="o">[</span>P.<span class="o">]</span>, seq 1:518, ack 1, win 502, options <span class="o">[</span>nop,nop,TS val <span class="m">3365688118</span> ecr 22099541<span class="o">]</span>, length <span class="m">517</span>
18:28:28.359281 IP 198.18.5.130.443 &gt; 172.17.0.4.54636: Flags <span class="o">[</span>.<span class="o">]</span>, ack 518, win 939, options <span class="o">[</span>nop,nop,TS val <span class="m">22099542</span> ecr 3365688118<span class="o">]</span>, length <span class="m">0</span>
18:28:28.726578 IP 198.18.5.130.443 &gt; 172.17.0.4.54636: Flags <span class="o">[</span>P.<span class="o">]</span>, seq 1:2622, ack 518, win 939, options <span class="o">[</span>nop,nop,TS val <span class="m">22099579</span> ecr 3365688118<span class="o">]</span>, length <span class="m">2621</span>
18:28:28.726610 IP 172.17.0.4.54636 &gt; 198.18.5.130.443: Flags <span class="o">[</span>.<span class="o">]</span>, ack 2622, win 482, options <span class="o">[</span>nop,nop,TS val <span class="m">3365688487</span> ecr 22099579<span class="o">]</span>, length <span class="m">0</span>
18:28:28.727633 IP 172.17.0.4.54636 &gt; 198.18.5.130.443: Flags <span class="o">[</span>P.<span class="o">]</span>, seq 518:598, ack 2622, win 501, options <span class="o">[</span>nop,nop,TS val <span class="m">3365688488</span> ecr 22099579<span class="o">]</span>, length <span class="m">80</span>
18:28:28.727798 IP 172.17.0.4.54636 &gt; 198.18.5.130.443: Flags <span class="o">[</span>P.<span class="o">]</span>, seq 598:644, ack 2622, win 501, options <span class="o">[</span>nop,nop,TS val <span class="m">3365688488</span> ecr 22099579<span class="o">]</span>, length <span class="m">46</span>
18:28:28.727825 IP 172.17.0.4.54636 &gt; 198.18.5.130.443: Flags <span class="o">[</span>P.<span class="o">]</span>, seq 644:693, ack 2622, win 501, options <span class="o">[</span>nop,nop,TS val <span class="m">3365688488</span> ecr 22099579<span class="o">]</span>, length <span class="m">49</span>
18:28:28.727847 IP 172.17.0.4.54636 &gt; 198.18.5.130.443: Flags <span class="o">[</span>P.<span class="o">]</span>, seq 693:728, ack 2622, win 501, options <span class="o">[</span>nop,nop,TS val <span class="m">3365688488</span> ecr 22099579<span class="o">]</span>, length <span class="m">35</span>
18:28:28.727871 IP 172.17.0.4.54636 &gt; 198.18.5.130.443: Flags <span class="o">[</span>P.<span class="o">]</span>, seq 728:812, ack 2622, win 501, options <span class="o">[</span>nop,nop,TS val <span class="m">3365688488</span> ecr 22099579<span class="o">]</span>, length <span class="m">84</span>
18:28:28.729308 IP 198.18.5.130.443 &gt; 172.17.0.4.54636: Flags <span class="o">[</span>.<span class="o">]</span>, ack 598, win 939, options <span class="o">[</span>nop,nop,TS val <span class="m">22099579</span> ecr 3365688488<span class="o">]</span>, length <span class="m">0</span>
18:28:28.729324 IP 198.18.5.130.443 &gt; 172.17.0.4.54636: Flags <span class="o">[</span>.<span class="o">]</span>, ack 644, win 939, options <span class="o">[</span>nop,nop,TS val <span class="m">22099579</span> ecr 3365688488<span class="o">]</span>, length <span class="m">0</span>
18:28:28.729328 IP 198.18.5.130.443 &gt; 172.17.0.4.54636: Flags <span class="o">[</span>.<span class="o">]</span>, ack 693, win 939, options <span class="o">[</span>nop,nop,TS val <span class="m">22099579</span> ecr 3365688488<span class="o">]</span>, length <span class="m">0</span>
</code></pre></td></tr></table>
</div>
</div><p>能看到数据确实在进入 docker0 网桥前，dst_ip 确实被从 <code>192.168.31.228</code>（wlp4s0 的 ip）被修改为了 <code>172.17.0.4</code>（<code>Container A</code> 的 ip）.</p>
<h3 id="nat-如何分配端口">NAT 如何分配端口？</h3>
<p>上一节我们实际测试发现，docker 容器的流量在经过 iptables 的 MASQUERADE 规则处理后，只有 src ip 被修改了，而 port 仍然是一致的。</p>
<p>但是如果 NAT 不修改连接的端口，实际上是会有问题的。如果有两个容器同时向 <code>ip: 198.18.5.130, port: 443</code> 发起请求，又恰好使用了同一个 src port，在宿主机上就会出现端口冲突！
因为这两个请求被 SNAT 时，如果只修改 src ip，那它们映射到的将是主机上的同一个连接！</p>
<p>这个问题 NAT 是如何解决的呢？我想如果遇到这种情况，NAT 应该会通过一定的规则选用一个不同的端口。</p>
<p>有空可以翻一波源码看看这个，待续&hellip;</p>
<h2 id="如何持久化-iptables-配置">如何持久化 iptables 配置</h2>
<p>首先需要注意的是，centos7/opensuse 15 都已经切换到了 firewalld 作为防火墙配置软件，
而 ubuntu18.04 lts 也换成了 ufw 来配置防火墙。</p>
<p>包括 docker 应该也是在启动的时候动态添加 iptables 配置。</p>
<p>对于上述新系统，还是建议直接使用 firewalld/ufw 配置防火墙吧，或者网上搜下关闭 ufw/firewalld、启用 iptables 持久化的解决方案。</p>
<p>本文主要目的在于理解 docker 容器网络的原理，以及为后面理解 kubernetes 网络插件 calico/flannel 打好基础，因此就不多介绍持久化了。</p>
<h2 id="如何使用-iptables--bridge--veth-实现容器网络">如何使用 iptables + bridge + veth 实现容器网络</h2>
<p>Docker/Podman 默认使用的都是 bridge 网络，它们的底层实现完全类似。下面以 docker 为例进行分析（Podman 的分析流程也基本一样）。</p>
<h3 id="通过-docker-run-运行容器">通过 docker run 运行容器</h3>
<p>首先，使用 <code>docker run</code> 运行几个容器，检查下网络状况：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span><span class="lnt">43
</span><span class="lnt">44
</span><span class="lnt">45
</span><span class="lnt">46
</span><span class="lnt">47
</span><span class="lnt">48
</span><span class="lnt">49
</span><span class="lnt">50
</span><span class="lnt">51
</span><span class="lnt">52
</span><span class="lnt">53
</span><span class="lnt">54
</span><span class="lnt">55
</span><span class="lnt">56
</span><span class="lnt">57
</span><span class="lnt">58
</span><span class="lnt">59
</span><span class="lnt">60
</span><span class="lnt">61
</span><span class="lnt">62
</span><span class="lnt">63
</span><span class="lnt">64
</span><span class="lnt">65
</span><span class="lnt">66
</span><span class="lnt">67
</span><span class="lnt">68
</span><span class="lnt">69
</span><span class="lnt">70
</span><span class="lnt">71
</span><span class="lnt">72
</span><span class="lnt">73
</span><span class="lnt">74
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-shell" data-lang="shell"><span class="c1"># 运行一个 debian 容器和一个 nginx</span>
❯ docker run -dit --name debian --rm debian:buster sleep <span class="m">1000000</span>
❯ docker run -dit --name nginx --rm nginx:1.19-alpine 

<span class="c1">#　查看网络接口，有两个 veth 接口（而且都没设 ip 地址），分别连接到两个容器的 eth0（dcoker0 网络架构图前面给过了，可以往前面翻翻对照下）</span>
❯ ip addr ls
...
5: docker0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu <span class="m">1500</span> qdisc noqueue state UP group default 
    link/ether 02:42:42:c7:12:ba brd ff:ff:ff:ff:ff:ff
    inet 172.17.0.1/16 brd 172.17.255.255 scope global docker0
       valid_lft forever preferred_lft forever
    inet6 fe80::42:42ff:fec7:12ba/64 scope link 
       valid_lft forever preferred_lft forever
100: veth16b37ea@if99: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu <span class="m">1500</span> qdisc noqueue master docker0 state UP group default 
    link/ether 42:af:34:ae:74:ae brd ff:ff:ff:ff:ff:ff link-netnsid <span class="m">0</span>
    inet6 fe80::40af:34ff:feae:74ae/64 scope link 
       valid_lft forever preferred_lft forever
102: veth4b4dada@if101: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu <span class="m">1500</span> qdisc noqueue master docker0 state UP group default 
    link/ether 9e:f1:58:1a:cf:ae brd ff:ff:ff:ff:ff:ff link-netnsid <span class="m">1</span>
    inet6 fe80::9cf1:58ff:fe1a:cfae/64 scope link 
       valid_lft forever preferred_lft forever

<span class="c1"># 两个 veth 接口都连接到了 docker0 上面，说明两个容器都使用了 docker 默认的 bridge 网络</span>
❯ sudo brctl show
bridge name     bridge id               STP enabled     interfaces
docker0         8000.024242c712ba       no              veth16b37ea
                                                        veth4b4dada

<span class="c1"># 查看路由规则</span>
❯ ip route ls
default via 192.168.31.1 dev wlp4s0 proto dhcp metric <span class="m">600</span>
<span class="c1">#下列路由规则将 `172.17.0.0/16` 网段的所有流量转发到 docker0</span>
172.17.0.0/16 dev docker0 proto kernel scope link src 172.17.0.1 linkdown 
192.168.31.0/24 dev wlp4s0 proto kernel scope link src 192.168.31.228 metric <span class="m">600</span> 

<span class="c1"># 查看　iptables 规则</span>
<span class="c1"># NAT 表</span>
❯ sudo iptables -t nat -S
-P PREROUTING ACCEPT
-P INPUT ACCEPT
-P OUTPUT ACCEPT
-P POSTROUTING ACCEPT
-N DOCKER
<span class="c1"># 所有目的地址在本机的，都先交给 DOCKER 链处理一波</span>
-A PREROUTING -m addrtype --dst-type LOCAL -j DOCKER
-A OUTPUT ! -d 127.0.0.0/8 -m addrtype --dst-type LOCAL -j DOCKER
<span class="c1"># （容器访问外部网络）所有出口不为 docker0 的流量，都做下 SNAT，把 src ip 换成出口接口的 ip 地址</span>
-A POSTROUTING -s 172.17.0.0/16 ! -o docker0 -j MASQUERADE
-A DOCKER -i docker0 -j RETURN

<span class="c1"># filter 表</span>
❯ sudo iptables -t filter -S
-P INPUT ACCEPT
-P FORWARD DROP
-P OUTPUT ACCEPT
-N DOCKER
-N DOCKER-ISOLATION-STAGE-1
-N DOCKER-ISOLATION-STAGE-2
-N DOCKER-USER
<span class="c1"># 所有流量都必须先经过如下两个表处理，没问题才能继续往下走</span>
-A FORWARD -j DOCKER-ISOLATION-STAGE-1
-A FORWARD -j DOCKER-USER
<span class="c1"># （容器访问外部网络）出去的流量走了 MASQUERADE，回来的流量会被 conntrack 识别并转发回来，这里允许返回的数据包通过。</span>
<span class="c1"># 这里直接 ACCEPT 被 conntrack 识别到的流量</span>
-A FORWARD -o docker0 -m conntrack --ctstate RELATED,ESTABLISHED -j ACCEPT
<span class="c1"># 将所有访问 docker0 的流量都转给自定义链 DOCKER 处理</span>
-A FORWARD -o docker0 -j DOCKER
<span class="c1"># 允许所有来自 docker0 的流量通过，不论下一跳是否是 docker0</span>
-A FORWARD -i docker0 ! -o docker0 -j ACCEPT
-A FORWARD -i docker0 -o docker0 -j ACCEPT
<span class="c1"># 下面三个表目前啥规则也没有，就是简单的 RETURN，交给后面的表继续处理</span>
-A DOCKER-ISOLATION-STAGE-1 -j RETURN
-A DOCKER-ISOLATION-STAGE-2 -j RETURN
-A DOCKER-USER -j RETURN
</code></pre></td></tr></table>
</div>
</div><p>接下来使用如下 docker-compose 配置启动一个 caddy　容器，添加自定义 network 和端口映射，待会就能验证 docker 是如何实现这两种网络的了。</p>
<p><code>docker-compose.yml</code> 内容：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-yaml" data-lang="yaml"><span class="nt">version</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;3.3&#34;</span><span class="w">
</span><span class="w"></span><span class="nt">services</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">caddy</span><span class="p">:</span><span class="w">
</span><span class="w">    </span><span class="nt">image</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;caddy:2.2.1-alpine&#34;</span><span class="w">
</span><span class="w">    </span><span class="nt">container_name</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;caddy&#34;</span><span class="w">
</span><span class="w">    </span><span class="nt">restart</span><span class="p">:</span><span class="w"> </span><span class="l">always</span><span class="w">
</span><span class="w">    </span><span class="nt">command</span><span class="p">:</span><span class="w"> </span><span class="l">caddy file-server --browse --root /data/static</span><span class="w">
</span><span class="w">    </span><span class="nt">ports</span><span class="p">:</span><span class="w">
</span><span class="w">      </span>- <span class="s2">&#34;8081:80&#34;</span><span class="w">
</span><span class="w">    </span><span class="nt">volumes</span><span class="p">:</span><span class="w">
</span><span class="w">      </span>- <span class="s2">&#34;/home/ryan/Downloads:/data/static&#34;</span><span class="w">
</span><span class="w">    </span><span class="nt">networks</span><span class="p">:</span><span class="w">
</span><span class="w">    </span>- <span class="l">caddy-1</span><span class="w">
</span><span class="w">
</span><span class="w"></span><span class="nt">networks</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">caddy-1</span><span class="p">:</span><span class="w">
</span></code></pre></td></tr></table>
</div>
</div><p>现在先用上面的配置启动 caddy 容器，然后再查看网络状况：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">  1
</span><span class="lnt">  2
</span><span class="lnt">  3
</span><span class="lnt">  4
</span><span class="lnt">  5
</span><span class="lnt">  6
</span><span class="lnt">  7
</span><span class="lnt">  8
</span><span class="lnt">  9
</span><span class="lnt"> 10
</span><span class="lnt"> 11
</span><span class="lnt"> 12
</span><span class="lnt"> 13
</span><span class="lnt"> 14
</span><span class="lnt"> 15
</span><span class="lnt"> 16
</span><span class="lnt"> 17
</span><span class="lnt"> 18
</span><span class="lnt"> 19
</span><span class="lnt"> 20
</span><span class="lnt"> 21
</span><span class="lnt"> 22
</span><span class="lnt"> 23
</span><span class="lnt"> 24
</span><span class="lnt"> 25
</span><span class="lnt"> 26
</span><span class="lnt"> 27
</span><span class="lnt"> 28
</span><span class="lnt"> 29
</span><span class="lnt"> 30
</span><span class="lnt"> 31
</span><span class="lnt"> 32
</span><span class="lnt"> 33
</span><span class="lnt"> 34
</span><span class="lnt"> 35
</span><span class="lnt"> 36
</span><span class="lnt"> 37
</span><span class="lnt"> 38
</span><span class="lnt"> 39
</span><span class="lnt"> 40
</span><span class="lnt"> 41
</span><span class="lnt"> 42
</span><span class="lnt"> 43
</span><span class="lnt"> 44
</span><span class="lnt"> 45
</span><span class="lnt"> 46
</span><span class="lnt"> 47
</span><span class="lnt"> 48
</span><span class="lnt"> 49
</span><span class="lnt"> 50
</span><span class="lnt"> 51
</span><span class="lnt"> 52
</span><span class="lnt"> 53
</span><span class="lnt"> 54
</span><span class="lnt"> 55
</span><span class="lnt"> 56
</span><span class="lnt"> 57
</span><span class="lnt"> 58
</span><span class="lnt"> 59
</span><span class="lnt"> 60
</span><span class="lnt"> 61
</span><span class="lnt"> 62
</span><span class="lnt"> 63
</span><span class="lnt"> 64
</span><span class="lnt"> 65
</span><span class="lnt"> 66
</span><span class="lnt"> 67
</span><span class="lnt"> 68
</span><span class="lnt"> 69
</span><span class="lnt"> 70
</span><span class="lnt"> 71
</span><span class="lnt"> 72
</span><span class="lnt"> 73
</span><span class="lnt"> 74
</span><span class="lnt"> 75
</span><span class="lnt"> 76
</span><span class="lnt"> 77
</span><span class="lnt"> 78
</span><span class="lnt"> 79
</span><span class="lnt"> 80
</span><span class="lnt"> 81
</span><span class="lnt"> 82
</span><span class="lnt"> 83
</span><span class="lnt"> 84
</span><span class="lnt"> 85
</span><span class="lnt"> 86
</span><span class="lnt"> 87
</span><span class="lnt"> 88
</span><span class="lnt"> 89
</span><span class="lnt"> 90
</span><span class="lnt"> 91
</span><span class="lnt"> 92
</span><span class="lnt"> 93
</span><span class="lnt"> 94
</span><span class="lnt"> 95
</span><span class="lnt"> 96
</span><span class="lnt"> 97
</span><span class="lnt"> 98
</span><span class="lnt"> 99
</span><span class="lnt">100
</span><span class="lnt">101
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-shell" data-lang="shell"><span class="c1"># 启动 caddy</span>
❯ docker-compose up -d
<span class="c1"># 查下 caddy 容器的 ip</span>
&gt; docker inspect caddy <span class="p">|</span> grep IPAddress
...
    <span class="s2">&#34;IPAddress&#34;</span>: <span class="s2">&#34;172.18.0.2&#34;</span>,

<span class="c1"># 查看网络接口，可以看到多了一个网桥，它就是上一行命令创建的 caddy-1 网络</span>
<span class="c1"># 还多了一个 veth，它连接到了 caddy 容器的 eth0(veth) 接口</span>
❯ ip addr ls
...
5: docker0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu <span class="m">1500</span> qdisc noqueue state UP group default 
    link/ether 02:42:42:c7:12:ba brd ff:ff:ff:ff:ff:ff
    inet 172.17.0.1/16 brd 172.17.255.255 scope global docker0
       valid_lft forever preferred_lft forever
    inet6 fe80::42:42ff:fec7:12ba/64 scope link 
       valid_lft forever preferred_lft forever
100: veth16b37ea@if99: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu <span class="m">1500</span> qdisc noqueue master docker0 state UP group default 
    link/ether 42:af:34:ae:74:ae brd ff:ff:ff:ff:ff:ff link-netnsid <span class="m">0</span>
    inet6 fe80::40af:34ff:feae:74ae/64 scope link 
       valid_lft forever preferred_lft forever
102: veth4b4dada@if101: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu <span class="m">1500</span> qdisc noqueue master docker0 state UP group default 
    link/ether 9e:f1:58:1a:cf:ae brd ff:ff:ff:ff:ff:ff link-netnsid <span class="m">1</span>
    inet6 fe80::9cf1:58ff:fe1a:cfae/64 scope link 
       valid_lft forever preferred_lft forever
103: br-ac3e0514d837: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu <span class="m">1500</span> qdisc noqueue state UP group default 
    link/ether 02:42:7d:95:ba:7e brd ff:ff:ff:ff:ff:ff
    inet 172.18.0.1/16 brd 172.18.255.255 scope global br-ac3e0514d837
       valid_lft forever preferred_lft forever
    inet6 fe80::42:7dff:fe95:ba7e/64 scope link 
       valid_lft forever preferred_lft forever
105: veth0c25c6f@if104: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu <span class="m">1500</span> qdisc noqueue master br-ac3e0514d837 state UP group default 
    link/ether 9a:03:e1:f0:26:ea brd ff:ff:ff:ff:ff:ff link-netnsid <span class="m">2</span>
    inet6 fe80::9803:e1ff:fef0:26ea/64 scope link 
       valid_lft forever preferred_lft forever


<span class="c1"># 查看网桥，能看到 caddy 容器的 veth 接口连在了 caddy-1 这个网桥上，没有加入到 docker0 网络</span>
❯ sudo brctl show
bridge name     bridge id               STP enabled     interfaces
br-ac3e0514d837         8000.02427d95ba7e       no              veth0c25c6f
docker0         8000.024242c712ba       no              veth16b37ea
                                                        veth4b4dada

<span class="c1"># 查看路由，能看到新网桥使用的地址段是 172.18.0.0/16，是 docker0 递增上来的 </span>
❯ ip route ls
default via 192.168.31.1 dev wlp4s0 proto dhcp metric <span class="m">600</span> 
172.17.0.0/16 dev docker0 proto kernel scope link src 172.17.0.1 
<span class="c1"># 多了一个网桥的</span>
172.18.0.0/16 dev br-ac3e0514d837 proto kernel scope link src 172.18.0.1 
192.168.31.0/24 dev wlp4s0 proto kernel scope link src 192.168.31.228 metric <span class="m">600</span> 

<span class="c1"># iptables 中也多了 caddy-1 网桥的 MASQUERADE 规则，以及端口映射的规则</span>
❯ sudo iptables -t nat -S
-P PREROUTING ACCEPT
-P INPUT ACCEPT
-P OUTPUT ACCEPT
-P POSTROUTING ACCEPT
-N DOCKER
-A PREROUTING -m addrtype --dst-type LOCAL -j DOCKER
-A OUTPUT ! -d 127.0.0.0/8 -m addrtype --dst-type LOCAL -j DOCKER
-A POSTROUTING -s 172.18.0.0/16 ! -o br-ac3e0514d837 -j MASQUERADE
-A POSTROUTING -s 172.17.0.0/16 ! -o docker0 -j MASQUERADE
<span class="c1"># 端口映射过来的入网流量，都做下 SNAT，把 src ip 换成出口 docker0 的 ip 地址</span>
-A POSTROUTING -s 172.18.0.2/32 -d 172.18.0.2/32 -p tcp -m tcp --dport <span class="m">80</span> -j MASQUERADE
-A DOCKER -i br-ac3e0514d837 -j RETURN
-A DOCKER -i docker0 -j RETURN
<span class="c1"># 主机上所有其他接口进来的 tcp 流量，只要目标端口是 8081，就转发到 caddy 容器去（端口映射）</span>
<span class="c1"># DOCKER 是被 PREROUTEING 链的 target，因此这会导致流量直接走了 FORWARD 链，直接绕过了通常设置在 INPUT 链的主机防火墙规则！</span>
-A DOCKER ! -i br-ac3e0514d837 -p tcp -m tcp --dport <span class="m">8081</span> -j DNAT --to-destination 172.18.0.2:80

❯ sudo iptables -t filter -S
-P INPUT ACCEPT
-P FORWARD DROP
-P OUTPUT ACCEPT
-N DOCKER
-N DOCKER-ISOLATION-STAGE-1
-N DOCKER-ISOLATION-STAGE-2
-N DOCKER-USER
-A FORWARD -j DOCKER-USER
-A FORWARD -j DOCKER-ISOLATION-STAGE-1
<span class="c1"># 给 caddy-1 bridge 网络添加的转发规则，与 docker0 的规则完全一一对应，就不多介绍了。</span>
-A FORWARD -o br-ac3e0514d837 -m conntrack --ctstate RELATED,ESTABLISHED -j ACCEPT
-A FORWARD -o br-ac3e0514d837 -j DOCKER
-A FORWARD -i br-ac3e0514d837 ! -o br-ac3e0514d837 -j ACCEPT
-A FORWARD -i br-ac3e0514d837 -o br-ac3e0514d837 -j ACCEPT
-A FORWARD -o docker0 -m conntrack --ctstate RELATED,ESTABLISHED -j ACCEPT
-A FORWARD -o docker0 -j DOCKER
-A FORWARD -i docker0 ! -o docker0 -j ACCEPT
-A FORWARD -i docker0 -o docker0 -j ACCEPT
<span class="c1"># 这一条仍然是端口映射相关的规则，接受所有从其他接口过来的，请求 80 端口且出口是 caddy-1 网桥的流量</span>
-A DOCKER -d 172.18.0.2/32 ! -i br-ac3e0514d837 -o br-ac3e0514d837 -p tcp -m tcp --dport <span class="m">80</span> -j ACCEPT
<span class="c1"># 当存在多个 bridge 网络的时候，docker 就会在下面两个 STAGE 链中处理将它们隔离开，禁止互相访问</span>
-A DOCKER-ISOLATION-STAGE-1 -i br-ac3e0514d837 ! -o br-ac3e0514d837 -j DOCKER-ISOLATION-STAGE-2
-A DOCKER-ISOLATION-STAGE-1 -i docker0 ! -o docker0 -j DOCKER-ISOLATION-STAGE-2
-A DOCKER-ISOLATION-STAGE-1 -j RETURN
<span class="c1"># 这里延续上面 STAGE-1 的处理，彻底隔离两个网桥的流量</span>
-A DOCKER-ISOLATION-STAGE-2 -o br-ac3e0514d837 -j DROP
-A DOCKER-ISOLATION-STAGE-2 -o docker0 -j DROP
-A DOCKER-ISOLATION-STAGE-2 -j RETURN
-A DOCKER-USER -j RETURN
</code></pre></td></tr></table>
</div>
</div><p>到这里，我们简单地分析了下 docker 如何通过 iptables 实现 bridge 网络和端口映射。
有了这个基础，后面就可以尝试深入分析 kubernetes 网络插件 flannel/calico/cilium 了哈哈。</p>
<h2 id="dockerpodman-的-macvlanipvlan-模式">Docker/Podman 的 macvlan/ipvlan 模式</h2>
<blockquote>
<p>注意：macvlan 和 wifi 好像不兼容，测试时不要使用无线网络的接口！</p>
</blockquote>
<p>我在前面介绍 Linux 虚拟网络接口的文章中，有介绍过 macvlan 和 ipvlan 两种新的虚拟接口。</p>
<p>目前 Podman/Docker 都支持使用 macvlan 来构建容器网络，这种模式下创建的容器直连外部网络，容器可以拥有独立的外部 IP，不需要端口映射，也不需要借助 iptables.</p>
<p>这和虚拟机的 Bridge 模式就很类似，主要适用于希望容器拥有独立外部 IP 的情况。</p>
<p>下面详细分析下 Docker 的 macvlan 网络（Podman 应该也完全类似）。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-shell" data-lang="shell"><span class="c1"># 首先创建一个 macvlan 网络</span>
<span class="c1"># subnet/gateway 的参数需要和物理网络一致</span>
<span class="c1"># 通过 -o parent 设定父接口，我本机的以太网口名称为 eno1</span>
$ docker network create -d macvlan <span class="se">\
</span><span class="se"></span>  --subnet<span class="o">=</span>192.168.31.0/24 <span class="se">\
</span><span class="se"></span>  --gateway<span class="o">=</span>192.168.31.1 <span class="se">\
</span><span class="se"></span>  -o <span class="nv">parent</span><span class="o">=</span>eno1 <span class="se">\
</span><span class="se"></span>  macnet0

<span class="c1"># 现在使用 macvlan 启动一个容器试试</span>
<span class="c1"># 建议和我一样，通过 --ip 手动配置静态 ip 地址，当然不配也可以，DHCP 会自动分配 IP</span>
$ docker run --network macnet0 --ip<span class="o">=</span>192.168.31.233 --rm -it buildpack-deps:buster-curl /bin/bash
<span class="c1"># 在容器中查看网络接口状况，能看到 eth0 是一个 macvlan 接口</span>
root@4319488cb5e7:/# ip -d addr ls
1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu <span class="m">65536</span> qdisc noqueue state UNKNOWN group default qlen <span class="m">1000</span>
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 promiscuity <span class="m">0</span> minmtu <span class="m">0</span> maxmtu <span class="m">0</span> numtxqueues <span class="m">1</span> numrxqueues <span class="m">1</span> gso_max_size <span class="m">65536</span> gso_max_segs <span class="m">65535</span> 
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
8: eth0@if2: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu <span class="m">1500</span> qdisc noqueue state UP group default 
    link/ether 02:42:c0:a8:1f:e9 brd ff:ff:ff:ff:ff:ff link-netnsid <span class="m">0</span> promiscuity <span class="m">0</span> minmtu <span class="m">68</span> maxmtu <span class="m">9194</span> 
    macvlan mode bridge numtxqueues <span class="m">1</span> numrxqueues <span class="m">1</span> gso_max_size <span class="m">64000</span> gso_max_segs <span class="m">64</span> 
    inet 192.168.31.233/24 brd 192.168.31.255 scope global eth0
       valid_lft forever preferred_lft forever
<span class="c1"># 路由表，默认 gateway 被自动配置进来了</span>
root@4319488cb5e7:/# ip route ls
default via 192.168.31.1 dev eth0 
192.168.31.0/24 dev eth0 proto kernel scope link src 192.168.31.233 

<span class="c1"># 可以正常访问 baidu</span>
root@4319488cb5e7:/# curl baidu.com
&lt;html&gt;
&lt;meta http-equiv<span class="o">=</span><span class="s2">&#34;refresh&#34;</span> <span class="nv">content</span><span class="o">=</span><span class="s2">&#34;0;url=http://www.baidu.com/&#34;</span>&gt;
&lt;/html&gt;
</code></pre></td></tr></table>
</div>
</div><p>Docker 支持的另一种网络模式是 ipvlan（ipvlan 和 macvlan 的区别我在前一篇文章中已经介绍过，不再赘言），创建命令和 macvlan 几乎一样：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-shell" data-lang="shell"><span class="c1"># 首先创建一个 macvlan 网络</span>
<span class="c1"># subnet/gateway 的参数需要和物理网络一致</span>
<span class="c1"># 通过 -o parent 设定父接口，我本机的以太网口名称为 eno1</span>
<span class="c1"># ipvlan_mode 默认为 l2，表示工作在数据链路层。</span>
$ docker network create -d ipvlan <span class="se">\
</span><span class="se"></span>  --subnet<span class="o">=</span>192.168.31.0/24 <span class="se">\
</span><span class="se"></span>  --gateway<span class="o">=</span>192.168.31.1 <span class="se">\
</span><span class="se"></span>  -o <span class="nv">parent</span><span class="o">=</span>eno1 <span class="se">\
</span><span class="se"></span>  -o <span class="nv">ipvlan_mode</span><span class="o">=</span>l2 <span class="se">\
</span><span class="se"></span>  ipvnet0

<span class="c1"># 现在使用 macvlan 启动一个容器试试</span>
<span class="c1"># 建议和我一样，通过 --ip 手动配置静态 ip 地址，当然不配也可以，DHCP 会自动分配 IP</span>
$ docker run --network ipvnet0 --ip<span class="o">=</span>192.168.31.234 --rm -it buildpack-deps:buster-curl /bin/bash
<span class="c1"># 在容器中查看网络接口状况，能看到 eth0 是一个 ipvlan 接口</span>
root@d0764ebbbf42:/# ip -d addr ls
1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu <span class="m">65536</span> qdisc noqueue state UNKNOWN group default qlen <span class="m">1000</span>
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 promiscuity <span class="m">0</span> minmtu <span class="m">0</span> maxmtu <span class="m">0</span> numtxqueues <span class="m">1</span> numrxqueues <span class="m">1</span> gso_max_size <span class="m">65536</span> gso_max_segs <span class="m">65535</span> 
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
12: eth0@if2: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu <span class="m">1500</span> qdisc noqueue state UNKNOWN group default 
    link/ether 38:f3:ab:a3:e6:71 brd ff:ff:ff:ff:ff:ff link-netnsid <span class="m">0</span> promiscuity <span class="m">0</span> minmtu <span class="m">68</span> maxmtu <span class="m">65535</span> 
    ipvlan  mode l2 bridge numtxqueues <span class="m">1</span> numrxqueues <span class="m">1</span> gso_max_size <span class="m">64000</span> gso_max_segs <span class="m">64</span> 
    inet 192.168.31.234/24 brd 192.168.31.255 scope global eth0
       valid_lft forever preferred_lft forever
<span class="c1"># 路由表，默认 gateway 被自动配置进来了</span>
root@d0764ebbbf42:/# ip route ls
default via 192.168.31.1 dev eth0 
192.168.31.0/24 dev eth0 proto kernel scope link src 192.168.31.234 

<span class="c1"># 可以正常访问 baidu</span>
root@d0764ebbbf42:/# curl baidu.com
&lt;html&gt;
&lt;meta http-equiv<span class="o">=</span><span class="s2">&#34;refresh&#34;</span> <span class="nv">content</span><span class="o">=</span><span class="s2">&#34;0;url=http://www.baidu.com/&#34;</span>&gt;
&lt;/html&gt;
</code></pre></td></tr></table>
</div>
</div><h2 id="rootless-容器的网络实现">Rootless 容器的网络实现</h2>
<p>如果容器运行时也在 Rootless 模式下运行，那它就没有权限在宿主机添加 bridge/veth 等虚拟网络接口，这种情况下，我们前面描述的容器网络就无法设置了。</p>
<p>那么 podman/containerd(nerdctl) 目前是如何在 Rootless 模式下构建容器网络的呢？</p>
<p>查看文档，发现它们都用到了 rootlesskit 相关的东西，而 rootlesskit 提供了 rootless 网络的几个实现，文档参见 <a href="https://github.com/rootless-containers/rootlesskit/blob/master/docs/network.md" target="_blank" rel="noopener noreferrer">rootlesskit/docs/network.md</a></p>
<p>其中目前推荐使用，而且 podman/containerd(nerdctl) 都默认使用的方案，是 <a href="https://github.com/rootless-containers/slirp4netns" target="_blank" rel="noopener noreferrer">rootless-containers/slirp4netns</a></p>
<p>以 containerd(nerdctl) 为例，按官方文档安装好后，随便启动几个容器，然后在宿主机查 <code>iptables</code>/<code>ip addr ls</code>，会发现啥也没有。
这显然是因为 rootless 模式下 containerd 改不了宿主机的 iptables 配置和虚拟网络接口。但是可以查看到宿主机 slirp4netns 在后台运行：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-shell" data-lang="shell">❯ ps aux <span class="p">|</span> grep tap
ryan     <span class="m">11644</span>  0.0  0.0   <span class="m">5288</span>  <span class="m">3312</span> ?        S    00:01   0:02 slirp4netns --mtu <span class="m">65520</span> -r <span class="m">3</span> --disable-host-loopback --enable-sandbox --enable-seccomp <span class="m">11625</span> tap0
</code></pre></td></tr></table>
</div>
</div><p>但是我看半天文档，只看到怎么使用 <code>rootlesskit</code>/<code>slirp4netns</code> 创建新的名字空间，没看到有介绍如何进入一个已存在的 <code>slirp4netns</code> 名字空间&hellip;</p>
<p>使用 <code>nsenter -a -t 11644</code> 也一直报错，任何程序都是 <code>no such binary</code>&hellip;</p>
<p>以后有空再重新研究一波&hellip;</p>
<p>总之能确定的是，它通过在虚拟的名字空间中创建了一个 <code>tap</code> 虚拟接口来实现容器网络，性能相比前面介绍的网络多少是要差一点的。</p>
<h2 id="nftables">nftables</h2>
<p>前面介绍了 iptables 以及其在 docker 和防火墙上的应用。但是实际上目前各大 Linux 发行版都已经不建议使用 iptables 了，甚至把 iptables 重命名为了 <code>iptables-leagacy</code>.</p>
<p>目前 opensuse/debian/opensuse 都已经预装了并且推荐使用 nftables，<strong>而且 firewalld 已经默认使用 nftables 作为它的后端了</strong>。</p>
<p>我在 opensuse tumbleweed 上实测，firewalld 添加的是 nftables 配置，而 docker 仍然在用旧的 iptables，也就是说我现在的机器上有两套 netfilter 工具并存：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback"># 查看 iptables 数据
&gt; iptables -S
-P INPUT ACCEPT
-P FORWARD DROP
-P OUTPUT ACCEPT
-N DOCKER
-N DOCKER-ISOLATION-STAGE-1
-N DOCKER-ISOLATION-STAGE-2
-N DOCKER-USER
-A FORWARD -j DOCKER-ISOLATION-STAGE-1
-A FORWARD -o br-e3fbbb7a1b3a -m conntrack --ctstate RELATED,ESTABLISHED -j ACCEPT
-A FORWARD -o br-e3fbbb7a1b3a -j DOCKER
...

# 确认下是否使用了 nftables 的兼容层，结果提示请我使用 iptables-legacy
&gt; iptables-nft -S
# Warning: iptables-legacy tables present, use iptables-legacy to see them
-P INPUT ACCEPT
-P FORWARD ACCEPT
-P OUTPUT ACCEPT

# 查看 nftables 规则，能看到三张 firewalld 生成的 table
&gt; nft list ruleset
table inet firewalld {
    ...
}
table ip firewalld {
    ...
}
table ip6 firewalld {
    ...
}
</code></pre></td></tr></table>
</div>
</div><p>但是现在 kubernetes/docker 都还是用的 iptables，nftables 我学了用处不大，以后有空再补充。</p>
<h2 id="参考">参考</h2>
<ul>
<li><a href="https://www.zsythink.net/archives/1199" target="_blank" rel="noopener noreferrer">iptables详解（1）：iptables概念</a></li>
<li><a href="https://linux.cn/article-13364-1.html" target="_blank" rel="noopener noreferrer">网络地址转换（NAT）之报文跟踪</a></li>
<li><a href="https://developer.aliyun.com/article/700923" target="_blank" rel="noopener noreferrer">容器安全拾遗 - Rootless Container初探</a></li>
<li><a href="https://en.wikipedia.org/wiki/Netfilter" target="_blank" rel="noopener noreferrer">netfilter - wikipedia</a></li>
</ul>
]]></description></item><item><title>Linux 中的虚拟网络接口</title><link>https://ryan4yin.space/posts/linux-virtual-network-interfaces/</link><pubDate>Sat, 14 Aug 2021 11:13:03 +0800</pubDate><author>xiaoyin_c@qq.com</author><dc:creator>ryan4yin</dc:creator><guid>https://ryan4yin.space/posts/linux-virtual-network-interfaces/</guid><description><![CDATA[<blockquote>
<p>本文用到的字符画工具：<a href="https://github.com/zenghongtu/vscode-asciiflow2" target="_blank" rel="noopener noreferrer">vscode-asciiflow2</a></p>
</blockquote>
<blockquote>
<p>注意: 本文中使用 <code>ip</code> 命令创建或修改的任何网络配置，都是未持久化的，主机重启即消失。</p>
</blockquote>
<p>Linux 具有强大的虚拟网络能力，这也是 openstack 网络、docker 容器网络以及 kubernetes 网络等虚拟网络的基础。</p>
<p>这里介绍 Linux 常用的虚拟网络接口类型：TUN/TAP、bridge、veth、ipvlan/macvlan、vlan 以及 vxlan/geneve.</p>
<h2 id="一tuntap-虚拟网络接口">一、tun/tap 虚拟网络接口</h2>
<p>tun/tap 是操作系统内核中的虚拟网络设备，他们为用户层程序提供数据的接收与传输。</p>
<p>普通的物理网络接口如 eth0，它的两端分别是内核协议栈和外面的物理网络。</p>
<p>而对于 TUN/TAP 虚拟接口如 tun0，它的一端一定是连接的用户层程序，另一端则视配置方式的不同而变化，可以直连内核协议栈，也可以是某个 bridge（后面会介绍）。
Linux 通过内核模块 TUN 提供 tun/tap 功能，该模块提供了一个设备接口 <code>/dev/net/tun</code> 供用户层程序读写，用户层程序通过 <code>/dev/net/tun</code> 读写主机内核协议栈的数据。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback">&gt; modinfo tun
filename:       /lib/modules/5.13.6-1-default/kernel/drivers/net/tun.ko.xz
alias:          devname:net/tun
alias:          char-major-10-200
license:        GPL
author:         (C) 1999-2004 Max Krasnyansky &lt;maxk@qualcomm.com&gt;
description:    Universal TUN/TAP device driver
...

&gt; ls /dev/net/tun
/dev/net/tun
</code></pre></td></tr></table>
</div>
</div><p>一个 TUN 设备的示例图如下：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback">             
+----------------------------------------------------------------------+
|                                                                      |
|  +--------------------+      +--------------------+                  |
|  | User Application A |      | User Application B +&lt;-----+           |
|  +------------+-------+      +-------+------------+      |           |
|               | 1                    | 5                 |           |
|...............+......................+...................|...........|
|               ↓                      ↓                   |           |
|         +----------+           +----------+              |           |
|         | socket A |           | socket B |              |           |
|         +-------+--+           +--+-------+              |           |
|                 | 2               | 6                    |           |
|.................+.................+......................|...........|
|                 ↓                 ↓                      |           |
|             +------------------------+          +--------+-------+   |
|             | Network Protocol Stack |          |  /dev/net/tun  |   |
|             +--+-------------------+-+          +--------+-------+   |
|                | 7                 | 3                   ^           |
|................+...................+.....................|...........|
|                ↓                   ↓                     |           |
|        +----------------+    +----------------+        4 |           |
|        |      eth0      |    |      tun0      |          |           |
|        +-------+--------+    +-----+----------+          |           |
|    10.32.0.11  |                   |   192.168.3.11      |           |
|                | 8                 +---------------------+           |
|                |                                                     |
+----------------+-----------------------------------------------------+
                 ↓
         Physical Network
</code></pre></td></tr></table>
</div>
</div><p>因为 TUN/TAP 设备的一端是内核协议栈，显然流入 tun0 的数据包是先经过本地的路由规则匹配的。</p>
<p>路由匹配成功，数据包被发送到 tun0 后，tun0 发现另一端是通过 <code>/dev/net/tun</code> 连接到应用程序 B，就会将数据丢给应用程序 B。</p>
<p>应用程序对数据包进行处理后，可能会构造新的数据包，通过物理网卡发送出去。比如常见的 VPN 程序就是把原来的数据包封装/加密一遍，再发送给 VPN 服务器。</p>
<h3 id="c-语言编程测试-tun-设备">C 语言编程测试 TUN 设备</h3>
<p>为了使用 tun/tap 设备，用户层程序需要通过系统调用打开 <code>/dev/net/tun</code> 获得一个读写该设备的文件描述符(FD)，并且调用 <code>ioctl()</code> 向内核注册一个 TUN 或 TAP 类型的虚拟网卡(实例化一个 tun/tap 设备)，其名称可能是 <code>tun0/tap0</code> 等。</p>
<p>此后，用户程序可以通过该 TUN/TAP 虚拟网卡与主机内核协议栈（或者其他网络设备）交互。当用户层程序关闭后，其注册的 TUN/TAP 虚拟网卡以及自动生成的路由表相关条目都会被内核释放。</p>
<p>可以把用户层程序看做是网络上另一台主机，他们通过 tun/tap 虚拟网卡相连。</p>
<p>一个简单的 C 程序示例如下，它每次收到数据后，都只单纯地打印一下收到的字节数：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span><span class="lnt">43
</span><span class="lnt">44
</span><span class="lnt">45
</span><span class="lnt">46
</span><span class="lnt">47
</span><span class="lnt">48
</span><span class="lnt">49
</span><span class="lnt">50
</span><span class="lnt">51
</span><span class="lnt">52
</span><span class="lnt">53
</span><span class="lnt">54
</span><span class="lnt">55
</span><span class="lnt">56
</span><span class="lnt">57
</span><span class="lnt">58
</span><span class="lnt">59
</span><span class="lnt">60
</span><span class="lnt">61
</span><span class="lnt">62
</span><span class="lnt">63
</span><span class="lnt">64
</span><span class="lnt">65
</span><span class="lnt">66
</span><span class="lnt">67
</span><span class="lnt">68
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-c" data-lang="c"><span class="cp">#include</span> <span class="cpf">&lt;linux/if.h&gt;</span><span class="cp">
</span><span class="cp">#include</span> <span class="cpf">&lt;linux/if_tun.h&gt;</span><span class="cp">
</span><span class="cp"></span>
<span class="cp">#include</span> <span class="cpf">&lt;sys/ioctl.h&gt;</span><span class="cp">
</span><span class="cp"></span>
<span class="cp">#include</span> <span class="cpf">&lt;fcntl.h&gt;</span><span class="cp">
</span><span class="cp">#include</span> <span class="cpf">&lt;string.h&gt;</span><span class="cp">
</span><span class="cp"></span>
<span class="cp">#include</span> <span class="cpf">&lt;unistd.h&gt;</span><span class="cp">
</span><span class="cp">#include</span><span class="cpf">&lt;stdlib.h&gt;</span><span class="cp">
</span><span class="cp">#include</span><span class="cpf">&lt;stdio.h&gt;</span><span class="cp">
</span><span class="cp"></span>
<span class="kt">int</span> <span class="nf">tun_alloc</span><span class="p">(</span><span class="kt">int</span> <span class="n">flags</span><span class="p">)</span>
<span class="p">{</span>

    <span class="k">struct</span> <span class="n">ifreq</span> <span class="n">ifr</span><span class="p">;</span>
    <span class="kt">int</span> <span class="n">fd</span><span class="p">,</span> <span class="n">err</span><span class="p">;</span>
    <span class="kt">char</span> <span class="o">*</span><span class="n">clonedev</span> <span class="o">=</span> <span class="s">&#34;/dev/net/tun&#34;</span><span class="p">;</span>

    <span class="c1">// 打开 tun 文件，获得 fd
</span><span class="c1"></span>    <span class="k">if</span> <span class="p">((</span><span class="n">fd</span> <span class="o">=</span> <span class="n">open</span><span class="p">(</span><span class="n">clonedev</span><span class="p">,</span> <span class="n">O_RDWR</span><span class="p">))</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">)</span> <span class="p">{</span>
        <span class="k">return</span> <span class="n">fd</span><span class="p">;</span>
    <span class="p">}</span>

    <span class="n">memset</span><span class="p">(</span><span class="o">&amp;</span><span class="n">ifr</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="k">sizeof</span><span class="p">(</span><span class="n">ifr</span><span class="p">));</span>
    <span class="n">ifr</span><span class="p">.</span><span class="n">ifr_flags</span> <span class="o">=</span> <span class="n">flags</span><span class="p">;</span>

    <span class="c1">// 向内核注册一个 TUN 网卡，并与前面拿到的 fd 关联起来
</span><span class="c1"></span>    <span class="c1">// 程序关闭时，注册的 tun 网卡及自动生成的相关路由策略，会被自动释放
</span><span class="c1"></span>    <span class="k">if</span> <span class="p">((</span><span class="n">err</span> <span class="o">=</span> <span class="n">ioctl</span><span class="p">(</span><span class="n">fd</span><span class="p">,</span> <span class="n">TUNSETIFF</span><span class="p">,</span> <span class="p">(</span><span class="kt">void</span> <span class="o">*</span><span class="p">)</span> <span class="o">&amp;</span><span class="n">ifr</span><span class="p">))</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">)</span> <span class="p">{</span>
        <span class="n">close</span><span class="p">(</span><span class="n">fd</span><span class="p">);</span>
        <span class="k">return</span> <span class="n">err</span><span class="p">;</span>
    <span class="p">}</span>

    <span class="n">printf</span><span class="p">(</span><span class="s">&#34;Open tun/tap device: %s for reading...</span><span class="se">\n</span><span class="s">&#34;</span><span class="p">,</span> <span class="n">ifr</span><span class="p">.</span><span class="n">ifr_name</span><span class="p">);</span>

    <span class="k">return</span> <span class="n">fd</span><span class="p">;</span>
<span class="p">}</span>

<span class="kt">int</span> <span class="nf">main</span><span class="p">()</span>
<span class="p">{</span>

    <span class="kt">int</span> <span class="n">tun_fd</span><span class="p">,</span> <span class="n">nread</span><span class="p">;</span>
    <span class="kt">char</span> <span class="n">buffer</span><span class="p">[</span><span class="mi">1500</span><span class="p">];</span>

    <span class="cm">/* Flags: IFF_TUN   - TUN device (no Ethernet headers)
</span><span class="cm">     *        IFF_TAP   - TAP device
</span><span class="cm">     *        IFF_NO_PI - Do not provide packet information
</span><span class="cm">     */</span>
    <span class="n">tun_fd</span> <span class="o">=</span> <span class="n">tun_alloc</span><span class="p">(</span><span class="n">IFF_TUN</span> <span class="o">|</span> <span class="n">IFF_NO_PI</span><span class="p">);</span>

    <span class="k">if</span> <span class="p">(</span><span class="n">tun_fd</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">)</span> <span class="p">{</span>
        <span class="n">perror</span><span class="p">(</span><span class="s">&#34;Allocating interface&#34;</span><span class="p">);</span>
        <span class="n">exit</span><span class="p">(</span><span class="mi">1</span><span class="p">);</span>
    <span class="p">}</span>

    <span class="k">while</span> <span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="p">{</span>
        <span class="n">nread</span> <span class="o">=</span> <span class="n">read</span><span class="p">(</span><span class="n">tun_fd</span><span class="p">,</span> <span class="n">buffer</span><span class="p">,</span> <span class="k">sizeof</span><span class="p">(</span><span class="n">buffer</span><span class="p">));</span>
        <span class="k">if</span> <span class="p">(</span><span class="n">nread</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">)</span> <span class="p">{</span>
            <span class="n">perror</span><span class="p">(</span><span class="s">&#34;Reading from interface&#34;</span><span class="p">);</span>
            <span class="n">close</span><span class="p">(</span><span class="n">tun_fd</span><span class="p">);</span>
            <span class="n">exit</span><span class="p">(</span><span class="mi">1</span><span class="p">);</span>
        <span class="p">}</span>

        <span class="n">printf</span><span class="p">(</span><span class="s">&#34;Read %d bytes from tun/tap device</span><span class="se">\n</span><span class="s">&#34;</span><span class="p">,</span> <span class="n">nread</span><span class="p">);</span>
    <span class="p">}</span>
    <span class="k">return</span> <span class="mi">0</span><span class="p">;</span>
<span class="p">}</span>
</code></pre></td></tr></table>
</div>
</div><p>接下来开启三个终端窗口来测试上述程序，分别运行上面的 tun 程序、tcpdump 和 iproute2 指令。</p>
<p>首先通过编译运行上述 c 程序，程序会阻塞住，等待数据到达：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback"># 编译，请忽略部分 warning
&gt; gcc mytun.c -o mytun

# 创建并监听 tun 设备需要 root 权限
&gt; sudo mytun 
Open tun/tap device: tun0 for reading...
</code></pre></td></tr></table>
</div>
</div><p>现在使用 iproute2 查看下链路层设备：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback"># 能发现最后面有列出名为 tun0 的接口，但是状态为 down
❯ ip addr ls
......
3: wlp4s0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP group default qlen 1000
    link/ether c0:3c:59:36:a4:16 brd ff:ff:ff:ff:ff:ff
    inet 192.168.31.228/24 brd 192.168.31.255 scope global dynamic noprefixroute wlp4s0
       valid_lft 41010sec preferred_lft 41010sec
    inet6 fe80::4ab0:130f:423b:5d37/64 scope link noprefixroute 
       valid_lft forever preferred_lft forever
7: tun0: &lt;POINTOPOINT,MULTICAST,NOARP&gt; mtu 1500 qdisc noop state DOWN group default qlen 500
    link/none 

# 为 tun0 设置 ip 地址，注意不要和其他接口在同一网段，会导致路由冲突
&gt; sudo ip addr add 172.21.22.23/24 dev tun0
# 启动 tun0 这个接口，这一步会自动向路由表中添加将 172.21.22.23/24 路由到 tun0 的策略
&gt; sudo ip link set tun0 up
#确认上一步添加的路由策略是否存在
❯ ip route ls
default via 192.168.31.1 dev wlp4s0 proto dhcp metric 600 
172.17.0.0/16 dev docker0 proto kernel scope link src 172.17.0.1 linkdown 
172.21.22.0/24 dev tun0 proto kernel scope link src 172.21.22.23 
192.168.31.0/24 dev wlp4s0 proto kernel scope link src 192.168.31.228 metric 600 

# 此时再查看接口，发现 tun0 状态为 unknown
&gt; ip addr ls
......
8: tun0: &lt;POINTOPOINT,MULTICAST,NOARP,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UNKNOWN group default qlen 500
    link/none 
    inet 172.21.22.23/24 scope global tun0
       valid_lft forever preferred_lft forever
    inet6 fe80::3d52:49b5:1cf3:38fd/64 scope link stable-privacy 
       valid_lft forever preferred_lft forever

# 使用 tcpdump 尝试抓下 tun0 的数据，会阻塞在这里，等待数据到达
&gt; tcpdump -i tun0
</code></pre></td></tr></table>
</div>
</div><p>现在再启动第三个窗口发点数据给 tun0，持续观察前面 <code>tcpdump</code> 和 <code>mytun</code> 的日志:</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback"># 直接 ping tun0 的地址，貌似有问题，数据没进 mytun 程序，而且还有响应
❯ ping -c 4 172.21.22.23
PING 172.21.22.23 (172.21.22.23) 56(84) bytes of data.
64 bytes from 172.21.22.23: icmp_seq=1 ttl=64 time=0.167 ms
64 bytes from 172.21.22.23: icmp_seq=2 ttl=64 time=0.180 ms
64 bytes from 172.21.22.23: icmp_seq=3 ttl=64 time=0.126 ms
64 bytes from 172.21.22.23: icmp_seq=4 ttl=64 time=0.141 ms

--- 172.21.22.23 ping statistics ---
4 packets transmitted, 4 received, 0% packet loss, time 3060ms
rtt min/avg/max/mdev = 0.126/0.153/0.180/0.021 ms

# 但是 ping 该网段下的其他地址，流量就会被转发给 mytun 程序，因为 mytun 啥数据也没回，自然丢包率 100%
# tcpdump 和 mytun 都会打印出相关日志
❯ ping -c 4 172.21.22.26
PING 172.21.22.26 (172.21.22.26) 56(84) bytes of data.

--- 172.21.22.26 ping statistics ---
4 packets transmitted, 0 received, 100% packet loss, time 3055ms
</code></pre></td></tr></table>
</div>
</div><p>下面给出 mytun 的输出：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback">Read 84 bytes from tun/tap device
Read 84 bytes from tun/tap device
Read 84 bytes from tun/tap device
Read 84 bytes from tun/tap device
</code></pre></td></tr></table>
</div>
</div><p>以及 tcpdump 的输出：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback">00:22:03.622684 IP (tos 0x0, ttl 64, id 37341, offset 0, flags [DF], proto ICMP (1), length 84)
    172.21.22.23 &gt; 172.21.22.26: ICMP echo request, id 11, seq 1, length 64
00:22:04.633394 IP (tos 0x0, ttl 64, id 37522, offset 0, flags [DF], proto ICMP (1), length 84)
    172.21.22.23 &gt; 172.21.22.26: ICMP echo request, id 11, seq 2, length 64
00:22:05.653356 IP (tos 0x0, ttl 64, id 37637, offset 0, flags [DF], proto ICMP (1), length 84)
    172.21.22.23 &gt; 172.21.22.26: ICMP echo request, id 11, seq 3, length 64
00:22:06.677341 IP (tos 0x0, ttl 64, id 37667, offset 0, flags [DF], proto ICMP (1), length 84)
    172.21.22.23 &gt; 172.21.22.26: ICMP echo request, id 11, seq 4, length 64
</code></pre></td></tr></table>
</div>
</div><p>更复杂的 tun 程序，可以参考</p>
<ul>
<li><a href="https://github.com/gregnietsky/simpletun" target="_blank" rel="noopener noreferrer">simpletun</a></li>
<li><a href="https://github.com/marywangran/simpletun" target="_blank" rel="noopener noreferrer">marywangran/simpletun</a></li>
<li><a href="https://github.com/marywangran/gotun-tunnel/blob/main/tun/tun.go" target="_blank" rel="noopener noreferrer">tun go 语言版</a></li>
</ul>
<h3 id="tun-与-tap-的区别">TUN 与 TAP 的区别</h3>
<p>TUN 和 TAP 的区别在于工作的网络层次不同，用户程序通过 TUN 设备只能读写网络层的 IP 数据包，而 TAP 设备则支持读写链路层的数据包（通常是以太网数据包，带有 Ethernet headers）。</p>
<p>TUN 与 TAP 的关系，就类似于 socket 和 raw socket.</p>
<p>TUN/TAP 应用最多的场景是 VPN 代理，比如:</p>
<ol>
<li><a href="https://github.com/ryan4yin/clash" target="_blank" rel="noopener noreferrer">clash</a>: 一个支持各种规则的隧道，也支持 TUN 模式</li>
<li><a href="https://github.com/xjasonlyu/tun2socks/wiki" target="_blank" rel="noopener noreferrer">tun2socks</a>: 一个全局透明代理，和 VPN 的工作模式一样，它通过创建虚拟网卡+修改路由表，在第三层网络层代理系统流量。</li>
</ol>
<h2 id="二veth">二、veth</h2>
<p>veth 接口总是成对出现，一对 veth 接口就类似一根网线，从一端进来的数据会从另一端出去。</p>
<p>同时 veth 又是一个虚拟网络接口，因此它和 TUN/TAP 或者其他物理网络接口一样，也都能配置 mac/ip 地址（但是并不是一定得配 mac/ip 地址）。</p>
<p>其主要作用就是连接不同的网络，比如在容器网络中，用于将容器的 namespace 与 root namespace 的网桥 br0 相连。
容器网络中，容器侧的 veth 自身设置了 ip/mac 地址并被重命名为 eth0，作为容器的网络接口使用，而主机侧的 veth 则直接连接在 docker0/br0 上面。</p>
<p>使用 veth 实现容器网络，需要结合下一小节介绍的 bridge，在下一小节将给出容器网络结构图。</p>
<h2 id="三bridge">三、bridge</h2>
<p>Linux Bridge 是工作在链路层的网络交换机，由 Linux 内核模块 <code>brige</code> 提供，它负责在所有连接到它的接口之间转发链路层数据包。</p>
<p>添加到 Bridge 上的设备被设置为只接受二层数据帧并且转发所有收到的数据包到 Bridge 中。
在 Bridge 中会进行一个类似物理交换机的查MAC端口映射表、转发、更新MAC端口映射表这样的处理逻辑，从而数据包可以被转发到另一个接口/丢弃/广播/发往上层协议栈，由此 Bridge 实现了数据转发的功能。</p>
<p>如果使用 tcpdump 在 Bridge 接口上抓包，可以抓到网桥上所有接口进出的包，因为这些数据包都要通过网桥进行转发。</p>
<p>与物理交换机不同的是，Bridge 本身可以设置 IP 地址，可以认为当使用 <code>brctl addbr br0</code> 新建一个 br0 网桥时，系统自动创建了一个同名的隐藏 <code>br0</code> 网络接口。<code>br0</code> 一旦设置 IP 地址，就意味着这个隐藏的 br0 接口可以作为路由接口设备，参与 IP 层的路由选择(可以使用 <code>route -n</code> 查看最后一列 <code>Iface</code>)。因此只有当 <code>br0</code> 设置 <code>IP</code> 地址时，Bridge 才有可能将数据包发往上层协议栈。</p>
<p>但被添加到 Bridge 上的网卡是不能配置 IP 地址的，他们工作在数据链路层，对路由系统不可见。</p>
<p>它常被用于在虚拟机、主机上不同的 namepsaces 之间转发数据。</p>
<h3 id="虚拟机场景桥接模式">虚拟机场景（桥接模式）</h3>
<p>以 qemu-kvm 为例，在虚拟机的桥接模式下，qemu-kvm 会为每个虚拟机创建一个 tun/tap 虚拟网卡并连接到 br0 网桥。
虚拟机内部的网络接口 <code>eth0</code> 是 qemu-kvm 软件模拟的，实际上虚拟机内网络数据的收发都会被 qemu-kvm 转换成对 <code>/dev/net/tun</code> 的读写。</p>
<p>以发送数据为例，整个流程如下：</p>
<ul>
<li>虚拟机发出去的数据包先到达 qemu-kvm 程序</li>
<li>数据被用户层程序 qemu-kvm 写入到 <code>/dev/net/tun</code>，到达 tap 设备</li>
<li>tap 设备把数据传送到 br0 网桥</li>
<li>br0 把数据交给 eth0 发送出去</li>
</ul>
<p>整个流程跑完，数据包都不需要经过宿主机的协议栈，效率高。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback">+------------------------------------------------+-----------------------------------+-----------------------------------+
|                       Host                     |           VirtualMachine1         |           VirtualMachine2         |
|                                                |                                   |                                   |
|    +--------------------------------------+    |    +-------------------------+    |    +-------------------------+    |
|    |         Network Protocol Stack       |    |    |  Network Protocol Stack |    |    |  Network Protocol Stack |    |
|    +--------------------------------------+    |    +-------------------------+    |    +-------------------------+    |
|                       ↑                        |                ↑                  |                 ↑                 |
|.......................|........................|................|..................|.................|.................|
|                       ↓                        |                ↓                  |                 ↓                 |
|                  +--------+                    |            +-------+              |             +-------+             |
|                  | .3.101 |                    |            | .3.102|              |             | .3.103|             |
|     +------+     +--------+     +-------+      |            +-------+              |             +-------+             |
|     | eth0 |&lt;---&gt;|   br0  |&lt;---&gt;|tun/tap|      |            | eth0  |              |             | eth0  |             |
|     +------+     +--------+     +-------+      |            +-------+              |             +-------+             |
|         ↑             ↑             ↑      +--------+           ↑                  |                 ↑                 |
|         |             |             +------|qemu-kvm|-----------+                  |                 |                 |
|         |             ↓                    +--------+                              |                 |                 |
|         |         +-------+                    |                                   |                 |                 |
|         |         |tun/tap|                    |                                   |                 |                 |
|         |         +-------+                    |                                   |                 |                 |
|         |             ↑                        |            +--------+             |                 |                 |
|         |             +-------------------------------------|qemu-kvm|-------------|-----------------+                 |
|         |                                      |            +--------+             |                                   |
|         |                                      |                                   |                                   |
+---------|--------------------------------------+-----------------------------------+-----------------------------------+
          ↓
    Physical Network  (192.168.3.0/24)
</code></pre></td></tr></table>
</div>
</div><h3 id="跨-namespace-通信场景容器网络nat-模式">跨 namespace 通信场景（容器网络，NAT 模式）</h3>
<blockquote>
<p>docker/podman 提供的 bridge 网络模式，就是使用 veth+bridge+iptalbes 实现的。我会在下一篇文章详细介绍「容器网络」。</p>
</blockquote>
<p>由于容器运行在自己单独的 network namespace 里面，所以和虚拟机一样，它们也都有自己单独的协议栈。</p>
<p>容器网络的结构和虚拟机差不多，但是它改用了 NAT 网络，并把 tun/tap 换成了 veth，导致 docker0 过来的数据，要先经过宿主机协议栈，然后才进入 veth 接口。</p>
<p>多了一层 NAT，以及多走了一层宿主机协议栈，都会导致性能下降。</p>
<p>示意图如下：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback">+-----------------------------------------------+-----------------------------------+-----------------------------------+
|                      Host                     |           Container 1             |           Container 2             |
|                                               |                                   |                                   |
|   +---------------------------------------+   |    +-------------------------+    |    +-------------------------+    |
|   |       Network Protocol Stack          |   |    |  Network Protocol Stack |    |    |  Network Protocol Stack |    |
|   +----+-------------+--------------------+   |    +-----------+-------------+    |    +------------+------------+    |
|        ^             ^                        |                ^                  |                 ^                 |
|........|.............|........................|................|..................|.................|.................|
|        v             v  ↓                     |                v                  |                 v                 |
|   +----+----+  +-----+------+                 |          +-----+-------+          |           +-----+-------+         |
|   | .31.101 |  | 172.17.0.1 |      +------+   |          | 172.17.0.2  |          |           |  172.17.0.3 |         |
|   +---------+  +-------------&lt;----&gt;+ veth |   |          +-------------+          |           +-------------+         |
|   |  eth0   |  |   docker0  |      +--+---+   |          | eth0(veth)  |          |           | eth0(veth)  |         |
|   +----+----+  +-----+------+         ^       |          +-----+-------+          |           +-----+-------+         |
|        ^             ^                |       |                ^                  |                 ^                 |
|        |             |                +------------------------+                  |                 |                 |
|        |             v                        |                                   |                 |                 |
|        |          +--+---+                    |                                   |                 |                 |
|        |          | veth |                    |                                   |                 |                 |
|        |          +--+---+                    |                                   |                 |                 |
|        |             ^                        |                                   |                 |                 |
|        |             +------------------------------------------------------------------------------+                 |
|        |                                      |                                   |                                   |
|        |                                      |                                   |                                   |
+-----------------------------------------------+-----------------------------------+-----------------------------------+
         v
    Physical Network  (192.168.31.0/24)
</code></pre></td></tr></table>
</div>
</div><p>每创建一个新容器，都会在容器的 namespace 里新建一个 veth 接口并命令为 eth0，同时在主 namespace 创建一个 veth，将容器的 eth0 与 docker0 连接。</p>
<p>可以在容器中通过 iproute2 查看到， eth0 的接口类型为 <code>veth</code>：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-shell" data-lang="shell">❯ docker run -it --rm debian:buster bash
root@5facbe4ddc1e:/# ip --details addr ls
1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu <span class="m">65536</span> qdisc noqueue state UNKNOWN group default qlen <span class="m">1000</span>
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 promiscuity <span class="m">0</span> minmtu <span class="m">0</span> maxmtu <span class="m">0</span> numtxqueues <span class="m">1</span> numrxqueues <span class="m">1</span> gso_max_size <span class="m">65536</span> gso_max_segs <span class="m">65535</span> 
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
20: eth0@if21: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu <span class="m">1500</span> qdisc noqueue state UP group default 
    link/ether 02:42:ac:11:00:02 brd ff:ff:ff:ff:ff:ff link-netnsid <span class="m">0</span> promiscuity <span class="m">0</span> minmtu <span class="m">68</span> maxmtu <span class="m">65535</span> 
    veth numtxqueues <span class="m">1</span> numrxqueues <span class="m">1</span> gso_max_size <span class="m">65536</span> gso_max_segs <span class="m">65535</span> 
    inet 172.17.0.2/16 brd 172.17.255.255 scope global eth0
       valid_lft forever preferred_lft forever
</code></pre></td></tr></table>
</div>
</div><p>同时在宿主机中能看到对应的 veth 设备是绑定到了 docker0 网桥的：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-shell" data-lang="shell">❯ sudo brctl show
bridge name     bridge id               STP enabled     interfaces
docker0         8000.0242fce99ef5       no              vethea4171a
</code></pre></td></tr></table>
</div>
</div><h2 id="四macvlan">四、macvlan</h2>
<blockquote>
<p>目前 docker/podman 都支持创建基于 macvlan 的 Linux 容器网络。</p>
</blockquote>
<blockquote>
<p>注意 macvlan 和 WiFi 存在兼容问题，如果使用笔记本测试，可能会遇到麻烦。</p>
</blockquote>
<blockquote>
<p>参考文档：<a href="https://cizixs.com/2017/02/14/network-virtualization-macvlan/" target="_blank" rel="noopener noreferrer">linux 网络虚拟化： macvlan</a></p>
</blockquote>
<p>macvlan 是比较新的 Linux 特性，需要内核版本 &gt;= 3.9，它被用于在主机的网络接口（父接口）上配置多个虚拟子接口，这些子接口都拥有各自独立的 mac 地址，也可以配上 ip 地址进行通讯。</p>
<p>macvlan 下的虚拟机或者容器网络和主机在同一个网段中，共享同一个广播域。macvlan 和 bridge 比较相似，但因为它省去了 bridge 的存在，所以配置和调试起来比较简单，而且效率也相对高。除此之外，macvlan 自身也完美支持 VLAN。</p>
<p>如果希望容器或者虚拟机放在主机相同的网络中，享受已经存在网络栈的各种优势，可以考虑 macvlan。</p>
<p>我会在下一篇文章对 docker 的 macvlan/ipvlan 做个分析，这里先略过了&hellip;</p>
<h2 id="五ipvlan">五、ipvlan</h2>
<blockquote>
<p><a href="https://cizixs.com/2017/02/17/network-virtualization-ipvlan/" target="_blank" rel="noopener noreferrer">linux 网络虚拟化： ipvlan</a></p>
</blockquote>
<blockquote>
<p>cilium 1.9 已经提供了基于 ipvlan 的网络（beta 特性），用于替换传统的 veth+bridge 容器网络。详见 <a href="https://docs.cilium.io/en/v1.9/gettingstarted/ipvlan/" target="_blank" rel="noopener noreferrer">IPVLAN based Networking (beta) - Cilium 1.9 Docs</a></p>
</blockquote>
<p>ipvlan 和 macvlan 的功能很类似，也是用于在主机的网络接口（父接口）上配置出多个虚拟的子接口。但不同的是，ipvlan 的各子接口没有独立的 mac 地址，它们和主机的父接口共享 mac 地址。</p>
<blockquote>
<p>因为 mac 地址共享，所以如果使用 DHCP，就要注意不能使用 mac 地址做 DHCP，需要额外配置唯一的 clientID.</p>
</blockquote>
<p>如果你遇到以下的情况，请考虑使用 ipvlan：</p>
<ul>
<li>父接口对 mac 地址数目有限制，或者在 mac 地址过多的情况下会造成严重的性能损失</li>
<li>工作在 802.11(wireless)无线网络中（macvlan 无法和无线网络共同工作）</li>
<li>希望搭建比较复杂的网络拓扑（不是简单的二层网络和 VLAN），比如要和 BGP 网络一起工作</li>
</ul>
<p>基于 ipvlan/macvlan 的容器网络，比 veth+bridge+iptables 的性能要更高。</p>
<p>我会在下一篇文章对 docker 的 macvlan/ipvlan 做个分析，这里先略过了&hellip;</p>
<h2 id="六vlan">六、vlan</h2>
<p>vlan 即虚拟局域网，是一个链路层的广播域隔离技术，可以用于切分局域网，解决广播泛滥和安全性问题。被隔离的广播域之间需要上升到第三层才能完成通讯。</p>
<p>常用的企业路由器如 ER-X 基本都可以设置 vlan，Linux 也直接支持了 vlan.</p>
<p>以太网数据包有一个专门的字段提供给 vlan 使用，vlan 数据包会在该位置记录它的 VLAN ID，交换机通过该 ID 来区分不同的 VLAN，只将该以太网报文广播到该 ID 对应的 VLAN 中。</p>
<h2 id="七vxlangeneve">七、vxlan/geneve</h2>
<blockquote>
<p><a href="https://datatracker.ietf.org/doc/html/rfc8926" target="_blank" rel="noopener noreferrer">rfc8926 - Geneve: Generic Network Virtualization Encapsulation</a>
<a href="https://datatracker.ietf.org/doc/html/rfc7348" target="_blank" rel="noopener noreferrer">rfc7348 - Virtual eXtensible Local Area Network (VXLAN)</a></p>
</blockquote>
<blockquote>
<p><a href="https://cizixs.com/2017/09/28/linux-vxlan/" target="_blank" rel="noopener noreferrer">linux 上实现 vxlan 网络</a></p>
</blockquote>
<p>在介绍 vxlan 前，先说明下两个名词的含义：</p>
<ul>
<li><strong>underlay 网络</strong>：即物理网络</li>
<li><strong>overlay 网络</strong>：指在现有的物理网络之上构建的虚拟网络。其实就是一种隧道技术，将原生态的二层数据帧报文进行封装后通过隧道进行传输。</li>
</ul>
<p>vxlan 与 geneve 都是 overlay 网络协议，它俩都是使用 UDP 包来封装链路层的以太网帧。</p>
<p>vxlan 在 2014 年标准化，而 geneve 在 2020 年底才通过草案阶段，目前尚未形成最终标准。但是目前 linux/cilium 都已经支持了 geneve.</p>
<p>geneve 相对 vxlan 最大的变化，是它更灵活——它的 header 长度是可变的。</p>
<p>目前所有 overlay 的跨主机容器网络方案，几乎都是基于 vxlan 实现的（例外：cilium 也支持 geneve）。</p>
<blockquote>
<p>我们在学习单机的容器网络时，不需要接触到 vxlan，但是在学习跨主机容器网络方案如 flannel/calico/cilium 时，那 vxlan(overlay) 及 BGP(underlay) 就不可避免地要接触了。</p>
</blockquote>
<p>先介绍下 vxlan 的数据包结构：</p>
<p><figure><a class="lightgallery" href="/images/linux-virtual-interfaces/vxlan-frame.png" title="/images/linux-virtual-interfaces/vxlan-frame.png" data-thumbnail="/images/linux-virtual-interfaces/vxlan-frame.png" data-sub-html="<h2>VXLAN 栈帧结构</h2>">
        
    </a><figcaption class="image-caption">VXLAN 栈帧结构</figcaption>
    </figure></p>
<p>在创建 vxlan 的 vtep 虚拟设备时，我们需要手动设置图中的如下属性：</p>
<ul>
<li>VXLAN 目标端口：即接收方 vtep 使用的端口，这里 IANA 定义的端口是 4789，但是只有 calico 的 vxlan 模式默认使用该端口 calico，而 cilium/flannel 的默认端口都是 Linux 默认的 8472.</li>
<li>VNID: 每个 VXLAN 网络接口都会被分配一个独立的 VNID</li>
</ul>
<p>一个点对点的 vxlan 网络架构图如下:</p>
<p><figure><a class="lightgallery" href="/images/linux-virtual-interfaces/vxlan-architecture.gif" title="/images/linux-virtual-interfaces/vxlan-architecture.gif" data-thumbnail="/images/linux-virtual-interfaces/vxlan-architecture.gif" data-sub-html="<h2>VXLAN 点对点网络架构</h2>">
        
    </a><figcaption class="image-caption">VXLAN 点对点网络架构</figcaption>
    </figure></p>
<p>可以看到每台虚拟机 VM 都会被分配一个唯一的 VNID，然后两台物理机之间通过 VTEP 虚拟网络设备建立了 VXLAN 隧道，所有 VXLAN 网络中的虚拟机，都通过 VTEP 来互相通信。</p>
<p>有了上面这些知识，我们就可以通过如下命令在两台 Linux 机器间建立一个<strong>点对点的 VXLAN 隧道</strong>：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-shell" data-lang="shell"><span class="c1"># 在主机 A 上创建 VTEP 设备 vxlan0</span>
<span class="c1"># 与另一个 vtep 接口 B（192.168.8.101）建立隧道</span>
<span class="c1"># 将 vxlan0 自身的 IP 地址设为 192.168.8.100</span>
<span class="c1"># 使用的 VXLAN 目标端口为 4789(IANA 标准)</span>
ip link add vxlan0 <span class="nb">type</span> vxlan <span class="se">\
</span><span class="se"></span>    id <span class="m">42</span> <span class="se">\
</span><span class="se"></span>    dstport <span class="m">4789</span> <span class="se">\
</span><span class="se"></span>    remote 192.168.8.101 <span class="se">\
</span><span class="se"></span>    <span class="nb">local</span> 192.168.8.100 <span class="se">\
</span><span class="se"></span>    dev enp0s8
<span class="c1"># 为我们的 VXLAN 网络设置虚拟网段，vxlan0 就是默认网关</span>
ip addr add 10.20.1.2/24 dev vxlan0
<span class="c1"># 启用我们的 vxlan0 设备，这会自动生成路由规则</span>
ip link <span class="nb">set</span> vxlan0 up

<span class="c1"># 现在在主机 B 上运行如下命令，同样创建一个 VTEP 设备 vxlan0，remote 和 local 的 ip 与前面用的命令刚好相反。</span>
<span class="c1"># 注意 VNID 和 dstport 必须和前面完全一致</span>
ip link add vxlan0 <span class="nb">type</span> vxlan <span class="se">\
</span><span class="se"></span>    id <span class="m">42</span> <span class="se">\
</span><span class="se"></span>    dstport <span class="m">4789</span> <span class="se">\
</span><span class="se"></span>    remote 192.168.8.100 <span class="se">\
</span><span class="se"></span>    <span class="nb">local</span> 192.168.8.101 <span class="se">\
</span><span class="se"></span>    dev enp0s8
<span class="c1"># 为我们的 VXLAN 网络设置虚拟网段，vxlan0 就是默认网关</span>
ip addr add 10.20.1.3/24 dev vxlan0
ip link <span class="nb">set</span> vxlan0 up

<span class="c1"># 到这里，两台机器就完成连接，可以通信了。可以在主机 B 上 ping 10.20.1.2 试试，应该能收到主机 A 的回应。</span>
ping 10.20.1.2
</code></pre></td></tr></table>
</div>
</div><p>点对点的 vxlan 隧道实际用处不大，如果集群中的每个节点都互相建 vxlan 隧道，代价太高了。</p>
<p>一种更好的方式，是使用 <strong>「组播模式」的 vxlan 隧道</strong>，这种模式下一个 vtep 可以一次与组内的所有 vtep 建立隧道。
示例命令如下（这里略过了如何设置组播地址 <code>239.1.1.1</code> 的信息）：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-shell" data-lang="shell">ip link add vxlan0 <span class="nb">type</span> vxlan <span class="se">\
</span><span class="se"></span>    id <span class="m">42</span> <span class="se">\
</span><span class="se"></span>    dstport <span class="m">4789</span> <span class="se">\
</span><span class="se"></span>    group 239.1.1.1 <span class="se">\
</span><span class="se"></span>    dev enp0s8 
ip addr add 10.20.1.2/24 dev vxlan0
ip link <span class="nb">set</span> vxlan0 up
</code></pre></td></tr></table>
</div>
</div><p>可以看到，只需要简单地把 local_ip/remote_ip 替换成一个组播地址就行。组播功能会将收到的数据包发送给组里的所有 vtep 接口，但是只有 VNID 能对上的 vtep 会处理该报文，其他 vtep 会直接丢弃数据。</p>
<p>接下来，为了能让所有的虚拟机/容器，都通过 vtep 通信，我们再添加一个 bridge 网络，充当 vtep 与容器间的交换机。架构如下：</p>
<p><figure><a class="lightgallery" href="/images/linux-virtual-interfaces/linux-vxlan-with-bridge.jpg" title="/images/linux-virtual-interfaces/linux-vxlan-with-bridge.jpg" data-thumbnail="/images/linux-virtual-interfaces/linux-vxlan-with-bridge.jpg" data-sub-html="<h2>VXLAN 多播网络架构</h2>">
        
    </a><figcaption class="image-caption">VXLAN 多播网络架构</figcaption>
    </figure></p>
<p>使用 ip 命令创建网桥、网络名字空间、veth pairs 组成上图中的容器网络：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-shell" data-lang="shell"><span class="c1"># 创建 br0 并将 vxlan0 绑定上去</span>
ip link add br0 <span class="nb">type</span> bridge
ip link <span class="nb">set</span> vxlan0 master bridge
ip link <span class="nb">set</span> vxlan0 up
ip link <span class="nb">set</span> br0 up

<span class="c1"># 模拟将容器加入到网桥中的操作</span>
ip netns add container1

<span class="c1">## 创建 veth pair，并把一端加到网桥上</span>
ip link add veth0 <span class="nb">type</span> veth peer name veth1
ip link <span class="nb">set</span> dev veth0 master br0
ip link <span class="nb">set</span> dev veth0 up

<span class="c1">## 配置容器内部的网络和 IP</span>
ip link <span class="nb">set</span> dev veth1 netns container1
ip netns <span class="nb">exec</span> container1 ip link <span class="nb">set</span> lo up

ip netns <span class="nb">exec</span> container1 ip link <span class="nb">set</span> veth1 name eth0
ip netns <span class="nb">exec</span> container1 ip addr add 10.20.1.11/24 dev eth0
ip netns <span class="nb">exec</span> container1 ip link <span class="nb">set</span> eth0 up
</code></pre></td></tr></table>
</div>
</div><p>然后在另一台机器上做同样的操作，并创建新容器，两个容器就能通过 vxlan 通信啦~</p>
<h3 id="比组播更高效的-vxlan-实现">比组播更高效的 vxlan 实现</h3>
<p>组播最大的问题在于，因为它不知道数据的目的地，所以每个 vtep 都发了一份。如果每次发数据时，如果能够精确到对应的 vtep，就能节约大量资源。</p>
<p>另一个问题是 ARP 查询也会被组播，要知道 vxlan 本身就是个 overlay 网络，ARP 的成本也很高。</p>
<p>上述问题都可以通过一个中心化的注册中心（如 etcd）来解决，所有容器、网络的注册与变更，都写入到这个注册中心，然后由程序自动维护 vtep 之间的隧道、fdb 表及 ARP 表.</p>
<h2 id="八虚拟网络接口的速率">八、虚拟网络接口的速率</h2>
<p>Loopback 和本章讲到的其他虚拟网络接口一样，都是一种软件模拟的网络设备。
他们的速率是不是也像物理链路一样，存在链路层（比如以太网）的带宽限制呢？</p>
<p>比如目前很多老旧的网络设备，都是只支持到百兆以太网，这就决定了它的带宽上限。
即使是较新的设备，目前基本也都只支持到千兆，也就是 1GbE 以太网标准，那本文提到的虚拟网络接口单纯在本机内部通信，是否也存在这样的制约呢？是否也只能跑到 1GbE?</p>
<p>使用 ethtool 检查：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback"># docker 容器的 veth 接口速率
&gt; ethtool vethe899841 | grep Speed
        Speed: 10000Mb/s

# 网桥看起来没有固定的速率
&gt; ethtool docker0 | grep Speed
        Speed: Unknown!

# tun0 设备的默认速率貌似是 10Mb/s ?
&gt; ethtool tun0 | grep Speed
        Speed: 10Mb/s

# 此外 ethtool 无法检查 lo 以及 wifi 的速率
</code></pre></td></tr></table>
</div>
</div><h3 id="网络性能实测">网络性能实测</h3>
<p>接下来实际测试一下，先给出机器参数：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback">❯ cat /etc/os-release 
NAME=&#34;openSUSE Tumbleweed&#34;
# VERSION=&#34;20210810&#34;
...

❯ uname -a
Linux legion-book 5.13.8-1-default #1 SMP Thu Aug 5 08:56:22 UTC 2021 (967c6a8) x86_64 x86_64 x86_64 GNU/Linux


❯ lscpu
Architecture:                    x86_64
CPU(s):                          16
Model name:                      AMD Ryzen 7 5800H with Radeon Graphics
...

# 内存，单位 MB
❯ free -m
               total        used        free      shared  buff/cache   available
Mem:           27929        4482       17324         249        6122       22797
Swap:           2048           0        2048
</code></pre></td></tr></table>
</div>
</div><p>使用 iperf3 测试：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span><span class="lnt">43
</span><span class="lnt">44
</span><span class="lnt">45
</span><span class="lnt">46
</span><span class="lnt">47
</span><span class="lnt">48
</span><span class="lnt">49
</span><span class="lnt">50
</span><span class="lnt">51
</span><span class="lnt">52
</span><span class="lnt">53
</span><span class="lnt">54
</span><span class="lnt">55
</span><span class="lnt">56
</span><span class="lnt">57
</span><span class="lnt">58
</span><span class="lnt">59
</span><span class="lnt">60
</span><span class="lnt">61
</span><span class="lnt">62
</span><span class="lnt">63
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-shell" data-lang="shell"><span class="c1"># 启动服务端</span>
iperf3 -s

-------------
<span class="c1"># 新窗口启动客户端，通过 loopback 接口访问 iperf3-server，大概 49Gb/s</span>
❯ iperf3 -c 127.0.0.1
Connecting to host 127.0.0.1, port <span class="m">5201</span>
<span class="o">[</span>  5<span class="o">]</span> <span class="nb">local</span> 127.0.0.1 port <span class="m">48656</span> connected to 127.0.0.1 port <span class="m">5201</span>
<span class="o">[</span> ID<span class="o">]</span> Interval           Transfer     Bitrate         Retr  Cwnd
<span class="o">[</span>  5<span class="o">]</span>   0.00-1.00   sec  4.46 GBytes  38.3 Gbits/sec    <span class="m">0</span>   1.62 MBytes       
<span class="o">[</span>  5<span class="o">]</span>   1.00-2.00   sec  4.61 GBytes  39.6 Gbits/sec    <span class="m">0</span>   1.62 MBytes       
<span class="o">[</span>  5<span class="o">]</span>   2.00-3.00   sec  5.69 GBytes  48.9 Gbits/sec    <span class="m">0</span>   1.62 MBytes       
<span class="o">[</span>  5<span class="o">]</span>   3.00-4.00   sec  6.11 GBytes  52.5 Gbits/sec    <span class="m">0</span>   1.62 MBytes       
<span class="o">[</span>  5<span class="o">]</span>   4.00-5.00   sec  6.04 GBytes  51.9 Gbits/sec    <span class="m">0</span>   1.62 MBytes       
<span class="o">[</span>  5<span class="o">]</span>   5.00-6.00   sec  6.05 GBytes  52.0 Gbits/sec    <span class="m">0</span>   1.62 MBytes       
<span class="o">[</span>  5<span class="o">]</span>   6.00-7.00   sec  6.01 GBytes  51.6 Gbits/sec    <span class="m">0</span>   1.62 MBytes       
<span class="o">[</span>  5<span class="o">]</span>   7.00-8.00   sec  6.05 GBytes  52.0 Gbits/sec    <span class="m">0</span>   1.62 MBytes       
<span class="o">[</span>  5<span class="o">]</span>   8.00-9.00   sec  6.34 GBytes  54.5 Gbits/sec    <span class="m">0</span>   1.62 MBytes       
<span class="o">[</span>  5<span class="o">]</span>   9.00-10.00  sec  5.91 GBytes  50.8 Gbits/sec    <span class="m">0</span>   1.62 MBytes       
- - - - - - - - - - - - - - - - - - - - - - - - -
<span class="o">[</span> ID<span class="o">]</span> Interval           Transfer     Bitrate         Retr
<span class="o">[</span>  5<span class="o">]</span>   0.00-10.00  sec  57.3 GBytes  49.2 Gbits/sec    <span class="m">0</span>             sender
<span class="o">[</span>  5<span class="o">]</span>   0.00-10.00  sec  57.3 GBytes  49.2 Gbits/sec                  receiver

<span class="c1"># 客户端通过 wlp4s0 wifi 网卡(192.168.31.228)访问 iperf3-server，实际还是走的本机，但是速度要比 loopback 快一点，可能是默认设置的问题</span>
❯ iperf3 -c 192.168.31.228
Connecting to host 192.168.31.228, port <span class="m">5201</span>
<span class="o">[</span>  5<span class="o">]</span> <span class="nb">local</span> 192.168.31.228 port <span class="m">43430</span> connected to 192.168.31.228 port <span class="m">5201</span>
<span class="o">[</span> ID<span class="o">]</span> Interval           Transfer     Bitrate         Retr  Cwnd
<span class="o">[</span>  5<span class="o">]</span>   0.00-1.00   sec  5.12 GBytes  43.9 Gbits/sec    <span class="m">0</span>   1.25 MBytes       
<span class="o">[</span>  5<span class="o">]</span>   1.00-2.00   sec  5.29 GBytes  45.5 Gbits/sec    <span class="m">0</span>   1.25 MBytes       
<span class="o">[</span>  5<span class="o">]</span>   2.00-3.00   sec  5.92 GBytes  50.9 Gbits/sec    <span class="m">0</span>   1.25 MBytes       
<span class="o">[</span>  5<span class="o">]</span>   3.00-4.00   sec  6.00 GBytes  51.5 Gbits/sec    <span class="m">0</span>   1.25 MBytes       
<span class="o">[</span>  5<span class="o">]</span>   4.00-5.00   sec  5.98 GBytes  51.4 Gbits/sec    <span class="m">0</span>   1.25 MBytes       
<span class="o">[</span>  5<span class="o">]</span>   5.00-6.00   sec  6.05 GBytes  52.0 Gbits/sec    <span class="m">0</span>   1.25 MBytes       
<span class="o">[</span>  5<span class="o">]</span>   6.00-7.00   sec  6.16 GBytes  52.9 Gbits/sec    <span class="m">0</span>   1.25 MBytes       
<span class="o">[</span>  5<span class="o">]</span>   7.00-8.00   sec  6.08 GBytes  52.2 Gbits/sec    <span class="m">0</span>   1.25 MBytes       
<span class="o">[</span>  5<span class="o">]</span>   8.00-9.00   sec  6.00 GBytes  51.6 Gbits/sec    <span class="m">0</span>   1.25 MBytes       
<span class="o">[</span>  5<span class="o">]</span>   9.00-10.00  sec  6.01 GBytes  51.6 Gbits/sec    <span class="m">0</span>   1.25 MBytes       
- - - - - - - - - - - - - - - - - - - - - - - - -
<span class="o">[</span> ID<span class="o">]</span> Interval           Transfer     Bitrate         Retr
<span class="o">[</span>  5<span class="o">]</span>   0.00-10.00  sec  58.6 GBytes  50.3 Gbits/sec    <span class="m">0</span>             sender
<span class="o">[</span>  5<span class="o">]</span>   0.00-10.00  sec  58.6 GBytes  50.3 Gbits/sec                  receiver

<span class="c1"># 从容器中访问宿主机的 iperf3-server，速度几乎没区别</span>
❯ docker run  -it --rm --name<span class="o">=</span>iperf3-server networkstatic/iperf3 -c 192.168.31.228
Connecting to host 192.168.31.228, port <span class="m">5201</span>
<span class="o">[</span>  5<span class="o">]</span> <span class="nb">local</span> 172.17.0.2 port <span class="m">43436</span> connected to 192.168.31.228 port <span class="m">5201</span>
<span class="o">[</span> ID<span class="o">]</span> Interval           Transfer     Bitrate         Retr  Cwnd
<span class="o">[</span>  5<span class="o">]</span>   0.00-1.00   sec  4.49 GBytes  38.5 Gbits/sec    <span class="m">0</span>    <span class="m">403</span> KBytes       
<span class="o">[</span>  5<span class="o">]</span>   1.00-2.00   sec  5.31 GBytes  45.6 Gbits/sec    <span class="m">0</span>    <span class="m">544</span> KBytes       
<span class="o">[</span>  5<span class="o">]</span>   2.00-3.00   sec  6.14 GBytes  52.8 Gbits/sec    <span class="m">0</span>    <span class="m">544</span> KBytes       
<span class="o">[</span>  5<span class="o">]</span>   3.00-4.00   sec  5.85 GBytes  50.3 Gbits/sec    <span class="m">0</span>    <span class="m">544</span> KBytes       
<span class="o">[</span>  5<span class="o">]</span>   4.00-5.00   sec  6.14 GBytes  52.7 Gbits/sec    <span class="m">0</span>    <span class="m">544</span> KBytes       
<span class="o">[</span>  5<span class="o">]</span>   5.00-6.00   sec  5.99 GBytes  51.5 Gbits/sec    <span class="m">0</span>    <span class="m">544</span> KBytes       
<span class="o">[</span>  5<span class="o">]</span>   6.00-7.00   sec  5.86 GBytes  50.4 Gbits/sec    <span class="m">0</span>    <span class="m">544</span> KBytes       
<span class="o">[</span>  5<span class="o">]</span>   7.00-8.00   sec  6.05 GBytes  52.0 Gbits/sec    <span class="m">0</span>    <span class="m">544</span> KBytes       
<span class="o">[</span>  5<span class="o">]</span>   8.00-9.00   sec  5.99 GBytes  51.5 Gbits/sec    <span class="m">0</span>    <span class="m">544</span> KBytes       
<span class="o">[</span>  5<span class="o">]</span>   9.00-10.00  sec  6.12 GBytes  52.5 Gbits/sec    <span class="m">0</span>    <span class="m">544</span> KBytes       
- - - - - - - - - - - - - - - - - - - - - - - - -
<span class="o">[</span> ID<span class="o">]</span> Interval           Transfer     Bitrate         Retr
<span class="o">[</span>  5<span class="o">]</span>   0.00-10.00  sec  58.0 GBytes  49.8 Gbits/sec    <span class="m">0</span>             sender
<span class="o">[</span>  5<span class="o">]</span>   0.00-10.00  sec  58.0 GBytes  49.8 Gbits/sec                  receiver
</code></pre></td></tr></table>
</div>
</div><p>把 iperf3-server 跑在容器里再测一遍：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span><span class="lnt">43
</span><span class="lnt">44
</span><span class="lnt">45
</span><span class="lnt">46
</span><span class="lnt">47
</span><span class="lnt">48
</span><span class="lnt">49
</span><span class="lnt">50
</span><span class="lnt">51
</span><span class="lnt">52
</span><span class="lnt">53
</span><span class="lnt">54
</span><span class="lnt">55
</span><span class="lnt">56
</span><span class="lnt">57
</span><span class="lnt">58
</span><span class="lnt">59
</span><span class="lnt">60
</span><span class="lnt">61
</span><span class="lnt">62
</span><span class="lnt">63
</span><span class="lnt">64
</span><span class="lnt">65
</span><span class="lnt">66
</span><span class="lnt">67
</span><span class="lnt">68
</span><span class="lnt">69
</span><span class="lnt">70
</span><span class="lnt">71
</span><span class="lnt">72
</span><span class="lnt">73
</span><span class="lnt">74
</span><span class="lnt">75
</span><span class="lnt">76
</span><span class="lnt">77
</span><span class="lnt">78
</span><span class="lnt">79
</span><span class="lnt">80
</span><span class="lnt">81
</span><span class="lnt">82
</span><span class="lnt">83
</span><span class="lnt">84
</span><span class="lnt">85
</span><span class="lnt">86
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-shell" data-lang="shell"><span class="c1"># 在容器中启动 iperf3-server，并映射到宿主机端口 6201</span>
&gt; docker run  -it --rm --name<span class="o">=</span>iperf3-server -p 6201:5201 networkstatic/iperf3 -s
&gt; docker inspect --format <span class="s2">&#34;{{ .NetworkSettings.IPAddress }}&#34;</span> iperf3-server
172.17.0.2
-----------------------------
<span class="c1"># 测试容器之间互访的速度，ip 为 iperf3-server 的容器 ip，速度要慢一些。</span>
<span class="c1"># 毕竟过了 veth -&gt; veth -&gt; docker0 -&gt; veth -&gt; veth 五层虚拟网络接口</span>
❯ docker run  -it --rm networkstatic/iperf3 -c 172.17.0.2
Connecting to host 172.17.0.2, port <span class="m">5201</span>
<span class="o">[</span>  5<span class="o">]</span> <span class="nb">local</span> 172.17.0.3 port <span class="m">40776</span> connected to 172.17.0.2 port <span class="m">5201</span>
<span class="o">[</span> ID<span class="o">]</span> Interval           Transfer     Bitrate         Retr  Cwnd
<span class="o">[</span>  5<span class="o">]</span>   0.00-1.00   sec  4.74 GBytes  40.7 Gbits/sec    <span class="m">0</span>    <span class="m">600</span> KBytes       
<span class="o">[</span>  5<span class="o">]</span>   1.00-2.00   sec  4.48 GBytes  38.5 Gbits/sec    <span class="m">0</span>    <span class="m">600</span> KBytes       
<span class="o">[</span>  5<span class="o">]</span>   2.00-3.00   sec  5.38 GBytes  46.2 Gbits/sec    <span class="m">0</span>    <span class="m">600</span> KBytes       
<span class="o">[</span>  5<span class="o">]</span>   3.00-4.00   sec  5.39 GBytes  46.3 Gbits/sec    <span class="m">0</span>    <span class="m">600</span> KBytes       
<span class="o">[</span>  5<span class="o">]</span>   4.00-5.00   sec  5.42 GBytes  46.6 Gbits/sec    <span class="m">0</span>    <span class="m">600</span> KBytes       
<span class="o">[</span>  5<span class="o">]</span>   5.00-6.00   sec  5.39 GBytes  46.3 Gbits/sec    <span class="m">0</span>    <span class="m">600</span> KBytes       
<span class="o">[</span>  5<span class="o">]</span>   6.00-7.00   sec  5.38 GBytes  46.2 Gbits/sec    <span class="m">0</span>    <span class="m">635</span> KBytes       
<span class="o">[</span>  5<span class="o">]</span>   7.00-8.00   sec  5.37 GBytes  46.1 Gbits/sec    <span class="m">0</span>    <span class="m">667</span> KBytes       
<span class="o">[</span>  5<span class="o">]</span>   8.00-9.00   sec  6.01 GBytes  51.7 Gbits/sec    <span class="m">0</span>    <span class="m">735</span> KBytes       
<span class="o">[</span>  5<span class="o">]</span>   9.00-10.00  sec  5.74 GBytes  49.3 Gbits/sec    <span class="m">0</span>    <span class="m">735</span> KBytes       
- - - - - - - - - - - - - - - - - - - - - - - - -
<span class="o">[</span> ID<span class="o">]</span> Interval           Transfer     Bitrate         Retr
<span class="o">[</span>  5<span class="o">]</span>   0.00-10.00  sec  53.3 GBytes  45.8 Gbits/sec    <span class="m">0</span>             sender
<span class="o">[</span>  5<span class="o">]</span>   0.00-10.00  sec  53.3 GBytes  45.8 Gbits/sec                  receiver

<span class="c1"># 本机直接访问容器 ip，走的是 docker0 网桥，居然还挺快</span>
❯ iperf3 -c 172.17.0.2
Connecting to host 172.17.0.2, port <span class="m">5201</span>
<span class="o">[</span>  5<span class="o">]</span> <span class="nb">local</span> 172.17.0.1 port <span class="m">56486</span> connected to 172.17.0.2 port <span class="m">5201</span>
<span class="o">[</span> ID<span class="o">]</span> Interval           Transfer     Bitrate         Retr  Cwnd
<span class="o">[</span>  5<span class="o">]</span>   0.00-1.00   sec  5.01 GBytes  43.0 Gbits/sec    <span class="m">0</span>    <span class="m">632</span> KBytes       
<span class="o">[</span>  5<span class="o">]</span>   1.00-2.00   sec  5.19 GBytes  44.6 Gbits/sec    <span class="m">0</span>    <span class="m">703</span> KBytes       
<span class="o">[</span>  5<span class="o">]</span>   2.00-3.00   sec  6.46 GBytes  55.5 Gbits/sec    <span class="m">0</span>    <span class="m">789</span> KBytes       
<span class="o">[</span>  5<span class="o">]</span>   3.00-4.00   sec  6.80 GBytes  58.4 Gbits/sec    <span class="m">0</span>    <span class="m">789</span> KBytes       
<span class="o">[</span>  5<span class="o">]</span>   4.00-5.00   sec  6.82 GBytes  58.6 Gbits/sec    <span class="m">0</span>    <span class="m">913</span> KBytes       
<span class="o">[</span>  5<span class="o">]</span>   5.00-6.00   sec  6.79 GBytes  58.3 Gbits/sec    <span class="m">0</span>   <span class="m">1007</span> KBytes       
<span class="o">[</span>  5<span class="o">]</span>   6.00-7.00   sec  6.63 GBytes  56.9 Gbits/sec    <span class="m">0</span>   1.04 MBytes       
<span class="o">[</span>  5<span class="o">]</span>   7.00-8.00   sec  6.75 GBytes  58.0 Gbits/sec    <span class="m">0</span>   1.04 MBytes       
<span class="o">[</span>  5<span class="o">]</span>   8.00-9.00   sec  6.19 GBytes  53.2 Gbits/sec    <span class="m">0</span>   1.04 MBytes       
<span class="o">[</span>  5<span class="o">]</span>   9.00-10.00  sec  6.55 GBytes  56.3 Gbits/sec    <span class="m">0</span>   1.04 MBytes       
- - - - - - - - - - - - - - - - - - - - - - - - -
<span class="o">[</span> ID<span class="o">]</span> Interval           Transfer     Bitrate         Retr
<span class="o">[</span>  5<span class="o">]</span>   0.00-10.00  sec  63.2 GBytes  54.3 Gbits/sec    <span class="m">0</span>             sender
<span class="o">[</span>  5<span class="o">]</span>   0.00-10.00  sec  63.2 GBytes  54.3 Gbits/sec                  receiver

<span class="c1"># 如果走本机 loopback 地址 + 容器端口映射，速度就慢了好多</span>
<span class="c1"># 或许是因为用 iptables 做端口映射导致的？</span>
❯ iperf3 -c 127.0.0.1 -p <span class="m">6201</span>
Connecting to host 127.0.0.1, port <span class="m">6201</span>
<span class="o">[</span>  5<span class="o">]</span> <span class="nb">local</span> 127.0.0.1 port <span class="m">48862</span> connected to 127.0.0.1 port <span class="m">6201</span>
<span class="o">[</span> ID<span class="o">]</span> Interval           Transfer     Bitrate         Retr  Cwnd
<span class="o">[</span>  5<span class="o">]</span>   0.00-1.00   sec  2.71 GBytes  23.3 Gbits/sec    <span class="m">0</span>   1.37 MBytes       
<span class="o">[</span>  5<span class="o">]</span>   1.00-2.00   sec  3.64 GBytes  31.3 Gbits/sec    <span class="m">0</span>   1.37 MBytes       
<span class="o">[</span>  5<span class="o">]</span>   2.00-3.00   sec  4.08 GBytes  35.0 Gbits/sec    <span class="m">0</span>   1.37 MBytes       
<span class="o">[</span>  5<span class="o">]</span>   3.00-4.00   sec  3.49 GBytes  30.0 Gbits/sec    <span class="m">0</span>   1.37 MBytes       
<span class="o">[</span>  5<span class="o">]</span>   4.00-5.00   sec  5.50 GBytes  47.2 Gbits/sec    <span class="m">2</span>   1.37 MBytes       
<span class="o">[</span>  5<span class="o">]</span>   5.00-6.00   sec  4.06 GBytes  34.9 Gbits/sec    <span class="m">0</span>   1.37 MBytes       
<span class="o">[</span>  5<span class="o">]</span>   6.00-7.00   sec  4.12 GBytes  35.4 Gbits/sec    <span class="m">0</span>   1.37 MBytes       
<span class="o">[</span>  5<span class="o">]</span>   7.00-8.00   sec  3.99 GBytes  34.3 Gbits/sec    <span class="m">0</span>   1.37 MBytes       
<span class="o">[</span>  5<span class="o">]</span>   8.00-9.00   sec  3.49 GBytes  30.0 Gbits/sec    <span class="m">0</span>   1.37 MBytes       
<span class="o">[</span>  5<span class="o">]</span>   9.00-10.00  sec  5.51 GBytes  47.3 Gbits/sec    <span class="m">0</span>   1.37 MBytes       
- - - - - - - - - - - - - - - - - - - - - - - - -
<span class="o">[</span> ID<span class="o">]</span> Interval           Transfer     Bitrate         Retr
<span class="o">[</span>  5<span class="o">]</span>   0.00-10.00  sec  40.6 GBytes  34.9 Gbits/sec    <span class="m">2</span>             sender
<span class="o">[</span>  5<span class="o">]</span>   0.00-10.00  sec  40.6 GBytes  34.9 Gbits/sec                  receiver

<span class="c1"># 可走 wlp4s0 + 容器端口映射，速度也不慢啊</span>
❯ iperf3 -c 192.168.31.228 -p <span class="m">6201</span>
Connecting to host 192.168.31.228, port <span class="m">6201</span>
<span class="o">[</span>  5<span class="o">]</span> <span class="nb">local</span> 192.168.31.228 port <span class="m">54582</span> connected to 192.168.31.228 port <span class="m">6201</span>
<span class="o">[</span> ID<span class="o">]</span> Interval           Transfer     Bitrate         Retr  Cwnd
<span class="o">[</span>  5<span class="o">]</span>   0.00-1.00   sec  4.34 GBytes  37.3 Gbits/sec    <span class="m">0</span>    <span class="m">795</span> KBytes       
<span class="o">[</span>  5<span class="o">]</span>   1.00-2.00   sec  4.78 GBytes  41.0 Gbits/sec    <span class="m">0</span>    <span class="m">834</span> KBytes       
<span class="o">[</span>  5<span class="o">]</span>   2.00-3.00   sec  6.26 GBytes  53.7 Gbits/sec    <span class="m">0</span>    <span class="m">834</span> KBytes       
<span class="o">[</span>  5<span class="o">]</span>   3.00-4.00   sec  6.30 GBytes  54.1 Gbits/sec    <span class="m">0</span>    <span class="m">875</span> KBytes       
<span class="o">[</span>  5<span class="o">]</span>   4.00-5.00   sec  6.26 GBytes  53.8 Gbits/sec    <span class="m">0</span>    <span class="m">875</span> KBytes       
<span class="o">[</span>  5<span class="o">]</span>   5.00-6.00   sec  5.75 GBytes  49.4 Gbits/sec    <span class="m">0</span>    <span class="m">875</span> KBytes       
<span class="o">[</span>  5<span class="o">]</span>   6.00-7.00   sec  5.49 GBytes  47.2 Gbits/sec    <span class="m">0</span>    <span class="m">966</span> KBytes       
<span class="o">[</span>  5<span class="o">]</span>   7.00-8.00   sec  5.72 GBytes  49.1 Gbits/sec    <span class="m">2</span>    <span class="m">966</span> KBytes       
<span class="o">[</span>  5<span class="o">]</span>   8.00-9.00   sec  4.81 GBytes  41.3 Gbits/sec    <span class="m">2</span>    <span class="m">966</span> KBytes       
<span class="o">[</span>  5<span class="o">]</span>   9.00-10.00  sec  5.98 GBytes  51.4 Gbits/sec    <span class="m">0</span>    <span class="m">966</span> KBytes       
- - - - - - - - - - - - - - - - - - - - - - - - -
<span class="o">[</span> ID<span class="o">]</span> Interval           Transfer     Bitrate         Retr
<span class="o">[</span>  5<span class="o">]</span>   0.00-10.00  sec  55.7 GBytes  47.8 Gbits/sec    <span class="m">4</span>             sender
<span class="o">[</span>  5<span class="o">]</span>   0.00-10.00  sec  55.7 GBytes  47.8 Gbits/sec                  receiver
</code></pre></td></tr></table>
</div>
</div><p>总的来看，loopback、bridge、veth 这几个接口基本上是没被限速的，veth 有查到上限为 10000Mb/s（10Gb/s） 感觉也是个假数字，
实际上测出来的数据基本在 35Gb/s 到 55Gb/s 之间，视情况浮动。</p>
<p>性能的变化和虚拟网络设备的链路和类型有关，或许和默认配置的区别也有关系。</p>
<p>另外 TUN 设备这里没有测，<code>ethtool tun0</code> 查到的值是比较离谱的 10Mb/s，但是感觉不太可能这么慢，有时间可以再测一波看看。</p>
<h2 id="参考">参考</h2>
<ul>
<li><a href="https://segmentfault.com/a/1190000009249039" target="_blank" rel="noopener noreferrer">Linux虚拟网络设备之tun/tap</a></li>
<li><a href="https://segmentfault.com/a/1190000009251098" target="_blank" rel="noopener noreferrer">Linux虚拟网络设备之veth</a></li>
<li><a href="https://opengers.github.io/openstack/openstack-base-virtual-network-devices-bridge-and-vlan/" target="_blank" rel="noopener noreferrer">云计算底层技术-虚拟网络设备(Bridge,VLAN)</a></li>
<li><a href="https://opengers.github.io/openstack/openstack-base-virtual-network-devices-tuntap-veth/" target="_blank" rel="noopener noreferrer">云计算底层技术-虚拟网络设备(tun/tap,veth)</a></li>
<li><a href="https://www.kernel.org/doc/Documentation/networking/tuntap.txt" target="_blank" rel="noopener noreferrer">Universal TUN/TAP device driver - Kernel Docs</a></li>
<li><a href="https://backreference.org/2010/03/26/tuntap-interface-tutorial/" target="_blank" rel="noopener noreferrer">Tun/Tap interface tutorial</a></li>
<li><a href="https://stackoverflow.com/questions/5832308/linux-loopback-performance-with-tcp-nodelay-enabled" target="_blank" rel="noopener noreferrer">Linux Loopback performance with TCP_NODELAY enabled</a></li>
</ul>
]]></description></item><item><title>Linux 网络工具中的瑞士军刀 - socat &amp; netcat</title><link>https://ryan4yin.space/posts/socat-netcat/</link><pubDate>Sun, 11 Apr 2021 16:38:13 +0800</pubDate><author>xiaoyin_c@qq.com</author><dc:creator>ryan4yin</dc:creator><guid>https://ryan4yin.space/posts/socat-netcat/</guid><description><![CDATA[<blockquote>
<p>文中的命令均在 macOS Big Sur 和 Opensuse Tumbleweed 上测试通过</p>
</blockquote>
<h2 id="socat--netcat">socat &amp; netcat</h2>
<p>netcat(network cat) 是一个历史悠久的网络工具包，被称作 TCP/IP 的瑞士军刀，各大 Linux 发行版都有默认安装 openbsd 版本的 netcat，它的命令行名称为 <code>nc</code>.</p>
<p>而 socat(socket cat)，官方文档描述它是 <code>&quot;netcat++&quot; (extended design, new implementation)</code>，项目比较活跃，kubernetes-client(kubectl) 底层就是使用的它做各种流量转发。</p>
<p>在不方便安装 socat 的环境中，我们可以使用系统自带的 netcat.
而在其他环境，可以考虑优先使用 socat.</p>
<h2 id="一简介">一、简介</h2>
<p>socat 的基本命令格式：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-shell" data-lang="shell">socat <span class="o">[</span>参数<span class="o">]</span> 地址1 地址2
</code></pre></td></tr></table>
</div>
</div><p>给 socat 提供两个地址，socat 干的活就是把两个地址的流对接起来。左边地址的输出传给右边，同时又把右边地址的输出传给左边，也就是一个<strong>双向的数据管道</strong>。</p>
<p>听起来好像没啥特别的，但是实际上计算机网络干的活也就是数据传输而已，却影响了整个世界，不可小觑它的功能。</p>
<p>socat 支持非常多的地址类型：<code>-</code>/stdio，TCP, TCP-LISTEN, UDP, UDP-LISTEN, OPEN, EXEC, SOCKS, PROXY 等等，可用于端口监听、链接，文件和进程读写，代理桥接等等。</p>
<p>socat 的功能就是这么简单，命令行参数也很简洁，唯一需要花点精力学习的就是它各种地址的定义和搭配写法。</p>
<p>而 netcat 定义貌似没这么严谨，可以简单的理解为网络版的 cat 命令 2333</p>
<h2 id="二安装方法">二、安装方法</h2>
<p>各发行版都自带 netcat，包名通常为 <code>nc-openbsd</code>，因此这里只介绍 socat 的安装方法：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-shell" data-lang="shell"><span class="c1"># Debian/Ubuntu</span>
sudo apt install socat

<span class="c1"># CentOS/RedHat</span>
sudo yum install socat

<span class="c1"># macOS</span>
brew install socat
</code></pre></td></tr></table>
</div>
</div><p>其他发行版基本也都可以使用包管理器安装 socat</p>
<h2 id="三常用命令">三、常用命令</h2>
<h3 id="1-网络调试">1. 网络调试</h3>
<h4 id="11-检测远程端口的可连接性确认防火墙没问题">1.1 检测远程端口的可连接性（确认防火墙没问题）</h4>
<blockquote>
<p>以前你可能学过如何用 telnet 来做这项测试，不过现在很多发行版基本都不自带 telnet 了，还需要额外安装。
telnet 差不多已经快寿终正寝了，还是建议使用更专业的 socat/netcat</p>
</blockquote>
<p>使用 socat/netcat 检测远程端口的可连接性：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-shell" data-lang="shell"><span class="c1"># -d[ddd] 增加日志详细程度，-dd  Prints fatal, error, warning, and notice messages.</span>
socat -dd - TCP:192.168.1.252:3306

<span class="c1"># -v 显示详细信息</span>
<span class="c1"># -z 不发送数据，效果为立即关闭连接，快速得出结果</span>
nc -vz 192.168.1.2 <span class="m">8080</span>

<span class="c1"># -vv 显示更详细的内容</span>
<span class="c1"># -w2 超时时间设为 2 秒</span>
<span class="c1"># 使用 nc 做简单的端口扫描</span>
nc -vv -w2 -z 192.168.1.2 20-500
</code></pre></td></tr></table>
</div>
</div><h4 id="12-测试本机端口是否能正常被外部访问检测防火墙路由">1.2 测试本机端口是否能正常被外部访问（检测防火墙、路由）</h4>
<p>在本机监听一个 TCP 端口，接收到的内容传到 stdout，同时将 stdin 的输入传给客户端：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-shell" data-lang="shell"><span class="c1"># 服务端启动命令，socat/nc 二选一</span>
socat TCP-LISTEN:7000 -
<span class="c1"># -l --listening</span>
nc -l <span class="m">7000</span>

<span class="c1"># 客户端连接命令，socat/nc 二选一</span>
socat TCP:192.168.31.123:7000 -
nc 192.168.11.123 <span class="m">7000</span>
</code></pre></td></tr></table>
</div>
</div><p>UDP 协议的测试也非常类似，使用 netcat 的示例如下：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-shell" data-lang="shell"><span class="c1"># 服务端，只监听 ipv4</span>
nc -u -l <span class="m">8080</span>

<span class="c1"># 客户端</span>
nc -u 192.168.31.123 <span class="m">8080</span>
<span class="c1"># 客户端本机测试，注意 localhost 会被优先解析为 ipv6! 这会导致服务端(ipv4)的 nc 接收不到数据！</span>
nc -u localhost <span class="m">8080</span>
</code></pre></td></tr></table>
</div>
</div><p>使用 socat 的 UDP 测试示例如下：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-shell" data-lang="shell">socat UDP-LISTEN:7000 -

socat UDP:192.168.31.123:7000 -
</code></pre></td></tr></table>
</div>
</div><h4 id="13-调试-tls-协议">1.3 调试 TLS 协议</h4>
<blockquote>
<p>参考 socat 官方文档：<a href="http://www.dest-unreach.org/socat/doc/socat-openssltunnel.html" target="_blank" rel="noopener noreferrer">Securing Traffic Between two Socat Instances Using SSL</a></p>
</blockquote>
<blockquote>
<p>测试证书及私钥的生成参见 <a href="https://ryan4yin.space/posts/about-tls-cert/" rel="">TLS 协议、TLS 证书、TLS 证书的配置方法、TLS 加密的破解手段</a></p>
</blockquote>
<p>模拟一个 mTLS 服务器，监听 4433 端口，接收到的数据同样输出到 stdout：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-shell" data-lang="shell"><span class="c1"># socat 需要使用同时包含证书和私钥的 pem 文件，生成方法如下</span>
cat server.key server.crt &gt; server.pem
cat client.key client.crt &gt; client.pem

<span class="c1"># 服务端启动命令</span>
socat openssl-listen:4433,reuseaddr,cert<span class="o">=</span>server.pem,cafile<span class="o">=</span>client.crt -

<span class="c1"># 客户端连接命令</span>
socat - openssl-connect:192.168.31.123:4433,cert<span class="o">=</span>client.pem,cafile<span class="o">=</span>server.crt
<span class="c1"># 或者使用 curl 连接(我们知道 ca.crt 和 server.crt 都能被用做 cacert/cafile)</span>
curl -v --cacert ca.crt --cert client.crt --key client.key --tls-max 1.2 https://192.168.31.123:4433
</code></pre></td></tr></table>
</div>
</div><p>上面的命令使用了 mTLS 双向认证的协议，可通过设定 <code>verify=0</code> 来关掉客户端认证，示例如下：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-shell" data-lang="shell">
<span class="c1"># socat 需要使用同时包含证书和私钥的 pem 文件，生成方法如下</span>
cat server.key server.crt &gt; server.pem

<span class="c1"># 服务端启动命令</span>
socat openssl-listen:4433,reuseaddr,cert<span class="o">=</span>server.pem,verify<span class="o">=</span><span class="m">0</span> -

<span class="c1"># 客户端连接命令，如果 ip/域名不受证书保护，就也需要添加 verify=0</span>
socat - openssl-connect:192.168.31.123:4433,cafile<span class="o">=</span>server.crt
<span class="c1"># 或者使用 curl 连接，证书无效请添加 -k 跳过证书验证</span>
curl -v --cacert server.crt https://192.168.31.123:4433
</code></pre></td></tr></table>
</div>
</div><h2 id="2-数据传输">2. 数据传输</h2>
<p>通常传输文件时，我都习惯使用 scp/ssh/rsync，但是 socat 其实也可以传输文件。</p>
<p>以将 demo.tar.gz 从主机 A 发送到主机 B 为例，
首先在数据发送方 A 执行如下命令：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-shell" data-lang="shell"><span class="c1"># -u 表示数据只从左边的地址单向传输给右边（socat 默认是一个双向管道）</span>
<span class="c1"># -U 和 -u 相反，数据只从右边单向传输给左边</span>
socat -u open:demo.tar.gz tcp-listen:2000,reuseaddr
</code></pre></td></tr></table>
</div>
</div><p>然后在数据接收方 B 执行如下命令，就能把文件接收到：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-shell" data-lang="shell">socat -u tcp:192.168.1.252:2000 open:demo.tar.gz,create
<span class="c1"># 如果觉得太繁琐，也可以直接通过 stdout 重定向</span>
socat -u tcp:192.168.1.252:2000 - &gt; demo.tar.gz
</code></pre></td></tr></table>
</div>
</div><p>使用 netcat 也可以实现数据传输：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-shell" data-lang="shell"><span class="c1"># 先在接收方启动服务端</span>
nc -l -p <span class="m">8080</span> &gt; demo.tar.gz
<span class="c1"># 再在发送方启动客户端发送数据</span>
nc 192.168.1.2 <span class="m">8080</span> &lt; demo.tar.gz
</code></pre></td></tr></table>
</div>
</div><h2 id="3-担当临时的-web-服务器">3. 担当临时的 web 服务器</h2>
<p>使用 <code>fork</code> <code>reuseaddr</code> <code>SYSTEM</code> 三个命令，再用 <code>systemd</code>/<code>supervisor</code> 管理一下，就可以用几行命令实现一个简单的后台服务器。</p>
<p>下面的命令将监听 8080 端口，并将数据流和 web.py 的 stdio 连接起来，可以直接使用浏览器访问 <code>http://&lt;ip&gt;:8080</code> 来查看效果。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-shell" data-lang="shell">socat TCP-LISTEN:8080,reuseaddr,fork SYSTEM:<span class="s2">&#34;python3 web.py&#34;</span>
</code></pre></td></tr></table>
</div>
</div><p>假设 <code>web.py</code> 的内容为：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-python" data-lang="python"><span class="k">print</span><span class="p">(</span><span class="s2">&#34;hello world&#34;</span><span class="p">)</span>
</code></pre></td></tr></table>
</div>
</div><p>那 <code>curl localhost:8080</code> 就应该会输出 <code>hello world</code></p>
<h2 id="4-端口转发">4. 端口转发</h2>
<p>监听 8080 端口，建立该端口与 <code>baidu.com:80</code> 之间的双向管道:</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-shell" data-lang="shell">socat TCP-LISTEN:8080,fork,reuseaddr  TCP:baidu.com:80
</code></pre></td></tr></table>
</div>
</div><p>拿 curl 命令测试一下，应该能正常访问到百度：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-shell" data-lang="shell"><span class="c1"># 注意指定 Host</span>
curl -v -H <span class="s1">&#39;Host: baidu.com&#39;</span> localhost:8080
</code></pre></td></tr></table>
</div>
</div><h2 id="参考">参考</h2>
<ul>
<li><a href="https://zhuanlan.zhihu.com/p/347722248" target="_blank" rel="noopener noreferrer">新版瑞士军刀：socat - 韦易笑 - 知乎</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/83959309" target="_blank" rel="noopener noreferrer">用好你的瑞士军刀/netcat - 韦易笑 - 知乎</a></li>
<li><a href="http://www.dest-unreach.org/socat/" target="_blank" rel="noopener noreferrer">socat - Multipurpose relay</a></li>
</ul>
]]></description></item><item><title>脚踏实地，仰望星空</title><link>https://ryan4yin.space/posts/no-more-dreams/</link><pubDate>Sat, 13 Feb 2021 10:32:56 +0800</pubDate><author>xiaoyin_c@qq.com</author><dc:creator>ryan4yin</dc:creator><guid>https://ryan4yin.space/posts/no-more-dreams/</guid><description><![CDATA[<!-- 每个人的一生都是一次远行 -->
<meting-js server="netease" type="song" id="176048" theme="#448aff"></meting-js>
<h2 id="年轻真好">年轻真好</h2>
<p>最近看了些前辈们的博客，很多是在计算机行业工作几十年的前辈，还有许嵩的文章。</p>
<p>我更深刻地认识到了一件事：我当下的很多文章，都能看得出我在很认真的思考、总结，但是总是有很明显的稚嫩的感觉在里面——我自认为这是「学生型思维」。</p>
<p>我总是喜欢讲「且行且寻」、「自己的眼界还太狭窄了，我对世界还很缺乏了解」、「根本看不清好坏，就无法独立做出决策」诸如此类。</p>
<p>我把这样的文章写出来，前辈们给我留言「博主只是沉淀的时间还远远不够。憋着急，年轻就是最大的资本。」、「只想说年轻真好，使劲折腾才知道要什么东西」。</p>
<p>嗯，我理解到了，因为我「<strong>年轻</strong>」，所以写出这样的文章没问题，可以使劲去折腾、去探索、去思考。</p>
<h2 id="三十而立">三十而立</h2>
<blockquote>
<p>子曰：吾，十有五，而志于学，三十而立，四十而不惑，五十而知天命，六十而耳顺，七十而从心所欲，不逾矩。</p>
</blockquote>
<p>孔子说：“我十五岁立志学习，三十岁在人生道路上站稳脚跟，四十岁心中不再迷惘，五十岁知道上天给我安排的命运，六十岁听到别人说话就能分辨是非真假，七十岁能随心所欲地说话做事，又不会超越规矩。”</p>
<p>「四十而不惑」对我而言可能还太远，但「三十而立」却是已经能预见到了的，没几年了。</p>
<p><strong>三十而立，人到了三十岁，就应该知道自己如何立身处世，尘世滚滚中能守住自己的一点本真不失</strong>。</p>
<p>三十岁，已不是一个年轻的年纪了。</p>
<p><strong>如果我到了三十岁，还去写些「自己的眼界还太狭窄了，我对世界还很缺乏了解」、「我根本看不清好坏，很多时候无法独立做出决策」，那就贻笑大方了</strong>。</p>
<p>所以即使说「年轻就是最大的资本」，也不是能随意挥霍的！人生这条道路上我们踽踽独行，道阻且长，眼光要放长远一点、多看一点，不要把自己限制住了，更不应该原地踏步！</p>
<h2 id="许嵩我没有梦想">许嵩——我没有梦想</h2>
<p>这两天看多了前辈们的博客，就想找点非虚构的书藉看看，补充点阅历。</p>
<p>昨天向朋友们讨书看，@rea.ink 就给我推荐许嵩的《海上灵光》。意外地发现了<a href="http://blog.sina.com.cn/vae" target="_blank" rel="noopener noreferrer">许嵩的新浪博客</a>。</p>
<p>博客的内容都很老了，最新的一篇是 2013 年。但是这并不妨碍其中见解的价值</p>
<div class="details admonition reference open">
        <div class="details-summary admonition-title">
            <i class="icon fas fa-pencil-alt fa-fw"></i>海上灵光——许嵩<i class="details-icon fas fa-angle-right fa-fw"></i>
        </div>
        <div class="details-content">
            <div class="admonition-content"><p>以前媒体问我接下来有什么计划或梦想时我总是很愣的回答，我没有梦想。</p>
<p>真的，一个年过半百的人还把梦想这种字眼挂在嘴上是很乏味的。</p>
<p>睁大眼看看眼前的生活，周遭的一切吧。</p>
<p>脚踏实地认真过好每一天的生活吧。</p>
<p>至于心底的信念——是决计不必拿出来高谈阔论的。</p>
</div>
        </div>
    </div>
<div class="details admonition reference open">
        <div class="details-summary admonition-title">
            <i class="icon fas fa-pencil-alt fa-fw"></i>出离心——许嵩<i class="details-icon fas fa-angle-right fa-fw"></i>
        </div>
        <div class="details-content">
            <div class="admonition-content"><p>这几个月，走过了不少地方。</p>
<p>每到一处，采访我的媒体通常会有这么一问：你的音乐理想是什么？</p>
<p>而当答案是“我从来没有理想”时，我迎接那些错愕的眼神。</p>
<p>年轻的时候，拥有一些世俗的念想（比如声名远播？）、一些物质上的期待（比如大房子好车子？）、一些精神上的憧憬（比如寻得佳偶？）、一些相对崇高的目标（比如造福子孙？！），似乎的确能让一些人更有动力的过活每一天。</p>
<p>但如果，岁月在你脸上已然留下不少年轮——你坐船的动机仍然只是到达一座岛，别人把岛上的一切美妙和宝藏说给你听就可以让你划船划的更带劲儿——那我能对你说些什么呢？</p>
</div>
        </div>
    </div>
<h2 id="池建强你老了">池建强——你老了</h2>
<p>这两天读到了一篇池建强写的<a href="http://macshuo.com/?p=1491" target="_blank" rel="noopener noreferrer">《你老了》</a>，作者是极客时间创始人，真的是年过半百的技术人了。</p>
<div class="details admonition reference open">
        <div class="details-summary admonition-title">
            <i class="icon fas fa-pencil-alt fa-fw"></i>你老了 - 池建强的随想录<i class="details-icon fas fa-angle-right fa-fw"></i>
        </div>
        <div class="details-content">
            <div class="admonition-content"><p>40 以后，不惑是不可能的，恐慌是与日俱增的。四十不惑，说得不是你想明白了，而是你想不明白的，可能就想不明白了，生日变成另一种仪式，它严肃的告诉你，同学，不要有任何幻想了，接受这个现实，你已经不再年轻了。再卖萌也改变不了这个事实。</p>
<p>人们总会长大，成熟，衰老，一如万事万物。今何在说，人从一出生开始，就踏上了自己的西游路，一路向西，到了尽头，就是虚无，人就没了。所有人都不可避免要奔向那个归宿，你没办法选择，没办法回头。</p>
</div>
        </div>
    </div>
<div class="details admonition reference open">
        <div class="details-summary admonition-title">
            <i class="icon fas fa-pencil-alt fa-fw"></i>你老了 - 池建强的随想录<i class="details-icon fas fa-angle-right fa-fw"></i>
        </div>
        <div class="details-content">
            <div class="admonition-content"><p>你跳不出这个世界，是因为你不知道这个世界有多大，一旦你知道了，你就超出了它。</p>
<p>年龄也是如此。</p>
</div>
        </div>
    </div>
<h2 id="梦想不要多的想看世界也不是靠说的">梦想不要多的，想看世界也不是靠说的</h2>
<p>既然说了要多走走看看，那就多看多想。</p>
<p>就像许嵩写的那样，不必去高谈阔论什么理想与信念，实际行动才是最有力的证明。</p>
<blockquote>
<p>Keep eyes on the stars, and feet on the ground.</p>
</blockquote>
]]></description></item><item><title>云原生流水线 Argo Workflows 的安装、使用以及个人体验</title><link>https://ryan4yin.space/posts/expirence-of-argo-workflow/</link><pubDate>Wed, 27 Jan 2021 15:37:27 +0800</pubDate><author>xiaoyin_c@qq.com</author><dc:creator>ryan4yin</dc:creator><guid>https://ryan4yin.space/posts/expirence-of-argo-workflow/</guid><description><![CDATA[<blockquote>
<p>注意：这篇文章并不是一篇入门教程，学习 Argo Workflows 请移步官方文档 <a href="https://argoproj.github.io/argo-workflows/" target="_blank" rel="noopener noreferrer">Argo Documentation</a></p>
</blockquote>
<p><a href="https://github.com/argoproj/argo-workflows/" target="_blank" rel="noopener noreferrer">Argo Workflows</a> 是一个云原生工作流引擎，专注于<strong>编排并行任务</strong>。它的特点如下：</p>
<ol>
<li>使用 Kubernetes 自定义资源(CR)定义工作流，其中工作流中的每个步骤都是一个容器。</li>
<li>将多步骤工作流建模为一系列任务，或者使用有向无环图（DAG）描述任务之间的依赖关系。</li>
<li>可以在短时间内轻松运行用于机器学习或数据处理的计算密集型作业。</li>
<li>Argo Workflows 可以看作 Tekton 的加强版，因此显然也可以通过 Argo Workflows 运行 CI/CD 流水线(Pipielines)。</li>
</ol>
<p>阿里云是 Argo Workflows 的深度使用者和贡献者，另外 Kubeflow 底层的工作流引擎也是 Argo Workflows.</p>
<h2 id="一argo-workflows-对比-jenkins">一、Argo Workflows 对比 Jenkins</h2>
<p>我们在切换到 Argo Workflows 之前，使用的 CI/CD 工具是 Jenkins，下面对 Argo Workflows 和 Jenkins 做一个比较详细的对比，
以了解 Argo Workflows 的优缺点。</p>
<h3 id="1-workflow-的定义">1. Workflow 的定义</h3>
<p><code>Workflow</code> 使用 kubernetes CR 进行定义，因此显然是一份 yaml 配置。</p>
<p>一个 Workflow，就是一个运行在 Kubernetes 上的流水线，对应 Jenkins 的一次 Build.</p>
<p>而 WorkflowTemplate 则是一个可重用的 Workflow 模板，对应 Jenkins 的一个 Job.</p>
<p><code>WorkflowTemplate</code> 的 yaml 定义和 <code>Workflow</code> 完全一致，只有 <code>Kind</code> 不同！</p>
<p>WorkflowTemplate 可以被其他 Workflow 引用并触发，也可以手动传参以生成一个 Workflow 工作流。</p>
<h3 id="2-workflow-的编排">2. Workflow 的编排</h3>
<p>Argo Workflows 相比其他流水线项目(Jenkins/Tekton/Drone/Gitlab-CI)而言，最大的特点，就是它强大的流水线编排能力。</p>
<p>其他流水线项目，对流水线之间的关联性考虑得很少，基本都假设流水线都是互相独立的。</p>
<p>而 Argo Workflows 则假设「任务」之间是有依赖关系的，针对这个依赖关系，它提供了两种协调编排「任务」的方法：Steps 和 DAG</p>
<p>再借助 <a href="https://argoproj.github.io/argo/workflow-templates/#referencing-other-workflowtemplates" target="_blank" rel="noopener noreferrer">templateRef</a> 或者 <a href="https://argoproj.github.io/argo/workflow-of-workflows/" target="_blank" rel="noopener noreferrer">Workflow of Workflows</a>，就能实现 Workflows 的编排了。</p>
<p><strong>我们之所以选择 Argo Workflows 而不是 Tekton，主要就是因为 Argo 的流水线编排能力比 Tekton 强大得多。</strong>（也许是因为我们的后端中台结构比较特殊，导致我们的 CI 流水线需要具备复杂的编排能力）</p>
<p>一个复杂工作流的示例如下：</p>
<p><figure><a class="lightgallery" href="/images/expirence-of-argo-workflow/complex-workflows.png" title="/images/expirence-of-argo-workflow/complex-workflows.png" data-thumbnail="/images/expirence-of-argo-workflow/complex-workflows.png" data-sub-html="<h2>https://github.com/argoproj/argo/issues/1088#issuecomment-445884543</h2>">
        
    </a><figcaption class="image-caption">https://github.com/argoproj/argo/issues/1088#issuecomment-445884543</figcaption>
    </figure></p>
<h3 id="3-workflow-的声明式配置">3. Workflow 的声明式配置</h3>
<p>Argo 使用 Kubernetes 自定义资源(CR)来定义 Workflow，熟悉 Kubernetes Yaml 的同学上手应该都很快。</p>
<p>下面对 Workflow 定义文件和 Jenkinsfile 做个对比：</p>
<ol>
<li>argo 完全使用 yaml 来定义流水线，学习成本比 Jenkinsfile 的 groovy 低。对熟悉 Kubernetes 的同学尤其如此。</li>
<li>将 jenkinsfile 用 argo 重写后，代码量出现了明显的膨胀。一个 20 行的 Jenkinsfile，用 Argo 重写可能就变成了 60 行。</li>
</ol>
<p>配置出现了膨胀是个问题，但是考虑到它的可读性还算不错，
而且 Argo 的 Workflow 编排功能，能替代掉我们目前维护的部分 Python 构建代码，以及一些其他优点，配置膨胀这个问题也就可以接受了。</p>
<h3 id="4-web-ui">4. Web UI</h3>
<p>Argo Workflows 的 Web UI 感觉还很原始。确实该支持的功能都有，但是它貌似不是面向「用户」的，功能比较底层。</p>
<p>它不像 Jenkins 一样，有很友好的使用界面(虽然说 Jenkins 的 UI 也很显老&hellip;)</p>
<p>另外它所有的 Workflow 都是相互独立的，没办法直观地找到一个 WorkflowTemplate 的所有构建记录，只能通过 label/namespace 进行分类，通过任务名称进行搜索。</p>
<p>而 Jenkins 可以很方便地看到同一个 Job 的所有构建历史。</p>
<h3 id="5-workflow-的分类">5. Workflow 的分类</h3>
<h4 id="为何需要对-workflow-做细致的分类">为何需要对 Workflow 做细致的分类</h4>
<p>常见的微服务项目，往往会拆分成众多 Git 仓库（微服务）进行开发，众多的 Git 仓库会使我们创建众多的 CI/CD 流水线。
如果没有任何的分类，这一大堆的流水线如何管理，就成了一个难题。</p>
<p>最显见的需求：前端和后端的流水线最好能区分一下，往下细分，前端的 Web 端和客户端最好也能区分，后端的业务层和中台最好也区分开来。</p>
<p>另外我们还希望将运维、自动化测试相关的任务也集成到这个系统中来（目前我们就是使用 Jenkins 完成运维、自动化测试任务的），
如果没有任何分类，这一大堆流水线将混乱无比。</p>
<h4 id="argo-workflows-的分类能力">Argo Workflows 的分类能力</h4>
<p>当 Workflow 越来越多的时候，如果不做分类，一堆 WorkflowTemplate 堆在一起就会显得特别混乱。（没错，我觉得 Drone 就有这个问题&hellip;）</p>
<p>Argo 是完全基于 Kubernetes 的，因此目前它也只能通过 namespace/labels 进行分类。</p>
<p>这样的分类结构和 Jenkins 的视图-文件夹体系大相径庭，目前感觉不是很好用（也可能纯粹是 Web UI 的锅）。</p>
<h3 id="6-触发构建的方式">6. 触发构建的方式</h3>
<p>Argo Workflows 的流水线有多种触发方式：</p>
<ul>
<li>手动触发：手动提交一个 Workflow，就能触发一次构建。可以通过 <a href="https://argoproj.github.io/argo/workflow-templates/#create-workflow-from-workflowtemplate-spec" target="_blank" rel="noopener noreferrer">workflowTemplateRef</a> 直接引用一个现成的流水线模板。</li>
<li>定时触发：<a href="https://argoproj.github.io/argo/cron-workflows/" target="_blank" rel="noopener noreferrer">CronWorkflow</a></li>
<li>通过 Git 仓库变更触发：借助 <a href="https://github.com/argoproj/argo-events" target="_blank" rel="noopener noreferrer">argo-events</a> 可以实现此功能，详见其文档。
<ul>
<li>另外目前也不清楚 WebHook 的可靠程度如何，会不会因为宕机、断网等故障，导致 Git 仓库变更了，而 Workflow 却没触发，而且还没有任何显眼的错误通知？如果这个错误就这样藏起来了，就可能会导致很严重的问题！</li>
</ul>
</li>
</ul>
<h3 id="7-secrets-管理">7. secrets 管理</h3>
<p>Argo Workflows 的流水线，可以从 kubernetes secrets/configmap 中获取信息，将信息注入到环境变量中、或者以文件形式挂载到 Pod 中。</p>
<p>Git 私钥、Harbor 仓库凭据、CD 需要的 kubeconfig，都可以直接从 secrets/configmap 中获取到。</p>
<p>另外因为 Vault 很流行，也可以将 secrets 保存在 Vault 中，再通过 vault agent 将配置注入进 Pod。</p>
<h3 id="8-artifacts">8. Artifacts</h3>
<p>Argo 支持接入对象存储，做全局的 Artifact 仓库，本地可以使用 MinIO.</p>
<p>使用对象存储存储 Artifact，最大的好处就是可以在 Pod 之间随意传数据，Pod 可以完全分布式地运行在 Kubernetes 集群的任何节点上。</p>
<p>另外也可以考虑借助 Artifact 仓库实现跨流水线的缓存复用（未测试），提升构建速度。</p>
<h3 id="9-容器镜像的构建">9. 容器镜像的构建</h3>
<p>借助 Buildkit 等容器镜像构建工具，可以实现容器镜像的分布式构建。</p>
<p>Buildkit 对构建缓存的支持也很好，可以直接将缓存存储在容器镜像仓库中。</p>
<blockquote>
<p>不建议使用 Google 的 Kaniko，它对缓存复用的支持不咋地，社区也不活跃。</p>
</blockquote>
<h3 id="10-客户端sdk">10. 客户端/SDK</h3>
<p>Argo 有提供一个命令行客户端，也有 HTTP API 可供使用。</p>
<p>如下项目值得试用：</p>
<ul>
<li><a href="https://github.com/argoproj-labs/argo-client-python" target="_blank" rel="noopener noreferrer">argo-client-python</a>: Argo Workflows 的 Python 客户端
<ul>
<li>说实话，感觉和 kubernetes-client/python 一样难用，毕竟都是 openapi-generator 生成出来的&hellip;</li>
</ul>
</li>
<li><a href="https://github.com/argoproj-labs/argo-python-dsl" target="_blank" rel="noopener noreferrer">argo-python-dsl</a>: 使用 Python DSL 编写 Argo Workflows
<ul>
<li>感觉使用难度比 yaml 高，也不太好用。</li>
</ul>
</li>
<li><a href="https://github.com/couler-proj/couler" target="_blank" rel="noopener noreferrer">couler</a>: 为  Argo/Tekton/Airflow 提供统一的构建与管理接口
<ul>
<li>理念倒是很好，待研究</li>
</ul>
</li>
</ul>
<p>感觉 couler 挺不错的，可以直接用 Python 写 WorkflowTemplate，这样就一步到位，所有 CI/CD 代码全部是 Python 了。</p>
<p>此外，因为 Argo Workflows 是 kubernetes 自定义资源 CR，也可以使用 helm/kustomize 来做 workflow 的生成。</p>
<p>目前我们一些步骤非常多，但是重复度也很高的 Argo 流水线配置，就是使用 helm 生成的——关键数据抽取到 values.yaml 中，使用 helm 模板 + <code>range</code> 循环来生成 workflow 配置。</p>
<h2 id="二安装-argo-workflowshttpsargoprojgithubioargoinstallation">二、<a href="https://argoproj.github.io/argo/installation/" target="_blank" rel="noopener noreferrer">安装 Argo Workflows</a></h2>
<p>安装一个集群版(cluster wide)的 Argo Workflows，使用 MinIO 做 artifacts 存储：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-shell" data-lang="shell">kubectl apply -f https://raw.githubusercontent.com/argoproj/argo/stable/manifests/install.yaml
</code></pre></td></tr></table>
</div>
</div><p>部署 MinIO:</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span><span class="lnt">9
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-shell" data-lang="shell">helm repo add minio https://helm.min.io/ <span class="c1"># official minio Helm charts</span>
<span class="c1"># 查看历史版本</span>
helm search repo minio/minio -l <span class="p">|</span> head
<span class="c1"># 下载并解压 chart</span>
helm pull minio/minio --untar --version 8.0.9

<span class="c1"># 编写 custom-values.yaml，然后部署 minio</span>
kubectl create namespace minio
helm install minio ./minio -n argo -f custom-values.yaml
</code></pre></td></tr></table>
</div>
</div><p>minio 部署好后，它会将默认的 <code>accesskey</code> 和 <code>secretkey</code> 保存在名为 <code>minio</code> 的 secret 中。
我们需要修改 argo 的配置，将 minio 作为它的默认 artifact 仓库。</p>
<p>在 configmap <code>workflow-controller-configmap</code> 的 data 中添加如下字段：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-shell" data-lang="shell">  artifactRepository: <span class="p">|</span>
    <span class="c1"># 是否将 main 容器的日志保存为 artifact，这样 pod 被删除后，仍然可以在 artifact 中找到日志</span>
    archiveLogs: <span class="nb">true</span>
    s3:
      bucket: argo-bucket   <span class="c1"># bucket 名称，这个 bucket 需要先手动创建好！</span>
      endpoint: minio:9000  <span class="c1"># minio 地址</span>
      insecure: <span class="nb">true</span>
      <span class="c1"># 从 minio 这个 secret 中获取 key/secret</span>
      accessKeySecret:
        name: minio
        key: accesskey
      secretKeySecret:
        name: minio
        key: secretkey
</code></pre></td></tr></table>
</div>
</div><p>现在还差最后一步：手动进入 minio 的 Web UI，创建好 <code>argo-bucket</code> 这个 bucket.
直接访问 minio 的 9000 端口（需要使用 nodeport/ingress 等方式暴露此端口）就能进入 Web UI，使用前面提到的 secret <code>minio</code> 中的 key/secret 登录，就能创建 bucket.</p>
<h3 id="serviceaccount-配置httpsargoprojgithubioargoservice-accounts"><a href="https://argoproj.github.io/argo/service-accounts/" target="_blank" rel="noopener noreferrer">ServiceAccount 配置</a></h3>
<p>Argo Workflows 依赖于 ServiceAccount 进行验证与授权，而且默认情况下，它使用所在 namespace 的 <code>default</code> ServiceAccount 运行 workflow.</p>
<p>可 <code>default</code> 这个 ServiceAccount 默认根本没有任何权限！所以 Argo 的 artifacts, outputs, access to secrets 等功能全都会因为权限不足而无法使用！</p>
<p>为此，Argo 的官方文档提供了两个解决方法。</p>
<p>方法一，直接给 default 绑定 <code>cluster-admin</code> ClusterRole，给它集群管理员的权限，只要一行命令（但是显然安全性堪忧）：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-shell" data-lang="shell">kubectl create rolebinding default-admin --clusterrole<span class="o">=</span>admin --serviceaccount<span class="o">=</span>&lt;namespace&gt;:default -n &lt;namespace&gt;
</code></pre></td></tr></table>
</div>
</div><p>方法二，官方给出了<a href="https://argoproj.github.io/argo/workflow-rbac/" target="_blank" rel="noopener noreferrer">Argo Workflows 需要的最小权限的 Role 定义</a>，方便起见我将它改成一个 ClusterRole:</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-yaml" data-lang="yaml"><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l">rbac.authorization.k8s.io/v1</span><span class="w">
</span><span class="w"></span><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l">ClusterRole</span><span class="w">
</span><span class="w"></span><span class="nt">metadata</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">argo-workflow-role</span><span class="w">
</span><span class="w"></span><span class="nt">rules</span><span class="p">:</span><span class="w">
</span><span class="w"></span><span class="c"># pod get/watch is used to identify the container IDs of the current pod</span><span class="w">
</span><span class="w"></span><span class="c"># pod patch is used to annotate the step&#39;s outputs back to controller (e.g. artifact location)</span><span class="w">
</span><span class="w"></span>- <span class="nt">apiGroups</span><span class="p">:</span><span class="w">
</span><span class="w">  </span>- <span class="s2">&#34;&#34;</span><span class="w">
</span><span class="w">  </span><span class="nt">resources</span><span class="p">:</span><span class="w">
</span><span class="w">  </span>- <span class="l">pods</span><span class="w">
</span><span class="w">  </span><span class="nt">verbs</span><span class="p">:</span><span class="w">
</span><span class="w">  </span>- <span class="l">get</span><span class="w">
</span><span class="w">  </span>- <span class="l">watch</span><span class="w">
</span><span class="w">  </span>- <span class="l">patch</span><span class="w">
</span><span class="w"></span><span class="c"># logs get/watch are used to get the pods logs for script outputs, and for log archival</span><span class="w">
</span><span class="w"></span>- <span class="nt">apiGroups</span><span class="p">:</span><span class="w">
</span><span class="w">  </span>- <span class="s2">&#34;&#34;</span><span class="w">
</span><span class="w">  </span><span class="nt">resources</span><span class="p">:</span><span class="w">
</span><span class="w">  </span>- <span class="l">pods/log</span><span class="w">
</span><span class="w">  </span><span class="nt">verbs</span><span class="p">:</span><span class="w">
</span><span class="w">  </span>- <span class="l">get</span><span class="w">
</span><span class="w">  </span>- <span class="l">watch</span><span class="w">
</span></code></pre></td></tr></table>
</div>
</div><p>创建好上面这个最小的 ClusterRole，然后为每个名字空间，跑一下如下命令，给 default 账号绑定这个 clusterrole:</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-shell" data-lang="shell">kubectl create rolebinding default-argo-workflow --clusterrole<span class="o">=</span>argo-workflow-role  --serviceaccount<span class="o">=</span>&lt;namespace&gt;:default -n &lt;namespace&gt;
</code></pre></td></tr></table>
</div>
</div><p>这样就能给 default 账号提供最小的 workflow 运行权限。</p>
<p>或者如果你希望使用别的 ServiceAccount 来运行 workflow，也可以自行创建 ServiceAccount，然后再走上面方法二的流程，但是最后，要记得在 workflow 的 <code>spec.serviceAccountName</code> 中设定好 ServiceAccount 名称。</p>
<h3 id="workflow-executorshttpsargoprojgithubioargoworkflow-executors"><a href="https://argoproj.github.io/argo/workflow-executors/" target="_blank" rel="noopener noreferrer">Workflow Executors</a></h3>
<p>Workflow Executor 是符合特定接口的一个进程(Process)，Argo 可以通过它执行一些动作，如监控 Pod 日志、收集 Artifacts、管理容器生命周期等等&hellip;</p>
<p>Workflow Executor 有多种实现，可以通过前面提到的 configmap <code>workflow-controller-configmap</code> 来选择。</p>
<p>可选项如下：</p>
<ol>
<li>docker(默认): 目前使用范围最广，但是安全性最差。它要求一定要挂载访问 <code>docker.sock</code>，因此一定要 root 权限！</li>
<li>kubelet: 应用非常少，目前功能也有些欠缺，目前也必须提供 root 权限</li>
<li>Kubernetes API (k8sapi): 直接通过调用 k8sapi 实现日志监控、Artifacts 手机等功能，非常安全，但是性能欠佳。</li>
<li>Process Namespace Sharing (pns): 安全性比 k8sapi 差一点，因为 Process 对其他所有容器都可见了。但是相对的性能好很多。</li>
</ol>
<p>在 docker 被 kubernetes 抛弃的当下，如果你已经改用 containerd 做为 kubernetes 运行时，那 argo 将会无法工作，因为它默认使用 docker 作为运行时！</p>
<p>我们建议将 workflow executore 改为 <code>pns</code>，兼顾安全性与性能，<code>workflow-controller-configmap</code> 按照如下方式修改：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-yaml" data-lang="yaml"><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l">v1</span><span class="w">
</span><span class="w"></span><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l">ConfigMap</span><span class="w">
</span><span class="w"></span><span class="nt">metadata</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">workflow-controller-configmap</span><span class="w">
</span><span class="w"></span><span class="nt">data</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">config</span><span class="p">:</span><span class="w"> </span><span class="p">|</span><span class="sd">
</span><span class="sd">    # ...省略若干配置...
</span><span class="sd">
</span><span class="sd">    # Specifies the container runtime interface to use (default: docker)
</span><span class="sd">    # must be one of: docker, kubelet, k8sapi, pns
</span><span class="sd">    containerRuntimeExecutor: pns
</span><span class="sd">    # ...</span><span class="w">    
</span></code></pre></td></tr></table>
</div>
</div><h2 id="三使用-argo-workflows-做-ci-工具">三、使用 Argo Workflows 做 CI 工具</h2>
<p>官方的 Reference 还算详细，也有提供非常多的 examples 供我们参考，这里提供我们几个常用的 workflow 定义。</p>
<ol>
<li>使用 buildkit 构建镜像：https://github.com/argoproj/argo-workflows/blob/master/examples/buildkit-template.yaml
<ol>
<li>buildkit 支持缓存，可以在这个 example 的基础上自定义参数</li>
<li>注意使用 PVC 来跨 step 共享存储空间这种手段，速度会比通过 artifacts 高很多。</li>
</ol>
</li>
</ol>
<h2 id="四常见问题">四、常见问题</h2>
<h3 id="1-workflow-默认使用-root-账号">1. workflow 默认使用 root 账号？</h3>
<p>workflow 的流程默认使用 root 账号，如果你的镜像默认使用非 root 账号，而且要修改文件，就很可能遇到 Permission Denined 的问题。</p>
<p>解决方法：通过 Pod Security Context 手动设定容器的 user/group:</p>
<ul>
<li><a href="https://argoproj.github.io/argo/workflow-pod-security-context/" target="_blank" rel="noopener noreferrer">Workflow Pod Security Context</a></li>
</ul>
<p>安全起见，我建议所有的 workflow 都手动设定 <code>securityContext</code>，示例：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-yaml" data-lang="yaml"><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l">argoproj.io/v1alpha1</span><span class="w">
</span><span class="w"></span><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l">WorkflowTemplate</span><span class="w">
</span><span class="w"></span><span class="nt">metadata</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">xxx</span><span class="w">
</span><span class="w"></span><span class="nt">spec</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">securityContext</span><span class="p">:</span><span class="w">
</span><span class="w">    </span><span class="nt">runAsNonRoot</span><span class="p">:</span><span class="w"> </span><span class="kc">true</span><span class="w">
</span><span class="w">    </span><span class="nt">runAsUser</span><span class="p">:</span><span class="w"> </span><span class="m">1000</span><span class="w">
</span></code></pre></td></tr></table>
</div>
</div><p>或者也可以通过 <code>workflow-controller-configmap</code> 的 <code>workflowDefaults</code> 设定默认的 workflow 配置。</p>
<h3 id="2-如何从-hashicorp-vault-中读取-secrets">2. 如何从 hashicorp vault 中读取 secrets?</h3>
<blockquote>
<p>参考 <a href="https://github.com/argoproj/argo/issues/3267#issuecomment-650119636" target="_blank" rel="noopener noreferrer">Support to get secrets from Vault</a></p>
</blockquote>
<p>hashicorp vault 目前可以说是云原生领域最受欢迎的 secrets 管理工具。
我们在生产环境用它做为分布式配置中心，同时在本地 CI/CD 中，也使用它存储相关的敏感信息。</p>
<p>现在迁移到 argo，我们当然希望能够有一个好的方法从 vault 中读取配置。</p>
<p>目前最推荐的方法，是使用 vault 的 vault-agent，将 secrets 以文件的形式注入到 pod 中。</p>
<p>通过 valut-policy - vault-role - k8s-serviceaccount 一系列认证授权配置，可以制定非常细粒度的 secrets 权限规则，而且配置信息阅后即焚，安全性很高。</p>
<h3 id="3-如何在多个名字空间中使用同一个-secrets">3. 如何在多个名字空间中使用同一个 secrets?</h3>
<p>使用 Namespace 对 workflow 进行分类时，遇到的一个常见问题就是，如何在多个名字空间使用 <code>private-git-creds</code>/<code>docker-config</code>/<code>minio</code>/<code>vault</code> 等 workflow 必要的 secrets.</p>
<p>常见的方法是把 secrets 在所有名字空间 create 一次。</p>
<p>但是也有更方便的 secrets 同步工具：</p>
<p>比如，使用 <a href="https://github.com/kyverno/kyverno" target="_blank" rel="noopener noreferrer">kyverno</a> 进行 secrets 同步的配置：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-yaml" data-lang="yaml"><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l">kyverno.io/v1</span><span class="w">
</span><span class="w"></span><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l">ClusterPolicy</span><span class="w">
</span><span class="w"></span><span class="nt">metadata</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">sync-secrets</span><span class="w">
</span><span class="w"></span><span class="nt">spec</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">background</span><span class="p">:</span><span class="w"> </span><span class="kc">false</span><span class="w">
</span><span class="w">  </span><span class="nt">rules</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="c"># 将 secret vault 从 argo Namespace 同步到其他所有 Namespace</span><span class="w">
</span><span class="w">  </span>- <span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">sync-vault-secret</span><span class="w">
</span><span class="w">    </span><span class="nt">match</span><span class="p">:</span><span class="w">
</span><span class="w">      </span><span class="nt">resources</span><span class="p">:</span><span class="w">
</span><span class="w">        </span><span class="nt">kinds</span><span class="p">:</span><span class="w">
</span><span class="w">        </span>- <span class="l">Namespace</span><span class="w">
</span><span class="w">    </span><span class="nt">generate</span><span class="p">:</span><span class="w">
</span><span class="w">      </span><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l">Secret</span><span class="w">
</span><span class="w">      </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">regcred</span><span class="w">
</span><span class="w">      </span><span class="nt">namespace</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;{{request.object.metadata.name}}&#34;</span><span class="w">
</span><span class="w">      </span><span class="nt">synchronize</span><span class="p">:</span><span class="w"> </span><span class="kc">true</span><span class="w">
</span><span class="w">      </span><span class="nt">clone</span><span class="p">:</span><span class="w">
</span><span class="w">        </span><span class="nt">namespace</span><span class="p">:</span><span class="w"> </span><span class="l">argo</span><span class="w">
</span><span class="w">        </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">vault</span><span class="w">
</span><span class="w">  </span><span class="c"># 可以配置多个 rules，每个 rules 同步一个 secret</span><span class="w">
</span></code></pre></td></tr></table>
</div>
</div><p>上面提供的 kyverno 配置，会实时地监控所有 Namespace 变更，一但有新 Namespace 被创建，它就会立即将 <code>vault</code> secret 同步到该 Namespace.</p>
<p>或者，使用专门的 secrets/configmap 复制工具：<a href="https://github.com/mittwald/kubernetes-replicator" target="_blank" rel="noopener noreferrer">kubernetes-replicator</a></p>
<h3 id="4-argo-对-cr-资源的验证不够严谨写错了-key-都不报错">4. Argo 对 CR 资源的验证不够严谨，写错了 key 都不报错</h3>
<p>待研究</p>
<h3 id="5-如何归档历史数据">5. 如何归档历史数据？</h3>
<p>Argo 用的时间长了，跑过的 Workflows/Pods 全都保存在 Kubernetes/Argo Server 中，导致 Argo 越用越慢。</p>
<p>为了解决这个问题，Argo 提供了一些配置来限制 Workflows 和 Pods 的数量，详见：<a href="https://argoproj.github.io/argo/cost-optimisation/#limit-the-total-number-of-workflows-and-pods" target="_blank" rel="noopener noreferrer">Limit The Total Number Of Workflows And Pods</a></p>
<p>这些限制都是 Workflow 的参数，如果希望设置一个全局默认的限制，可以按照如下示例修改 argo 的 <code>workflow-controller-configmap</code> 这个 configmap:</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-yaml" data-lang="yaml"><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l">v1</span><span class="w">
</span><span class="w"></span><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l">ConfigMap</span><span class="w">
</span><span class="w"></span><span class="nt">metadata</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">workflow-controller-configmap</span><span class="w">
</span><span class="w"></span><span class="nt">data</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">config</span><span class="p">:</span><span class="w"> </span><span class="p">|</span><span class="sd">
</span><span class="sd">    # Default values that will apply to all Workflows from this controller, unless overridden on the Workflow-level
</span><span class="sd">    # See more: docs/default-workflow-specs.md
</span><span class="sd">    workflowDefaults:
</span><span class="sd">      spec:
</span><span class="sd">        # must complete in 8h (28,800 seconds)
</span><span class="sd">        activeDeadlineSeconds: 28800
</span><span class="sd">        # keep workflows for 1d (86,400 seconds)
</span><span class="sd">        ttlStrategy:
</span><span class="sd">          secondsAfterCompletion: 86400
</span><span class="sd">          # secondsAfterSuccess: 5
</span><span class="sd">          # secondsAfterFailure: 500
</span><span class="sd">        # delete all pods as soon as they complete
</span><span class="sd">        podGC:
</span><span class="sd">          # 可选项：&#34;OnPodCompletion&#34;, &#34;OnPodSuccess&#34;, &#34;OnWorkflowCompletion&#34;, &#34;OnWorkflowSuccess&#34;
</span><span class="sd">          strategy: OnPodCompletion</span><span class="w">    
</span></code></pre></td></tr></table>
</div>
</div><h3 id="6-argo-的其他进阶配置">6. Argo 的其他进阶配置</h3>
<p>Argo Workflows 的配置，都保存在 <code>workflow-controller-configmap</code> 这个 configmap 中，我们前面已经接触到了它的部分内容。</p>
<p>这里给出此配置文件的完整 examples: <a href="https://github.com/argoproj/argo/blob/master/docs/workflow-controller-configmap.yaml">https://github.com/argoproj/argo/blob/master/docs/workflow-controller-configmap.yaml</a></p>
<p>其中一些可能需要自定义的参数如下：</p>
<ul>
<li><code>parallelism</code>: workflow 的最大并行数量</li>
<li><code>persistence</code>: 将完成的 workflows 保存到 postgresql/mysql 中，这样即使 k8s 中的 workflow 被删除了，还能查看 workflow 记录
<ul>
<li>也支持配置过期时间</li>
</ul>
</li>
<li><code>sso</code>: 启用单点登录</li>
</ul>
<h3 id="7-是否应该尽量使用-cicd-工具提供的功能">7. 是否应该尽量使用 CI/CD 工具提供的功能？</h3>
<p>我从同事以及网络上，了解到部分 DevOps 人员主张尽量自己使用 Python/Go 来实现 CI/CD 流水线，CI/CD 工具提供的功能能不使用就不要使用。</p>
<p>因此有此一问。下面做下详细的分析：</p>
<p>尽量使用 CI/CD 工具提供的插件/功能，好处是不需要自己去实现，可以降低维护成本。
但是相对的运维人员就需要深入学习这个 CI/CD 工具的使用，另外还会和 CI/CD 工具绑定，会增加迁移难度。</p>
<p>而尽量自己用 Python 等代码去实现流水线，让 CI/CD 工具只负责调度与运行这些 Python 代码，
那 CI/CD 就可以很方便地随便换，运维人员也不需要去深入学习 CI/CD 工具的使用。
缺点是可能会增加 CI/CD 代码的复杂性。</p>
<p>我观察到 argo/drone 的一些 examples，发现它们的特征是：</p>
<ol>
<li>所有 CI/CD 相关的逻辑，全都实现在流水线中，不需要其他构建代码</li>
<li>每一个 step 都使用专用镜像：golang/nodejs/python
<ol>
<li>比如先使用 golang 镜像进行测试、构建，再使用 kaniko 将打包成容器镜像</li>
</ol>
</li>
</ol>
<p>那是否应该尽量使用 CI/CD 工具提供的功能呢？
<strong>其实这就是有多种方法实现同一件事，该用哪种方法的问题。这个问题在各个领域都很常见。</strong></p>
<p>以我目前的经验来看，需要具体问题具体分析，以 Argo Workflows 为例：</p>
<ol>
<li>流水线本身非常简单，那完全可以直接使用 argo 来实现，没必要自己再搞个 python 脚本
<ol>
<li>简单的流水线，迁移起来往往也非常简单。没必要为了可迁移性，非要用 argo 去调用 python 脚本。</li>
</ol>
</li>
<li>流水线的步骤之间包含很多逻辑判断/数据传递，那很可能是你的流水线设计有问题！
<ol>
<li><strong>流水线的步骤之间传递的数据应该尽可能少！复杂的逻辑判断应该尽量封装在其中一个步骤中！</strong></li>
<li>这种情况下，就应该使用 python 脚本来封装复杂的逻辑，而不应该将这些逻辑暴露到 Argo Workflows 中！</li>
</ol>
</li>
<li>我需要批量运行很多的流水线，而且它们之间还有复杂的依赖关系：那显然应该利用上 argo wrokflow 的高级特性。
<ol>
<li>argo 的 dag/steps 和 workflow of workflows 这两个功能结合，可以简单地实现上述功能。</li>
</ol>
</li>
</ol>
<h2 id="8-如何提升-argo-workflows-的创建和销毁速度">8. 如何提升 Argo Workflows 的创建和销毁速度？</h2>
<p>我们发现 workflow 的 pod，创建和销毁消耗了大量时间，尤其是销毁。
这导致我们单个流水线在 argo 上跑，还没在 jenkins 上跑更快。</p>
<h2 id="使用体验">使用体验</h2>
<p>目前已经使用 Argo Workflows 一个月多了，总的来说，最难用的就是 Web UI。</p>
<p>其他的都是小问题，只有 Web UI 是真的超难用，感觉根本就没有好好做过设计&hellip;</p>
<p>急需一个第三方 Web UI&hellip;</p>
<h2 id="画外---如何处理其他-kubernetes-资源之间的依赖关系">画外 - 如何处理其他 Kubernetes 资源之间的依赖关系</h2>
<p>Argo 相比其他 CI 工具，最大的特点，是它假设「任务」之间是有依赖关系的，因此它提供了多种协调编排「任务」的方法。</p>
<p>但是貌似 Argo CD 并没有继承这个理念，Argo CD 部署时，并不能在 kubernetes 资源之间，通过 DAG 等方法定义依赖关系。</p>
<p>微服务之间存在依赖关系，希望能按依赖关系进行部署，而 ArgoCD/FluxCD 部署 kubernetes yaml 时都是不考虑任何依赖关系的。这里就存在一些矛盾。</p>
<p>解决这个矛盾的方法有很多，我查阅了很多资料，也自己做了一些思考，得到的最佳实践来自<a href="https://developer.aliyun.com/article/573791" target="_blank" rel="noopener noreferrer">解决服务依赖 - 阿里云 ACK 容器服务</a>，它给出了两种方案：</p>
<ol>
<li><strong>应用端服务依赖检查</strong>: 即在微服务的入口添加依赖检查逻辑，确保所有依赖的微服务/数据库都可访问了，就续探针才能返回 200. 如果超时就直接 Crash</li>
<li><strong>独立的服务依赖检查逻辑</strong>: 部分遗留代码使用方法一改造起来或许会很困难，这时可以考虑使用 <strong>pod initContainer</strong> 或者容器的启动脚本中，加入依赖检查逻辑。</li>
</ol>
<p>但是这两个方案也还是存在一些问题，在说明问题前，我先说明一下我们「<strong>按序部署</strong>」的应用场景。</p>
<p>我们是一个很小的团队，后端做 RPC 接口升级时，通常是直接在开发环境做全量升级+测试。
因此运维这边也是，每次都是做全量升级。</p>
<p>因为没有协议协商机制，新的微服务的「RPC 服务端」将兼容 v1 v2 新旧两种协议，而新的「RPC 客户端」将直接使用 v2 协议去请求其他微服务。
这就导致我们<strong>必须先升级「RPC 服务端」，然后才能升级「RPC 客户端」</strong>。</p>
<p>为此，在进行微服务的全量升级时，就需要沿着 RPC 调用链路按序升级，这里就涉及到了 Kubernetes 资源之间的依赖关系。</p>
<blockquote>
<p>我目前获知的关键问题在于：我们使用的并不是真正的微服务开发模式，而是在把整个微服务系统当成一个「单体服务」在看待，所以引申出了这样的依赖关键的问题。
我进入的新公司完全没有这样的问题，所有的服务之间在 CI/CD 这个阶段都是解耦的，CI/CD 不需要考虑服务之间的依赖关系，也没有自动按照依赖关系进行微服务批量发布的功能，这些都由开发人员自行维护。
或许这才是正确的使用姿势，如果动不动就要批量更新一大批服务，那微服务体系的设计、拆分肯定是有问题了，生产环境也不会允许这么轻率的更新。</p>
</blockquote>
<p>前面讲了，阿里云提供的「应用端服务依赖检查」和「独立的服务依赖检查逻辑」是最佳实践。它们的优点有：</p>
<ol>
<li>简化部署逻辑，每次直接做全量部署就 OK。</li>
<li>提升部署速度，具体体现在：GitOps 部署流程只需要走一次（按序部署要很多次）、所有镜像都提前拉取好了、所有 Pod 也都提前启动了。</li>
</ol>
<p>但是这里有个问题是「灰度发布」或者「滚动更新」，这两种情况下都存在<strong>新旧版本共存</strong>的问题。</p>
<p>如果出现了 RPC 接口升级，那就必须先完成「RPC 服务端」的「灰度发布」或者「滚动更新」，再去更新「RPC 客户端」。</p>
<p>否则如果直接对所有微服务做灰度更新，只依靠「服务依赖检查」，就会出现这样的问题——「RPC 服务端」处于「薛定谔」状态，你调用到的服务端版本是新还是旧，取决于负载均衡的策略和概率。</p>
<p>**因此在做 RPC 接口的全量升级时，只依靠「服务依赖检查」是行不通的。**我目前想到的方案，有如下几种：</p>
<ul>
<li>我们当前的使用方案：<strong>直接在 yaml 部署这一步实现按序部署</strong>，每次部署后就轮询 kube-apiserver，确认全部灰度完成，再进行下一阶段的 yaml 部署。</li>
<li><strong>让后端加个参数来控制客户端使用的 RPC 协议版本，或者搞一个协议协商</strong>。这样就不需要控制微服务发布顺序了。</li>
<li>社区很多有状态应用的部署都涉及到部署顺序等复杂操作，目前流行的解决方案是<strong>使用 Operator+CRD 来实现这类应用的部署</strong>。Operator 会自行处理好各个组件的部署顺序。</li>
</ul>
<h2 id="参考文档">参考文档</h2>
<ul>
<li><a href="https://www.infoq.cn/article/fFZPvrKtbykg53x03IaH" target="_blank" rel="noopener noreferrer">Argo加入CNCF孵化器，一文解析Kubernetes原生工作流</a></li>
</ul>
<p>视频:</p>
<ul>
<li><a href="https://www.youtube.com/watch?v=fKiU7txd4RI&amp;list=PLj6h78yzYM2Pn8RxfLh2qrXBDftr6Qjut&amp;index=149" target="_blank" rel="noopener noreferrer">How to Multiply the Power of Argo Projects By Using Them Together - Hong Wang</a></li>
</ul>]]></description></item><item><title>secrets 管理工具 Vault 的介绍、安装及使用</title><link>https://ryan4yin.space/posts/expirence-of-vault/</link><pubDate>Sun, 24 Jan 2021 09:31:41 +0800</pubDate><author>xiaoyin_c@qq.com</author><dc:creator>ryan4yin</dc:creator><guid>https://ryan4yin.space/posts/expirence-of-vault/</guid><description><![CDATA[<p><a href="https://github.com/hashicorp/vault" target="_blank" rel="noopener noreferrer">Vault</a> 是 hashicorp 推出的 secrets 管理、加密即服务与权限管理工具。它的功能简介如下：</p>
<ol>
<li>secrets 管理：支持保存各种自定义信息、自动生成各类密钥，vault 自动生成的密钥还能自动轮转(rotate)</li>
<li>认证方式：支持接入各大云厂商的账号体系（比如阿里云RAM子账号体系）或者 LDAP 等进行身份验证，不需要创建额外的账号体系。</li>
<li>权限管理：通过 policy，可以设定非常细致的 ACL 权限。</li>
<li>密钥引擎：也支持接管各大云厂商的账号体系（比如阿里云RAM子账号体系），实现 API Key 的自动轮转。</li>
<li>支持接入 kubernetes rbac 权限体系，通过 serviceaccount+role 为每个 Pod 单独配置权限。</li>
</ol>
<ul>
<li>支持通过 sidecar/init-container 将 secrets 注入到 pod 中，或者通过 k8s operator 将 vault 数据同步到 k8s secrets 中</li>
</ul>
<p>在使用 Vault 之前，我们是以携程开源的 <a href="https://github.com/ctripcorp/apollo" target="_blank" rel="noopener noreferrer">Apollo</a> 作为微服务的分布式配置中心。</p>
<p>Apollo 在国内非常流行。它功能强大，支持配置的继承，也有提供 HTTP API 方便自动化。
缺点是权限管理和 secrets 管理比较弱，也不支持信息加密，不适合直接存储敏感信息。因此我们现在切换到了 Vault.</p>
<p>目前我们本地的 CI/CD 流水线和云上的微服务体系，都是使用的 Vault 做 secrets 管理.</p>
<h2 id="一vault-基础概念">一、Vault 基础概念</h2>
<blockquote>
<p>「基本概念」这一节，基本都翻译自官方文档: <a href="https://www.vaultproject.io/docs/internals/architecture">https://www.vaultproject.io/docs/internals/architecture</a></p>
</blockquote>
<p>首先看一下 Vault 的架构图：</p>
<p><figure><a class="lightgallery" href="/images/expirence-of-vault/vault-layers.png" title="/images/expirence-of-vault/vault-layers.png" data-thumbnail="/images/expirence-of-vault/vault-layers.png" data-sub-html="<h2>vault layers</h2>">
        
    </a><figcaption class="image-caption">vault layers</figcaption>
    </figure></p>
<p>可以看到，几乎所有的 Vault 组件都被统称为「屏障(Barrier)」，
Vault 可以简单地被划分为 Storage Backend、Barrier 和 HTTP/S API 三个部分。</p>
<p>类比银行金库，「屏障」就是 Vault(金库) 周围的「钢铁」和「混凝土」，Storage Backend 和客户端之间的所有数据流动都需要经过它。</p>
<p>「屏障」确保只有加密数据会被写入 Storage Backend，加密数据在经过「屏障」被读出的过程中被验证与解密。</p>
<p>和银行金库的大门非常类似，Barrier 也必须先解封，才能解密 Storage Backend 中的数据。</p>
<h3 id="1-数据存储及加密解密">1. 数据存储及加密解密</h3>
<p>Storage Backend(后端存储): Vault 自身不存储数据，因此需要为它配置一个「Storage Backend」。
「Storage Backend」是不受信任的，只用于存储加密数据。</p>
<p>Initialization(初始化): Vault 在首次启动时需要初始化，这一步生成一个「加密密钥(Encryption Key)」用于加密数据，加密完成的数据才能被保存到 Storage Backend.</p>
<p>Unseal(解封): Vault 启动后，因为不知道「加密密钥」，它会进入「封印(Sealed)」状态，在「解封(Unseal)」前无法进行任何操作。</p>
<p>「加密密钥」被「master key」保护，我们必须提供「master key」才能完成 Unseal 操作。</p>
<p>默认情况下，Vault 使用<a href="https://medium.com/taipei-ethereum-meetup/%E7%A7%81%E9%91%B0%E5%88%86%E5%89%B2-shamirs-secret-sharing-7a70c8abf664" target="_blank" rel="noopener noreferrer">沙米尔密钥共享算法</a>
将「master key」分割成五个「Key Shares(分享密钥)」，必须要提供其中任意三个「Key Shares」才能重建出「master key」从而完成 Unseal.</p>
<p><figure><a class="lightgallery" href="/images/expirence-of-vault/vault-shamir-secret-sharing.svg" title="/images/expirence-of-vault/vault-shamir-secret-sharing.svg" data-thumbnail="/images/expirence-of-vault/vault-shamir-secret-sharing.svg" data-sub-html="<h2>vault-shamir-secret-sharing</h2>">
        
    </a><figcaption class="image-caption">vault-shamir-secret-sharing</figcaption>
    </figure></p>
<blockquote>
<p>「Key Shares」的数量，以及重建「master key」最少需要的 key shares 数量，都是可以调整的。
沙米尔密钥共享算法也可以关闭，这样 master key 将被直接用于 Unseal.</p>
</blockquote>
<h3 id="2-认证系统及权限系统">2. 认证系统及权限系统</h3>
<p>在解封完成后，Vault 就可以开始处理请求了。</p>
<p>HTTP 请求进入后的整个处理流程都由 vault core 管理，core 会强制进行 ACL 检查，并确保审计日志(audit logging)完成记录。</p>
<p>客户端首次连接 vault 时，需要先完成身份认证，vault 的「auth methods」模块有很多身份认证方法可选：</p>
<ol>
<li>用户友好的认证方法，适合管理员使用：username/password、云服务商、ldap
<ol>
<li>在创建 user 的时候，需要为 user 绑定 policy，给予合适的权限。</li>
</ol>
</li>
<li>应用友好的方法，适合应用程序使用：public/private keys、tokens、kubernetes、jwt</li>
</ol>
<p>身份验证请求流经 Core 并进入 auth methods，auth methods 确定请求是否有效并返回「关联策略(policies)」的列表。</p>
<p>ACL Policies 由 policy store 负责管理与存储，由 core 进行 ACL 检查。
ACL 的默认行为是拒绝，这意味着除非明确配置 Policy 允许某项操作，否则该操作将被拒绝。</p>
<p>在通过 auth methods 完成了身份认证，并且返回的「关联策略」也没毛病之后，「token store」将会生成并管理一个新的 token，
这个 token 会被返回给客户端，用于进行后续请求。</p>
<p>类似 web 网站的 cookie，token 也都存在一个 lease 租期或者说有效期，这加强了安全性。</p>
<p>token 关联了相关的策略 policies，这些策略将被用于验证请求的权限。</p>
<p>请求经过验证后，将被路由到 secret engine。如果 secret engine 返回了一个 secret（由 vault 自动生成的 secret），
Core 会将其注册到 expiration manager，并给它附加一个 lease ID。lease ID 被客户端用于更新(renew)或吊销(revoke)它得到的 secret.</p>
<p>如果客户端允许租约(lease)到期，expiration manager 将自动吊销这个 secret.</p>
<p>Core 负责处理审核代理(audit broker)的请求及响应日志，将请求发送到所有已配置的审核设备(audit devices)。</p>
<h3 id="3-secret-engine">3. Secret Engine</h3>
<p>Secret Engine 是保存、生成或者加密数据的组件，它非常灵活。</p>
<p>有的 Secret Engines 只是单纯地存储与读取数据，比如 kv 就可以看作一个加密的 Redis。
而其他的 Secret Engines 则连接到其他的服务并按需生成动态凭证。</p>
<p>还有些 Secret Engines 提供「加密即服务(encryption as a service)」的能力，如 transit、证书管理等。</p>
<p>常用的 engine 举例：</p>
<ol>
<li>AliCloud Secrets Engine: 基于 RAM 策略动态生成 AliCloud Access Token，或基于 RAM 角色动态生成 AliCloud STS 凭据
<ul>
<li>Access Token 会自动更新(Renew)，而 STS 凭据是临时使用的，过期后就失效了。</li>
</ul>
</li>
<li>kv: 键值存储，可用于存储一些静态的配置。它一定程度上能替代掉携程的 Apollo 配置中心。</li>
<li>Transit Secrets Engine: 提供加密即服务的功能，它只负责加密和解密，不负责存储。主要应用场景是帮 app 加解密数据，但是数据仍旧存储在 MySQL 等数据库中。</li>
</ol>
<h2 id="二部署-vault">二、部署 Vault</h2>
<p>官方建议<a href="https://www.vaultproject.io/docs/platform/k8s/helm/run" target="_blank" rel="noopener noreferrer">通过 Helm 部署 vault</a>，大概流程：</p>
<ol>
<li>使用 helm/docker 部署运行 vault.</li>
<li>初始化/解封 vault: vault 安全措施，每次重启必须解封(可设置自动解封).</li>
</ol>
<h3 id="0-如何选择存储后端">0. 如何选择存储后端？</h3>
<p>首先，我们肯定需要 HA，至少要保留能升级到 HA 的能力，所以不建议选择不支持 HA 的后端。</p>
<p>而具体的选择，就因团队经验而异了，人们往往倾向于使用自己熟悉的、知根知底的后端，或者选用云服务。</p>
<p>比如我们对 MySQL/PostgreSQL 比较熟悉，而且使用云服务提供的数据库不需要考虑太多的维护问题，MySQL 作为一个通用协议也不会被云厂商绑架，那我们就倾向于使用 MySQL/PostgreSQL.</p>
<p>而如果你们是本地自建，那你可能更倾向于使用 Etcd/Consul/Raft 做后端存储。</p>
<h3 id="1-docker-compose-部署非-ha">1. docker-compose 部署（非 HA）</h3>
<blockquote>
<p>推荐用于本地开发测试环境，或者其他不需要高可用的环境。</p>
</blockquote>
<p><code>docker-compose.yml</code> 示例如下：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-yaml" data-lang="yaml"><span class="nt">version</span><span class="p">:</span><span class="w"> </span><span class="s1">&#39;3.3&#39;</span><span class="w">
</span><span class="w"></span><span class="nt">services</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">vault</span><span class="p">:</span><span class="w">
</span><span class="w">    </span><span class="c"># 文档：https://hub.docker.com/_/vault</span><span class="w">
</span><span class="w">    </span><span class="nt">image</span><span class="p">:</span><span class="w"> </span><span class="l">vault:1.6.0</span><span class="w">
</span><span class="w">    </span><span class="nt">container_name</span><span class="p">:</span><span class="w"> </span><span class="l">vault</span><span class="w">
</span><span class="w">    </span><span class="nt">ports</span><span class="p">:</span><span class="w">
</span><span class="w">      </span><span class="c"># rootless 容器，内部不能使用标准端口 443</span><span class="w">
</span><span class="w">      </span>- <span class="s2">&#34;443:8200&#34;</span><span class="w">
</span><span class="w">    </span><span class="nt">restart</span><span class="p">:</span><span class="w"> </span><span class="l">always</span><span class="w">
</span><span class="w">    </span><span class="nt">volumes</span><span class="p">:</span><span class="w">
</span><span class="w">      </span><span class="c"># 审计日志存储目录，默认不写审计日志，启用 `file` audit backend 时必须提供一个此文件夹下的路径</span><span class="w">
</span><span class="w">      </span>- <span class="l">./logs:/vault/logs</span><span class="w">
</span><span class="w">      </span><span class="c"># 当使用 file data storage 插件时，数据被存储在这里。默认不往这写任何数据。</span><span class="w">
</span><span class="w">      </span>- <span class="l">./file:/vault/file</span><span class="w">
</span><span class="w">      </span><span class="c"># 配置目录，vault 默认 `/valut/config/` 中所有以 .hcl/.json 结尾的文件</span><span class="w">
</span><span class="w">      </span><span class="c"># config.hcl 文件内容，参考 cutom-vaules.yaml</span><span class="w">
</span><span class="w">      </span>- <span class="l">./config.hcl:/vault/config/config.hcl</span><span class="w">
</span><span class="w">      </span><span class="c"># TLS 证书</span><span class="w">
</span><span class="w">      </span>- <span class="l">./certs:/certs</span><span class="w">
</span><span class="w">    </span><span class="c"># vault 需要锁定内存以防止敏感值信息被交换(swapped)到磁盘中</span><span class="w">
</span><span class="w">    </span><span class="c"># 为此需要添加如下能力</span><span class="w">
</span><span class="w">    </span><span class="nt">cap_add</span><span class="p">:</span><span class="w">
</span><span class="w">      </span>- <span class="l">IPC_LOCK</span><span class="w">
</span><span class="w">    </span><span class="c"># 必须手动设置 entrypoint，否则 vault 将以 development 模式运行</span><span class="w">
</span><span class="w">    </span><span class="nt">entrypoint</span><span class="p">:</span><span class="w"> </span><span class="l">vault server -config /vault/config/config.hcl</span><span class="w">
</span></code></pre></td></tr></table>
</div>
</div><p><code>config.hcl</code> 内容如下：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-hcl" data-lang="hcl"><span class="n">ui</span> <span class="o">=</span> <span class="kt">true</span>

<span class="err">//</span> <span class="k">使用文件做数据存储</span><span class="err">（</span><span class="k">单节点</span><span class="err">）</span>
<span class="k">storage</span> <span class="s2">&#34;file&#34;</span> {
<span class="n">  path</span>    <span class="o">=</span> <span class="s2">&#34;/vault/file&#34;</span>
}

<span class="k">listener</span> <span class="s2">&#34;tcp&#34;</span> {
<span class="n">  address</span> <span class="o">=</span> <span class="s2">&#34;[::]:8200&#34;</span>

<span class="n">  tls_disable</span> <span class="o">=</span> <span class="kt">false</span>
<span class="n">  tls_cert_file</span> <span class="o">=</span> <span class="s2">&#34;/certs/server.crt&#34;</span>
<span class="n">  tls_key_file</span>  <span class="o">=</span> <span class="s2">&#34;/certs/server.key&#34;</span>
}
</code></pre></td></tr></table>
</div>
</div><p>将如上两份配置保存在同一非文件夹内，同时在 <code>./certs</code> 中提供 TLS 证书 <code>server.crt</code> 和私钥 <code>server.key</code>。</p>
<p>然后 <code>docker-compose up -d</code> 就能启动运行一个 vault 实例。</p>
<h3 id="install-by-helm">2. 通过 helm 部署高可用的 vault</h3>
<blockquote>
<p>推荐用于生产环境</p>
</blockquote>
<p>通过 helm 部署：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-shell" data-lang="shell"><span class="c1"># 添加 valut 仓库</span>
helm repo add hashicorp https://helm.releases.hashicorp.com
<span class="c1"># 查看 vault 版本号</span>
helm search repo hashicorp/vault -l <span class="p">|</span> head
<span class="c1"># 下载某个版本号的 vault</span>
helm pull hashicorp/vault --version  0.11.0 --untar
</code></pre></td></tr></table>
</div>
</div><p>参照下载下来的 <code>./vault/values.yaml</code> 编写 <code>custom-values.yaml</code>，
部署一个以 <code>mysql</code> 为后端存储的 HA vault，配置示例如下:</p>
<blockquote>
<p>配置内容虽然多，但是大都是直接拷贝自 <code>./vault/values.yaml</code>，改动很少。
测试 Vault 时可以忽略掉其中大多数的配置项。</p>
</blockquote>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">  1
</span><span class="lnt">  2
</span><span class="lnt">  3
</span><span class="lnt">  4
</span><span class="lnt">  5
</span><span class="lnt">  6
</span><span class="lnt">  7
</span><span class="lnt">  8
</span><span class="lnt">  9
</span><span class="lnt"> 10
</span><span class="lnt"> 11
</span><span class="lnt"> 12
</span><span class="lnt"> 13
</span><span class="lnt"> 14
</span><span class="lnt"> 15
</span><span class="lnt"> 16
</span><span class="lnt"> 17
</span><span class="lnt"> 18
</span><span class="lnt"> 19
</span><span class="lnt"> 20
</span><span class="lnt"> 21
</span><span class="lnt"> 22
</span><span class="lnt"> 23
</span><span class="lnt"> 24
</span><span class="lnt"> 25
</span><span class="lnt"> 26
</span><span class="lnt"> 27
</span><span class="lnt"> 28
</span><span class="lnt"> 29
</span><span class="lnt"> 30
</span><span class="lnt"> 31
</span><span class="lnt"> 32
</span><span class="lnt"> 33
</span><span class="lnt"> 34
</span><span class="lnt"> 35
</span><span class="lnt"> 36
</span><span class="lnt"> 37
</span><span class="lnt"> 38
</span><span class="lnt"> 39
</span><span class="lnt"> 40
</span><span class="lnt"> 41
</span><span class="lnt"> 42
</span><span class="lnt"> 43
</span><span class="lnt"> 44
</span><span class="lnt"> 45
</span><span class="lnt"> 46
</span><span class="lnt"> 47
</span><span class="lnt"> 48
</span><span class="lnt"> 49
</span><span class="lnt"> 50
</span><span class="lnt"> 51
</span><span class="lnt"> 52
</span><span class="lnt"> 53
</span><span class="lnt"> 54
</span><span class="lnt"> 55
</span><span class="lnt"> 56
</span><span class="lnt"> 57
</span><span class="lnt"> 58
</span><span class="lnt"> 59
</span><span class="lnt"> 60
</span><span class="lnt"> 61
</span><span class="lnt"> 62
</span><span class="lnt"> 63
</span><span class="lnt"> 64
</span><span class="lnt"> 65
</span><span class="lnt"> 66
</span><span class="lnt"> 67
</span><span class="lnt"> 68
</span><span class="lnt"> 69
</span><span class="lnt"> 70
</span><span class="lnt"> 71
</span><span class="lnt"> 72
</span><span class="lnt"> 73
</span><span class="lnt"> 74
</span><span class="lnt"> 75
</span><span class="lnt"> 76
</span><span class="lnt"> 77
</span><span class="lnt"> 78
</span><span class="lnt"> 79
</span><span class="lnt"> 80
</span><span class="lnt"> 81
</span><span class="lnt"> 82
</span><span class="lnt"> 83
</span><span class="lnt"> 84
</span><span class="lnt"> 85
</span><span class="lnt"> 86
</span><span class="lnt"> 87
</span><span class="lnt"> 88
</span><span class="lnt"> 89
</span><span class="lnt"> 90
</span><span class="lnt"> 91
</span><span class="lnt"> 92
</span><span class="lnt"> 93
</span><span class="lnt"> 94
</span><span class="lnt"> 95
</span><span class="lnt"> 96
</span><span class="lnt"> 97
</span><span class="lnt"> 98
</span><span class="lnt"> 99
</span><span class="lnt">100
</span><span class="lnt">101
</span><span class="lnt">102
</span><span class="lnt">103
</span><span class="lnt">104
</span><span class="lnt">105
</span><span class="lnt">106
</span><span class="lnt">107
</span><span class="lnt">108
</span><span class="lnt">109
</span><span class="lnt">110
</span><span class="lnt">111
</span><span class="lnt">112
</span><span class="lnt">113
</span><span class="lnt">114
</span><span class="lnt">115
</span><span class="lnt">116
</span><span class="lnt">117
</span><span class="lnt">118
</span><span class="lnt">119
</span><span class="lnt">120
</span><span class="lnt">121
</span><span class="lnt">122
</span><span class="lnt">123
</span><span class="lnt">124
</span><span class="lnt">125
</span><span class="lnt">126
</span><span class="lnt">127
</span><span class="lnt">128
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-yaml" data-lang="yaml"><span class="nt">global</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="c"># enabled is the master enabled switch. Setting this to true or false</span><span class="w">
</span><span class="w">  </span><span class="c"># will enable or disable all the components within this chart by default.</span><span class="w">
</span><span class="w">  </span><span class="nt">enabled</span><span class="p">:</span><span class="w"> </span><span class="kc">true</span><span class="w">
</span><span class="w">  </span><span class="c"># TLS for end-to-end encrypted transport</span><span class="w">
</span><span class="w">  </span><span class="nt">tlsDisable</span><span class="p">:</span><span class="w"> </span><span class="kc">false</span><span class="w">
</span><span class="w">
</span><span class="w"></span><span class="nt">injector</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="c"># True if you want to enable vault agent injection.</span><span class="w">
</span><span class="w">  </span><span class="nt">enabled</span><span class="p">:</span><span class="w"> </span><span class="kc">true</span><span class="w">
</span><span class="w">
</span><span class="w">  </span><span class="nt">replicas</span><span class="p">:</span><span class="w"> </span><span class="m">1</span><span class="w">
</span><span class="w">
</span><span class="w">  </span><span class="c"># If true, will enable a node exporter metrics endpoint at /metrics.</span><span class="w">
</span><span class="w">  </span><span class="nt">metrics</span><span class="p">:</span><span class="w">
</span><span class="w">    </span><span class="nt">enabled</span><span class="p">:</span><span class="w"> </span><span class="kc">false</span><span class="w">
</span><span class="w">
</span><span class="w">  </span><span class="c"># Mount Path of the Vault Kubernetes Auth Method.</span><span class="w">
</span><span class="w">  </span><span class="nt">authPath</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;auth/kubernetes&#34;</span><span class="w">
</span><span class="w">
</span><span class="w">  </span><span class="nt">certs</span><span class="p">:</span><span class="w">
</span><span class="w">    </span><span class="c"># secretName is the name of the secret that has the TLS certificate and</span><span class="w">
</span><span class="w">    </span><span class="c"># private key to serve the injector webhook. If this is null, then the</span><span class="w">
</span><span class="w">    </span><span class="c"># injector will default to its automatic management mode that will assign</span><span class="w">
</span><span class="w">    </span><span class="c"># a service account to the injector to generate its own certificates.</span><span class="w">
</span><span class="w">    </span><span class="nt">secretName</span><span class="p">:</span><span class="w"> </span><span class="kc">null</span><span class="w">
</span><span class="w">
</span><span class="w">    </span><span class="c"># caBundle is a base64-encoded PEM-encoded certificate bundle for the</span><span class="w">
</span><span class="w">    </span><span class="c"># CA that signed the TLS certificate that the webhook serves. This must</span><span class="w">
</span><span class="w">    </span><span class="c"># be set if secretName is non-null.</span><span class="w">
</span><span class="w">    </span><span class="nt">caBundle</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;&#34;</span><span class="w">
</span><span class="w">
</span><span class="w">    </span><span class="c"># certName and keyName are the names of the files within the secret for</span><span class="w">
</span><span class="w">    </span><span class="c"># the TLS cert and private key, respectively. These have reasonable</span><span class="w">
</span><span class="w">    </span><span class="c"># defaults but can be customized if necessary.</span><span class="w">
</span><span class="w">    </span><span class="nt">certName</span><span class="p">:</span><span class="w"> </span><span class="l">tls.crt</span><span class="w">
</span><span class="w">    </span><span class="nt">keyName</span><span class="p">:</span><span class="w"> </span><span class="l">tls.key</span><span class="w">
</span><span class="w">
</span><span class="w"></span><span class="nt">server</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="c"># Resource requests, limits, etc. for the server cluster placement. This</span><span class="w">
</span><span class="w">  </span><span class="c"># should map directly to the value of the resources field for a PodSpec.</span><span class="w">
</span><span class="w">  </span><span class="c"># By default no direct resource request is made.</span><span class="w">
</span><span class="w">
</span><span class="w">  </span><span class="c"># Enables a headless service to be used by the Vault Statefulset</span><span class="w">
</span><span class="w">  </span><span class="nt">service</span><span class="p">:</span><span class="w">
</span><span class="w">    </span><span class="nt">enabled</span><span class="p">:</span><span class="w"> </span><span class="kc">true</span><span class="w">
</span><span class="w">    </span><span class="c"># Port on which Vault server is listening</span><span class="w">
</span><span class="w">    </span><span class="nt">port</span><span class="p">:</span><span class="w"> </span><span class="m">8200</span><span class="w">
</span><span class="w">    </span><span class="c"># Target port to which the service should be mapped to</span><span class="w">
</span><span class="w">    </span><span class="nt">targetPort</span><span class="p">:</span><span class="w"> </span><span class="m">8200</span><span class="w">
</span><span class="w">
</span><span class="w">
</span><span class="w">  </span><span class="c"># This configures the Vault Statefulset to create a PVC for audit</span><span class="w">
</span><span class="w">  </span><span class="c"># logs.  Once Vault is deployed, initialized and unseal, Vault must</span><span class="w">
</span><span class="w">  </span><span class="c"># be configured to use this for audit logs.  This will be mounted to</span><span class="w">
</span><span class="w">  </span><span class="c"># /vault/audit</span><span class="w">
</span><span class="w">  </span><span class="c"># See https://www.vaultproject.io/docs/audit/index.html to know more</span><span class="w">
</span><span class="w">  </span><span class="nt">auditStorage</span><span class="p">:</span><span class="w">
</span><span class="w">    </span><span class="nt">enabled</span><span class="p">:</span><span class="w"> </span><span class="kc">false</span><span class="w">
</span><span class="w">
</span><span class="w">  </span><span class="c"># Run Vault in &#34;HA&#34; mode. There are no storage requirements unless audit log</span><span class="w">
</span><span class="w">  </span><span class="c"># persistence is required.  In HA mode Vault will configure itself to use Consul</span><span class="w">
</span><span class="w">  </span><span class="c"># for its storage backend.  The default configuration provided will work the Consul</span><span class="w">
</span><span class="w">  </span><span class="c"># Helm project by default.  It is possible to manually configure Vault to use a</span><span class="w">
</span><span class="w">  </span><span class="c"># different HA backend.</span><span class="w">
</span><span class="w">  </span><span class="nt">ha</span><span class="p">:</span><span class="w">
</span><span class="w">    </span><span class="nt">enabled</span><span class="p">:</span><span class="w"> </span><span class="kc">true</span><span class="w">
</span><span class="w">    </span><span class="nt">replicas</span><span class="p">:</span><span class="w"> </span><span class="m">3</span><span class="w">
</span><span class="w">
</span><span class="w">    </span><span class="c"># Set the api_addr configuration for Vault HA</span><span class="w">
</span><span class="w">    </span><span class="c"># See https://www.vaultproject.io/docs/configuration#api_addr</span><span class="w">
</span><span class="w">    </span><span class="c"># If set to null, this will be set to the Pod IP Address</span><span class="w">
</span><span class="w">    </span><span class="nt">apiAddr</span><span class="p">:</span><span class="w"> </span><span class="kc">null</span><span class="w">
</span><span class="w">
</span><span class="w">    </span><span class="c"># config is a raw string of default configuration when using a Stateful</span><span class="w">
</span><span class="w">    </span><span class="c"># deployment. Default is to use a Consul for its HA storage backend.</span><span class="w">
</span><span class="w">    </span><span class="c"># This should be HCL.</span><span class="w">
</span><span class="w">    
</span><span class="w">    </span><span class="c"># Note: Configuration files are stored in ConfigMaps so sensitive data </span><span class="w">
</span><span class="w">    </span><span class="c"># such as passwords should be either mounted through extraSecretEnvironmentVars</span><span class="w">
</span><span class="w">    </span><span class="c"># or through a Kube secret.  For more information see: </span><span class="w">
</span><span class="w">    </span><span class="c"># https://www.vaultproject.io/docs/platform/k8s/helm/run#protecting-sensitive-vault-configurations</span><span class="w">
</span><span class="w">    </span><span class="nt">config</span><span class="p">:</span><span class="w"> </span><span class="p">|</span><span class="sd">
</span><span class="sd">      ui = true
</span><span class="sd">
</span><span class="sd">      listener &#34;tcp&#34; {
</span><span class="sd">        address = &#34;[::]:8200&#34;
</span><span class="sd">        cluster_address = &#34;[::]:8201&#34;
</span><span class="sd">
</span><span class="sd">        # 注意，这个值要和 helm 的参数 global.tlsDisable 一致
</span><span class="sd">        tls_disable = false
</span><span class="sd">        tls_cert_file = &#34;/etc/certs/vault.crt&#34;
</span><span class="sd">        tls_key_file  = &#34;/etc/certs/vault.key&#34;
</span><span class="sd">      }
</span><span class="sd">
</span><span class="sd">      # storage &#34;postgresql&#34; {
</span><span class="sd">      #   connection_url = &#34;postgres://username:password@&lt;host&gt;:5432/vault?sslmode=disable&#34;
</span><span class="sd">      #   ha_enabled = true
</span><span class="sd">      # }
</span><span class="sd">
</span><span class="sd">      service_registration &#34;kubernetes&#34; {}
</span><span class="sd">
</span><span class="sd">      # Example configuration for using auto-unseal, using AWS KMS. 
</span><span class="sd">      # the cluster must have a service account that is authorized to access AWS KMS, throught an IAM Role.
</span><span class="sd">      # seal &#34;awskms&#34; {
</span><span class="sd">      #   region     = &#34;us-east-1&#34;
</span><span class="sd">      #   kms_key_id = &#34;&lt;some-key-id&gt;&#34;
</span><span class="sd">      #   默认情况下插件会使用 awskms 的公网 enpoint，但是也可以使用如下参数，改用自行创建的 vpc 内网 endpoint
</span><span class="sd">      #   endpoint   = &#34;https://&lt;vpc-endpoint-id&gt;.kms.us-east-1.vpce.amazonaws.com&#34;
</span><span class="sd">      # }</span><span class="w">      
</span><span class="w">
</span><span class="w">  </span><span class="c"># Definition of the serviceAccount used to run Vault.</span><span class="w">
</span><span class="w">  </span><span class="c"># These options are also used when using an external Vault server to validate</span><span class="w">
</span><span class="w">  </span><span class="c"># Kubernetes tokens.</span><span class="w">
</span><span class="w">  </span><span class="nt">serviceAccount</span><span class="p">:</span><span class="w">
</span><span class="w">    </span><span class="nt">create</span><span class="p">:</span><span class="w"> </span><span class="kc">true</span><span class="w">
</span><span class="w">    </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;vault&#34;</span><span class="w">
</span><span class="w">    </span><span class="nt">annotations</span><span class="p">:</span><span class="w">
</span><span class="w">      </span><span class="c"># 如果要使用 auto unseal 的话，这个填写拥有 awskms 权限的 AWS IAM Role</span><span class="w">
</span><span class="w">      </span><span class="nt">eks.amazonaws.com/role-arn</span><span class="p">:</span><span class="w"> </span><span class="l">&lt;role-arn&gt;</span><span class="w">
</span><span class="w">
</span><span class="w"></span><span class="c"># Vault UI</span><span class="w">
</span><span class="w"></span><span class="nt">ui</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">enabled</span><span class="p">:</span><span class="w"> </span><span class="kc">true</span><span class="w">
</span><span class="w">  </span><span class="nt">publishNotReadyAddresses</span><span class="p">:</span><span class="w"> </span><span class="kc">true</span><span class="w">
</span><span class="w">  </span><span class="nt">serviceType</span><span class="p">:</span><span class="w"> </span><span class="l">ClusterIP</span><span class="w">
</span><span class="w">  </span><span class="nt">activeVaultPodOnly</span><span class="p">:</span><span class="w"> </span><span class="kc">true</span><span class="w">
</span><span class="w">  </span><span class="nt">externalPort</span><span class="p">:</span><span class="w"> </span><span class="m">8200</span><span class="w">
</span></code></pre></td></tr></table>
</div>
</div><p>现在使用自定义的 <code>custom-values.yaml</code> 部署 vautl:</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-shell" data-lang="shell">kubectl create namespace vault
<span class="c1"># 安装/升级 valut</span>
helm upgrade --install vault ./vault --namespace vault -f custom-values.yaml
</code></pre></td></tr></table>
</div>
</div><h3 id="3-初始化并解封-vault">3. 初始化并解封 vault</h3>
<blockquote>
<p>官方文档：<a href="https://learn.hashicorp.com/tutorials/vault/kubernetes-raft-deployment-guide?in=vault/kubernetes#install-vault" target="_blank" rel="noopener noreferrer">Initialize and unseal Vault - Vault on Kubernetes Deployment Guide</a></p>
</blockquote>
<p>通过 helm 部署 vault，默认会部署一个三副本的 StatefulSet，但是这三个副本都会处于 NotReady 状态（docker 方式部署的也一样）。
接下来还需要手动初始化并解封 vault，才能 <code>Ready</code>:</p>
<ol>
<li>第一步：从三个副本中随便选择一个，运行 vault 的初始化命令：<code>kubectl exec -ti vault-0 -- vault operator init</code>
<ol>
<li>初始化操作会返回 5 个 unseal keys，以及一个 Initial Root Token，这些数据非常敏感非常重要，一定要保存到安全的地方！</li>
</ol>
</li>
<li>第二步：在每个副本上，使用任意三个 unseal keys 进行解封操作。
<ol>
<li>一共有三个副本，也就是说要解封 3*3 次，才能完成 vault 的完整解封！</li>
</ol>
</li>
</ol>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-shell" data-lang="shell"><span class="c1"># 每个实例都需要解封三次！</span>
<span class="c1">## Unseal the first vault server until it reaches the key threshold</span>
$ kubectl <span class="nb">exec</span> -ti vault-0 -- vault operator unseal <span class="c1"># ... Unseal Key 1</span>
$ kubectl <span class="nb">exec</span> -ti vault-0 -- vault operator unseal <span class="c1"># ... Unseal Key 2</span>
$ kubectl <span class="nb">exec</span> -ti vault-0 -- vault operator unseal <span class="c1"># ... Unseal Key 3</span>
</code></pre></td></tr></table>
</div>
</div><p>这样就完成了部署，但是要注意，<strong>vault 实例每次重启后，都需要重新解封！也就是重新进行第二步操作！</strong></p>
<h3 id="4-初始化并设置自动解封">4. 初始化并设置自动解封</h3>
<p>在未设置 auto unseal 的情况下，vault 每次重启都要手动解封所有 vault 实例，实在是很麻烦，在云上自动扩缩容的情况下，vault 实例会被自动调度，这种情况就更麻烦了。</p>
<p>为了简化这个流程，可以考虑配置 auto unseal 让 vault 自动解封。</p>
<p>自动解封目前有两种方法：</p>
<ol>
<li>使用阿里云/AWS/Azure 等云服务提供的密钥库来管理 encryption key
<ol>
<li>AWS: <a href="https://www.vaultproject.io/docs/configuration/seal/awskms" target="_blank" rel="noopener noreferrer">awskms Seal</a>
<ol>
<li>如果是 k8s 集群，vault 使用的 ServiceAccount 需要有权限使用 AWS KMS，它可替代掉 config.hcl 中的 access_key/secret_key 两个属性</li>
</ol>
</li>
<li>阿里云：<a href="https://www.vaultproject.io/docs/configuration/seal/alicloudkms" target="_blank" rel="noopener noreferrer">alicloudkms Seal</a></li>
</ol>
</li>
<li>如果你不想用云服务，那可以考虑 <a href="https://learn.hashicorp.com/tutorials/vault/autounseal-transit" target="_blank" rel="noopener noreferrer">autounseal-transit</a>，这种方法使用另一个 vault 实例提供的 transit 引擎来实现 auto-unseal.</li>
<li>简单粗暴：直接写个 crontab 或者在 CI 平台上加个定时任务去执行解封命令，以实现自动解封。不过这样安全性就不好说了。</li>
</ol>
<p>以使用 awskms 为例，首先创建 aws IAM 的 policy 内容如下:</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-json" data-lang="json"><span class="p">{</span>
    <span class="nt">&#34;Version&#34;</span><span class="p">:</span> <span class="s2">&#34;2012-10-17&#34;</span><span class="p">,</span>
    <span class="nt">&#34;Statement&#34;</span><span class="p">:</span> <span class="p">[</span>
        <span class="p">{</span>
            <span class="nt">&#34;Sid&#34;</span><span class="p">:</span> <span class="s2">&#34;VaultKMSUnseal&#34;</span><span class="p">,</span>
            <span class="nt">&#34;Effect&#34;</span><span class="p">:</span> <span class="s2">&#34;Allow&#34;</span><span class="p">,</span>
            <span class="nt">&#34;Action&#34;</span><span class="p">:</span> <span class="p">[</span>
                <span class="s2">&#34;kms:Decrypt&#34;</span><span class="p">,</span>
                <span class="s2">&#34;kms:Encrypt&#34;</span><span class="p">,</span>
                <span class="s2">&#34;kms:DescribeKey&#34;</span>
            <span class="p">],</span>
            <span class="nt">&#34;Resource&#34;</span><span class="p">:</span> <span class="s2">&#34;*&#34;</span>
        <span class="p">}</span>
    <span class="p">]</span>
<span class="p">}</span>
</code></pre></td></tr></table>
</div>
</div><p>然后创建 IAM Role 绑定上面的 policy，并为 vault 的 k8s serviceaccount 创建一个 IAM Role，绑定上这个 policy.</p>
<p>这样 vault 使用的 serviceaccount 自身就拥有了访问 awskms 的权限，也就不需要额外通过 access_key/secret_key 来访问 awskms.</p>
<p>关于 IAM Role 和 k8s serviceaccount 如何绑定，参见官方文档：<a href="https://docs.aws.amazon.com/eks/latest/userguide/iam-roles-for-service-accounts.html" target="_blank" rel="noopener noreferrer">IAM roles for EKS service accounts</a></p>
<p>完事后再修改好前面提供的 helm 配置，部署它，最后使用如下命令初始化一下：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-shell" data-lang="shell"><span class="c1"># 初始化命令和普通模式并无不同</span>
kubectl <span class="nb">exec</span> -ti vault-0 -- vault operator init
<span class="c1"># 会打印出一个 root token，以及五个 Recovery Key（而不是 Unseal Key）</span>
<span class="c1"># Recover Key 不再用于解封，但是重新生成 root token 等操作仍然会需要用到它.</span>
</code></pre></td></tr></table>
</div>
</div><p>然后就大功告成了，可以尝试下删除 vault 的 pod，新建的 Pod 应该会自动解封。</p>
<h2 id="三vault-自身的配置管理">三、Vault 自身的配置管理</h2>
<p>Vault 本身是一个复杂的 secrets 工具，它提供了 <strong>Web UI</strong> 和 <strong>CLI</strong> 用于手动管理与查看 Vault 的内容。</p>
<p>但是作为一名 DevOps，我们当然更喜欢自动化的方法，这有两种选择:</p>
<ul>
<li>使用 vault 的 sdk: python-<a href="https://github.com/hvac/hvac" target="_blank" rel="noopener noreferrer">hvac</a></li>
<li>使用 <a href="https://github.com/hashicorp/terraform-provider-vault" target="_blank" rel="noopener noreferrer">terraform-provider-vault</a> 或者 <a href="https://github.com/pulumi/pulumi-vault" target="_blank" rel="noopener noreferrer">pulumi-vault</a> 实现 vault 配置的自动化管理。</li>
</ul>
<p>Web UI 适合手工操作，而 sdk/<code>terraform-provider-vault</code> 则适合用于自动化管理 vault.</p>
<p>我们的测试环境就是使用 <code>pulumi-vault</code> 完成的自动化配置 vault policy 和 kubernetes role，然后自动化注入所有测试用的 secrets.</p>
<h3 id="1-使用-pulumi-自动化配置-vault">1. 使用 pulumi 自动化配置 vault</h3>
<p>使用 pulumi 管理 vault 配置的优势是很大的，因为云上资源的敏感信息（数据库账号密码、资源 ID、RAM子账号）都是 pulumi 创建的。</p>
<p>再结合使用 pulumi_valut，就能实现敏感信息自动生成后，立即保存到 vault 中，实现完全自动化。</p>
<p>后续微服务就可以通过 kubernetes 认证，直接从 vault 读取敏感信息。</p>
<p>或者是写入到本地的 vault 中留做备份，在需要的时候，管理员能登入进去查看相关敏感信息。</p>
<h4 id="11-token-的生成">1.1 Token 的生成</h4>
<p>pulumi_vault 本身挺简单的，声明式的配置嘛，直接用就是了。</p>
<p>但是它一定要求提供 <code>VAULT_TOKEN</code> 作为身份认证的凭证（实测 userpass/approle 都不能直接使用，会报错 <code>no vault token found</code>），而且 pulumi 还会先生成临时用的 child token，然后用这个 child token
进行后续的操作。</p>
<p>首先安全起见，肯定不应该直接提供 root token！root token 应该封存，除了紧急情况不应该启用。</p>
<p>那么应该如何生成一个权限有限的 token 给 vault 使用呢？
我的方法是创建一个 userpass 账号，通过 policy 给予它有限的权限。
然后先手动(或者自动)登录获取到 token，再将 token 提供给 pulumi_vault 使用。</p>
<p>这里面有个坑，就是必须给 userpass 账号创建 child token 的权限：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-hcl" data-lang="hcl"><span class="k">path</span> <span class="s2">&#34;local/*&#34;</span> {
<span class="n">  capabilities</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&#34;read&#34;, &#34;list&#34;</span><span class="p">]</span>
}

<span class="err">//</span> <span class="k">允许创建</span> <span class="k">child</span> <span class="k">token</span>
<span class="k">path</span> <span class="s2">&#34;auth/token/create&#34;</span> {
<span class="n">  capabilities</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&#34;create&#34;, &#34;read&#34;, &#34;update&#34;, &#34;delete&#34;, &#34;list&#34;</span><span class="p">]</span>
}
</code></pre></td></tr></table>
</div>
</div><p>不给这个权限，pulumi_vault 就会一直报错。。</p>
<p>然后还得给它「自动化配置」需要的权限，比如自动创建/更新 policy/secrets/kubernetes 等等，示例如下:</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-hcl" data-lang="hcl"><span class="c1"># To list policies - Step 3
</span><span class="c1"></span><span class="k">path</span> <span class="s2">&#34;sys/policy&#34;</span>
{
<span class="n">  capabilities</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&#34;read&#34;</span><span class="p">]</span>
}<span class="c1">
</span><span class="c1">
</span><span class="c1"># Create and manage ACL policies broadly across Vault
</span><span class="c1"></span><span class="k">path</span> <span class="s2">&#34;sys/policy/*&#34;</span>
{
<span class="n">  capabilities</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&#34;create&#34;, &#34;read&#34;, &#34;update&#34;, &#34;delete&#34;, &#34;list&#34;, &#34;sudo&#34;</span><span class="p">]</span>
}<span class="c1">
</span><span class="c1">
</span><span class="c1"># List, create, update, and delete key/value secrets
</span><span class="c1"></span><span class="k">path</span> <span class="s2">&#34;secret/*&#34;</span>
{
<span class="n">  capabilities</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&#34;create&#34;, &#34;read&#34;, &#34;update&#34;, &#34;delete&#34;, &#34;list&#34;, &#34;sudo&#34;</span><span class="p">]</span>
}

<span class="k">path</span> <span class="s2">&#34;auth/kubernetes/role/*&#34;</span>
{
<span class="n">  capabilities</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&#34;create&#34;, &#34;read&#34;, &#34;update&#34;, &#34;list&#34;</span><span class="p">]</span>
}
</code></pre></td></tr></table>
</div>
</div><h2 id="四在-kubernetes-中使用-vault-注入-secrets">四、在 Kubernetes 中使用 vault 注入 secrets</h2>
<p><figure><a class="lightgallery" href="/images/expirence-of-vault/vault-k8s-auth-workflow.png" title="/images/expirence-of-vault/vault-k8s-auth-workflow.png" data-thumbnail="/images/expirence-of-vault/vault-k8s-auth-workflow.png" data-sub-html="<h2>vault-k8s-auth-workflow</h2>">
        
    </a><figcaption class="image-caption">vault-k8s-auth-workflow</figcaption>
    </figure></p>
<p>前面提到过 vault 支持通过 Kubernetes 的 ServiceAccount 为每个 Pod 单独分配权限。</p>
<p>应用程序有两种方式去读取 vault 中的配置：</p>
<ol>
<li>借助 Vault Sidecar，将 secrets 以文件的形式自动注入到 Pod 中，比如 <code>/vault/secrets/config.json</code>
<ul>
<li>vault sidecar 在常驻模式下每 15 秒更新一次配置，应用程序可以使用 <code>watchdog</code> 实时监控 secrets 文件的变更。</li>
</ul>
</li>
<li>应用程序自己使用 SDK 直接访问 vault api 获取 secrets</li>
</ol>
<p>上述两种方式，都可以借助 Kubernetes ServiceAccount 进行身份验证和权限分配。</p>
<p>下面以 Sidecar 模式为例，介绍如何将 secrets 以文件形式注入到 Pod 中。</p>
<h3 id="1-部署并配置-vault-agent">1. 部署并配置 vault agent</h3>
<p>首先启用 Vault 的 Kubernetes 身份验证:</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-shell" data-lang="shell"><span class="c1"># 配置身份认证需要在 vault pod 中执行，启动 vault-0 的交互式会话</span>
kubectl <span class="nb">exec</span> -n vault -it vault-0 -- /bin/sh
<span class="nb">export</span> <span class="nv">VAULT_TOKEN</span><span class="o">=</span><span class="s1">&#39;&lt;your-root-token&gt;&#39;</span>
<span class="nb">export</span> <span class="nv">VAULT_ADDR</span><span class="o">=</span><span class="s1">&#39;http://localhost:8200&#39;</span>
 
<span class="c1"># 启用 Kubernetes 身份验证</span>
vault auth <span class="nb">enable</span> kubernetes

<span class="c1"># kube-apiserver API 配置，vault 需要通过 kube-apiserver 完成对 serviceAccount 的身份验证</span>
vault write auth/kubernetes/config <span class="se">\
</span><span class="se"></span>    <span class="nv">token_reviewer_jwt</span><span class="o">=</span><span class="s2">&#34;</span><span class="k">$(</span>cat /var/run/secrets/kubernetes.io/serviceaccount/token<span class="k">)</span><span class="s2">&#34;</span> <span class="se">\
</span><span class="se"></span>    <span class="nv">kubernetes_host</span><span class="o">=</span><span class="s2">&#34;https://</span><span class="nv">$KUBERNETES_PORT_443_TCP_ADDR</span><span class="s2">:443&#34;</span> <span class="se">\
</span><span class="se"></span>    <span class="nv">kubernetes_ca_cert</span><span class="o">=</span>@/var/run/secrets/kubernetes.io/serviceaccount/ca.crt
</code></pre></td></tr></table>
</div>
</div><h4 id="11-使用集群外部的-valut-实例">1.1 使用集群外部的 valut 实例</h4>
<blockquote>
<p>如果你没这个需求，请跳过这一节。</p>
</blockquote>
<blockquote>
<p>详见 <a href="https://learn.hashicorp.com/tutorials/vault/kubernetes-external-vault?in=vault/kubernetes#install-the-vault-helm-chart-configured-to-address-an-external-vault" target="_blank" rel="noopener noreferrer">Install the Vault Helm chart configured to address an external Vault</a></p>
</blockquote>
<p>kubernetes 也可以和外部的 vault 实例集成，集群中只部署 vault-agent.</p>
<p>这适用于多个 kubernetes 集群以及其他 APP 共用一个 vault 实例的情况，比如我们本地的多个开发测试集群，就都共用着同一个 vault 实例，方便统一管理应用的 secrets.</p>
<p>首先，使用 helm chart 部署 vault-agent，接入外部的 vault 实例。使用的 <code>custom-values.yaml</code> 示例如下：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span><span class="lnt">43
</span><span class="lnt">44
</span><span class="lnt">45
</span><span class="lnt">46
</span><span class="lnt">47
</span><span class="lnt">48
</span><span class="lnt">49
</span><span class="lnt">50
</span><span class="lnt">51
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-yaml" data-lang="yaml"><span class="nt">global</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="c"># enabled is the master enabled switch. Setting this to true or false</span><span class="w">
</span><span class="w">  </span><span class="c"># will enable or disable all the components within this chart by default.</span><span class="w">
</span><span class="w">  </span><span class="nt">enabled</span><span class="p">:</span><span class="w"> </span><span class="kc">true</span><span class="w">
</span><span class="w">  </span><span class="c"># TLS for end-to-end encrypted transport</span><span class="w">
</span><span class="w">  </span><span class="nt">tlsDisable</span><span class="p">:</span><span class="w"> </span><span class="kc">false</span><span class="w">
</span><span class="w">
</span><span class="w"></span><span class="nt">injector</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="c"># True if you want to enable vault agent injection.</span><span class="w">
</span><span class="w">  </span><span class="nt">enabled</span><span class="p">:</span><span class="w"> </span><span class="kc">true</span><span class="w">
</span><span class="w">
</span><span class="w">  </span><span class="nt">replicas</span><span class="p">:</span><span class="w"> </span><span class="m">1</span><span class="w">
</span><span class="w">
</span><span class="w">  </span><span class="c"># If multiple replicas are specified, by default a leader-elector side-car</span><span class="w">
</span><span class="w">  </span><span class="c"># will be created so that only one injector attempts to create TLS certificates.</span><span class="w">
</span><span class="w">  </span><span class="nt">leaderElector</span><span class="p">:</span><span class="w">
</span><span class="w">    </span><span class="nt">enabled</span><span class="p">:</span><span class="w"> </span><span class="kc">true</span><span class="w">
</span><span class="w">    </span><span class="nt">image</span><span class="p">:</span><span class="w">
</span><span class="w">      </span><span class="nt">repository</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;gcr.io/google_containers/leader-elector&#34;</span><span class="w">
</span><span class="w">      </span><span class="nt">tag</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;0.4&#34;</span><span class="w">
</span><span class="w">    </span><span class="nt">ttl</span><span class="p">:</span><span class="w"> </span><span class="l">60s</span><span class="w">
</span><span class="w">
</span><span class="w">  </span><span class="c"># If true, will enable a node exporter metrics endpoint at /metrics.</span><span class="w">
</span><span class="w">  </span><span class="nt">metrics</span><span class="p">:</span><span class="w">
</span><span class="w">    </span><span class="nt">enabled</span><span class="p">:</span><span class="w"> </span><span class="kc">false</span><span class="w">
</span><span class="w">
</span><span class="w">  </span><span class="c"># External vault server address for the injector to use. Setting this will</span><span class="w">
</span><span class="w">  </span><span class="c"># disable deployment of a  vault server along with the injector.</span><span class="w">
</span><span class="w">  </span><span class="c"># TODO 这里的 https ca.crt 要怎么设置？mTLS 又该如何配置？</span><span class="w">
</span><span class="w">  </span><span class="nt">externalVaultAddr</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;https://&lt;external-vault-url&gt;&#34;</span><span class="w">
</span><span class="w">
</span><span class="w">  </span><span class="c"># Mount Path of the Vault Kubernetes Auth Method.</span><span class="w">
</span><span class="w">  </span><span class="nt">authPath</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;auth/kubernetes&#34;</span><span class="w">
</span><span class="w">
</span><span class="w">  </span><span class="nt">certs</span><span class="p">:</span><span class="w">
</span><span class="w">    </span><span class="c"># secretName is the name of the secret that has the TLS certificate and</span><span class="w">
</span><span class="w">    </span><span class="c"># private key to serve the injector webhook. If this is null, then the</span><span class="w">
</span><span class="w">    </span><span class="c"># injector will default to its automatic management mode that will assign</span><span class="w">
</span><span class="w">    </span><span class="c"># a service account to the injector to generate its own certificates.</span><span class="w">
</span><span class="w">    </span><span class="nt">secretName</span><span class="p">:</span><span class="w"> </span><span class="kc">null</span><span class="w">
</span><span class="w">
</span><span class="w">    </span><span class="c"># caBundle is a base64-encoded PEM-encoded certificate bundle for the</span><span class="w">
</span><span class="w">    </span><span class="c"># CA that signed the TLS certificate that the webhook serves. This must</span><span class="w">
</span><span class="w">    </span><span class="c"># be set if secretName is non-null.</span><span class="w">
</span><span class="w">    </span><span class="nt">caBundle</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;&#34;</span><span class="w">
</span><span class="w">
</span><span class="w">    </span><span class="c"># certName and keyName are the names of the files within the secret for</span><span class="w">
</span><span class="w">    </span><span class="c"># the TLS cert and private key, respectively. These have reasonable</span><span class="w">
</span><span class="w">    </span><span class="c"># defaults but can be customized if necessary.</span><span class="w">
</span><span class="w">    </span><span class="nt">certName</span><span class="p">:</span><span class="w"> </span><span class="l">tls.crt</span><span class="w">
</span><span class="w">    </span><span class="nt">keyName</span><span class="p">:</span><span class="w"> </span><span class="l">tls.key</span><span class="w">
</span></code></pre></td></tr></table>
</div>
</div><p>部署命令和 <a href="#install-by-helm" rel="">通过 helm 部署 vault</a> 一致，只要更换 <code>custom-values.yaml</code> 就行。</p>
<p>vault-agent 部署完成后，第二步是为 vault 创建 serviceAccount、secret 和 ClusterRoleBinding，以允许 vault 审查 kubernetes 的 token, 完成对 pod 的身份验证. yaml 配置如下：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-yaml" data-lang="yaml"><span class="nn">---</span><span class="w">
</span><span class="w"></span><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l">v1</span><span class="w">
</span><span class="w"></span><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l">ServiceAccount</span><span class="w">
</span><span class="w"></span><span class="nt">metadata</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">vault-auth</span><span class="w">
</span><span class="w">  </span><span class="nt">namespace</span><span class="p">:</span><span class="w"> </span><span class="l">vault</span><span class="w">
</span><span class="w"></span><span class="nn">---</span><span class="w">
</span><span class="w"></span><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l">v1</span><span class="w">
</span><span class="w"></span><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l">Secret</span><span class="w">
</span><span class="w"></span><span class="nt">metadata</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">vault-auth</span><span class="w">
</span><span class="w">  </span><span class="nt">namespace</span><span class="p">:</span><span class="w"> </span><span class="l">vault</span><span class="w">
</span><span class="w">  </span><span class="nt">annotations</span><span class="p">:</span><span class="w">
</span><span class="w">    </span><span class="nt">kubernetes.io/service-account.name</span><span class="p">:</span><span class="w"> </span><span class="l">vault-auth</span><span class="w">
</span><span class="w"></span><span class="nt">type</span><span class="p">:</span><span class="w"> </span><span class="l">kubernetes.io/service-account-token</span><span class="w">
</span><span class="w"></span><span class="nn">---</span><span class="w">
</span><span class="w"></span><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l">rbac.authorization.k8s.io/v1beta1</span><span class="w">
</span><span class="w"></span><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l">ClusterRoleBinding</span><span class="w">
</span><span class="w"></span><span class="nt">metadata</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">role-tokenreview-binding</span><span class="w">
</span><span class="w"></span><span class="nt">roleRef</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">apiGroup</span><span class="p">:</span><span class="w"> </span><span class="l">rbac.authorization.k8s.io</span><span class="w">
</span><span class="w">  </span><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l">ClusterRole</span><span class="w">
</span><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">system:auth-delegator</span><span class="w">
</span><span class="w"></span><span class="nt">subjects</span><span class="p">:</span><span class="w">
</span><span class="w">  </span>- <span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l">ServiceAccount</span><span class="w">
</span><span class="w">    </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">vault-auth</span><span class="w">
</span><span class="w">    </span><span class="nt">namespace</span><span class="p">:</span><span class="w"> </span><span class="l">vault</span><span class="w">
</span></code></pre></td></tr></table>
</div>
</div><p>现在在 vault 实例这边，启用 kubernetes 身份验证，在 vault 实例内，执行如下命令：</p>
<blockquote>
<p>vault 实例内显然没有 kubectl 和 kubeconfig，简便起见，下列的 vault 命令也可以通过 Web UI 完成。</p>
</blockquote>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-shell" data-lang="shell"><span class="nb">export</span> <span class="nv">VAULT_TOKEN</span><span class="o">=</span><span class="s1">&#39;&lt;your-root-token&gt;&#39;</span>
<span class="nb">export</span> <span class="nv">VAULT_ADDR</span><span class="o">=</span><span class="s1">&#39;http://localhost:8200&#39;</span>
 
<span class="c1"># 启用 Kubernetes 身份验证</span>
vault auth <span class="nb">enable</span> kubernetes
 
<span class="c1"># kube-apiserver API 配置，vault 需要通过 kube-apiserver 完成对 serviceAccount 的身份验证</span>
<span class="c1"># TOKEN_REVIEW_JWT: 就是我们前面创建的 secret `vault-auth`</span>
<span class="nv">TOKEN_REVIEW_JWT</span><span class="o">=</span><span class="k">$(</span>kubectl -n vault get secret vault-auth -o go-template<span class="o">=</span><span class="s1">&#39;{{ .data.token }}&#39;</span> <span class="p">|</span> base64 --decode<span class="k">)</span>
<span class="c1"># kube-apiserver 的 ca 证书</span>
<span class="nv">KUBE_CA_CERT</span><span class="o">=</span><span class="k">$(</span>kubectl -n vault config view --raw --minify --flatten -o <span class="nv">jsonpath</span><span class="o">=</span><span class="s1">&#39;{.clusters[].cluster.certificate-authority-data}&#39;</span> <span class="p">|</span> base64 --decode<span class="k">)</span>
<span class="c1"># kube-apiserver 的 url</span>
<span class="nv">KUBE_HOST</span><span class="o">=</span><span class="k">$(</span>kubectl config view --raw --minify --flatten -o <span class="nv">jsonpath</span><span class="o">=</span><span class="s1">&#39;{.clusters[].cluster.server}&#39;</span><span class="k">)</span>

vault write auth/kubernetes/config <span class="se">\
</span><span class="se"></span>        <span class="nv">token_reviewer_jwt</span><span class="o">=</span><span class="s2">&#34;</span><span class="nv">$TOKEN_REVIEW_JWT</span><span class="s2">&#34;</span> <span class="se">\
</span><span class="se"></span>        <span class="nv">kubernetes_host</span><span class="o">=</span><span class="s2">&#34;</span><span class="nv">$KUBE_HOST</span><span class="s2">&#34;</span> <span class="se">\
</span><span class="se"></span>        <span class="nv">kubernetes_ca_cert</span><span class="o">=</span><span class="s2">&#34;</span><span class="nv">$KUBE_CA_CERT</span><span class="s2">&#34;</span>
</code></pre></td></tr></table>
</div>
</div><p>这样，就完成了 kubernetes 与外部 vault 的集成！</p>
<h3 id="2-关联-k8s-rbac-权限系统和-vault">2. 关联 k8s rbac 权限系统和 vault</h3>
<p>接下来需要做的事：</p>
<ol start="2">
<li>通过 vault policy 定义好每个 role（微服务）能访问哪些资源。</li>
<li>为每个微服务生成一个 role，这个 role 需要绑定对应的 vault policy 及 kubernetes serviceaccount
<ol>
<li>这个 role 是 vault 的 kubernetes 插件自身的属性，它和 kubernetes role 没有半毛钱关系。</li>
</ol>
</li>
<li>创建一个 ServiceAccount，并使用这个 使用这个 ServiceAccount 部署微服务</li>
</ol>
<p>其中第一步和第二步都可以通过 vault api 自动化完成.
第三步可以通过 kubectl 部署时完成。</p>
<p>方便起见，vault policy / role / k8s serviceaccount 这三个配置，都建议和微服务使用相同的名称。</p>
<blockquote>
<p>上述配置中，role 起到一个承上启下的作用，它关联了 k8s serviceaccount 和 vault policy 两个配置。</p>
</blockquote>
<p>比如创建一个名为 <code>my-app-policy</code> 的 vault policy，内容为:</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-hcl" data-lang="hcl"><span class="c1"># 允许读取数据
</span><span class="c1"></span><span class="k">path</span> <span class="s2">&#34;my-app/data/*&#34;</span> {
<span class="n">   capabilities</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&#34;read&#34;, &#34;list&#34;</span><span class="p">]</span>
}
<span class="err">//</span> <span class="k">允许列出</span> <span class="k">myapp</span> <span class="k">中的所有数据</span><span class="p">(</span><span class="k">kv</span> <span class="k">v2</span><span class="p">)</span>
<span class="k">path</span> <span class="s2">&#34;myapp/metadata/*&#34;</span> {
<span class="n">    capabilities</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&#34;read&#34;, &#34;list&#34;</span><span class="p">]</span>
}
</code></pre></td></tr></table>
</div>
</div><p>然后在 vault 的 kuberntes 插件配置中，创建 role <code>my-app-role</code>，配置如下:</p>
<ol>
<li>关联 k8s default 名字空间中的 serviceaccount <code>my-app-account</code>，并创建好这个 serviceaccount.</li>
<li>关联 vault token policy，这就是前面创建的 <code>my-app-policy</code></li>
<li>设置 token period（有效期）</li>
</ol>
<p>这之后，每个微服务就能通过 serviceaccount 从 vault 中读取 <code>my-app</code> 中的所有信息了。</p>
<h3 id="3-部署-pod">3. 部署 Pod</h3>
<blockquote>
<p>参考文档：<a href="https://www.vaultproject.io/docs/platform/k8s/injector">https://www.vaultproject.io/docs/platform/k8s/injector</a></p>
</blockquote>
<p>下一步就是将配置注入到微服务容器中，这需要使用到 Agent Sidecar Injector。
vault 通过 sidecar 实现配置的自动注入与动态更新。</p>
<p>具体而言就是在 Pod 上加上一堆 Agent Sidecar Injector 的注解，如果配置比较多，也可以使用 configmap 保存，在注解中引用。</p>
<p>需要注意的是 vault-inject-agent 有两种运行模式：</p>
<ol>
<li>init 模式: 仅在 Pod 启动前初始化一次，跑完就退出（Completed）</li>
<li>常驻模式: 容器不退出，持续监控 vault 的配置更新，维持 Pod 配置和 vualt 配置的同步。</li>
</ol>
<p>示例：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-yaml" data-lang="yaml"><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l">apps/v1</span><span class="w">
</span><span class="w"></span><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l">Deployment</span><span class="w">
</span><span class="w"></span><span class="nt">metadata</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">labels</span><span class="p">:</span><span class="w">
</span><span class="w">    </span><span class="nt">app</span><span class="p">:</span><span class="w"> </span><span class="l">my-app</span><span class="w">
</span><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">my-app</span><span class="w">
</span><span class="w">  </span><span class="nt">namespace</span><span class="p">:</span><span class="w"> </span><span class="l">default</span><span class="w">
</span><span class="w"></span><span class="nt">spec</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">minReadySeconds</span><span class="p">:</span><span class="w"> </span><span class="m">3</span><span class="w">
</span><span class="w">  </span><span class="nt">progressDeadlineSeconds</span><span class="p">:</span><span class="w"> </span><span class="m">60</span><span class="w">
</span><span class="w">  </span><span class="nt">revisionHistoryLimit</span><span class="p">:</span><span class="w"> </span><span class="m">3</span><span class="w">
</span><span class="w">  </span><span class="nt">selector</span><span class="p">:</span><span class="w">
</span><span class="w">    </span><span class="nt">matchLabels</span><span class="p">:</span><span class="w">
</span><span class="w">      </span><span class="nt">app</span><span class="p">:</span><span class="w"> </span><span class="l">my-app</span><span class="w">
</span><span class="w">  </span><span class="nt">strategy</span><span class="p">:</span><span class="w">
</span><span class="w">    </span><span class="nt">rollingUpdate</span><span class="p">:</span><span class="w">
</span><span class="w">      </span><span class="nt">maxUnavailable</span><span class="p">:</span><span class="w"> </span><span class="m">1</span><span class="w">
</span><span class="w">    </span><span class="nt">type</span><span class="p">:</span><span class="w"> </span><span class="l">RollingUpdate</span><span class="w">
</span><span class="w">  </span><span class="nt">template</span><span class="p">:</span><span class="w">
</span><span class="w">    </span><span class="nt">metadata</span><span class="p">:</span><span class="w">
</span><span class="w">      </span><span class="nt">annotations</span><span class="p">:</span><span class="w">
</span><span class="w">        </span><span class="nt">vault.hashicorp.com/agent-init-first</span><span class="p">:</span><span class="w"> </span><span class="s1">&#39;true&#39;</span><span class="w">  </span><span class="c"># 是否使用 initContainer 提前初始化配置文件</span><span class="w">
</span><span class="w">        </span><span class="nt">vault.hashicorp.com/agent-inject</span><span class="p">:</span><span class="w"> </span><span class="s1">&#39;true&#39;</span><span class="w">
</span><span class="w">        </span><span class="nt">vault.hashicorp.com/secret-volume-path</span><span class="p">:</span><span class="w"> </span><span class="l">vault</span><span class="w">
</span><span class="w">        </span><span class="nt">vault.hashicorp.com/role</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;my-app-role&#34;</span><span class="w">  </span><span class="c"># vault kubernetes 插件的 role 名称</span><span class="w">
</span><span class="w">        </span><span class="nt">vault.hashicorp.com/agent-inject-template-config.json</span><span class="p">:</span><span class="w"> </span><span class="p">|</span><span class="sd">
</span><span class="sd">          </span><span class="w">          </span><span class="c"># 渲染模板的语法在后面介绍</span><span class="w">
</span><span class="w">        </span><span class="nt">vault.hashicorp.com/agent-limits-cpu</span><span class="p">:</span><span class="w"> </span><span class="l">250m</span><span class="w">
</span><span class="w">        </span><span class="nt">vault.hashicorp.com/agent-requests-cpu</span><span class="p">:</span><span class="w"> </span><span class="l">100m</span><span class="w">
</span><span class="w">        </span><span class="c"># 包含 vault 配置的 configmap，可以做更精细的控制</span><span class="w">
</span><span class="w">        </span><span class="c"># vault.hashicorp.com/agent-configmap: my-app-vault-config</span><span class="w">
</span><span class="w">      </span><span class="nt">labels</span><span class="p">:</span><span class="w">
</span><span class="w">        </span><span class="nt">app</span><span class="p">:</span><span class="w"> </span><span class="l">my-app</span><span class="w">
</span><span class="w">    </span><span class="nt">spec</span><span class="p">:</span><span class="w">
</span><span class="w">      </span><span class="nt">containers</span><span class="p">:</span><span class="w">
</span><span class="w">      </span>- <span class="nt">image</span><span class="p">:</span><span class="w"> </span><span class="l">registry.svc.local/xx/my-app:latest</span><span class="w">
</span><span class="w">        </span><span class="nt">imagePullPolicy</span><span class="p">:</span><span class="w"> </span><span class="l">IfNotPresent</span><span class="w">
</span><span class="w">        </span><span class="c"># 此处省略若干配置...</span><span class="w">
</span><span class="w">      </span><span class="nt">serviceAccountName</span><span class="p">:</span><span class="w"> </span><span class="l">my-app-account</span><span class="w">
</span></code></pre></td></tr></table>
</div>
</div><p>常见错误：</p>
<ul>
<li>vault-agent(sidecar) 报错: <code>namespace not authorized</code>
<ul>
<li><code>auth/kubernetes/config</code> 中的 role 没有绑定 Pod 的 namespace</li>
</ul>
</li>
<li>vault-agent(sidecar) 报错: <code>permission denied</code>
<ul>
<li>检查 <code>vault</code> 实例的日志，应该有对应的错误日志，很可能是 <code>auth/kubernetes/config</code> 没配对，vault 无法验证 kube-apiserver 的 tls 证书，或者使用的 kubernetes token 没有权限。</li>
</ul>
</li>
<li>vault-agent(sidecar) 报错: <code>service account not authorized</code>
<ul>
<li><code>auth/kubernetes/config</code> 中的 role 没有绑定 Pod 使用的 serviceAccount</li>
</ul>
</li>
</ul>
<h3 id="4-vault-agent-配置">4. vault agent 配置</h3>
<p>vault-agent 的配置，需要注意的有：</p>
<ol>
<li>如果使用 configmap 提供完整的 <code>config.hcl</code> 配置，注意 <code>agent-init</code></li>
</ol>
<p>vautl-agent 的 template 说明：</p>
<p>目前来说最流行的配置文件格式应该是 json/yaml，以 json 为例，
对每个微服务的 kv 数据，可以考虑将它所有的个性化配置都保存在 <code>&lt;engine-name&gt;/&lt;service-name&gt;/</code> 下面，然后使用如下 template 注入配置：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback">{
    {{ range secrets &#34;&lt;engine-name&gt;/metadata/&lt;service-name&gt;/&#34; }}
        &#34;{{ printf &#34;%s&#34; . }}&#34;: 
        {{ with secret (printf &#34;&lt;engine-name&gt;/&lt;service-name&gt;/%s&#34; .) }}
        {{ .Data.data | toJSONPretty }},
        {{ end }}
    {{ end }}
}
</code></pre></td></tr></table>
</div>
</div><blockquote>
<p>template 的详细语法参见: <a href="https://github.com/hashicorp/consul-template#secret">https://github.com/hashicorp/consul-template#secret</a></p>
</blockquote>
<blockquote>
<p>注意：v2 版本的 kv secrets，它的 list 接口有变更，因此在遍历 v2 kv secrets 时，
必须要写成 <code>range secrets &quot;&lt;engine-name&gt;/metadata/&lt;service-name&gt;/&quot;</code>，也就是中间要插入 <code>metadata</code>，而且 policy 中必须开放 <code>&lt;engine-name&gt;/metadata/&lt;service-name&gt;/</code> 的 read/list 权限！
官方文档完全没提到这一点，我通过 wireshark 抓包调试，对照官方的 <a href="https://www.vaultproject.io/api-docs/secret/kv/kv-v2" target="_blank" rel="noopener noreferrer">KV Secrets Engine - Version 2 (API)</a> 才搞明白这个。</p>
</blockquote>
<p>这样生成出来的内容将是 json 格式，不过有个不兼容的地方：最后一个 secrets 的末尾有逗号 <code>,</code>
渲染出的效果示例：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-json" data-lang="json"><span class="p">{</span>
    <span class="nt">&#34;secret-a&#34;</span><span class="p">:</span> <span class="p">{</span>
  <span class="nt">&#34;a&#34;</span><span class="p">:</span> <span class="s2">&#34;b&#34;</span><span class="p">,</span>
  <span class="nt">&#34;c&#34;</span><span class="p">:</span> <span class="s2">&#34;d&#34;</span>
<span class="p">},</span>
    <span class="nt">&#34;secret-b&#34;</span><span class="p">:</span> <span class="p">{</span>
  <span class="nt">&#34;v&#34;</span><span class="p">:</span> <span class="s2">&#34;g&#34;</span><span class="p">,</span>
  <span class="nt">&#34;r&#34;</span><span class="p">:</span> <span class="s2">&#34;c&#34;</span>
<span class="p">},</span>
<span class="p">}</span>
</code></pre></td></tr></table>
</div>
</div><p>因为存在尾部逗号(trailing comma)，直接使用 json 标准库解析它会报错。
那该如何去解析它呢？我在万能的 stackoverflow 上找到了解决方案：<strong>yaml 完全兼容 json 语法，并且支持尾部逗号！</strong></p>
<p>以 python 为例，直接 <code>yaml.safe_load()</code> 就能完美解析 vault 生成出的 json 内容。</p>
<h3 id="5-拓展在-kubernetes-中使用-vault-的其他姿势">5. 拓展：在 kubernetes 中使用 vault 的其他姿势</h3>
<p>除了使用官方提供的 sidecar 模式进行 secrets 注入，社区也提供了一些别的方案，可以参考：</p>
<ul>
<li><a href="https://github.com/hashicorp/vault-csi-provider" target="_blank" rel="noopener noreferrer">hashicorp/vault-csi-provider</a>: 官方的 Beta 项目，通过 Secrets Store CSI 驱动将 vault secrets 以数据卷的形式挂载到 pod 中</li>
<li><a href="https://github.com/external-secrets/kubernetes-external-secrets" target="_blank" rel="noopener noreferrer">kubernetes-external-secrets</a>: 提供 CRD 定义，根据定义将 secret 从 vault 中同步到 kubernetes secrets</li>
</ul>
<p>官方的 sidecar/init-container 模式仍然是最推荐使用的。</p>
<h2 id="五使用-vault-实现-aws-iam-credentials-的自动轮转">五、使用 vault 实现 AWS IAM Credentials 的自动轮转</h2>
<p>待续。。。</p>
]]></description></item></channel></rss>